{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.4/782.4 KB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: requests in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, tqdm, regex, protobuf, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 protobuf-4.24.3 regex-2023.8.8 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece protobuf transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils import _pytree as pytree\n",
    "import textwrap\n",
    "AUTH_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 614/614 [00:00<00:00, 3.24MB/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 36.2MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [02:05<00:00, 79.7MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:43<00:00, 81.1MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [02:48<00:00, 84.45s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Downloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 727kB/s]\n"
     ]
    }
   ],
   "source": [
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.float,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 3.76MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 45.2MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 2.92MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    use_fast=False,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results):\n",
    "    past_key_values, _ = pytree.tree_flatten(results.past_key_values)\n",
    "    #print(\"Logits:\", pytree.tree_map(lambda x: x.shape, results.logits))\n",
    "    #print(f\"PKV (len={len(past_key_values)}):\")\n",
    "    count = 0\n",
    "    prev = \"\"\n",
    "    for s in pytree.tree_map(lambda x: repr(x.shape), past_key_values):\n",
    "        if s == prev:\n",
    "            count += 1\n",
    "            continue\n",
    "        elif count:\n",
    "            print(\" \", s, f\"* {count+1}\" if count else \"\")\n",
    "            count = 0\n",
    "        prev = s\n",
    "    if count:\n",
    "        print(\" \", s, f\"* {count+1}\" if count else \"\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: {'input_ids': tensor([[    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
      "          3629,  3022,   895, 29889,   887,   526,   263,  8444, 29892,  3390,\n",
      "          1319,   322, 15993, 20255, 29889,   960,   263,  1139,   947,   451,\n",
      "          1207,   738,  4060, 29892,   470,   338,   451,  2114,  1474, 16165,\n",
      "           261,   296, 29892,  5649,  2020,  2012,   310, 22862,  1554,   451,\n",
      "          1959, 29889,   960,   366,  1016, 29915, 29873,  1073,   278,  1234,\n",
      "           304,   263,  1139, 29892,  3113,  1016, 29915, 29873,  6232,  2089,\n",
      "          2472, 29889,    13, 29966,   829, 14816, 29903,  6778,    13,    13,\n",
      "          2918,   825,   526,   366, 29973,   518, 29914, 25580, 29962,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "  Shape: torch.Size([1, 90])\n",
      "  torch.Size([1, 32, 90, 128]) * 64\n"
     ]
    }
   ],
   "source": [
    "prompt = (\"\"\"<s>[INST] <<SYS>>\n",
    "Be concise. You are a helpful, respectful and honest assistant. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "hi what are you? [/INST]\n",
    "\"\"\"\n",
    ")\n",
    "conversation = prompt\n",
    "initial_input = tokenizer(conversation, return_tensors=\"pt\")\n",
    "print(\"Example input:\", initial_input)\n",
    "print(\"  Shape:\", initial_input.input_ids.shape)\n",
    "initial_results = mdl.forward(initial_input.input_ids)\n",
    "summarize_results(initial_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Decoded: (tensor([10994]))\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "all_detoks = []\n",
    "def decode_token(results, index=-1, store=True):\n",
    "    #print(\"Logits:\", results.logits.shape)\n",
    "    #print(\"Logits reshaped:\", results.logits[:, index, :].shape)\n",
    "    token = torch.argmax(results.logits[:, index, :], dim=1)\n",
    "    detok = tokenizer.decode(token, skip_special_tokens=False)\n",
    "    print(f\"--> Decoded: ({token})\")\n",
    "    if store:\n",
    "        all_tokens.append(token[0])\n",
    "        all_detoks.append(detok)\n",
    "    return token, detok\n",
    "\n",
    "# Decode initial token\n",
    "# for i in range(initial_results.logits.shape[1]):\n",
    "#     token, detok = decode_token(initial_results, index=i)\n",
    "token, detok = decode_token(initial_results, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next input token: tensor([[10994]])\n",
      "  torch.Size([1, 32, 91, 128]) * 64\n",
      "--> Decoded: (tensor([29991]))\n",
      "Next input token: tensor([[29991]])\n",
      "  torch.Size([1, 32, 92, 128]) * 64\n",
      "--> Decoded: (tensor([306]))\n",
      "Next input token: tensor([[306]])\n",
      "  torch.Size([1, 32, 93, 128]) * 64\n",
      "--> Decoded: (tensor([29915]))\n",
      "Next input token: tensor([[29915]])\n",
      "  torch.Size([1, 32, 94, 128]) * 64\n",
      "--> Decoded: (tensor([29885]))\n",
      "Next input token: tensor([[29885]])\n",
      "  torch.Size([1, 32, 95, 128]) * 64\n",
      "--> Decoded: (tensor([925]))\n",
      "All tokens: [tensor(10994), tensor(29991), tensor(306), tensor(29915), tensor(29885), tensor(925)]\n",
      "All detoks: ['Hello', '!', 'I', \"'\", 'm', 'just']\n",
      "<s>[INST] <<SYS>>\n",
      "Be concise. You are a helpful, respectful and honest assistant. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "hi what are you? [/INST]\n",
      "\n",
      "Hello! I'm just\n"
     ]
    }
   ],
   "source": [
    "# Decode loop for subsequent tokens.\n",
    "current_results = initial_results\n",
    "for _ in range(5):\n",
    "    prior_pkvs, _ = pytree.tree_flatten(current_results.past_key_values)\n",
    "    next_input_token = torch.reshape(token, [1, 1])\n",
    "    print(\"Next input token:\", next_input_token)\n",
    "    step_results = mdl.forward(next_input_token, past_key_values=current_results.past_key_values)\n",
    "    summarize_results(step_results)\n",
    "    token, detok = decode_token(step_results)\n",
    "    if token[0] == 2:\n",
    "        break\n",
    "    current_results = step_results\n",
    "\n",
    "    current_pkvs, _ = pytree.tree_flatten(current_results.past_key_values)\n",
    "    pkv_len = prior_pkvs[0].shape[2]\n",
    "    for check_step in range(pkv_len):\n",
    "        for left, right in zip(prior_pkvs, current_pkvs):\n",
    "            if not torch.equal(left[:, :, check_step, :], right[:, :, check_step, :]):\n",
    "                print(f\"PKVS MISMATCH AT STEP {check_step}!\")\n",
    "\n",
    "print(\"All tokens:\", all_tokens)\n",
    "print(\"All detoks:\", all_detoks)\n",
    "\n",
    "print(conversation)\n",
    "print(tokenizer.decode(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to FX Trace the initialization graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5275, -1.7372,  5.9657,  ..., -2.0697, -0.9659, -2.2212]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch._export.constraints import constrain_as_size, constrain_as_value\n",
    "BATCH_SIZE = 1\n",
    "MAX_STEP_SEQ = 512\n",
    "NUM_HEADS = 64\n",
    "_pkv_ex = initial_results.past_key_values[0][0]\n",
    "hidden_dim = _pkv_ex.shape[3]\n",
    "dtype = _pkv_ex.dtype\n",
    "empty_states = torch.zeros(NUM_HEADS, \n",
    "                           BATCH_SIZE, \n",
    "                           _pkv_ex.shape[1], \n",
    "                           MAX_STEP_SEQ, \n",
    "                           hidden_dim, \n",
    "                           dtype=dtype)\n",
    "\n",
    "\n",
    "class StatefulModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.decoder_states = empty_states\n",
    "        self.step_seq = torch.zeros(1, dtype=torch.int32)\n",
    "\n",
    "    def initialize(self, input_ids: torch.Tensor): \n",
    "        torch.sym_constrain_range(input_ids.shape[1], min=2, max=4095)\n",
    "        results = self.base_model.forward(input_ids)\n",
    "        pkvs = results.past_key_values\n",
    "        pkv_example = pkvs[0][0]\n",
    "        seq_length = pkv_example.shape[2]\n",
    "        constrain_as_value(seq_length)\n",
    "        self._update_states(pkvs, seq_length) \n",
    "        return results.logits, seq_length\n",
    "\n",
    "    #probably combine this into forward. Missing constrain_as_value?\n",
    "    def forward(self, token):\n",
    "        \n",
    "        next_input_token = torch.zeros(1,1,dtype=token.dtype)\n",
    "        next_input_token[0,0] = token\n",
    "        bundled_pkvs = [\n",
    "            [self.decoder_states[x,:,:,:,:],\n",
    "             self.decoder_states[x+1,:,:,:,:]] for x in range(0,NUM_HEADS,2)]\n",
    "        results = self.base_model.forward(next_input_token, past_key_values= bundled_pkvs)\n",
    "        #always returns 1 token\n",
    "        #self._update_states(results.past_key_values, torch.ones(1, dtype=torch.int32))\n",
    "        return results.logits\n",
    "\n",
    "    def _update_states(self, update_states, seq_length):\n",
    "        updates_flat, _ = pytree.tree_flatten(update_states)\n",
    "        update_copy = [torch.empty_like(x).copy_(x) for x in updates_flat]\n",
    "        start = self.step_seq[0]\n",
    "            \n",
    "        stop = self.step_seq + seq_length\n",
    "        range = torch.arange(start.item(), stop.item())\n",
    "        for i, update in enumerate(update_copy):\n",
    "\n",
    "            temp = torch.index_select(update, 2, range)\n",
    "            self.decoder_states[i, :, :, start:stop, :] = temp\n",
    "        self.step_seq += seq_length\n",
    "\n",
    "sm = StatefulModel(mdl)\n",
    "input_ids = initial_input.input_ids\n",
    "_, seq_length = input_ids.shape\n",
    "sm.initialize(input_ids)\n",
    "sm.forward(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90])\n",
      "<function export.<locals>.inner at 0x7f82207877e0>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand: the requested shape has too few dimensions!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m exp \u001b[39m=\u001b[39m dynamo\u001b[39m.\u001b[39mexport(sm\u001b[39m.\u001b[39mforward, aten_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, constraints\u001b[39m=\u001b[39m[\n\u001b[1;32m     11\u001b[0m     \u001b[39m# dynamic_dim(input_ids, 1) >= 1,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# dynamic_dim(input_ids, 1) < 4096,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ])\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(exp)\n\u001b[0;32m---> 15\u001b[0m g, guards \u001b[39m=\u001b[39m exp(input_ids)\n\u001b[1;32m     16\u001b[0m g\u001b[39m.\u001b[39mprint_readable()\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1140\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideffects and reject.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     result_traced \u001b[39m=\u001b[39m opt_f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1141\u001b[0m \u001b[39mexcept\u001b[39;00m ConstraintViolationError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1142\u001b[0m     constraint_violation_error \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36mStatefulModel.forward\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m results\u001b[39m.\u001b[39mlogits, seq_length\n\u001b[1;32m     33\u001b[0m \u001b[39m#probably combine this into forward. Missing constrain_as_value?\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, token):\n\u001b[1;32m     36\u001b[0m     next_input_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,dtype\u001b[39m=\u001b[39mtoken\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     37\u001b[0m     next_input_token[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m token\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1106\u001b[0m, in \u001b[0;36mexport.<locals>.inner.<locals>.dynamo_normalization_capturing_compiler.<locals>.result_capturing_wrapper\u001b[0;34m(*graph_inputs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         fake_params_buffers[name] \u001b[39m=\u001b[39m ambient_fake_mode\u001b[39m.\u001b[39mfrom_tensor(\n\u001b[1;32m   1100\u001b[0m             value, static_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m         )\n\u001b[1;32m   1103\u001b[0m     fake_graph_inputs \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map(\n\u001b[1;32m   1104\u001b[0m         ambient_fake_mode\u001b[39m.\u001b[39mfrom_tensor, graph_inputs\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m     graph_captured_result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfunc\u001b[39m.\u001b[39;49mfunctional_call(\n\u001b[1;32m   1107\u001b[0m         graph, fake_params_buffers, fake_graph_inputs\n\u001b[1;32m   1108\u001b[0m     )\n\u001b[1;32m   1110\u001b[0m \u001b[39mreturn\u001b[39;00m graph_captured_result\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_functorch/functional_call.py:143\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mstateless\u001b[39m.\u001b[39;49m_functional_call(\n\u001b[1;32m    144\u001b[0m     module,\n\u001b[1;32m    145\u001b[0m     parameters_and_buffers,\n\u001b[1;32m    146\u001b[0m     args,\n\u001b[1;32m    147\u001b[0m     kwargs,\n\u001b[1;32m    148\u001b[0m     tie_weights\u001b[39m=\u001b[39;49mtie_weights,\n\u001b[1;32m    149\u001b[0m     strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[1;32m    150\u001b[0m )\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/nn/utils/stateless.py:264\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    260\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m    261\u001b[0m \u001b[39mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    262\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[39m=\u001b[39mtie_weights, strict\u001b[39m=\u001b[39mstrict\n\u001b[1;32m    263\u001b[0m ):\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/fx/graph_module.py:678\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 678\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/fx/graph_module.py:284\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/fx/graph_module.py:274\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    273\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<eval_with_key>.0:7\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, L_token_)\u001b[0m\n\u001b[1;32m      5\u001b[0m l_token_ \u001b[39m=\u001b[39m L_token_\n\u001b[1;32m      6\u001b[0m zeros \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mint64)\n\u001b[0;32m----> 7\u001b[0m zeros[(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m)] \u001b[39m=\u001b[39m l_token_;  setitem \u001b[39m=\u001b[39m zeros;  l_token_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m l__self___decoder_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL__self___decoder_states\n\u001b[1;32m      9\u001b[0m getitem \u001b[39m=\u001b[39m l__self___decoder_states[(\u001b[39m0\u001b[39m, \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m), \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m), \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m), \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m))]\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1290\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m   1287\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_dispatch_mode(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_TorchDispatchModeKey\u001b[39m.\u001b[39mFAKE) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m ), func\n\u001b[1;32m   1289\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m   1291\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     log\u001b[39m.\u001b[39mexception(\u001b[39m\"\u001b[39m\u001b[39mfake tensor raised TypeError\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1495\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m decomposition_table \u001b[39mand\u001b[39;00m (\n\u001b[1;32m   1486\u001b[0m     has_symbolic_sizes\n\u001b[1;32m   1487\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     )\n\u001b[1;32m   1493\u001b[0m ):\n\u001b[1;32m   1494\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m-> 1495\u001b[0m         \u001b[39mreturn\u001b[39;00m decomposition_table[func](\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1497\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m   1498\u001b[0m     \u001b[39m# Decomposes CompositeImplicitAutograd ops\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m     r \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39mdecompose(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/_refs/__init__.py:2845\u001b[0m, in \u001b[0;36mexpand\u001b[0;34m(a, *shape)\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(shape[\u001b[39m0\u001b[39m], Sequence):\n\u001b[1;32m   2843\u001b[0m     shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(shape[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2845\u001b[0m torch\u001b[39m.\u001b[39;49m_check(\n\u001b[1;32m   2846\u001b[0m     \u001b[39mlen\u001b[39;49m(shape) \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(a\u001b[39m.\u001b[39;49mshape),\n\u001b[1;32m   2847\u001b[0m     \u001b[39mlambda\u001b[39;49;00m: \u001b[39m\"\u001b[39;49m\u001b[39mexpand: the requested shape has too few dimensions!\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2848\u001b[0m )\n\u001b[1;32m   2850\u001b[0m offset \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(shape) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(a\u001b[39m.\u001b[39mshape)\n\u001b[1;32m   2851\u001b[0m shape_ \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(shape)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/__init__.py:1007\u001b[0m, in \u001b[0;36m_check\u001b[0;34m(cond, message)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check\u001b[39m(cond, message\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    993\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m    is False.\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39m            message. Default: ``None``\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m     _check_with(\u001b[39mRuntimeError\u001b[39;49;00m, cond, message)\n",
      "File \u001b[0;32m~/src/venv/Turbine/lib/python3.11/site-packages/torch/__init__.py:990\u001b[0m, in \u001b[0;36m_check_with\u001b[0;34m(error_type, cond, message)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmessage must be a callable\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    988\u001b[0m     message_evaluated \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(message())\n\u001b[0;32m--> 990\u001b[0m \u001b[39mraise\u001b[39;00m error_type(message_evaluated)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand: the requested shape has too few dimensions!"
     ]
    }
   ],
   "source": [
    "import torch._dynamo as dynamo\n",
    "from torch._export import dynamic_dim\n",
    "from torch._export.constraints import constrain_as_size, constrain_as_value\n",
    "import os\n",
    "\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamo\"\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "\n",
    "print(input_ids.size())\n",
    "exp = dynamo.export(sm.forward, aten_graph=True, constraints=[\n",
    "    # dynamic_dim(input_ids, 1) >= 1,\n",
    "    # dynamic_dim(input_ids, 1) < 4096,\n",
    "])\n",
    "print(exp)\n",
    "g, guards = exp(input_ids)\n",
    "g.print_readable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
