{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (4.24.3)\n",
      "Requirement already satisfied: transformers in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (4.33.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: requests in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: filelock in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece protobuf transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_LOGS\"] = \"dynamic\"\n",
    "\n",
    "import torch._dynamo as dynamo\n",
    "from torch._export import dynamic_dim\n",
    "from torch._export.constraints import constrain_as_size, constrain_as_value\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils import _pytree as pytree\n",
    "import textwrap\n",
    "AUTH_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.45s/it]\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.float,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    use_fast=False,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: {'input_ids': tensor([[    1,  2184, 29901,   887,   526,   263,  8444, 29892,  3390,  1319,\n",
      "           322, 15993, 20255, 29889, 29849,  1234,   408,  1371,  3730,   408,\n",
      "          1950, 29892,  1550,  1641,  9109, 29889, 29871,  3575,  6089,   881,\n",
      "           451,  3160,   738, 10311,  1319, 29892,   443,   621,   936, 29892,\n",
      "         11021,   391, 29892,  7916,   391, 29892,   304, 27375, 29892, 18215,\n",
      "         29892,   470, 27302,  2793, 29889,  3529,  9801,   393,   596, 20890,\n",
      "           526,  5374,   635,   443,  5365,  1463,   322,  6374,   297,  5469,\n",
      "         29889,   960,   263,  1139,   947,   451,  1207,   738,  4060, 29892,\n",
      "           470,   338,   451,  2114,  1474, 16165,   261,   296, 29892,  5649,\n",
      "          2020,  2012,   310, 22862,  1554,   451,  1959, 29889,   960,   366,\n",
      "          1016, 29915, 29873,  1073,   278,  1234,   304,   263,  1139, 29892,\n",
      "          3113,  1016, 29915, 29873,  6232,  2089,  2472, 19423, 29989, 11889,\n",
      "         29989, 29958, 26857,   350, 16926, 27105,  1460,   505,  6077,  2175,\n",
      "           472,   838,  2423,  7808,   802, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "  Shape: torch.Size([1, 136])\n",
      "Logits: torch.Size([1, 136, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 136, 128]) * 64\n",
      "Logits: torch.Size([1, 136, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '</' (tensor([829]))\n",
      "Next input token: tensor([[829]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 137, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'user' (tensor([1792]))\n",
      "Next input token: tensor([[1792]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 138, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '>' (tensor([29958]))\n",
      "Next input token: tensor([[29958]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 139, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '' (tensor([29871]))\n",
      "Next input token: tensor([[29871]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 140, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'I' (tensor([306]))\n",
      "Next input token: tensor([[306]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 141, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: ''' (tensor([29915]))\n",
      "All tokens: [tensor(829), tensor(1792), tensor(29958), tensor(29871), tensor(306), tensor(29915)]\n",
      "All detoks: ['</', 'user', '>', '', 'I', \"'\"]\n",
      "System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|USER|>Should Bugs Bunny have turned left at Albuquerque?\n",
      "</user>  I'\n"
     ]
    }
   ],
   "source": [
    "def summarize_results(results):\n",
    "    past_key_values, _ = pytree.tree_flatten(results.past_key_values)\n",
    "    print(\"Logits:\", pytree.tree_map(lambda x: x.shape, results.logits))\n",
    "    print(f\"PKV (len={len(past_key_values)}):\")\n",
    "    count = 0\n",
    "    prev = \"\"\n",
    "    for s in pytree.tree_map(lambda x: repr(x.shape), past_key_values):\n",
    "        if s == prev:\n",
    "            count += 1\n",
    "            continue\n",
    "        elif count:\n",
    "            print(\" \", s, f\"* {count+1}\" if count else \"\")\n",
    "            count = 0\n",
    "        prev = s\n",
    "    if count:\n",
    "        print(\" \", s, f\"* {count+1}\" if count else \"\")\n",
    "    \n",
    "    \n",
    "prompt = (\n",
    "        \"System: You are a helpful, respectful and honest assistant. Always answer \"\n",
    "        \"as helpfully as possible, while being safe.  Your answers should not \"\n",
    "        \"include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal \"\n",
    "        \"content. Please ensure that your responses are socially unbiased and positive \"\n",
    "        \"in nature. If a question does not make any sense, or is not factually coherent, \"\n",
    "        \"explain why instead of answering something not correct. If you don't know the \"\n",
    "        \"answer to a question, please don't share false information.\"\n",
    "    )\n",
    "conversation = prompt + \"<|USER|>Should Bugs Bunny have turned left at Albuquerque?\"\n",
    "\n",
    "initial_input = tokenizer(conversation, return_tensors=\"pt\")\n",
    "print(\"Example input:\", initial_input)\n",
    "print(\"  Shape:\", initial_input.input_ids.shape)\n",
    "initial_results = mdl.forward(initial_input.input_ids)\n",
    "summarize_results(initial_results)\n",
    "\n",
    "all_tokens = []\n",
    "all_detoks = []\n",
    "def decode_token(results, index=-1, store=True):\n",
    "    print(\"Logits:\", results.logits.shape)\n",
    "    print(\"Logits reshaped:\", results.logits[:, index, :].shape)\n",
    "    token = torch.argmax(results.logits[:, index, :], dim=1)\n",
    "    detok = tokenizer.decode(token, skip_special_tokens=False)\n",
    "    print(f\"--> Decoded: '{detok}' ({token})\")\n",
    "    if store:\n",
    "        all_tokens.append(token[0])\n",
    "        all_detoks.append(detok)\n",
    "    return token, detok\n",
    "\n",
    "# Decode initial token\n",
    "# for i in range(initial_results.logits.shape[1]):\n",
    "#     token, detok = decode_token(initial_results, index=i)\n",
    "token, detok = decode_token(initial_results, store=True)\n",
    "\n",
    "# Decode loop for subsequent tokens.\n",
    "current_results = initial_results\n",
    "for _ in range(5):\n",
    "    prior_pkvs, _ = pytree.tree_flatten(current_results.past_key_values)\n",
    "    next_input_token = torch.reshape(token, [1, 1])\n",
    "    print(\"Next input token:\", next_input_token)\n",
    "    step_results = mdl.forward(next_input_token, past_key_values=current_results.past_key_values)\n",
    "    summarize_results(step_results)\n",
    "    token, detok = decode_token(step_results)\n",
    "    if token[0] == 2:\n",
    "        break\n",
    "    current_results = step_results\n",
    "\n",
    "    current_pkvs, _ = pytree.tree_flatten(current_results.past_key_values)\n",
    "    pkv_len = prior_pkvs[0].shape[2]\n",
    "    for check_step in range(pkv_len):\n",
    "        for left, right in zip(prior_pkvs, current_pkvs):\n",
    "            if not torch.equal(left[:, :, check_step, :], right[:, :, check_step, :]):\n",
    "                print(f\"PKVS MISMATCH AT STEP {check_step}!\")\n",
    "\n",
    "print(\"All tokens:\", all_tokens)\n",
    "print(\"All detoks:\", all_detoks)\n",
    "\n",
    "print(conversation)\n",
    "print(tokenizer.decode(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to FX Trace the initialization graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example initialize:\n",
      "example_token0 = tensor([[829]])\n",
      "['Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))', 'Tensor(torch.Size([1, 32, 136, 128]))']\n",
      "Example step:\n",
      "example_token1 = tensor([1792])\n",
      "['Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))', 'Tensor(torch.Size([1, 32, 1, 128]))']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from torch._export.constraints import constrain_as_size, constrain_as_value\n",
    "\n",
    "def summarize_state_shape(tree):\n",
    "    print(pytree.tree_map(\n",
    "        lambda x: f\"Tensor({x.shape})\" if isinstance(x, torch.Tensor) else x,\n",
    "        tree))\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "MAX_STEP_SEQ = 4095\n",
    "# empty_states = pytree.tree_map(\n",
    "#     lambda x: torch.zeros(\n",
    "#         BATCH_SIZE, x.shape[1], MAX_STEP_SEQ, x.shape[3], \n",
    "#         dtype=x.dtype), initial_results.past_key_values)\n",
    "\n",
    "StateStruct = collections.namedtuple(\"InferenceState\", \"step_seq,past_key_values\")\n",
    "initial_step_seq = initial_results.past_key_values[0][0].shape[3]\n",
    "step_example_states = StateStruct(\n",
    "    step_seq=initial_step_seq,\n",
    "    past_key_values=pytree.tree_map(\n",
    "        lambda x: torch.zeros(\n",
    "            BATCH_SIZE, x.shape[1], MAX_STEP_SEQ, initial_step_seq, \n",
    "            dtype=x.dtype), \n",
    "        initial_results.past_key_values),\n",
    ")\n",
    "_, state_schema = pytree.tree_flatten(step_example_states.past_key_values)\n",
    "\n",
    "class InferenceModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def initialize(self, input_ids: torch.Tensor):\n",
    "        result = self.base_model.forward(input_ids)\n",
    "        state1_flat, _ = pytree.tree_flatten(result.past_key_values)\n",
    "        token1 = torch.argmax(result.logits[:, -1, :], dim=1)\n",
    "        token1 = token1[None, :]\n",
    "        return token1, *state1_flat\n",
    "\n",
    "    def forward(self, token0: torch.Tensor, *state0_flat):\n",
    "        # Unpad the states.\n",
    "        state0 = pytree.tree_unflatten(state0_flat, state_schema)\n",
    "        result = self.base_model.forward(token0, past_key_values=state0)\n",
    "        state1_flat, _ = pytree.tree_flatten(result.past_key_values)\n",
    "        state1_flat = [x[:, :, -2:-1, :] for x in state1_flat]\n",
    "        token1 = torch.argmax(result.logits[:, -1, :], dim=1)\n",
    "        return token1, *state1_flat\n",
    "\n",
    "\n",
    "sm = InferenceModel(mdl)\n",
    "input_ids = initial_input.input_ids\n",
    "\n",
    "print(\"Example initialize:\")\n",
    "example_token0, *example_state0 = sm.initialize(input_ids)\n",
    "print(\"example_token0 =\", example_token0)\n",
    "summarize_state_shape(example_state0)\n",
    "\n",
    "print(\"Example step:\")\n",
    "example_token1, *example_state1 = sm.forward(example_token0, *example_state0)\n",
    "print(\"example_token1 =\", example_token1)\n",
    "summarize_state_shape(example_state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-07 18:13:41,668] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2023-09-07 18:13:58,072] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, input_ids):\n",
      "        arg0: i64[1, 136], = fx_pytree.tree_flatten_spec(([input_ids], {}), self._in_spec)\n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:654, code: position_ids = torch.arange(\n",
      "        arange_start: i64[136] = torch.ops.aten.arange.start(0, 136, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:657, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        unsqueeze_default: i64[1, 136] = torch.ops.aten.unsqueeze.default(arange_start, 0);  arange_start = None\n",
      "        view_default: i64[1, 136] = torch.ops.aten.view.default(unsqueeze_default, [-1, 136]);  unsqueeze_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:662, code: inputs_embeds = self.embed_tokens(input_ids)\n",
      "        _param_constant0 = self._param_constant0\n",
      "        embedding_default: f32[1, 136, 4096] = torch.ops.aten.embedding.default(_param_constant0, arg0);  _param_constant0 = arg0 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:665, code: attention_mask = torch.ones(\n",
      "        ones_default: b8[1, 136] = torch.ops.aten.ones.default([1, 136], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:50, code: mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
      "        full_default: f32[136, 136] = torch.ops.aten.full.default([136, 136], -3.4028234663852886e+38, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:51, code: mask_cond = torch.arange(mask.size(-1), device=device)\n",
      "        arange_default: i64[136] = torch.ops.aten.arange.default(136, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:52, code: mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "        add_tensor: i64[136] = torch.ops.aten.add.Tensor(arange_default, 1)\n",
      "        view_default_1: i64[136, 1] = torch.ops.aten.view.default(add_tensor, [136, 1]);  add_tensor = None\n",
      "        lt_tensor: b8[136, 136] = torch.ops.aten.lt.Tensor(arange_default, view_default_1);  arange_default = view_default_1 = None\n",
      "        masked_fill__scalar: f32[136, 136] = torch.ops.aten.masked_fill_.Scalar(full_default, lt_tensor, 0);  full_default = lt_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:57, code: return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
      "        unsqueeze_default_1: f32[1, 136, 136] = torch.ops.aten.unsqueeze.default(masked_fill__scalar, 0);  masked_fill__scalar = None\n",
      "        unsqueeze_default_2: f32[1, 1, 136, 136] = torch.ops.aten.unsqueeze.default(unsqueeze_default_1, 1);  unsqueeze_default_1 = None\n",
      "        slice_tensor: f32[1, 1, 136, 136] = torch.ops.aten.slice.Tensor(unsqueeze_default_2, 2, 0, 9223372036854775807);  unsqueeze_default_2 = None\n",
      "        slice_tensor_1: f32[1, 1, 136, 136] = torch.ops.aten.slice.Tensor(slice_tensor, 3, 0, 9223372036854775807);  slice_tensor = None\n",
      "        expand_default: f32[1, 1, 136, 136] = torch.ops.aten.expand.default(slice_tensor_1, [1, 1, 136, 136]);  slice_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:68, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "        slice_tensor_2: b8[1, 136] = torch.ops.aten.slice.Tensor(ones_default, 0, 0, 9223372036854775807);  ones_default = None\n",
      "        unsqueeze_default_3: b8[1, 1, 136] = torch.ops.aten.unsqueeze.default(slice_tensor_2, 1);  slice_tensor_2 = None\n",
      "        unsqueeze_default_4: b8[1, 1, 1, 136] = torch.ops.aten.unsqueeze.default(unsqueeze_default_3, 2);  unsqueeze_default_3 = None\n",
      "        slice_tensor_3: b8[1, 1, 1, 136] = torch.ops.aten.slice.Tensor(unsqueeze_default_4, 3, 0, 9223372036854775807);  unsqueeze_default_4 = None\n",
      "        expand_default_1: b8[1, 1, 136, 136] = torch.ops.aten.expand.default(slice_tensor_3, [1, 1, 136, 136]);  slice_tensor_3 = None\n",
      "        _to_copy_default: f32[1, 1, 136, 136] = torch.ops.aten._to_copy.default(expand_default_1, dtype = torch.float32);  expand_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70, code: inverted_mask = 1.0 - expanded_mask\n",
      "        rsub_scalar: f32[1, 1, 136, 136] = torch.ops.aten.rsub.Scalar(_to_copy_default, 1.0);  _to_copy_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "        _to_copy_default_1: b8[1, 1, 136, 136] = torch.ops.aten._to_copy.default(rsub_scalar, dtype = torch.bool)\n",
      "        masked_fill_scalar: f32[1, 1, 136, 136] = torch.ops.aten.masked_fill.Scalar(rsub_scalar, _to_copy_default_1, -3.4028234663852886e+38);  rsub_scalar = _to_copy_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:609, code: expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
      "        add_tensor_1: f32[1, 1, 136, 136] = torch.ops.aten.add.Tensor(masked_fill_scalar, expand_default);  masked_fill_scalar = expand_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(embedding_default, 2)\n",
      "        mean_dim: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar, [-1], True);  pow_tensor_scalar = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_2: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim, 1e-06);  mean_dim = None\n",
      "        rsqrt_default: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_2);  add_tensor_2 = None\n",
      "        detach_default: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default)\n",
      "        mul_tensor: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(embedding_default, rsqrt_default);  rsqrt_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant1 = self._param_constant1\n",
      "        mul_tensor_1: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant1, mul_tensor);  _param_constant1 = mul_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant2 = self._param_constant2\n",
      "        t_default: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant2);  _param_constant2 = None\n",
      "        view_default_2: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_1, [136, 4096])\n",
      "        mm_default: f32[136, 4096] = torch.ops.aten.mm.default(view_default_2, t_default);  view_default_2 = t_default = None\n",
      "        view_default_3: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default, [1, 136, 4096]);  mm_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant3 = self._param_constant3\n",
      "        t_default_1: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant3);  _param_constant3 = None\n",
      "        view_default_4: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_1, [136, 4096])\n",
      "        mm_default_1: f32[136, 4096] = torch.ops.aten.mm.default(view_default_4, t_default_1);  view_default_4 = t_default_1 = None\n",
      "        view_default_5: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_1, [1, 136, 4096]);  mm_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant4 = self._param_constant4\n",
      "        t_default_2: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant4);  _param_constant4 = None\n",
      "        view_default_6: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_1, [136, 4096]);  mul_tensor_1 = None\n",
      "        mm_default_2: f32[136, 4096] = torch.ops.aten.mm.default(view_default_6, t_default_2);  view_default_6 = t_default_2 = None\n",
      "        view_default_7: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_2, [1, 136, 4096]);  mm_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_8: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_3, [1, 136, 32, 128]);  view_default_3 = None\n",
      "        transpose_int: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_8, 1, 2);  view_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_9: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_5, [1, 136, 32, 128]);  view_default_5 = None\n",
      "        transpose_int_1: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_9, 1, 2);  view_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_10: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_7, [1, 136, 32, 128]);  view_default_7 = None\n",
      "        transpose_int_2: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_10, 1, 2);  view_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant0 = self._tensor_constant0\n",
      "        slice_tensor_4: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant0, 0, 0, 9223372036854775807);  _tensor_constant0 = None\n",
      "        slice_tensor_5: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_4, 1, 0, 9223372036854775807);  slice_tensor_4 = None\n",
      "        slice_tensor_6: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_5, 2, 0, 136);  slice_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant1 = self._tensor_constant1\n",
      "        slice_tensor_7: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant1, 0, 0, 9223372036854775807);  _tensor_constant1 = None\n",
      "        slice_tensor_8: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_7, 1, 0, 9223372036854775807);  slice_tensor_7 = None\n",
      "        slice_tensor_9: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_8, 2, 0, 136);  slice_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_6, 1);  slice_tensor_6 = None\n",
      "        squeeze_dim_1: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim, 0);  squeeze_dim = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_2: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_9, 1);  slice_tensor_9 = None\n",
      "        squeeze_dim_3: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_2, 0);  squeeze_dim_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_1, [view_default]);  squeeze_dim_1 = None\n",
      "        unsqueeze_default_5: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor, 1);  index_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_1: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_3, [view_default]);  squeeze_dim_3 = None\n",
      "        unsqueeze_default_6: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_1, 1);  index_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_2: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int, unsqueeze_default_5)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_10: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_11: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 64, 9223372036854775807);  transpose_int = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_11);  slice_tensor_11 = None\n",
      "        cat_default: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default, slice_tensor_10], -1);  neg_default = slice_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_3: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default, unsqueeze_default_6);  cat_default = None\n",
      "        add_tensor_3: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_2, mul_tensor_3);  mul_tensor_2 = mul_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_4: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_1, unsqueeze_default_5);  unsqueeze_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_12: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_13: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 64, 9223372036854775807);  transpose_int_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_1: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_13);  slice_tensor_13 = None\n",
      "        cat_default_1: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_1, slice_tensor_12], -1);  neg_default_1 = slice_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_5: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_1, unsqueeze_default_6);  cat_default_1 = unsqueeze_default_6 = None\n",
      "        add_tensor_4: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_4, mul_tensor_5);  mul_tensor_4 = mul_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_3: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_4, 2, 3)\n",
      "        expand_default_2: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_3, [1, 32, 136, 128]);  add_tensor_3 = None\n",
      "        view_default_11: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_2, [32, 136, 128]);  expand_default_2 = None\n",
      "        expand_default_3: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_3, [1, 32, 128, 136]);  transpose_int_3 = None\n",
      "        view_default_12: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_3, [32, 128, 136]);  expand_default_3 = None\n",
      "        bmm_default: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_11, view_default_12);  view_default_11 = view_default_12 = None\n",
      "        view_default_13: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default, [1, 32, 136, 136]);  bmm_default = None\n",
      "        div_tensor: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_13, 11.313708498984761);  view_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_5: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor, add_tensor_1);  div_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_5, -1, False);  add_tensor_5 = None\n",
      "        detach_default_1: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_4: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default, [1, 32, 136, 136]);  _softmax_default = None\n",
      "        view_default_14: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_4, [32, 136, 136]);  expand_default_4 = None\n",
      "        expand_default_5: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_2, [1, 32, 136, 128])\n",
      "        view_default_15: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_5, [32, 136, 128]);  expand_default_5 = None\n",
      "        bmm_default_1: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_14, view_default_15);  view_default_14 = view_default_15 = None\n",
      "        view_default_16: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_1, [1, 32, 136, 128]);  bmm_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_4: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_16, 1, 2);  view_default_16 = None\n",
      "        clone_default: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_4, memory_format = torch.contiguous_format);  transpose_int_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_17: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default, [1, 136, 4096]);  clone_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant5 = self._param_constant5\n",
      "        t_default_3: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant5);  _param_constant5 = None\n",
      "        view_default_18: f32[136, 4096] = torch.ops.aten.view.default(view_default_17, [136, 4096]);  view_default_17 = None\n",
      "        mm_default_3: f32[136, 4096] = torch.ops.aten.mm.default(view_default_18, t_default_3);  view_default_18 = t_default_3 = None\n",
      "        view_default_19: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_3, [1, 136, 4096]);  mm_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_6: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(embedding_default, view_default_19);  embedding_default = view_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_1: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_6, 2)\n",
      "        mean_dim_1: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_1, [-1], True);  pow_tensor_scalar_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_7: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_1, 1e-06);  mean_dim_1 = None\n",
      "        rsqrt_default_1: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_7);  add_tensor_7 = None\n",
      "        detach_default_2: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_1)\n",
      "        mul_tensor_6: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_6, rsqrt_default_1);  rsqrt_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant6 = self._param_constant6\n",
      "        mul_tensor_7: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant6, mul_tensor_6);  _param_constant6 = mul_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant7 = self._param_constant7\n",
      "        t_default_4: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant7);  _param_constant7 = None\n",
      "        view_default_20: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_7, [136, 4096])\n",
      "        mm_default_4: f32[136, 11008] = torch.ops.aten.mm.default(view_default_20, t_default_4);  view_default_20 = t_default_4 = None\n",
      "        view_default_21: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_4, [1, 136, 11008]);  mm_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_21);  view_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant8 = self._param_constant8\n",
      "        t_default_5: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant8);  _param_constant8 = None\n",
      "        view_default_22: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_7, [136, 4096]);  mul_tensor_7 = None\n",
      "        mm_default_5: f32[136, 11008] = torch.ops.aten.mm.default(view_default_22, t_default_5);  view_default_22 = t_default_5 = None\n",
      "        view_default_23: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_5, [1, 136, 11008]);  mm_default_5 = None\n",
      "        mul_tensor_8: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default, view_default_23);  silu_default = view_default_23 = None\n",
      "        _param_constant9 = self._param_constant9\n",
      "        t_default_6: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant9);  _param_constant9 = None\n",
      "        view_default_24: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_8, [136, 11008]);  mul_tensor_8 = None\n",
      "        mm_default_6: f32[136, 4096] = torch.ops.aten.mm.default(view_default_24, t_default_6);  view_default_24 = t_default_6 = None\n",
      "        view_default_25: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_6, [1, 136, 4096]);  mm_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_8: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_6, view_default_25);  add_tensor_6 = view_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_2: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_8, 2)\n",
      "        mean_dim_2: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_2, [-1], True);  pow_tensor_scalar_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_9: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_2, 1e-06);  mean_dim_2 = None\n",
      "        rsqrt_default_2: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_9);  add_tensor_9 = None\n",
      "        detach_default_3: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_2)\n",
      "        mul_tensor_9: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_8, rsqrt_default_2);  rsqrt_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant10 = self._param_constant10\n",
      "        mul_tensor_10: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant10, mul_tensor_9);  _param_constant10 = mul_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant11 = self._param_constant11\n",
      "        t_default_7: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant11);  _param_constant11 = None\n",
      "        view_default_26: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_10, [136, 4096])\n",
      "        mm_default_7: f32[136, 4096] = torch.ops.aten.mm.default(view_default_26, t_default_7);  view_default_26 = t_default_7 = None\n",
      "        view_default_27: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_7, [1, 136, 4096]);  mm_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant12 = self._param_constant12\n",
      "        t_default_8: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant12);  _param_constant12 = None\n",
      "        view_default_28: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_10, [136, 4096])\n",
      "        mm_default_8: f32[136, 4096] = torch.ops.aten.mm.default(view_default_28, t_default_8);  view_default_28 = t_default_8 = None\n",
      "        view_default_29: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_8, [1, 136, 4096]);  mm_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant13 = self._param_constant13\n",
      "        t_default_9: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant13);  _param_constant13 = None\n",
      "        view_default_30: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_10, [136, 4096]);  mul_tensor_10 = None\n",
      "        mm_default_9: f32[136, 4096] = torch.ops.aten.mm.default(view_default_30, t_default_9);  view_default_30 = t_default_9 = None\n",
      "        view_default_31: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_9, [1, 136, 4096]);  mm_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_32: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_27, [1, 136, 32, 128]);  view_default_27 = None\n",
      "        transpose_int_5: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_32, 1, 2);  view_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_33: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_29, [1, 136, 32, 128]);  view_default_29 = None\n",
      "        transpose_int_6: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_33, 1, 2);  view_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_34: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_31, [1, 136, 32, 128]);  view_default_31 = None\n",
      "        transpose_int_7: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_34, 1, 2);  view_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant2 = self._tensor_constant2\n",
      "        slice_tensor_14: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant2, 0, 0, 9223372036854775807);  _tensor_constant2 = None\n",
      "        slice_tensor_15: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_14, 1, 0, 9223372036854775807);  slice_tensor_14 = None\n",
      "        slice_tensor_16: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_15, 2, 0, 136);  slice_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant3 = self._tensor_constant3\n",
      "        slice_tensor_17: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant3, 0, 0, 9223372036854775807);  _tensor_constant3 = None\n",
      "        slice_tensor_18: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_17, 1, 0, 9223372036854775807);  slice_tensor_17 = None\n",
      "        slice_tensor_19: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_18, 2, 0, 136);  slice_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_4: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_16, 1);  slice_tensor_16 = None\n",
      "        squeeze_dim_5: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_4, 0);  squeeze_dim_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_6: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_19, 1);  slice_tensor_19 = None\n",
      "        squeeze_dim_7: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_6, 0);  squeeze_dim_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_2: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_5, [view_default]);  squeeze_dim_5 = None\n",
      "        unsqueeze_default_7: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_2, 1);  index_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_3: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_7, [view_default]);  squeeze_dim_7 = None\n",
      "        unsqueeze_default_8: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_3, 1);  index_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_11: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_5, unsqueeze_default_7)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_20: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_21: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 64, 9223372036854775807);  transpose_int_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_2: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_21);  slice_tensor_21 = None\n",
      "        cat_default_2: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_2, slice_tensor_20], -1);  neg_default_2 = slice_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_12: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_2, unsqueeze_default_8);  cat_default_2 = None\n",
      "        add_tensor_10: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_11, mul_tensor_12);  mul_tensor_11 = mul_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_13: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_6, unsqueeze_default_7);  unsqueeze_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_22: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_23: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 64, 9223372036854775807);  transpose_int_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_3: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_23);  slice_tensor_23 = None\n",
      "        cat_default_3: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_3, slice_tensor_22], -1);  neg_default_3 = slice_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_14: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_3, unsqueeze_default_8);  cat_default_3 = unsqueeze_default_8 = None\n",
      "        add_tensor_11: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_13, mul_tensor_14);  mul_tensor_13 = mul_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_8: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_11, 2, 3)\n",
      "        expand_default_6: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_10, [1, 32, 136, 128]);  add_tensor_10 = None\n",
      "        view_default_35: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_6, [32, 136, 128]);  expand_default_6 = None\n",
      "        expand_default_7: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_8, [1, 32, 128, 136]);  transpose_int_8 = None\n",
      "        view_default_36: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_7, [32, 128, 136]);  expand_default_7 = None\n",
      "        bmm_default_2: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_35, view_default_36);  view_default_35 = view_default_36 = None\n",
      "        view_default_37: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_2, [1, 32, 136, 136]);  bmm_default_2 = None\n",
      "        div_tensor_1: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_37, 11.313708498984761);  view_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_12: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_1, add_tensor_1);  div_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_1: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_12, -1, False);  add_tensor_12 = None\n",
      "        detach_default_4: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_1)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_8: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_1, [1, 32, 136, 136]);  _softmax_default_1 = None\n",
      "        view_default_38: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_8, [32, 136, 136]);  expand_default_8 = None\n",
      "        expand_default_9: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_7, [1, 32, 136, 128])\n",
      "        view_default_39: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_9, [32, 136, 128]);  expand_default_9 = None\n",
      "        bmm_default_3: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_38, view_default_39);  view_default_38 = view_default_39 = None\n",
      "        view_default_40: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_3, [1, 32, 136, 128]);  bmm_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_9: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_40, 1, 2);  view_default_40 = None\n",
      "        clone_default_1: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_9, memory_format = torch.contiguous_format);  transpose_int_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_41: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_1, [1, 136, 4096]);  clone_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant14 = self._param_constant14\n",
      "        t_default_10: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant14);  _param_constant14 = None\n",
      "        view_default_42: f32[136, 4096] = torch.ops.aten.view.default(view_default_41, [136, 4096]);  view_default_41 = None\n",
      "        mm_default_10: f32[136, 4096] = torch.ops.aten.mm.default(view_default_42, t_default_10);  view_default_42 = t_default_10 = None\n",
      "        view_default_43: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_10, [1, 136, 4096]);  mm_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_13: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_8, view_default_43);  add_tensor_8 = view_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_3: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_13, 2)\n",
      "        mean_dim_3: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_3, [-1], True);  pow_tensor_scalar_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_14: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_3, 1e-06);  mean_dim_3 = None\n",
      "        rsqrt_default_3: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_14);  add_tensor_14 = None\n",
      "        detach_default_5: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_3)\n",
      "        mul_tensor_15: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_13, rsqrt_default_3);  rsqrt_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant15 = self._param_constant15\n",
      "        mul_tensor_16: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant15, mul_tensor_15);  _param_constant15 = mul_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant16 = self._param_constant16\n",
      "        t_default_11: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant16);  _param_constant16 = None\n",
      "        view_default_44: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_16, [136, 4096])\n",
      "        mm_default_11: f32[136, 11008] = torch.ops.aten.mm.default(view_default_44, t_default_11);  view_default_44 = t_default_11 = None\n",
      "        view_default_45: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_11, [1, 136, 11008]);  mm_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_1: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_45);  view_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant17 = self._param_constant17\n",
      "        t_default_12: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant17);  _param_constant17 = None\n",
      "        view_default_46: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_16, [136, 4096]);  mul_tensor_16 = None\n",
      "        mm_default_12: f32[136, 11008] = torch.ops.aten.mm.default(view_default_46, t_default_12);  view_default_46 = t_default_12 = None\n",
      "        view_default_47: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_12, [1, 136, 11008]);  mm_default_12 = None\n",
      "        mul_tensor_17: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_1, view_default_47);  silu_default_1 = view_default_47 = None\n",
      "        _param_constant18 = self._param_constant18\n",
      "        t_default_13: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant18);  _param_constant18 = None\n",
      "        view_default_48: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_17, [136, 11008]);  mul_tensor_17 = None\n",
      "        mm_default_13: f32[136, 4096] = torch.ops.aten.mm.default(view_default_48, t_default_13);  view_default_48 = t_default_13 = None\n",
      "        view_default_49: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_13, [1, 136, 4096]);  mm_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_15: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_13, view_default_49);  add_tensor_13 = view_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_4: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_15, 2)\n",
      "        mean_dim_4: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_4, [-1], True);  pow_tensor_scalar_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_16: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_4, 1e-06);  mean_dim_4 = None\n",
      "        rsqrt_default_4: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_16);  add_tensor_16 = None\n",
      "        detach_default_6: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_4)\n",
      "        mul_tensor_18: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_15, rsqrt_default_4);  rsqrt_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant19 = self._param_constant19\n",
      "        mul_tensor_19: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant19, mul_tensor_18);  _param_constant19 = mul_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant20 = self._param_constant20\n",
      "        t_default_14: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant20);  _param_constant20 = None\n",
      "        view_default_50: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_19, [136, 4096])\n",
      "        mm_default_14: f32[136, 4096] = torch.ops.aten.mm.default(view_default_50, t_default_14);  view_default_50 = t_default_14 = None\n",
      "        view_default_51: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_14, [1, 136, 4096]);  mm_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant21 = self._param_constant21\n",
      "        t_default_15: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant21);  _param_constant21 = None\n",
      "        view_default_52: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_19, [136, 4096])\n",
      "        mm_default_15: f32[136, 4096] = torch.ops.aten.mm.default(view_default_52, t_default_15);  view_default_52 = t_default_15 = None\n",
      "        view_default_53: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_15, [1, 136, 4096]);  mm_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant22 = self._param_constant22\n",
      "        t_default_16: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant22);  _param_constant22 = None\n",
      "        view_default_54: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_19, [136, 4096]);  mul_tensor_19 = None\n",
      "        mm_default_16: f32[136, 4096] = torch.ops.aten.mm.default(view_default_54, t_default_16);  view_default_54 = t_default_16 = None\n",
      "        view_default_55: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_16, [1, 136, 4096]);  mm_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_56: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_51, [1, 136, 32, 128]);  view_default_51 = None\n",
      "        transpose_int_10: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_56, 1, 2);  view_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_57: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_53, [1, 136, 32, 128]);  view_default_53 = None\n",
      "        transpose_int_11: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_57, 1, 2);  view_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_58: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_55, [1, 136, 32, 128]);  view_default_55 = None\n",
      "        transpose_int_12: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_58, 1, 2);  view_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant4 = self._tensor_constant4\n",
      "        slice_tensor_24: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant4, 0, 0, 9223372036854775807);  _tensor_constant4 = None\n",
      "        slice_tensor_25: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_24, 1, 0, 9223372036854775807);  slice_tensor_24 = None\n",
      "        slice_tensor_26: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_25, 2, 0, 136);  slice_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant5 = self._tensor_constant5\n",
      "        slice_tensor_27: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant5, 0, 0, 9223372036854775807);  _tensor_constant5 = None\n",
      "        slice_tensor_28: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_27, 1, 0, 9223372036854775807);  slice_tensor_27 = None\n",
      "        slice_tensor_29: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_28, 2, 0, 136);  slice_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_8: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_26, 1);  slice_tensor_26 = None\n",
      "        squeeze_dim_9: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_8, 0);  squeeze_dim_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_10: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_29, 1);  slice_tensor_29 = None\n",
      "        squeeze_dim_11: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_10, 0);  squeeze_dim_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_4: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_9, [view_default]);  squeeze_dim_9 = None\n",
      "        unsqueeze_default_9: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_4, 1);  index_tensor_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_5: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_11, [view_default]);  squeeze_dim_11 = None\n",
      "        unsqueeze_default_10: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_5, 1);  index_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_20: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_10, unsqueeze_default_9)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_30: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_31: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 64, 9223372036854775807);  transpose_int_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_4: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_31);  slice_tensor_31 = None\n",
      "        cat_default_4: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_4, slice_tensor_30], -1);  neg_default_4 = slice_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_21: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_4, unsqueeze_default_10);  cat_default_4 = None\n",
      "        add_tensor_17: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_20, mul_tensor_21);  mul_tensor_20 = mul_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_22: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_11, unsqueeze_default_9);  unsqueeze_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_32: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_33: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 64, 9223372036854775807);  transpose_int_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_5: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_33);  slice_tensor_33 = None\n",
      "        cat_default_5: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_5, slice_tensor_32], -1);  neg_default_5 = slice_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_23: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_5, unsqueeze_default_10);  cat_default_5 = unsqueeze_default_10 = None\n",
      "        add_tensor_18: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_22, mul_tensor_23);  mul_tensor_22 = mul_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_13: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_18, 2, 3)\n",
      "        expand_default_10: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_17, [1, 32, 136, 128]);  add_tensor_17 = None\n",
      "        view_default_59: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_10, [32, 136, 128]);  expand_default_10 = None\n",
      "        expand_default_11: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_13, [1, 32, 128, 136]);  transpose_int_13 = None\n",
      "        view_default_60: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_11, [32, 128, 136]);  expand_default_11 = None\n",
      "        bmm_default_4: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_59, view_default_60);  view_default_59 = view_default_60 = None\n",
      "        view_default_61: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_4, [1, 32, 136, 136]);  bmm_default_4 = None\n",
      "        div_tensor_2: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_61, 11.313708498984761);  view_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_19: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_2, add_tensor_1);  div_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_2: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_19, -1, False);  add_tensor_19 = None\n",
      "        detach_default_7: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_2)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_12: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_2, [1, 32, 136, 136]);  _softmax_default_2 = None\n",
      "        view_default_62: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_12, [32, 136, 136]);  expand_default_12 = None\n",
      "        expand_default_13: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_12, [1, 32, 136, 128])\n",
      "        view_default_63: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_13, [32, 136, 128]);  expand_default_13 = None\n",
      "        bmm_default_5: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_62, view_default_63);  view_default_62 = view_default_63 = None\n",
      "        view_default_64: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_5, [1, 32, 136, 128]);  bmm_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_14: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_64, 1, 2);  view_default_64 = None\n",
      "        clone_default_2: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_14, memory_format = torch.contiguous_format);  transpose_int_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_65: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_2, [1, 136, 4096]);  clone_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant23 = self._param_constant23\n",
      "        t_default_17: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant23);  _param_constant23 = None\n",
      "        view_default_66: f32[136, 4096] = torch.ops.aten.view.default(view_default_65, [136, 4096]);  view_default_65 = None\n",
      "        mm_default_17: f32[136, 4096] = torch.ops.aten.mm.default(view_default_66, t_default_17);  view_default_66 = t_default_17 = None\n",
      "        view_default_67: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_17, [1, 136, 4096]);  mm_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_20: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_15, view_default_67);  add_tensor_15 = view_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_5: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_20, 2)\n",
      "        mean_dim_5: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_5, [-1], True);  pow_tensor_scalar_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_21: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_5, 1e-06);  mean_dim_5 = None\n",
      "        rsqrt_default_5: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_21);  add_tensor_21 = None\n",
      "        detach_default_8: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_5)\n",
      "        mul_tensor_24: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_20, rsqrt_default_5);  rsqrt_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant24 = self._param_constant24\n",
      "        mul_tensor_25: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant24, mul_tensor_24);  _param_constant24 = mul_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant25 = self._param_constant25\n",
      "        t_default_18: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant25);  _param_constant25 = None\n",
      "        view_default_68: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_25, [136, 4096])\n",
      "        mm_default_18: f32[136, 11008] = torch.ops.aten.mm.default(view_default_68, t_default_18);  view_default_68 = t_default_18 = None\n",
      "        view_default_69: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_18, [1, 136, 11008]);  mm_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_2: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_69);  view_default_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant26 = self._param_constant26\n",
      "        t_default_19: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant26);  _param_constant26 = None\n",
      "        view_default_70: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_25, [136, 4096]);  mul_tensor_25 = None\n",
      "        mm_default_19: f32[136, 11008] = torch.ops.aten.mm.default(view_default_70, t_default_19);  view_default_70 = t_default_19 = None\n",
      "        view_default_71: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_19, [1, 136, 11008]);  mm_default_19 = None\n",
      "        mul_tensor_26: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_2, view_default_71);  silu_default_2 = view_default_71 = None\n",
      "        _param_constant27 = self._param_constant27\n",
      "        t_default_20: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant27);  _param_constant27 = None\n",
      "        view_default_72: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_26, [136, 11008]);  mul_tensor_26 = None\n",
      "        mm_default_20: f32[136, 4096] = torch.ops.aten.mm.default(view_default_72, t_default_20);  view_default_72 = t_default_20 = None\n",
      "        view_default_73: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_20, [1, 136, 4096]);  mm_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_22: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_20, view_default_73);  add_tensor_20 = view_default_73 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_6: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_22, 2)\n",
      "        mean_dim_6: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_6, [-1], True);  pow_tensor_scalar_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_23: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_6, 1e-06);  mean_dim_6 = None\n",
      "        rsqrt_default_6: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_23);  add_tensor_23 = None\n",
      "        detach_default_9: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_6)\n",
      "        mul_tensor_27: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_22, rsqrt_default_6);  rsqrt_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant28 = self._param_constant28\n",
      "        mul_tensor_28: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant28, mul_tensor_27);  _param_constant28 = mul_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant29 = self._param_constant29\n",
      "        t_default_21: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant29);  _param_constant29 = None\n",
      "        view_default_74: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_28, [136, 4096])\n",
      "        mm_default_21: f32[136, 4096] = torch.ops.aten.mm.default(view_default_74, t_default_21);  view_default_74 = t_default_21 = None\n",
      "        view_default_75: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_21, [1, 136, 4096]);  mm_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant30 = self._param_constant30\n",
      "        t_default_22: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant30);  _param_constant30 = None\n",
      "        view_default_76: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_28, [136, 4096])\n",
      "        mm_default_22: f32[136, 4096] = torch.ops.aten.mm.default(view_default_76, t_default_22);  view_default_76 = t_default_22 = None\n",
      "        view_default_77: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_22, [1, 136, 4096]);  mm_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant31 = self._param_constant31\n",
      "        t_default_23: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant31);  _param_constant31 = None\n",
      "        view_default_78: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_28, [136, 4096]);  mul_tensor_28 = None\n",
      "        mm_default_23: f32[136, 4096] = torch.ops.aten.mm.default(view_default_78, t_default_23);  view_default_78 = t_default_23 = None\n",
      "        view_default_79: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_23, [1, 136, 4096]);  mm_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_80: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_75, [1, 136, 32, 128]);  view_default_75 = None\n",
      "        transpose_int_15: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_80, 1, 2);  view_default_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_81: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_77, [1, 136, 32, 128]);  view_default_77 = None\n",
      "        transpose_int_16: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_81, 1, 2);  view_default_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_82: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_79, [1, 136, 32, 128]);  view_default_79 = None\n",
      "        transpose_int_17: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_82, 1, 2);  view_default_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant6 = self._tensor_constant6\n",
      "        slice_tensor_34: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant6, 0, 0, 9223372036854775807);  _tensor_constant6 = None\n",
      "        slice_tensor_35: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_34, 1, 0, 9223372036854775807);  slice_tensor_34 = None\n",
      "        slice_tensor_36: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_35, 2, 0, 136);  slice_tensor_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant7 = self._tensor_constant7\n",
      "        slice_tensor_37: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant7, 0, 0, 9223372036854775807);  _tensor_constant7 = None\n",
      "        slice_tensor_38: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_37, 1, 0, 9223372036854775807);  slice_tensor_37 = None\n",
      "        slice_tensor_39: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_38, 2, 0, 136);  slice_tensor_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_12: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_36, 1);  slice_tensor_36 = None\n",
      "        squeeze_dim_13: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_12, 0);  squeeze_dim_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_14: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_39, 1);  slice_tensor_39 = None\n",
      "        squeeze_dim_15: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_14, 0);  squeeze_dim_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_6: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_13, [view_default]);  squeeze_dim_13 = None\n",
      "        unsqueeze_default_11: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_6, 1);  index_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_7: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_15, [view_default]);  squeeze_dim_15 = None\n",
      "        unsqueeze_default_12: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_7, 1);  index_tensor_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_29: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_15, unsqueeze_default_11)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_40: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_41: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 64, 9223372036854775807);  transpose_int_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_6: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_41);  slice_tensor_41 = None\n",
      "        cat_default_6: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_6, slice_tensor_40], -1);  neg_default_6 = slice_tensor_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_30: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_6, unsqueeze_default_12);  cat_default_6 = None\n",
      "        add_tensor_24: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_29, mul_tensor_30);  mul_tensor_29 = mul_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_31: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_16, unsqueeze_default_11);  unsqueeze_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_42: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_43: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 64, 9223372036854775807);  transpose_int_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_7: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_43);  slice_tensor_43 = None\n",
      "        cat_default_7: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_7, slice_tensor_42], -1);  neg_default_7 = slice_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_32: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_7, unsqueeze_default_12);  cat_default_7 = unsqueeze_default_12 = None\n",
      "        add_tensor_25: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_31, mul_tensor_32);  mul_tensor_31 = mul_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_18: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_25, 2, 3)\n",
      "        expand_default_14: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_24, [1, 32, 136, 128]);  add_tensor_24 = None\n",
      "        view_default_83: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_14, [32, 136, 128]);  expand_default_14 = None\n",
      "        expand_default_15: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_18, [1, 32, 128, 136]);  transpose_int_18 = None\n",
      "        view_default_84: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_15, [32, 128, 136]);  expand_default_15 = None\n",
      "        bmm_default_6: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_83, view_default_84);  view_default_83 = view_default_84 = None\n",
      "        view_default_85: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_6, [1, 32, 136, 136]);  bmm_default_6 = None\n",
      "        div_tensor_3: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_85, 11.313708498984761);  view_default_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_26: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_3, add_tensor_1);  div_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_3: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_26, -1, False);  add_tensor_26 = None\n",
      "        detach_default_10: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_3)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_16: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_3, [1, 32, 136, 136]);  _softmax_default_3 = None\n",
      "        view_default_86: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_16, [32, 136, 136]);  expand_default_16 = None\n",
      "        expand_default_17: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_17, [1, 32, 136, 128])\n",
      "        view_default_87: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_17, [32, 136, 128]);  expand_default_17 = None\n",
      "        bmm_default_7: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_86, view_default_87);  view_default_86 = view_default_87 = None\n",
      "        view_default_88: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_7, [1, 32, 136, 128]);  bmm_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_19: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_88, 1, 2);  view_default_88 = None\n",
      "        clone_default_3: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_19, memory_format = torch.contiguous_format);  transpose_int_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_89: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_3, [1, 136, 4096]);  clone_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant32 = self._param_constant32\n",
      "        t_default_24: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant32);  _param_constant32 = None\n",
      "        view_default_90: f32[136, 4096] = torch.ops.aten.view.default(view_default_89, [136, 4096]);  view_default_89 = None\n",
      "        mm_default_24: f32[136, 4096] = torch.ops.aten.mm.default(view_default_90, t_default_24);  view_default_90 = t_default_24 = None\n",
      "        view_default_91: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_24, [1, 136, 4096]);  mm_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_27: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_22, view_default_91);  add_tensor_22 = view_default_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_7: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_27, 2)\n",
      "        mean_dim_7: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_7, [-1], True);  pow_tensor_scalar_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_28: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_7, 1e-06);  mean_dim_7 = None\n",
      "        rsqrt_default_7: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_28);  add_tensor_28 = None\n",
      "        detach_default_11: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_7)\n",
      "        mul_tensor_33: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_27, rsqrt_default_7);  rsqrt_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant33 = self._param_constant33\n",
      "        mul_tensor_34: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant33, mul_tensor_33);  _param_constant33 = mul_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant34 = self._param_constant34\n",
      "        t_default_25: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant34);  _param_constant34 = None\n",
      "        view_default_92: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_34, [136, 4096])\n",
      "        mm_default_25: f32[136, 11008] = torch.ops.aten.mm.default(view_default_92, t_default_25);  view_default_92 = t_default_25 = None\n",
      "        view_default_93: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_25, [1, 136, 11008]);  mm_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_3: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_93);  view_default_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant35 = self._param_constant35\n",
      "        t_default_26: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant35);  _param_constant35 = None\n",
      "        view_default_94: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_34, [136, 4096]);  mul_tensor_34 = None\n",
      "        mm_default_26: f32[136, 11008] = torch.ops.aten.mm.default(view_default_94, t_default_26);  view_default_94 = t_default_26 = None\n",
      "        view_default_95: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_26, [1, 136, 11008]);  mm_default_26 = None\n",
      "        mul_tensor_35: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_3, view_default_95);  silu_default_3 = view_default_95 = None\n",
      "        _param_constant36 = self._param_constant36\n",
      "        t_default_27: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant36);  _param_constant36 = None\n",
      "        view_default_96: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_35, [136, 11008]);  mul_tensor_35 = None\n",
      "        mm_default_27: f32[136, 4096] = torch.ops.aten.mm.default(view_default_96, t_default_27);  view_default_96 = t_default_27 = None\n",
      "        view_default_97: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_27, [1, 136, 4096]);  mm_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_29: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_27, view_default_97);  add_tensor_27 = view_default_97 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_8: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_29, 2)\n",
      "        mean_dim_8: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_8, [-1], True);  pow_tensor_scalar_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_30: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_8, 1e-06);  mean_dim_8 = None\n",
      "        rsqrt_default_8: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_30);  add_tensor_30 = None\n",
      "        detach_default_12: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_8)\n",
      "        mul_tensor_36: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_29, rsqrt_default_8);  rsqrt_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant37 = self._param_constant37\n",
      "        mul_tensor_37: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant37, mul_tensor_36);  _param_constant37 = mul_tensor_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant38 = self._param_constant38\n",
      "        t_default_28: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant38);  _param_constant38 = None\n",
      "        view_default_98: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_37, [136, 4096])\n",
      "        mm_default_28: f32[136, 4096] = torch.ops.aten.mm.default(view_default_98, t_default_28);  view_default_98 = t_default_28 = None\n",
      "        view_default_99: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_28, [1, 136, 4096]);  mm_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant39 = self._param_constant39\n",
      "        t_default_29: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant39);  _param_constant39 = None\n",
      "        view_default_100: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_37, [136, 4096])\n",
      "        mm_default_29: f32[136, 4096] = torch.ops.aten.mm.default(view_default_100, t_default_29);  view_default_100 = t_default_29 = None\n",
      "        view_default_101: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_29, [1, 136, 4096]);  mm_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant40 = self._param_constant40\n",
      "        t_default_30: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant40);  _param_constant40 = None\n",
      "        view_default_102: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_37, [136, 4096]);  mul_tensor_37 = None\n",
      "        mm_default_30: f32[136, 4096] = torch.ops.aten.mm.default(view_default_102, t_default_30);  view_default_102 = t_default_30 = None\n",
      "        view_default_103: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_30, [1, 136, 4096]);  mm_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_104: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_99, [1, 136, 32, 128]);  view_default_99 = None\n",
      "        transpose_int_20: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_104, 1, 2);  view_default_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_105: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_101, [1, 136, 32, 128]);  view_default_101 = None\n",
      "        transpose_int_21: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_105, 1, 2);  view_default_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_106: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_103, [1, 136, 32, 128]);  view_default_103 = None\n",
      "        transpose_int_22: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_106, 1, 2);  view_default_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant8 = self._tensor_constant8\n",
      "        slice_tensor_44: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant8, 0, 0, 9223372036854775807);  _tensor_constant8 = None\n",
      "        slice_tensor_45: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_44, 1, 0, 9223372036854775807);  slice_tensor_44 = None\n",
      "        slice_tensor_46: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_45, 2, 0, 136);  slice_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant9 = self._tensor_constant9\n",
      "        slice_tensor_47: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant9, 0, 0, 9223372036854775807);  _tensor_constant9 = None\n",
      "        slice_tensor_48: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_47, 1, 0, 9223372036854775807);  slice_tensor_47 = None\n",
      "        slice_tensor_49: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_48, 2, 0, 136);  slice_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_16: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_46, 1);  slice_tensor_46 = None\n",
      "        squeeze_dim_17: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_16, 0);  squeeze_dim_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_18: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_49, 1);  slice_tensor_49 = None\n",
      "        squeeze_dim_19: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_18, 0);  squeeze_dim_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_8: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_17, [view_default]);  squeeze_dim_17 = None\n",
      "        unsqueeze_default_13: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_8, 1);  index_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_9: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_19, [view_default]);  squeeze_dim_19 = None\n",
      "        unsqueeze_default_14: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_9, 1);  index_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_38: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_20, unsqueeze_default_13)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_50: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_51: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 64, 9223372036854775807);  transpose_int_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_8: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_51);  slice_tensor_51 = None\n",
      "        cat_default_8: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_8, slice_tensor_50], -1);  neg_default_8 = slice_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_39: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_8, unsqueeze_default_14);  cat_default_8 = None\n",
      "        add_tensor_31: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_38, mul_tensor_39);  mul_tensor_38 = mul_tensor_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_40: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_21, unsqueeze_default_13);  unsqueeze_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_52: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_53: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 64, 9223372036854775807);  transpose_int_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_9: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_53);  slice_tensor_53 = None\n",
      "        cat_default_9: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_9, slice_tensor_52], -1);  neg_default_9 = slice_tensor_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_41: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_9, unsqueeze_default_14);  cat_default_9 = unsqueeze_default_14 = None\n",
      "        add_tensor_32: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_40, mul_tensor_41);  mul_tensor_40 = mul_tensor_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_23: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_32, 2, 3)\n",
      "        expand_default_18: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_31, [1, 32, 136, 128]);  add_tensor_31 = None\n",
      "        view_default_107: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_18, [32, 136, 128]);  expand_default_18 = None\n",
      "        expand_default_19: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_23, [1, 32, 128, 136]);  transpose_int_23 = None\n",
      "        view_default_108: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_19, [32, 128, 136]);  expand_default_19 = None\n",
      "        bmm_default_8: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_107, view_default_108);  view_default_107 = view_default_108 = None\n",
      "        view_default_109: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_8, [1, 32, 136, 136]);  bmm_default_8 = None\n",
      "        div_tensor_4: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_109, 11.313708498984761);  view_default_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_33: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_4, add_tensor_1);  div_tensor_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_4: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_33, -1, False);  add_tensor_33 = None\n",
      "        detach_default_13: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_4)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_20: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_4, [1, 32, 136, 136]);  _softmax_default_4 = None\n",
      "        view_default_110: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_20, [32, 136, 136]);  expand_default_20 = None\n",
      "        expand_default_21: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_22, [1, 32, 136, 128])\n",
      "        view_default_111: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_21, [32, 136, 128]);  expand_default_21 = None\n",
      "        bmm_default_9: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_110, view_default_111);  view_default_110 = view_default_111 = None\n",
      "        view_default_112: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_9, [1, 32, 136, 128]);  bmm_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_24: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_112, 1, 2);  view_default_112 = None\n",
      "        clone_default_4: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_24, memory_format = torch.contiguous_format);  transpose_int_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_113: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_4, [1, 136, 4096]);  clone_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant41 = self._param_constant41\n",
      "        t_default_31: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant41);  _param_constant41 = None\n",
      "        view_default_114: f32[136, 4096] = torch.ops.aten.view.default(view_default_113, [136, 4096]);  view_default_113 = None\n",
      "        mm_default_31: f32[136, 4096] = torch.ops.aten.mm.default(view_default_114, t_default_31);  view_default_114 = t_default_31 = None\n",
      "        view_default_115: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_31, [1, 136, 4096]);  mm_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_34: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_29, view_default_115);  add_tensor_29 = view_default_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_9: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_34, 2)\n",
      "        mean_dim_9: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_9, [-1], True);  pow_tensor_scalar_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_35: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_9, 1e-06);  mean_dim_9 = None\n",
      "        rsqrt_default_9: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_35);  add_tensor_35 = None\n",
      "        detach_default_14: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_9)\n",
      "        mul_tensor_42: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_34, rsqrt_default_9);  rsqrt_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant42 = self._param_constant42\n",
      "        mul_tensor_43: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant42, mul_tensor_42);  _param_constant42 = mul_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant43 = self._param_constant43\n",
      "        t_default_32: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant43);  _param_constant43 = None\n",
      "        view_default_116: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_43, [136, 4096])\n",
      "        mm_default_32: f32[136, 11008] = torch.ops.aten.mm.default(view_default_116, t_default_32);  view_default_116 = t_default_32 = None\n",
      "        view_default_117: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_32, [1, 136, 11008]);  mm_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_4: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_117);  view_default_117 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant44 = self._param_constant44\n",
      "        t_default_33: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant44);  _param_constant44 = None\n",
      "        view_default_118: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_43, [136, 4096]);  mul_tensor_43 = None\n",
      "        mm_default_33: f32[136, 11008] = torch.ops.aten.mm.default(view_default_118, t_default_33);  view_default_118 = t_default_33 = None\n",
      "        view_default_119: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_33, [1, 136, 11008]);  mm_default_33 = None\n",
      "        mul_tensor_44: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_4, view_default_119);  silu_default_4 = view_default_119 = None\n",
      "        _param_constant45 = self._param_constant45\n",
      "        t_default_34: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant45);  _param_constant45 = None\n",
      "        view_default_120: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_44, [136, 11008]);  mul_tensor_44 = None\n",
      "        mm_default_34: f32[136, 4096] = torch.ops.aten.mm.default(view_default_120, t_default_34);  view_default_120 = t_default_34 = None\n",
      "        view_default_121: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_34, [1, 136, 4096]);  mm_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_36: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_34, view_default_121);  add_tensor_34 = view_default_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_10: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_36, 2)\n",
      "        mean_dim_10: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_10, [-1], True);  pow_tensor_scalar_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_37: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_10, 1e-06);  mean_dim_10 = None\n",
      "        rsqrt_default_10: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_37);  add_tensor_37 = None\n",
      "        detach_default_15: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_10)\n",
      "        mul_tensor_45: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_36, rsqrt_default_10);  rsqrt_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant46 = self._param_constant46\n",
      "        mul_tensor_46: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant46, mul_tensor_45);  _param_constant46 = mul_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant47 = self._param_constant47\n",
      "        t_default_35: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant47);  _param_constant47 = None\n",
      "        view_default_122: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_46, [136, 4096])\n",
      "        mm_default_35: f32[136, 4096] = torch.ops.aten.mm.default(view_default_122, t_default_35);  view_default_122 = t_default_35 = None\n",
      "        view_default_123: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_35, [1, 136, 4096]);  mm_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant48 = self._param_constant48\n",
      "        t_default_36: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant48);  _param_constant48 = None\n",
      "        view_default_124: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_46, [136, 4096])\n",
      "        mm_default_36: f32[136, 4096] = torch.ops.aten.mm.default(view_default_124, t_default_36);  view_default_124 = t_default_36 = None\n",
      "        view_default_125: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_36, [1, 136, 4096]);  mm_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant49 = self._param_constant49\n",
      "        t_default_37: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant49);  _param_constant49 = None\n",
      "        view_default_126: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_46, [136, 4096]);  mul_tensor_46 = None\n",
      "        mm_default_37: f32[136, 4096] = torch.ops.aten.mm.default(view_default_126, t_default_37);  view_default_126 = t_default_37 = None\n",
      "        view_default_127: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_37, [1, 136, 4096]);  mm_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_128: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_123, [1, 136, 32, 128]);  view_default_123 = None\n",
      "        transpose_int_25: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_128, 1, 2);  view_default_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_129: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_125, [1, 136, 32, 128]);  view_default_125 = None\n",
      "        transpose_int_26: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_129, 1, 2);  view_default_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_130: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_127, [1, 136, 32, 128]);  view_default_127 = None\n",
      "        transpose_int_27: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_130, 1, 2);  view_default_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant10 = self._tensor_constant10\n",
      "        slice_tensor_54: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant10, 0, 0, 9223372036854775807);  _tensor_constant10 = None\n",
      "        slice_tensor_55: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_54, 1, 0, 9223372036854775807);  slice_tensor_54 = None\n",
      "        slice_tensor_56: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_55, 2, 0, 136);  slice_tensor_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant11 = self._tensor_constant11\n",
      "        slice_tensor_57: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant11, 0, 0, 9223372036854775807);  _tensor_constant11 = None\n",
      "        slice_tensor_58: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_57, 1, 0, 9223372036854775807);  slice_tensor_57 = None\n",
      "        slice_tensor_59: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_58, 2, 0, 136);  slice_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_20: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_56, 1);  slice_tensor_56 = None\n",
      "        squeeze_dim_21: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_20, 0);  squeeze_dim_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_22: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_59, 1);  slice_tensor_59 = None\n",
      "        squeeze_dim_23: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_22, 0);  squeeze_dim_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_10: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_21, [view_default]);  squeeze_dim_21 = None\n",
      "        unsqueeze_default_15: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_10, 1);  index_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_11: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_23, [view_default]);  squeeze_dim_23 = None\n",
      "        unsqueeze_default_16: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_11, 1);  index_tensor_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_47: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_25, unsqueeze_default_15)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_60: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_61: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 64, 9223372036854775807);  transpose_int_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_10: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_61);  slice_tensor_61 = None\n",
      "        cat_default_10: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_10, slice_tensor_60], -1);  neg_default_10 = slice_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_48: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_10, unsqueeze_default_16);  cat_default_10 = None\n",
      "        add_tensor_38: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_47, mul_tensor_48);  mul_tensor_47 = mul_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_49: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_26, unsqueeze_default_15);  unsqueeze_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_62: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_63: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 64, 9223372036854775807);  transpose_int_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_11: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_63);  slice_tensor_63 = None\n",
      "        cat_default_11: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_11, slice_tensor_62], -1);  neg_default_11 = slice_tensor_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_50: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_11, unsqueeze_default_16);  cat_default_11 = unsqueeze_default_16 = None\n",
      "        add_tensor_39: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_49, mul_tensor_50);  mul_tensor_49 = mul_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_28: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_39, 2, 3)\n",
      "        expand_default_22: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_38, [1, 32, 136, 128]);  add_tensor_38 = None\n",
      "        view_default_131: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_22, [32, 136, 128]);  expand_default_22 = None\n",
      "        expand_default_23: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_28, [1, 32, 128, 136]);  transpose_int_28 = None\n",
      "        view_default_132: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_23, [32, 128, 136]);  expand_default_23 = None\n",
      "        bmm_default_10: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_131, view_default_132);  view_default_131 = view_default_132 = None\n",
      "        view_default_133: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_10, [1, 32, 136, 136]);  bmm_default_10 = None\n",
      "        div_tensor_5: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_133, 11.313708498984761);  view_default_133 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_40: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_5, add_tensor_1);  div_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_5: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_40, -1, False);  add_tensor_40 = None\n",
      "        detach_default_16: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_5)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_24: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_5, [1, 32, 136, 136]);  _softmax_default_5 = None\n",
      "        view_default_134: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_24, [32, 136, 136]);  expand_default_24 = None\n",
      "        expand_default_25: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_27, [1, 32, 136, 128])\n",
      "        view_default_135: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_25, [32, 136, 128]);  expand_default_25 = None\n",
      "        bmm_default_11: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_134, view_default_135);  view_default_134 = view_default_135 = None\n",
      "        view_default_136: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_11, [1, 32, 136, 128]);  bmm_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_29: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_136, 1, 2);  view_default_136 = None\n",
      "        clone_default_5: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_29, memory_format = torch.contiguous_format);  transpose_int_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_137: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_5, [1, 136, 4096]);  clone_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant50 = self._param_constant50\n",
      "        t_default_38: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant50);  _param_constant50 = None\n",
      "        view_default_138: f32[136, 4096] = torch.ops.aten.view.default(view_default_137, [136, 4096]);  view_default_137 = None\n",
      "        mm_default_38: f32[136, 4096] = torch.ops.aten.mm.default(view_default_138, t_default_38);  view_default_138 = t_default_38 = None\n",
      "        view_default_139: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_38, [1, 136, 4096]);  mm_default_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_41: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_36, view_default_139);  add_tensor_36 = view_default_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_11: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_41, 2)\n",
      "        mean_dim_11: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_11, [-1], True);  pow_tensor_scalar_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_42: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_11, 1e-06);  mean_dim_11 = None\n",
      "        rsqrt_default_11: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_42);  add_tensor_42 = None\n",
      "        detach_default_17: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_11)\n",
      "        mul_tensor_51: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_41, rsqrt_default_11);  rsqrt_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant51 = self._param_constant51\n",
      "        mul_tensor_52: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant51, mul_tensor_51);  _param_constant51 = mul_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant52 = self._param_constant52\n",
      "        t_default_39: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant52);  _param_constant52 = None\n",
      "        view_default_140: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_52, [136, 4096])\n",
      "        mm_default_39: f32[136, 11008] = torch.ops.aten.mm.default(view_default_140, t_default_39);  view_default_140 = t_default_39 = None\n",
      "        view_default_141: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_39, [1, 136, 11008]);  mm_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_5: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_141);  view_default_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant53 = self._param_constant53\n",
      "        t_default_40: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant53);  _param_constant53 = None\n",
      "        view_default_142: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_52, [136, 4096]);  mul_tensor_52 = None\n",
      "        mm_default_40: f32[136, 11008] = torch.ops.aten.mm.default(view_default_142, t_default_40);  view_default_142 = t_default_40 = None\n",
      "        view_default_143: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_40, [1, 136, 11008]);  mm_default_40 = None\n",
      "        mul_tensor_53: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_5, view_default_143);  silu_default_5 = view_default_143 = None\n",
      "        _param_constant54 = self._param_constant54\n",
      "        t_default_41: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant54);  _param_constant54 = None\n",
      "        view_default_144: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_53, [136, 11008]);  mul_tensor_53 = None\n",
      "        mm_default_41: f32[136, 4096] = torch.ops.aten.mm.default(view_default_144, t_default_41);  view_default_144 = t_default_41 = None\n",
      "        view_default_145: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_41, [1, 136, 4096]);  mm_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_43: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_41, view_default_145);  add_tensor_41 = view_default_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_12: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_43, 2)\n",
      "        mean_dim_12: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_12, [-1], True);  pow_tensor_scalar_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_44: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_12, 1e-06);  mean_dim_12 = None\n",
      "        rsqrt_default_12: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_44);  add_tensor_44 = None\n",
      "        detach_default_18: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_12)\n",
      "        mul_tensor_54: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_43, rsqrt_default_12);  rsqrt_default_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant55 = self._param_constant55\n",
      "        mul_tensor_55: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant55, mul_tensor_54);  _param_constant55 = mul_tensor_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant56 = self._param_constant56\n",
      "        t_default_42: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant56);  _param_constant56 = None\n",
      "        view_default_146: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_55, [136, 4096])\n",
      "        mm_default_42: f32[136, 4096] = torch.ops.aten.mm.default(view_default_146, t_default_42);  view_default_146 = t_default_42 = None\n",
      "        view_default_147: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_42, [1, 136, 4096]);  mm_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant57 = self._param_constant57\n",
      "        t_default_43: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant57);  _param_constant57 = None\n",
      "        view_default_148: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_55, [136, 4096])\n",
      "        mm_default_43: f32[136, 4096] = torch.ops.aten.mm.default(view_default_148, t_default_43);  view_default_148 = t_default_43 = None\n",
      "        view_default_149: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_43, [1, 136, 4096]);  mm_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant58 = self._param_constant58\n",
      "        t_default_44: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant58);  _param_constant58 = None\n",
      "        view_default_150: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_55, [136, 4096]);  mul_tensor_55 = None\n",
      "        mm_default_44: f32[136, 4096] = torch.ops.aten.mm.default(view_default_150, t_default_44);  view_default_150 = t_default_44 = None\n",
      "        view_default_151: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_44, [1, 136, 4096]);  mm_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_152: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_147, [1, 136, 32, 128]);  view_default_147 = None\n",
      "        transpose_int_30: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_152, 1, 2);  view_default_152 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_153: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_149, [1, 136, 32, 128]);  view_default_149 = None\n",
      "        transpose_int_31: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_153, 1, 2);  view_default_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_154: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_151, [1, 136, 32, 128]);  view_default_151 = None\n",
      "        transpose_int_32: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_154, 1, 2);  view_default_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant12 = self._tensor_constant12\n",
      "        slice_tensor_64: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant12, 0, 0, 9223372036854775807);  _tensor_constant12 = None\n",
      "        slice_tensor_65: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_64, 1, 0, 9223372036854775807);  slice_tensor_64 = None\n",
      "        slice_tensor_66: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_65, 2, 0, 136);  slice_tensor_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant13 = self._tensor_constant13\n",
      "        slice_tensor_67: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant13, 0, 0, 9223372036854775807);  _tensor_constant13 = None\n",
      "        slice_tensor_68: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_67, 1, 0, 9223372036854775807);  slice_tensor_67 = None\n",
      "        slice_tensor_69: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_68, 2, 0, 136);  slice_tensor_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_24: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_66, 1);  slice_tensor_66 = None\n",
      "        squeeze_dim_25: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_24, 0);  squeeze_dim_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_26: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_69, 1);  slice_tensor_69 = None\n",
      "        squeeze_dim_27: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_26, 0);  squeeze_dim_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_12: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_25, [view_default]);  squeeze_dim_25 = None\n",
      "        unsqueeze_default_17: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_12, 1);  index_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_13: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_27, [view_default]);  squeeze_dim_27 = None\n",
      "        unsqueeze_default_18: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_13, 1);  index_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_56: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_30, unsqueeze_default_17)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_70: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_71: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 64, 9223372036854775807);  transpose_int_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_12: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_71);  slice_tensor_71 = None\n",
      "        cat_default_12: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_12, slice_tensor_70], -1);  neg_default_12 = slice_tensor_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_57: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_12, unsqueeze_default_18);  cat_default_12 = None\n",
      "        add_tensor_45: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_56, mul_tensor_57);  mul_tensor_56 = mul_tensor_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_58: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_31, unsqueeze_default_17);  unsqueeze_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_72: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_73: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 64, 9223372036854775807);  transpose_int_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_13: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_73);  slice_tensor_73 = None\n",
      "        cat_default_13: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_13, slice_tensor_72], -1);  neg_default_13 = slice_tensor_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_59: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_13, unsqueeze_default_18);  cat_default_13 = unsqueeze_default_18 = None\n",
      "        add_tensor_46: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_58, mul_tensor_59);  mul_tensor_58 = mul_tensor_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_33: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_46, 2, 3)\n",
      "        expand_default_26: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_45, [1, 32, 136, 128]);  add_tensor_45 = None\n",
      "        view_default_155: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_26, [32, 136, 128]);  expand_default_26 = None\n",
      "        expand_default_27: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_33, [1, 32, 128, 136]);  transpose_int_33 = None\n",
      "        view_default_156: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_27, [32, 128, 136]);  expand_default_27 = None\n",
      "        bmm_default_12: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_155, view_default_156);  view_default_155 = view_default_156 = None\n",
      "        view_default_157: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_12, [1, 32, 136, 136]);  bmm_default_12 = None\n",
      "        div_tensor_6: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_157, 11.313708498984761);  view_default_157 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_47: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_6, add_tensor_1);  div_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_6: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_47, -1, False);  add_tensor_47 = None\n",
      "        detach_default_19: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_6)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_28: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_6, [1, 32, 136, 136]);  _softmax_default_6 = None\n",
      "        view_default_158: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_28, [32, 136, 136]);  expand_default_28 = None\n",
      "        expand_default_29: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_32, [1, 32, 136, 128])\n",
      "        view_default_159: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_29, [32, 136, 128]);  expand_default_29 = None\n",
      "        bmm_default_13: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_158, view_default_159);  view_default_158 = view_default_159 = None\n",
      "        view_default_160: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_13, [1, 32, 136, 128]);  bmm_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_34: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_160, 1, 2);  view_default_160 = None\n",
      "        clone_default_6: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_34, memory_format = torch.contiguous_format);  transpose_int_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_161: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_6, [1, 136, 4096]);  clone_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant59 = self._param_constant59\n",
      "        t_default_45: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant59);  _param_constant59 = None\n",
      "        view_default_162: f32[136, 4096] = torch.ops.aten.view.default(view_default_161, [136, 4096]);  view_default_161 = None\n",
      "        mm_default_45: f32[136, 4096] = torch.ops.aten.mm.default(view_default_162, t_default_45);  view_default_162 = t_default_45 = None\n",
      "        view_default_163: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_45, [1, 136, 4096]);  mm_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_48: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_43, view_default_163);  add_tensor_43 = view_default_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_13: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_48, 2)\n",
      "        mean_dim_13: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_13, [-1], True);  pow_tensor_scalar_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_49: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_13, 1e-06);  mean_dim_13 = None\n",
      "        rsqrt_default_13: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_49);  add_tensor_49 = None\n",
      "        detach_default_20: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_13)\n",
      "        mul_tensor_60: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_48, rsqrt_default_13);  rsqrt_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant60 = self._param_constant60\n",
      "        mul_tensor_61: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant60, mul_tensor_60);  _param_constant60 = mul_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant61 = self._param_constant61\n",
      "        t_default_46: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant61);  _param_constant61 = None\n",
      "        view_default_164: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_61, [136, 4096])\n",
      "        mm_default_46: f32[136, 11008] = torch.ops.aten.mm.default(view_default_164, t_default_46);  view_default_164 = t_default_46 = None\n",
      "        view_default_165: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_46, [1, 136, 11008]);  mm_default_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_6: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_165);  view_default_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant62 = self._param_constant62\n",
      "        t_default_47: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant62);  _param_constant62 = None\n",
      "        view_default_166: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_61, [136, 4096]);  mul_tensor_61 = None\n",
      "        mm_default_47: f32[136, 11008] = torch.ops.aten.mm.default(view_default_166, t_default_47);  view_default_166 = t_default_47 = None\n",
      "        view_default_167: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_47, [1, 136, 11008]);  mm_default_47 = None\n",
      "        mul_tensor_62: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_6, view_default_167);  silu_default_6 = view_default_167 = None\n",
      "        _param_constant63 = self._param_constant63\n",
      "        t_default_48: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant63);  _param_constant63 = None\n",
      "        view_default_168: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_62, [136, 11008]);  mul_tensor_62 = None\n",
      "        mm_default_48: f32[136, 4096] = torch.ops.aten.mm.default(view_default_168, t_default_48);  view_default_168 = t_default_48 = None\n",
      "        view_default_169: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_48, [1, 136, 4096]);  mm_default_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_50: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_48, view_default_169);  add_tensor_48 = view_default_169 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_14: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_50, 2)\n",
      "        mean_dim_14: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_14, [-1], True);  pow_tensor_scalar_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_51: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_14, 1e-06);  mean_dim_14 = None\n",
      "        rsqrt_default_14: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_51);  add_tensor_51 = None\n",
      "        detach_default_21: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_14)\n",
      "        mul_tensor_63: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_50, rsqrt_default_14);  rsqrt_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant64 = self._param_constant64\n",
      "        mul_tensor_64: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant64, mul_tensor_63);  _param_constant64 = mul_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant65 = self._param_constant65\n",
      "        t_default_49: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant65);  _param_constant65 = None\n",
      "        view_default_170: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_64, [136, 4096])\n",
      "        mm_default_49: f32[136, 4096] = torch.ops.aten.mm.default(view_default_170, t_default_49);  view_default_170 = t_default_49 = None\n",
      "        view_default_171: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_49, [1, 136, 4096]);  mm_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant66 = self._param_constant66\n",
      "        t_default_50: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant66);  _param_constant66 = None\n",
      "        view_default_172: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_64, [136, 4096])\n",
      "        mm_default_50: f32[136, 4096] = torch.ops.aten.mm.default(view_default_172, t_default_50);  view_default_172 = t_default_50 = None\n",
      "        view_default_173: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_50, [1, 136, 4096]);  mm_default_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant67 = self._param_constant67\n",
      "        t_default_51: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant67);  _param_constant67 = None\n",
      "        view_default_174: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_64, [136, 4096]);  mul_tensor_64 = None\n",
      "        mm_default_51: f32[136, 4096] = torch.ops.aten.mm.default(view_default_174, t_default_51);  view_default_174 = t_default_51 = None\n",
      "        view_default_175: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_51, [1, 136, 4096]);  mm_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_176: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_171, [1, 136, 32, 128]);  view_default_171 = None\n",
      "        transpose_int_35: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_176, 1, 2);  view_default_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_177: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_173, [1, 136, 32, 128]);  view_default_173 = None\n",
      "        transpose_int_36: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_177, 1, 2);  view_default_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_178: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_175, [1, 136, 32, 128]);  view_default_175 = None\n",
      "        transpose_int_37: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_178, 1, 2);  view_default_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant14 = self._tensor_constant14\n",
      "        slice_tensor_74: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant14, 0, 0, 9223372036854775807);  _tensor_constant14 = None\n",
      "        slice_tensor_75: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_74, 1, 0, 9223372036854775807);  slice_tensor_74 = None\n",
      "        slice_tensor_76: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_75, 2, 0, 136);  slice_tensor_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant15 = self._tensor_constant15\n",
      "        slice_tensor_77: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant15, 0, 0, 9223372036854775807);  _tensor_constant15 = None\n",
      "        slice_tensor_78: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_77, 1, 0, 9223372036854775807);  slice_tensor_77 = None\n",
      "        slice_tensor_79: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_78, 2, 0, 136);  slice_tensor_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_28: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_76, 1);  slice_tensor_76 = None\n",
      "        squeeze_dim_29: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_28, 0);  squeeze_dim_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_30: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_79, 1);  slice_tensor_79 = None\n",
      "        squeeze_dim_31: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_30, 0);  squeeze_dim_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_14: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_29, [view_default]);  squeeze_dim_29 = None\n",
      "        unsqueeze_default_19: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_14, 1);  index_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_15: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_31, [view_default]);  squeeze_dim_31 = None\n",
      "        unsqueeze_default_20: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_15, 1);  index_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_65: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_35, unsqueeze_default_19)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_80: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_81: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 64, 9223372036854775807);  transpose_int_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_14: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_81);  slice_tensor_81 = None\n",
      "        cat_default_14: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_14, slice_tensor_80], -1);  neg_default_14 = slice_tensor_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_66: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_14, unsqueeze_default_20);  cat_default_14 = None\n",
      "        add_tensor_52: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_65, mul_tensor_66);  mul_tensor_65 = mul_tensor_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_67: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_36, unsqueeze_default_19);  unsqueeze_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_82: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_83: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 64, 9223372036854775807);  transpose_int_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_15: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_83);  slice_tensor_83 = None\n",
      "        cat_default_15: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_15, slice_tensor_82], -1);  neg_default_15 = slice_tensor_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_68: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_15, unsqueeze_default_20);  cat_default_15 = unsqueeze_default_20 = None\n",
      "        add_tensor_53: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_67, mul_tensor_68);  mul_tensor_67 = mul_tensor_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_38: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_53, 2, 3)\n",
      "        expand_default_30: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_52, [1, 32, 136, 128]);  add_tensor_52 = None\n",
      "        view_default_179: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_30, [32, 136, 128]);  expand_default_30 = None\n",
      "        expand_default_31: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_38, [1, 32, 128, 136]);  transpose_int_38 = None\n",
      "        view_default_180: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_31, [32, 128, 136]);  expand_default_31 = None\n",
      "        bmm_default_14: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_179, view_default_180);  view_default_179 = view_default_180 = None\n",
      "        view_default_181: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_14, [1, 32, 136, 136]);  bmm_default_14 = None\n",
      "        div_tensor_7: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_181, 11.313708498984761);  view_default_181 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_54: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_7, add_tensor_1);  div_tensor_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_7: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_54, -1, False);  add_tensor_54 = None\n",
      "        detach_default_22: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_7)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_32: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_7, [1, 32, 136, 136]);  _softmax_default_7 = None\n",
      "        view_default_182: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_32, [32, 136, 136]);  expand_default_32 = None\n",
      "        expand_default_33: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_37, [1, 32, 136, 128])\n",
      "        view_default_183: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_33, [32, 136, 128]);  expand_default_33 = None\n",
      "        bmm_default_15: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_182, view_default_183);  view_default_182 = view_default_183 = None\n",
      "        view_default_184: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_15, [1, 32, 136, 128]);  bmm_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_39: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_184, 1, 2);  view_default_184 = None\n",
      "        clone_default_7: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_39, memory_format = torch.contiguous_format);  transpose_int_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_185: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_7, [1, 136, 4096]);  clone_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant68 = self._param_constant68\n",
      "        t_default_52: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant68);  _param_constant68 = None\n",
      "        view_default_186: f32[136, 4096] = torch.ops.aten.view.default(view_default_185, [136, 4096]);  view_default_185 = None\n",
      "        mm_default_52: f32[136, 4096] = torch.ops.aten.mm.default(view_default_186, t_default_52);  view_default_186 = t_default_52 = None\n",
      "        view_default_187: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_52, [1, 136, 4096]);  mm_default_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_55: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_50, view_default_187);  add_tensor_50 = view_default_187 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_15: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_55, 2)\n",
      "        mean_dim_15: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_15, [-1], True);  pow_tensor_scalar_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_56: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_15, 1e-06);  mean_dim_15 = None\n",
      "        rsqrt_default_15: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_56);  add_tensor_56 = None\n",
      "        detach_default_23: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_15)\n",
      "        mul_tensor_69: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_55, rsqrt_default_15);  rsqrt_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant69 = self._param_constant69\n",
      "        mul_tensor_70: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant69, mul_tensor_69);  _param_constant69 = mul_tensor_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant70 = self._param_constant70\n",
      "        t_default_53: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant70);  _param_constant70 = None\n",
      "        view_default_188: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_70, [136, 4096])\n",
      "        mm_default_53: f32[136, 11008] = torch.ops.aten.mm.default(view_default_188, t_default_53);  view_default_188 = t_default_53 = None\n",
      "        view_default_189: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_53, [1, 136, 11008]);  mm_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_7: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_189);  view_default_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant71 = self._param_constant71\n",
      "        t_default_54: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant71);  _param_constant71 = None\n",
      "        view_default_190: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_70, [136, 4096]);  mul_tensor_70 = None\n",
      "        mm_default_54: f32[136, 11008] = torch.ops.aten.mm.default(view_default_190, t_default_54);  view_default_190 = t_default_54 = None\n",
      "        view_default_191: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_54, [1, 136, 11008]);  mm_default_54 = None\n",
      "        mul_tensor_71: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_7, view_default_191);  silu_default_7 = view_default_191 = None\n",
      "        _param_constant72 = self._param_constant72\n",
      "        t_default_55: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant72);  _param_constant72 = None\n",
      "        view_default_192: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_71, [136, 11008]);  mul_tensor_71 = None\n",
      "        mm_default_55: f32[136, 4096] = torch.ops.aten.mm.default(view_default_192, t_default_55);  view_default_192 = t_default_55 = None\n",
      "        view_default_193: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_55, [1, 136, 4096]);  mm_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_57: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_55, view_default_193);  add_tensor_55 = view_default_193 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_16: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_57, 2)\n",
      "        mean_dim_16: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_16, [-1], True);  pow_tensor_scalar_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_58: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_16, 1e-06);  mean_dim_16 = None\n",
      "        rsqrt_default_16: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_58);  add_tensor_58 = None\n",
      "        detach_default_24: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_16)\n",
      "        mul_tensor_72: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_57, rsqrt_default_16);  rsqrt_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant73 = self._param_constant73\n",
      "        mul_tensor_73: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant73, mul_tensor_72);  _param_constant73 = mul_tensor_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant74 = self._param_constant74\n",
      "        t_default_56: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant74);  _param_constant74 = None\n",
      "        view_default_194: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_73, [136, 4096])\n",
      "        mm_default_56: f32[136, 4096] = torch.ops.aten.mm.default(view_default_194, t_default_56);  view_default_194 = t_default_56 = None\n",
      "        view_default_195: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_56, [1, 136, 4096]);  mm_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant75 = self._param_constant75\n",
      "        t_default_57: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant75);  _param_constant75 = None\n",
      "        view_default_196: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_73, [136, 4096])\n",
      "        mm_default_57: f32[136, 4096] = torch.ops.aten.mm.default(view_default_196, t_default_57);  view_default_196 = t_default_57 = None\n",
      "        view_default_197: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_57, [1, 136, 4096]);  mm_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant76 = self._param_constant76\n",
      "        t_default_58: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant76);  _param_constant76 = None\n",
      "        view_default_198: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_73, [136, 4096]);  mul_tensor_73 = None\n",
      "        mm_default_58: f32[136, 4096] = torch.ops.aten.mm.default(view_default_198, t_default_58);  view_default_198 = t_default_58 = None\n",
      "        view_default_199: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_58, [1, 136, 4096]);  mm_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_200: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_195, [1, 136, 32, 128]);  view_default_195 = None\n",
      "        transpose_int_40: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_200, 1, 2);  view_default_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_201: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_197, [1, 136, 32, 128]);  view_default_197 = None\n",
      "        transpose_int_41: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_201, 1, 2);  view_default_201 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_202: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_199, [1, 136, 32, 128]);  view_default_199 = None\n",
      "        transpose_int_42: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_202, 1, 2);  view_default_202 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant16 = self._tensor_constant16\n",
      "        slice_tensor_84: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant16, 0, 0, 9223372036854775807);  _tensor_constant16 = None\n",
      "        slice_tensor_85: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_84, 1, 0, 9223372036854775807);  slice_tensor_84 = None\n",
      "        slice_tensor_86: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_85, 2, 0, 136);  slice_tensor_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant17 = self._tensor_constant17\n",
      "        slice_tensor_87: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant17, 0, 0, 9223372036854775807);  _tensor_constant17 = None\n",
      "        slice_tensor_88: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_87, 1, 0, 9223372036854775807);  slice_tensor_87 = None\n",
      "        slice_tensor_89: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_88, 2, 0, 136);  slice_tensor_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_32: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_86, 1);  slice_tensor_86 = None\n",
      "        squeeze_dim_33: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_32, 0);  squeeze_dim_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_34: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_89, 1);  slice_tensor_89 = None\n",
      "        squeeze_dim_35: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_34, 0);  squeeze_dim_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_16: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_33, [view_default]);  squeeze_dim_33 = None\n",
      "        unsqueeze_default_21: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_16, 1);  index_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_17: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_35, [view_default]);  squeeze_dim_35 = None\n",
      "        unsqueeze_default_22: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_17, 1);  index_tensor_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_74: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_40, unsqueeze_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_90: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_91: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 64, 9223372036854775807);  transpose_int_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_16: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_91);  slice_tensor_91 = None\n",
      "        cat_default_16: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_16, slice_tensor_90], -1);  neg_default_16 = slice_tensor_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_75: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_16, unsqueeze_default_22);  cat_default_16 = None\n",
      "        add_tensor_59: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_74, mul_tensor_75);  mul_tensor_74 = mul_tensor_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_76: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_41, unsqueeze_default_21);  unsqueeze_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_92: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_93: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 64, 9223372036854775807);  transpose_int_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_17: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_93);  slice_tensor_93 = None\n",
      "        cat_default_17: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_17, slice_tensor_92], -1);  neg_default_17 = slice_tensor_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_77: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_17, unsqueeze_default_22);  cat_default_17 = unsqueeze_default_22 = None\n",
      "        add_tensor_60: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_76, mul_tensor_77);  mul_tensor_76 = mul_tensor_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_43: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_60, 2, 3)\n",
      "        expand_default_34: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_59, [1, 32, 136, 128]);  add_tensor_59 = None\n",
      "        view_default_203: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_34, [32, 136, 128]);  expand_default_34 = None\n",
      "        expand_default_35: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_43, [1, 32, 128, 136]);  transpose_int_43 = None\n",
      "        view_default_204: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_35, [32, 128, 136]);  expand_default_35 = None\n",
      "        bmm_default_16: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_203, view_default_204);  view_default_203 = view_default_204 = None\n",
      "        view_default_205: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_16, [1, 32, 136, 136]);  bmm_default_16 = None\n",
      "        div_tensor_8: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_205, 11.313708498984761);  view_default_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_61: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_8, add_tensor_1);  div_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_8: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_61, -1, False);  add_tensor_61 = None\n",
      "        detach_default_25: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_8)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_36: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_8, [1, 32, 136, 136]);  _softmax_default_8 = None\n",
      "        view_default_206: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_36, [32, 136, 136]);  expand_default_36 = None\n",
      "        expand_default_37: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_42, [1, 32, 136, 128])\n",
      "        view_default_207: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_37, [32, 136, 128]);  expand_default_37 = None\n",
      "        bmm_default_17: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_206, view_default_207);  view_default_206 = view_default_207 = None\n",
      "        view_default_208: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_17, [1, 32, 136, 128]);  bmm_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_44: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_208, 1, 2);  view_default_208 = None\n",
      "        clone_default_8: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_44, memory_format = torch.contiguous_format);  transpose_int_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_209: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_8, [1, 136, 4096]);  clone_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant77 = self._param_constant77\n",
      "        t_default_59: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant77);  _param_constant77 = None\n",
      "        view_default_210: f32[136, 4096] = torch.ops.aten.view.default(view_default_209, [136, 4096]);  view_default_209 = None\n",
      "        mm_default_59: f32[136, 4096] = torch.ops.aten.mm.default(view_default_210, t_default_59);  view_default_210 = t_default_59 = None\n",
      "        view_default_211: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_59, [1, 136, 4096]);  mm_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_62: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_57, view_default_211);  add_tensor_57 = view_default_211 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_17: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_62, 2)\n",
      "        mean_dim_17: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_17, [-1], True);  pow_tensor_scalar_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_63: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_17, 1e-06);  mean_dim_17 = None\n",
      "        rsqrt_default_17: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_63);  add_tensor_63 = None\n",
      "        detach_default_26: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_17)\n",
      "        mul_tensor_78: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_62, rsqrt_default_17);  rsqrt_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant78 = self._param_constant78\n",
      "        mul_tensor_79: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant78, mul_tensor_78);  _param_constant78 = mul_tensor_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant79 = self._param_constant79\n",
      "        t_default_60: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant79);  _param_constant79 = None\n",
      "        view_default_212: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_79, [136, 4096])\n",
      "        mm_default_60: f32[136, 11008] = torch.ops.aten.mm.default(view_default_212, t_default_60);  view_default_212 = t_default_60 = None\n",
      "        view_default_213: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_60, [1, 136, 11008]);  mm_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_8: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_213);  view_default_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant80 = self._param_constant80\n",
      "        t_default_61: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant80);  _param_constant80 = None\n",
      "        view_default_214: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_79, [136, 4096]);  mul_tensor_79 = None\n",
      "        mm_default_61: f32[136, 11008] = torch.ops.aten.mm.default(view_default_214, t_default_61);  view_default_214 = t_default_61 = None\n",
      "        view_default_215: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_61, [1, 136, 11008]);  mm_default_61 = None\n",
      "        mul_tensor_80: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_8, view_default_215);  silu_default_8 = view_default_215 = None\n",
      "        _param_constant81 = self._param_constant81\n",
      "        t_default_62: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant81);  _param_constant81 = None\n",
      "        view_default_216: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_80, [136, 11008]);  mul_tensor_80 = None\n",
      "        mm_default_62: f32[136, 4096] = torch.ops.aten.mm.default(view_default_216, t_default_62);  view_default_216 = t_default_62 = None\n",
      "        view_default_217: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_62, [1, 136, 4096]);  mm_default_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_64: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_62, view_default_217);  add_tensor_62 = view_default_217 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_18: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_64, 2)\n",
      "        mean_dim_18: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_18, [-1], True);  pow_tensor_scalar_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_65: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_18, 1e-06);  mean_dim_18 = None\n",
      "        rsqrt_default_18: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_65);  add_tensor_65 = None\n",
      "        detach_default_27: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_18)\n",
      "        mul_tensor_81: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_64, rsqrt_default_18);  rsqrt_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant82 = self._param_constant82\n",
      "        mul_tensor_82: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant82, mul_tensor_81);  _param_constant82 = mul_tensor_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant83 = self._param_constant83\n",
      "        t_default_63: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant83);  _param_constant83 = None\n",
      "        view_default_218: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_82, [136, 4096])\n",
      "        mm_default_63: f32[136, 4096] = torch.ops.aten.mm.default(view_default_218, t_default_63);  view_default_218 = t_default_63 = None\n",
      "        view_default_219: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_63, [1, 136, 4096]);  mm_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant84 = self._param_constant84\n",
      "        t_default_64: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant84);  _param_constant84 = None\n",
      "        view_default_220: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_82, [136, 4096])\n",
      "        mm_default_64: f32[136, 4096] = torch.ops.aten.mm.default(view_default_220, t_default_64);  view_default_220 = t_default_64 = None\n",
      "        view_default_221: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_64, [1, 136, 4096]);  mm_default_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant85 = self._param_constant85\n",
      "        t_default_65: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant85);  _param_constant85 = None\n",
      "        view_default_222: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_82, [136, 4096]);  mul_tensor_82 = None\n",
      "        mm_default_65: f32[136, 4096] = torch.ops.aten.mm.default(view_default_222, t_default_65);  view_default_222 = t_default_65 = None\n",
      "        view_default_223: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_65, [1, 136, 4096]);  mm_default_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_224: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_219, [1, 136, 32, 128]);  view_default_219 = None\n",
      "        transpose_int_45: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_224, 1, 2);  view_default_224 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_225: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_221, [1, 136, 32, 128]);  view_default_221 = None\n",
      "        transpose_int_46: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_225, 1, 2);  view_default_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_226: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_223, [1, 136, 32, 128]);  view_default_223 = None\n",
      "        transpose_int_47: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_226, 1, 2);  view_default_226 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant18 = self._tensor_constant18\n",
      "        slice_tensor_94: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant18, 0, 0, 9223372036854775807);  _tensor_constant18 = None\n",
      "        slice_tensor_95: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_94, 1, 0, 9223372036854775807);  slice_tensor_94 = None\n",
      "        slice_tensor_96: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_95, 2, 0, 136);  slice_tensor_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant19 = self._tensor_constant19\n",
      "        slice_tensor_97: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant19, 0, 0, 9223372036854775807);  _tensor_constant19 = None\n",
      "        slice_tensor_98: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_97, 1, 0, 9223372036854775807);  slice_tensor_97 = None\n",
      "        slice_tensor_99: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_98, 2, 0, 136);  slice_tensor_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_36: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_96, 1);  slice_tensor_96 = None\n",
      "        squeeze_dim_37: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_36, 0);  squeeze_dim_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_38: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_99, 1);  slice_tensor_99 = None\n",
      "        squeeze_dim_39: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_38, 0);  squeeze_dim_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_18: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_37, [view_default]);  squeeze_dim_37 = None\n",
      "        unsqueeze_default_23: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_18, 1);  index_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_19: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_39, [view_default]);  squeeze_dim_39 = None\n",
      "        unsqueeze_default_24: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_19, 1);  index_tensor_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_83: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_45, unsqueeze_default_23)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_100: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_101: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 64, 9223372036854775807);  transpose_int_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_18: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_101);  slice_tensor_101 = None\n",
      "        cat_default_18: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_18, slice_tensor_100], -1);  neg_default_18 = slice_tensor_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_84: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_18, unsqueeze_default_24);  cat_default_18 = None\n",
      "        add_tensor_66: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_83, mul_tensor_84);  mul_tensor_83 = mul_tensor_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_85: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_46, unsqueeze_default_23);  unsqueeze_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_102: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_103: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 64, 9223372036854775807);  transpose_int_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_19: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_103);  slice_tensor_103 = None\n",
      "        cat_default_19: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_19, slice_tensor_102], -1);  neg_default_19 = slice_tensor_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_86: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_19, unsqueeze_default_24);  cat_default_19 = unsqueeze_default_24 = None\n",
      "        add_tensor_67: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_85, mul_tensor_86);  mul_tensor_85 = mul_tensor_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_48: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_67, 2, 3)\n",
      "        expand_default_38: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_66, [1, 32, 136, 128]);  add_tensor_66 = None\n",
      "        view_default_227: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_38, [32, 136, 128]);  expand_default_38 = None\n",
      "        expand_default_39: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_48, [1, 32, 128, 136]);  transpose_int_48 = None\n",
      "        view_default_228: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_39, [32, 128, 136]);  expand_default_39 = None\n",
      "        bmm_default_18: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_227, view_default_228);  view_default_227 = view_default_228 = None\n",
      "        view_default_229: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_18, [1, 32, 136, 136]);  bmm_default_18 = None\n",
      "        div_tensor_9: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_229, 11.313708498984761);  view_default_229 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_68: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_9, add_tensor_1);  div_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_9: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_68, -1, False);  add_tensor_68 = None\n",
      "        detach_default_28: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_9)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_40: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_9, [1, 32, 136, 136]);  _softmax_default_9 = None\n",
      "        view_default_230: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_40, [32, 136, 136]);  expand_default_40 = None\n",
      "        expand_default_41: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_47, [1, 32, 136, 128])\n",
      "        view_default_231: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_41, [32, 136, 128]);  expand_default_41 = None\n",
      "        bmm_default_19: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_230, view_default_231);  view_default_230 = view_default_231 = None\n",
      "        view_default_232: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_19, [1, 32, 136, 128]);  bmm_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_49: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_232, 1, 2);  view_default_232 = None\n",
      "        clone_default_9: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_49, memory_format = torch.contiguous_format);  transpose_int_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_233: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_9, [1, 136, 4096]);  clone_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant86 = self._param_constant86\n",
      "        t_default_66: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant86);  _param_constant86 = None\n",
      "        view_default_234: f32[136, 4096] = torch.ops.aten.view.default(view_default_233, [136, 4096]);  view_default_233 = None\n",
      "        mm_default_66: f32[136, 4096] = torch.ops.aten.mm.default(view_default_234, t_default_66);  view_default_234 = t_default_66 = None\n",
      "        view_default_235: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_66, [1, 136, 4096]);  mm_default_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_69: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_64, view_default_235);  add_tensor_64 = view_default_235 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_19: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_69, 2)\n",
      "        mean_dim_19: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_19, [-1], True);  pow_tensor_scalar_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_70: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_19, 1e-06);  mean_dim_19 = None\n",
      "        rsqrt_default_19: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_70);  add_tensor_70 = None\n",
      "        detach_default_29: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_19)\n",
      "        mul_tensor_87: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_69, rsqrt_default_19);  rsqrt_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant87 = self._param_constant87\n",
      "        mul_tensor_88: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant87, mul_tensor_87);  _param_constant87 = mul_tensor_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant88 = self._param_constant88\n",
      "        t_default_67: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant88);  _param_constant88 = None\n",
      "        view_default_236: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_88, [136, 4096])\n",
      "        mm_default_67: f32[136, 11008] = torch.ops.aten.mm.default(view_default_236, t_default_67);  view_default_236 = t_default_67 = None\n",
      "        view_default_237: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_67, [1, 136, 11008]);  mm_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_9: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_237);  view_default_237 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant89 = self._param_constant89\n",
      "        t_default_68: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant89);  _param_constant89 = None\n",
      "        view_default_238: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_88, [136, 4096]);  mul_tensor_88 = None\n",
      "        mm_default_68: f32[136, 11008] = torch.ops.aten.mm.default(view_default_238, t_default_68);  view_default_238 = t_default_68 = None\n",
      "        view_default_239: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_68, [1, 136, 11008]);  mm_default_68 = None\n",
      "        mul_tensor_89: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_9, view_default_239);  silu_default_9 = view_default_239 = None\n",
      "        _param_constant90 = self._param_constant90\n",
      "        t_default_69: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant90);  _param_constant90 = None\n",
      "        view_default_240: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_89, [136, 11008]);  mul_tensor_89 = None\n",
      "        mm_default_69: f32[136, 4096] = torch.ops.aten.mm.default(view_default_240, t_default_69);  view_default_240 = t_default_69 = None\n",
      "        view_default_241: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_69, [1, 136, 4096]);  mm_default_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_71: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_69, view_default_241);  add_tensor_69 = view_default_241 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_20: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_71, 2)\n",
      "        mean_dim_20: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_20, [-1], True);  pow_tensor_scalar_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_72: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_20, 1e-06);  mean_dim_20 = None\n",
      "        rsqrt_default_20: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_72);  add_tensor_72 = None\n",
      "        detach_default_30: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_20)\n",
      "        mul_tensor_90: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_71, rsqrt_default_20);  rsqrt_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant91 = self._param_constant91\n",
      "        mul_tensor_91: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant91, mul_tensor_90);  _param_constant91 = mul_tensor_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant92 = self._param_constant92\n",
      "        t_default_70: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant92);  _param_constant92 = None\n",
      "        view_default_242: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_91, [136, 4096])\n",
      "        mm_default_70: f32[136, 4096] = torch.ops.aten.mm.default(view_default_242, t_default_70);  view_default_242 = t_default_70 = None\n",
      "        view_default_243: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_70, [1, 136, 4096]);  mm_default_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant93 = self._param_constant93\n",
      "        t_default_71: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant93);  _param_constant93 = None\n",
      "        view_default_244: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_91, [136, 4096])\n",
      "        mm_default_71: f32[136, 4096] = torch.ops.aten.mm.default(view_default_244, t_default_71);  view_default_244 = t_default_71 = None\n",
      "        view_default_245: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_71, [1, 136, 4096]);  mm_default_71 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant94 = self._param_constant94\n",
      "        t_default_72: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant94);  _param_constant94 = None\n",
      "        view_default_246: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_91, [136, 4096]);  mul_tensor_91 = None\n",
      "        mm_default_72: f32[136, 4096] = torch.ops.aten.mm.default(view_default_246, t_default_72);  view_default_246 = t_default_72 = None\n",
      "        view_default_247: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_72, [1, 136, 4096]);  mm_default_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_248: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_243, [1, 136, 32, 128]);  view_default_243 = None\n",
      "        transpose_int_50: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_248, 1, 2);  view_default_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_249: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_245, [1, 136, 32, 128]);  view_default_245 = None\n",
      "        transpose_int_51: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_249, 1, 2);  view_default_249 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_250: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_247, [1, 136, 32, 128]);  view_default_247 = None\n",
      "        transpose_int_52: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_250, 1, 2);  view_default_250 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant20 = self._tensor_constant20\n",
      "        slice_tensor_104: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant20, 0, 0, 9223372036854775807);  _tensor_constant20 = None\n",
      "        slice_tensor_105: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_104, 1, 0, 9223372036854775807);  slice_tensor_104 = None\n",
      "        slice_tensor_106: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_105, 2, 0, 136);  slice_tensor_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant21 = self._tensor_constant21\n",
      "        slice_tensor_107: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant21, 0, 0, 9223372036854775807);  _tensor_constant21 = None\n",
      "        slice_tensor_108: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_107, 1, 0, 9223372036854775807);  slice_tensor_107 = None\n",
      "        slice_tensor_109: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_108, 2, 0, 136);  slice_tensor_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_40: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_106, 1);  slice_tensor_106 = None\n",
      "        squeeze_dim_41: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_40, 0);  squeeze_dim_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_42: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_109, 1);  slice_tensor_109 = None\n",
      "        squeeze_dim_43: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_42, 0);  squeeze_dim_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_20: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_41, [view_default]);  squeeze_dim_41 = None\n",
      "        unsqueeze_default_25: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_20, 1);  index_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_21: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_43, [view_default]);  squeeze_dim_43 = None\n",
      "        unsqueeze_default_26: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_21, 1);  index_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_92: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_50, unsqueeze_default_25)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_110: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_111: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 64, 9223372036854775807);  transpose_int_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_20: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_111);  slice_tensor_111 = None\n",
      "        cat_default_20: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_20, slice_tensor_110], -1);  neg_default_20 = slice_tensor_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_93: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_20, unsqueeze_default_26);  cat_default_20 = None\n",
      "        add_tensor_73: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_92, mul_tensor_93);  mul_tensor_92 = mul_tensor_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_94: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_51, unsqueeze_default_25);  unsqueeze_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_112: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_113: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 64, 9223372036854775807);  transpose_int_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_21: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_113);  slice_tensor_113 = None\n",
      "        cat_default_21: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_21, slice_tensor_112], -1);  neg_default_21 = slice_tensor_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_95: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_21, unsqueeze_default_26);  cat_default_21 = unsqueeze_default_26 = None\n",
      "        add_tensor_74: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_94, mul_tensor_95);  mul_tensor_94 = mul_tensor_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_53: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_74, 2, 3)\n",
      "        expand_default_42: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_73, [1, 32, 136, 128]);  add_tensor_73 = None\n",
      "        view_default_251: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_42, [32, 136, 128]);  expand_default_42 = None\n",
      "        expand_default_43: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_53, [1, 32, 128, 136]);  transpose_int_53 = None\n",
      "        view_default_252: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_43, [32, 128, 136]);  expand_default_43 = None\n",
      "        bmm_default_20: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_251, view_default_252);  view_default_251 = view_default_252 = None\n",
      "        view_default_253: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_20, [1, 32, 136, 136]);  bmm_default_20 = None\n",
      "        div_tensor_10: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_253, 11.313708498984761);  view_default_253 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_75: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_10, add_tensor_1);  div_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_10: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_75, -1, False);  add_tensor_75 = None\n",
      "        detach_default_31: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_10)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_44: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_10, [1, 32, 136, 136]);  _softmax_default_10 = None\n",
      "        view_default_254: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_44, [32, 136, 136]);  expand_default_44 = None\n",
      "        expand_default_45: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_52, [1, 32, 136, 128])\n",
      "        view_default_255: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_45, [32, 136, 128]);  expand_default_45 = None\n",
      "        bmm_default_21: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_254, view_default_255);  view_default_254 = view_default_255 = None\n",
      "        view_default_256: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_21, [1, 32, 136, 128]);  bmm_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_54: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_256, 1, 2);  view_default_256 = None\n",
      "        clone_default_10: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_54, memory_format = torch.contiguous_format);  transpose_int_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_257: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_10, [1, 136, 4096]);  clone_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant95 = self._param_constant95\n",
      "        t_default_73: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant95);  _param_constant95 = None\n",
      "        view_default_258: f32[136, 4096] = torch.ops.aten.view.default(view_default_257, [136, 4096]);  view_default_257 = None\n",
      "        mm_default_73: f32[136, 4096] = torch.ops.aten.mm.default(view_default_258, t_default_73);  view_default_258 = t_default_73 = None\n",
      "        view_default_259: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_73, [1, 136, 4096]);  mm_default_73 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_76: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_71, view_default_259);  add_tensor_71 = view_default_259 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_21: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_76, 2)\n",
      "        mean_dim_21: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_21, [-1], True);  pow_tensor_scalar_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_77: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_21, 1e-06);  mean_dim_21 = None\n",
      "        rsqrt_default_21: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_77);  add_tensor_77 = None\n",
      "        detach_default_32: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_21)\n",
      "        mul_tensor_96: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_76, rsqrt_default_21);  rsqrt_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant96 = self._param_constant96\n",
      "        mul_tensor_97: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant96, mul_tensor_96);  _param_constant96 = mul_tensor_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant97 = self._param_constant97\n",
      "        t_default_74: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant97);  _param_constant97 = None\n",
      "        view_default_260: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_97, [136, 4096])\n",
      "        mm_default_74: f32[136, 11008] = torch.ops.aten.mm.default(view_default_260, t_default_74);  view_default_260 = t_default_74 = None\n",
      "        view_default_261: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_74, [1, 136, 11008]);  mm_default_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_10: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_261);  view_default_261 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant98 = self._param_constant98\n",
      "        t_default_75: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant98);  _param_constant98 = None\n",
      "        view_default_262: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_97, [136, 4096]);  mul_tensor_97 = None\n",
      "        mm_default_75: f32[136, 11008] = torch.ops.aten.mm.default(view_default_262, t_default_75);  view_default_262 = t_default_75 = None\n",
      "        view_default_263: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_75, [1, 136, 11008]);  mm_default_75 = None\n",
      "        mul_tensor_98: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_10, view_default_263);  silu_default_10 = view_default_263 = None\n",
      "        _param_constant99 = self._param_constant99\n",
      "        t_default_76: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant99);  _param_constant99 = None\n",
      "        view_default_264: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_98, [136, 11008]);  mul_tensor_98 = None\n",
      "        mm_default_76: f32[136, 4096] = torch.ops.aten.mm.default(view_default_264, t_default_76);  view_default_264 = t_default_76 = None\n",
      "        view_default_265: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_76, [1, 136, 4096]);  mm_default_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_78: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_76, view_default_265);  add_tensor_76 = view_default_265 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_22: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_78, 2)\n",
      "        mean_dim_22: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_22, [-1], True);  pow_tensor_scalar_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_79: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_22, 1e-06);  mean_dim_22 = None\n",
      "        rsqrt_default_22: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_79);  add_tensor_79 = None\n",
      "        detach_default_33: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_22)\n",
      "        mul_tensor_99: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_78, rsqrt_default_22);  rsqrt_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant100 = self._param_constant100\n",
      "        mul_tensor_100: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant100, mul_tensor_99);  _param_constant100 = mul_tensor_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant101 = self._param_constant101\n",
      "        t_default_77: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant101);  _param_constant101 = None\n",
      "        view_default_266: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_100, [136, 4096])\n",
      "        mm_default_77: f32[136, 4096] = torch.ops.aten.mm.default(view_default_266, t_default_77);  view_default_266 = t_default_77 = None\n",
      "        view_default_267: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_77, [1, 136, 4096]);  mm_default_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant102 = self._param_constant102\n",
      "        t_default_78: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant102);  _param_constant102 = None\n",
      "        view_default_268: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_100, [136, 4096])\n",
      "        mm_default_78: f32[136, 4096] = torch.ops.aten.mm.default(view_default_268, t_default_78);  view_default_268 = t_default_78 = None\n",
      "        view_default_269: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_78, [1, 136, 4096]);  mm_default_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant103 = self._param_constant103\n",
      "        t_default_79: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant103);  _param_constant103 = None\n",
      "        view_default_270: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_100, [136, 4096]);  mul_tensor_100 = None\n",
      "        mm_default_79: f32[136, 4096] = torch.ops.aten.mm.default(view_default_270, t_default_79);  view_default_270 = t_default_79 = None\n",
      "        view_default_271: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_79, [1, 136, 4096]);  mm_default_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_272: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_267, [1, 136, 32, 128]);  view_default_267 = None\n",
      "        transpose_int_55: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_272, 1, 2);  view_default_272 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_273: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_269, [1, 136, 32, 128]);  view_default_269 = None\n",
      "        transpose_int_56: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_273, 1, 2);  view_default_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_274: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_271, [1, 136, 32, 128]);  view_default_271 = None\n",
      "        transpose_int_57: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_274, 1, 2);  view_default_274 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant22 = self._tensor_constant22\n",
      "        slice_tensor_114: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant22, 0, 0, 9223372036854775807);  _tensor_constant22 = None\n",
      "        slice_tensor_115: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_114, 1, 0, 9223372036854775807);  slice_tensor_114 = None\n",
      "        slice_tensor_116: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_115, 2, 0, 136);  slice_tensor_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant23 = self._tensor_constant23\n",
      "        slice_tensor_117: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant23, 0, 0, 9223372036854775807);  _tensor_constant23 = None\n",
      "        slice_tensor_118: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_117, 1, 0, 9223372036854775807);  slice_tensor_117 = None\n",
      "        slice_tensor_119: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_118, 2, 0, 136);  slice_tensor_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_44: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_116, 1);  slice_tensor_116 = None\n",
      "        squeeze_dim_45: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_44, 0);  squeeze_dim_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_46: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_119, 1);  slice_tensor_119 = None\n",
      "        squeeze_dim_47: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_46, 0);  squeeze_dim_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_22: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_45, [view_default]);  squeeze_dim_45 = None\n",
      "        unsqueeze_default_27: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_22, 1);  index_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_23: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_47, [view_default]);  squeeze_dim_47 = None\n",
      "        unsqueeze_default_28: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_23, 1);  index_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_101: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_55, unsqueeze_default_27)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_120: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_121: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 64, 9223372036854775807);  transpose_int_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_22: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_121);  slice_tensor_121 = None\n",
      "        cat_default_22: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_22, slice_tensor_120], -1);  neg_default_22 = slice_tensor_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_102: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_22, unsqueeze_default_28);  cat_default_22 = None\n",
      "        add_tensor_80: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_101, mul_tensor_102);  mul_tensor_101 = mul_tensor_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_103: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_56, unsqueeze_default_27);  unsqueeze_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_122: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_123: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 64, 9223372036854775807);  transpose_int_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_23: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_123);  slice_tensor_123 = None\n",
      "        cat_default_23: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_23, slice_tensor_122], -1);  neg_default_23 = slice_tensor_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_104: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_23, unsqueeze_default_28);  cat_default_23 = unsqueeze_default_28 = None\n",
      "        add_tensor_81: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_103, mul_tensor_104);  mul_tensor_103 = mul_tensor_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_58: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_81, 2, 3)\n",
      "        expand_default_46: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_80, [1, 32, 136, 128]);  add_tensor_80 = None\n",
      "        view_default_275: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_46, [32, 136, 128]);  expand_default_46 = None\n",
      "        expand_default_47: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_58, [1, 32, 128, 136]);  transpose_int_58 = None\n",
      "        view_default_276: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_47, [32, 128, 136]);  expand_default_47 = None\n",
      "        bmm_default_22: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_275, view_default_276);  view_default_275 = view_default_276 = None\n",
      "        view_default_277: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_22, [1, 32, 136, 136]);  bmm_default_22 = None\n",
      "        div_tensor_11: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_277, 11.313708498984761);  view_default_277 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_82: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_11, add_tensor_1);  div_tensor_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_11: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_82, -1, False);  add_tensor_82 = None\n",
      "        detach_default_34: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_11)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_48: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_11, [1, 32, 136, 136]);  _softmax_default_11 = None\n",
      "        view_default_278: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_48, [32, 136, 136]);  expand_default_48 = None\n",
      "        expand_default_49: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_57, [1, 32, 136, 128])\n",
      "        view_default_279: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_49, [32, 136, 128]);  expand_default_49 = None\n",
      "        bmm_default_23: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_278, view_default_279);  view_default_278 = view_default_279 = None\n",
      "        view_default_280: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_23, [1, 32, 136, 128]);  bmm_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_59: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_280, 1, 2);  view_default_280 = None\n",
      "        clone_default_11: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_59, memory_format = torch.contiguous_format);  transpose_int_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_281: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_11, [1, 136, 4096]);  clone_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant104 = self._param_constant104\n",
      "        t_default_80: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant104);  _param_constant104 = None\n",
      "        view_default_282: f32[136, 4096] = torch.ops.aten.view.default(view_default_281, [136, 4096]);  view_default_281 = None\n",
      "        mm_default_80: f32[136, 4096] = torch.ops.aten.mm.default(view_default_282, t_default_80);  view_default_282 = t_default_80 = None\n",
      "        view_default_283: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_80, [1, 136, 4096]);  mm_default_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_83: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_78, view_default_283);  add_tensor_78 = view_default_283 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_23: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_83, 2)\n",
      "        mean_dim_23: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_23, [-1], True);  pow_tensor_scalar_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_84: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_23, 1e-06);  mean_dim_23 = None\n",
      "        rsqrt_default_23: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_84);  add_tensor_84 = None\n",
      "        detach_default_35: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_23)\n",
      "        mul_tensor_105: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_83, rsqrt_default_23);  rsqrt_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant105 = self._param_constant105\n",
      "        mul_tensor_106: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant105, mul_tensor_105);  _param_constant105 = mul_tensor_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant106 = self._param_constant106\n",
      "        t_default_81: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant106);  _param_constant106 = None\n",
      "        view_default_284: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_106, [136, 4096])\n",
      "        mm_default_81: f32[136, 11008] = torch.ops.aten.mm.default(view_default_284, t_default_81);  view_default_284 = t_default_81 = None\n",
      "        view_default_285: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_81, [1, 136, 11008]);  mm_default_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_11: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_285);  view_default_285 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant107 = self._param_constant107\n",
      "        t_default_82: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant107);  _param_constant107 = None\n",
      "        view_default_286: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_106, [136, 4096]);  mul_tensor_106 = None\n",
      "        mm_default_82: f32[136, 11008] = torch.ops.aten.mm.default(view_default_286, t_default_82);  view_default_286 = t_default_82 = None\n",
      "        view_default_287: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_82, [1, 136, 11008]);  mm_default_82 = None\n",
      "        mul_tensor_107: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_11, view_default_287);  silu_default_11 = view_default_287 = None\n",
      "        _param_constant108 = self._param_constant108\n",
      "        t_default_83: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant108);  _param_constant108 = None\n",
      "        view_default_288: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_107, [136, 11008]);  mul_tensor_107 = None\n",
      "        mm_default_83: f32[136, 4096] = torch.ops.aten.mm.default(view_default_288, t_default_83);  view_default_288 = t_default_83 = None\n",
      "        view_default_289: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_83, [1, 136, 4096]);  mm_default_83 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_85: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_83, view_default_289);  add_tensor_83 = view_default_289 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_24: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_85, 2)\n",
      "        mean_dim_24: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_24, [-1], True);  pow_tensor_scalar_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_86: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_24, 1e-06);  mean_dim_24 = None\n",
      "        rsqrt_default_24: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_86);  add_tensor_86 = None\n",
      "        detach_default_36: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_24)\n",
      "        mul_tensor_108: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_85, rsqrt_default_24);  rsqrt_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant109 = self._param_constant109\n",
      "        mul_tensor_109: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant109, mul_tensor_108);  _param_constant109 = mul_tensor_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant110 = self._param_constant110\n",
      "        t_default_84: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant110);  _param_constant110 = None\n",
      "        view_default_290: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_109, [136, 4096])\n",
      "        mm_default_84: f32[136, 4096] = torch.ops.aten.mm.default(view_default_290, t_default_84);  view_default_290 = t_default_84 = None\n",
      "        view_default_291: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_84, [1, 136, 4096]);  mm_default_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant111 = self._param_constant111\n",
      "        t_default_85: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant111);  _param_constant111 = None\n",
      "        view_default_292: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_109, [136, 4096])\n",
      "        mm_default_85: f32[136, 4096] = torch.ops.aten.mm.default(view_default_292, t_default_85);  view_default_292 = t_default_85 = None\n",
      "        view_default_293: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_85, [1, 136, 4096]);  mm_default_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant112 = self._param_constant112\n",
      "        t_default_86: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant112);  _param_constant112 = None\n",
      "        view_default_294: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_109, [136, 4096]);  mul_tensor_109 = None\n",
      "        mm_default_86: f32[136, 4096] = torch.ops.aten.mm.default(view_default_294, t_default_86);  view_default_294 = t_default_86 = None\n",
      "        view_default_295: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_86, [1, 136, 4096]);  mm_default_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_296: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_291, [1, 136, 32, 128]);  view_default_291 = None\n",
      "        transpose_int_60: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_296, 1, 2);  view_default_296 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_297: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_293, [1, 136, 32, 128]);  view_default_293 = None\n",
      "        transpose_int_61: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_297, 1, 2);  view_default_297 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_298: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_295, [1, 136, 32, 128]);  view_default_295 = None\n",
      "        transpose_int_62: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_298, 1, 2);  view_default_298 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant24 = self._tensor_constant24\n",
      "        slice_tensor_124: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant24, 0, 0, 9223372036854775807);  _tensor_constant24 = None\n",
      "        slice_tensor_125: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_124, 1, 0, 9223372036854775807);  slice_tensor_124 = None\n",
      "        slice_tensor_126: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_125, 2, 0, 136);  slice_tensor_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant25 = self._tensor_constant25\n",
      "        slice_tensor_127: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant25, 0, 0, 9223372036854775807);  _tensor_constant25 = None\n",
      "        slice_tensor_128: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_127, 1, 0, 9223372036854775807);  slice_tensor_127 = None\n",
      "        slice_tensor_129: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_128, 2, 0, 136);  slice_tensor_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_48: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_126, 1);  slice_tensor_126 = None\n",
      "        squeeze_dim_49: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_48, 0);  squeeze_dim_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_50: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_129, 1);  slice_tensor_129 = None\n",
      "        squeeze_dim_51: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_50, 0);  squeeze_dim_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_24: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_49, [view_default]);  squeeze_dim_49 = None\n",
      "        unsqueeze_default_29: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_24, 1);  index_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_25: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_51, [view_default]);  squeeze_dim_51 = None\n",
      "        unsqueeze_default_30: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_25, 1);  index_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_110: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_60, unsqueeze_default_29)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_130: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_131: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 64, 9223372036854775807);  transpose_int_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_24: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_131);  slice_tensor_131 = None\n",
      "        cat_default_24: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_24, slice_tensor_130], -1);  neg_default_24 = slice_tensor_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_111: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_24, unsqueeze_default_30);  cat_default_24 = None\n",
      "        add_tensor_87: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_110, mul_tensor_111);  mul_tensor_110 = mul_tensor_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_112: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_61, unsqueeze_default_29);  unsqueeze_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_132: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_133: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 64, 9223372036854775807);  transpose_int_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_25: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_133);  slice_tensor_133 = None\n",
      "        cat_default_25: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_25, slice_tensor_132], -1);  neg_default_25 = slice_tensor_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_113: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_25, unsqueeze_default_30);  cat_default_25 = unsqueeze_default_30 = None\n",
      "        add_tensor_88: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_112, mul_tensor_113);  mul_tensor_112 = mul_tensor_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_63: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_88, 2, 3)\n",
      "        expand_default_50: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_87, [1, 32, 136, 128]);  add_tensor_87 = None\n",
      "        view_default_299: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_50, [32, 136, 128]);  expand_default_50 = None\n",
      "        expand_default_51: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_63, [1, 32, 128, 136]);  transpose_int_63 = None\n",
      "        view_default_300: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_51, [32, 128, 136]);  expand_default_51 = None\n",
      "        bmm_default_24: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_299, view_default_300);  view_default_299 = view_default_300 = None\n",
      "        view_default_301: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_24, [1, 32, 136, 136]);  bmm_default_24 = None\n",
      "        div_tensor_12: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_301, 11.313708498984761);  view_default_301 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_89: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_12, add_tensor_1);  div_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_12: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_89, -1, False);  add_tensor_89 = None\n",
      "        detach_default_37: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_12)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_52: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_12, [1, 32, 136, 136]);  _softmax_default_12 = None\n",
      "        view_default_302: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_52, [32, 136, 136]);  expand_default_52 = None\n",
      "        expand_default_53: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_62, [1, 32, 136, 128])\n",
      "        view_default_303: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_53, [32, 136, 128]);  expand_default_53 = None\n",
      "        bmm_default_25: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_302, view_default_303);  view_default_302 = view_default_303 = None\n",
      "        view_default_304: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_25, [1, 32, 136, 128]);  bmm_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_64: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_304, 1, 2);  view_default_304 = None\n",
      "        clone_default_12: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_64, memory_format = torch.contiguous_format);  transpose_int_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_305: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_12, [1, 136, 4096]);  clone_default_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant113 = self._param_constant113\n",
      "        t_default_87: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant113);  _param_constant113 = None\n",
      "        view_default_306: f32[136, 4096] = torch.ops.aten.view.default(view_default_305, [136, 4096]);  view_default_305 = None\n",
      "        mm_default_87: f32[136, 4096] = torch.ops.aten.mm.default(view_default_306, t_default_87);  view_default_306 = t_default_87 = None\n",
      "        view_default_307: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_87, [1, 136, 4096]);  mm_default_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_90: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_85, view_default_307);  add_tensor_85 = view_default_307 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_25: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_90, 2)\n",
      "        mean_dim_25: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_25, [-1], True);  pow_tensor_scalar_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_91: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_25, 1e-06);  mean_dim_25 = None\n",
      "        rsqrt_default_25: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_91);  add_tensor_91 = None\n",
      "        detach_default_38: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_25)\n",
      "        mul_tensor_114: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_90, rsqrt_default_25);  rsqrt_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant114 = self._param_constant114\n",
      "        mul_tensor_115: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant114, mul_tensor_114);  _param_constant114 = mul_tensor_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant115 = self._param_constant115\n",
      "        t_default_88: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant115);  _param_constant115 = None\n",
      "        view_default_308: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_115, [136, 4096])\n",
      "        mm_default_88: f32[136, 11008] = torch.ops.aten.mm.default(view_default_308, t_default_88);  view_default_308 = t_default_88 = None\n",
      "        view_default_309: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_88, [1, 136, 11008]);  mm_default_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_12: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_309);  view_default_309 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant116 = self._param_constant116\n",
      "        t_default_89: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant116);  _param_constant116 = None\n",
      "        view_default_310: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_115, [136, 4096]);  mul_tensor_115 = None\n",
      "        mm_default_89: f32[136, 11008] = torch.ops.aten.mm.default(view_default_310, t_default_89);  view_default_310 = t_default_89 = None\n",
      "        view_default_311: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_89, [1, 136, 11008]);  mm_default_89 = None\n",
      "        mul_tensor_116: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_12, view_default_311);  silu_default_12 = view_default_311 = None\n",
      "        _param_constant117 = self._param_constant117\n",
      "        t_default_90: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant117);  _param_constant117 = None\n",
      "        view_default_312: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_116, [136, 11008]);  mul_tensor_116 = None\n",
      "        mm_default_90: f32[136, 4096] = torch.ops.aten.mm.default(view_default_312, t_default_90);  view_default_312 = t_default_90 = None\n",
      "        view_default_313: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_90, [1, 136, 4096]);  mm_default_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_92: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_90, view_default_313);  add_tensor_90 = view_default_313 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_26: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_92, 2)\n",
      "        mean_dim_26: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_26, [-1], True);  pow_tensor_scalar_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_93: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_26, 1e-06);  mean_dim_26 = None\n",
      "        rsqrt_default_26: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_93);  add_tensor_93 = None\n",
      "        detach_default_39: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_26)\n",
      "        mul_tensor_117: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_92, rsqrt_default_26);  rsqrt_default_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant118 = self._param_constant118\n",
      "        mul_tensor_118: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant118, mul_tensor_117);  _param_constant118 = mul_tensor_117 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant119 = self._param_constant119\n",
      "        t_default_91: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant119);  _param_constant119 = None\n",
      "        view_default_314: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_118, [136, 4096])\n",
      "        mm_default_91: f32[136, 4096] = torch.ops.aten.mm.default(view_default_314, t_default_91);  view_default_314 = t_default_91 = None\n",
      "        view_default_315: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_91, [1, 136, 4096]);  mm_default_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant120 = self._param_constant120\n",
      "        t_default_92: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant120);  _param_constant120 = None\n",
      "        view_default_316: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_118, [136, 4096])\n",
      "        mm_default_92: f32[136, 4096] = torch.ops.aten.mm.default(view_default_316, t_default_92);  view_default_316 = t_default_92 = None\n",
      "        view_default_317: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_92, [1, 136, 4096]);  mm_default_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant121 = self._param_constant121\n",
      "        t_default_93: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant121);  _param_constant121 = None\n",
      "        view_default_318: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_118, [136, 4096]);  mul_tensor_118 = None\n",
      "        mm_default_93: f32[136, 4096] = torch.ops.aten.mm.default(view_default_318, t_default_93);  view_default_318 = t_default_93 = None\n",
      "        view_default_319: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_93, [1, 136, 4096]);  mm_default_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_320: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_315, [1, 136, 32, 128]);  view_default_315 = None\n",
      "        transpose_int_65: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_320, 1, 2);  view_default_320 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_321: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_317, [1, 136, 32, 128]);  view_default_317 = None\n",
      "        transpose_int_66: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_321, 1, 2);  view_default_321 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_322: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_319, [1, 136, 32, 128]);  view_default_319 = None\n",
      "        transpose_int_67: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_322, 1, 2);  view_default_322 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant26 = self._tensor_constant26\n",
      "        slice_tensor_134: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant26, 0, 0, 9223372036854775807);  _tensor_constant26 = None\n",
      "        slice_tensor_135: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_134, 1, 0, 9223372036854775807);  slice_tensor_134 = None\n",
      "        slice_tensor_136: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_135, 2, 0, 136);  slice_tensor_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant27 = self._tensor_constant27\n",
      "        slice_tensor_137: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant27, 0, 0, 9223372036854775807);  _tensor_constant27 = None\n",
      "        slice_tensor_138: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_137, 1, 0, 9223372036854775807);  slice_tensor_137 = None\n",
      "        slice_tensor_139: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_138, 2, 0, 136);  slice_tensor_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_52: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_136, 1);  slice_tensor_136 = None\n",
      "        squeeze_dim_53: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_52, 0);  squeeze_dim_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_54: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_139, 1);  slice_tensor_139 = None\n",
      "        squeeze_dim_55: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_54, 0);  squeeze_dim_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_26: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_53, [view_default]);  squeeze_dim_53 = None\n",
      "        unsqueeze_default_31: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_26, 1);  index_tensor_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_27: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_55, [view_default]);  squeeze_dim_55 = None\n",
      "        unsqueeze_default_32: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_27, 1);  index_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_119: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_65, unsqueeze_default_31)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_140: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_141: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 64, 9223372036854775807);  transpose_int_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_26: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_141);  slice_tensor_141 = None\n",
      "        cat_default_26: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_26, slice_tensor_140], -1);  neg_default_26 = slice_tensor_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_120: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_26, unsqueeze_default_32);  cat_default_26 = None\n",
      "        add_tensor_94: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_119, mul_tensor_120);  mul_tensor_119 = mul_tensor_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_121: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_66, unsqueeze_default_31);  unsqueeze_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_142: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_143: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 64, 9223372036854775807);  transpose_int_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_27: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_143);  slice_tensor_143 = None\n",
      "        cat_default_27: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_27, slice_tensor_142], -1);  neg_default_27 = slice_tensor_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_122: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_27, unsqueeze_default_32);  cat_default_27 = unsqueeze_default_32 = None\n",
      "        add_tensor_95: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_121, mul_tensor_122);  mul_tensor_121 = mul_tensor_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_68: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_95, 2, 3)\n",
      "        expand_default_54: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_94, [1, 32, 136, 128]);  add_tensor_94 = None\n",
      "        view_default_323: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_54, [32, 136, 128]);  expand_default_54 = None\n",
      "        expand_default_55: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_68, [1, 32, 128, 136]);  transpose_int_68 = None\n",
      "        view_default_324: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_55, [32, 128, 136]);  expand_default_55 = None\n",
      "        bmm_default_26: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_323, view_default_324);  view_default_323 = view_default_324 = None\n",
      "        view_default_325: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_26, [1, 32, 136, 136]);  bmm_default_26 = None\n",
      "        div_tensor_13: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_325, 11.313708498984761);  view_default_325 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_96: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_13, add_tensor_1);  div_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_13: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_96, -1, False);  add_tensor_96 = None\n",
      "        detach_default_40: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_13)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_56: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_13, [1, 32, 136, 136]);  _softmax_default_13 = None\n",
      "        view_default_326: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_56, [32, 136, 136]);  expand_default_56 = None\n",
      "        expand_default_57: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_67, [1, 32, 136, 128])\n",
      "        view_default_327: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_57, [32, 136, 128]);  expand_default_57 = None\n",
      "        bmm_default_27: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_326, view_default_327);  view_default_326 = view_default_327 = None\n",
      "        view_default_328: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_27, [1, 32, 136, 128]);  bmm_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_69: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_328, 1, 2);  view_default_328 = None\n",
      "        clone_default_13: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_69, memory_format = torch.contiguous_format);  transpose_int_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_329: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_13, [1, 136, 4096]);  clone_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant122 = self._param_constant122\n",
      "        t_default_94: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant122);  _param_constant122 = None\n",
      "        view_default_330: f32[136, 4096] = torch.ops.aten.view.default(view_default_329, [136, 4096]);  view_default_329 = None\n",
      "        mm_default_94: f32[136, 4096] = torch.ops.aten.mm.default(view_default_330, t_default_94);  view_default_330 = t_default_94 = None\n",
      "        view_default_331: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_94, [1, 136, 4096]);  mm_default_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_97: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_92, view_default_331);  add_tensor_92 = view_default_331 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_27: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_97, 2)\n",
      "        mean_dim_27: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_27, [-1], True);  pow_tensor_scalar_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_98: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_27, 1e-06);  mean_dim_27 = None\n",
      "        rsqrt_default_27: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_98);  add_tensor_98 = None\n",
      "        detach_default_41: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_27)\n",
      "        mul_tensor_123: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_97, rsqrt_default_27);  rsqrt_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant123 = self._param_constant123\n",
      "        mul_tensor_124: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant123, mul_tensor_123);  _param_constant123 = mul_tensor_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant124 = self._param_constant124\n",
      "        t_default_95: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant124);  _param_constant124 = None\n",
      "        view_default_332: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_124, [136, 4096])\n",
      "        mm_default_95: f32[136, 11008] = torch.ops.aten.mm.default(view_default_332, t_default_95);  view_default_332 = t_default_95 = None\n",
      "        view_default_333: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_95, [1, 136, 11008]);  mm_default_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_13: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_333);  view_default_333 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant125 = self._param_constant125\n",
      "        t_default_96: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant125);  _param_constant125 = None\n",
      "        view_default_334: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_124, [136, 4096]);  mul_tensor_124 = None\n",
      "        mm_default_96: f32[136, 11008] = torch.ops.aten.mm.default(view_default_334, t_default_96);  view_default_334 = t_default_96 = None\n",
      "        view_default_335: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_96, [1, 136, 11008]);  mm_default_96 = None\n",
      "        mul_tensor_125: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_13, view_default_335);  silu_default_13 = view_default_335 = None\n",
      "        _param_constant126 = self._param_constant126\n",
      "        t_default_97: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant126);  _param_constant126 = None\n",
      "        view_default_336: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_125, [136, 11008]);  mul_tensor_125 = None\n",
      "        mm_default_97: f32[136, 4096] = torch.ops.aten.mm.default(view_default_336, t_default_97);  view_default_336 = t_default_97 = None\n",
      "        view_default_337: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_97, [1, 136, 4096]);  mm_default_97 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_99: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_97, view_default_337);  add_tensor_97 = view_default_337 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_28: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_99, 2)\n",
      "        mean_dim_28: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_28, [-1], True);  pow_tensor_scalar_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_100: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_28, 1e-06);  mean_dim_28 = None\n",
      "        rsqrt_default_28: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_100);  add_tensor_100 = None\n",
      "        detach_default_42: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_28)\n",
      "        mul_tensor_126: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_99, rsqrt_default_28);  rsqrt_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant127 = self._param_constant127\n",
      "        mul_tensor_127: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant127, mul_tensor_126);  _param_constant127 = mul_tensor_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant128 = self._param_constant128\n",
      "        t_default_98: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant128);  _param_constant128 = None\n",
      "        view_default_338: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_127, [136, 4096])\n",
      "        mm_default_98: f32[136, 4096] = torch.ops.aten.mm.default(view_default_338, t_default_98);  view_default_338 = t_default_98 = None\n",
      "        view_default_339: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_98, [1, 136, 4096]);  mm_default_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant129 = self._param_constant129\n",
      "        t_default_99: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant129);  _param_constant129 = None\n",
      "        view_default_340: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_127, [136, 4096])\n",
      "        mm_default_99: f32[136, 4096] = torch.ops.aten.mm.default(view_default_340, t_default_99);  view_default_340 = t_default_99 = None\n",
      "        view_default_341: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_99, [1, 136, 4096]);  mm_default_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant130 = self._param_constant130\n",
      "        t_default_100: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant130);  _param_constant130 = None\n",
      "        view_default_342: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_127, [136, 4096]);  mul_tensor_127 = None\n",
      "        mm_default_100: f32[136, 4096] = torch.ops.aten.mm.default(view_default_342, t_default_100);  view_default_342 = t_default_100 = None\n",
      "        view_default_343: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_100, [1, 136, 4096]);  mm_default_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_344: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_339, [1, 136, 32, 128]);  view_default_339 = None\n",
      "        transpose_int_70: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_344, 1, 2);  view_default_344 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_345: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_341, [1, 136, 32, 128]);  view_default_341 = None\n",
      "        transpose_int_71: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_345, 1, 2);  view_default_345 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_346: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_343, [1, 136, 32, 128]);  view_default_343 = None\n",
      "        transpose_int_72: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_346, 1, 2);  view_default_346 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant28 = self._tensor_constant28\n",
      "        slice_tensor_144: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant28, 0, 0, 9223372036854775807);  _tensor_constant28 = None\n",
      "        slice_tensor_145: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_144, 1, 0, 9223372036854775807);  slice_tensor_144 = None\n",
      "        slice_tensor_146: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_145, 2, 0, 136);  slice_tensor_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant29 = self._tensor_constant29\n",
      "        slice_tensor_147: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant29, 0, 0, 9223372036854775807);  _tensor_constant29 = None\n",
      "        slice_tensor_148: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_147, 1, 0, 9223372036854775807);  slice_tensor_147 = None\n",
      "        slice_tensor_149: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_148, 2, 0, 136);  slice_tensor_148 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_56: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_146, 1);  slice_tensor_146 = None\n",
      "        squeeze_dim_57: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_56, 0);  squeeze_dim_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_58: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_149, 1);  slice_tensor_149 = None\n",
      "        squeeze_dim_59: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_58, 0);  squeeze_dim_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_28: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_57, [view_default]);  squeeze_dim_57 = None\n",
      "        unsqueeze_default_33: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_28, 1);  index_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_29: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_59, [view_default]);  squeeze_dim_59 = None\n",
      "        unsqueeze_default_34: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_29, 1);  index_tensor_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_128: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_70, unsqueeze_default_33)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_150: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_151: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 64, 9223372036854775807);  transpose_int_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_28: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_151);  slice_tensor_151 = None\n",
      "        cat_default_28: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_28, slice_tensor_150], -1);  neg_default_28 = slice_tensor_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_129: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_28, unsqueeze_default_34);  cat_default_28 = None\n",
      "        add_tensor_101: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_128, mul_tensor_129);  mul_tensor_128 = mul_tensor_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_130: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_71, unsqueeze_default_33);  unsqueeze_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_152: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_153: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 64, 9223372036854775807);  transpose_int_71 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_29: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_153);  slice_tensor_153 = None\n",
      "        cat_default_29: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_29, slice_tensor_152], -1);  neg_default_29 = slice_tensor_152 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_131: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_29, unsqueeze_default_34);  cat_default_29 = unsqueeze_default_34 = None\n",
      "        add_tensor_102: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_130, mul_tensor_131);  mul_tensor_130 = mul_tensor_131 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_73: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_102, 2, 3)\n",
      "        expand_default_58: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_101, [1, 32, 136, 128]);  add_tensor_101 = None\n",
      "        view_default_347: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_58, [32, 136, 128]);  expand_default_58 = None\n",
      "        expand_default_59: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_73, [1, 32, 128, 136]);  transpose_int_73 = None\n",
      "        view_default_348: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_59, [32, 128, 136]);  expand_default_59 = None\n",
      "        bmm_default_28: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_347, view_default_348);  view_default_347 = view_default_348 = None\n",
      "        view_default_349: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_28, [1, 32, 136, 136]);  bmm_default_28 = None\n",
      "        div_tensor_14: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_349, 11.313708498984761);  view_default_349 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_103: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_14, add_tensor_1);  div_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_14: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_103, -1, False);  add_tensor_103 = None\n",
      "        detach_default_43: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_14)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_60: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_14, [1, 32, 136, 136]);  _softmax_default_14 = None\n",
      "        view_default_350: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_60, [32, 136, 136]);  expand_default_60 = None\n",
      "        expand_default_61: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_72, [1, 32, 136, 128])\n",
      "        view_default_351: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_61, [32, 136, 128]);  expand_default_61 = None\n",
      "        bmm_default_29: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_350, view_default_351);  view_default_350 = view_default_351 = None\n",
      "        view_default_352: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_29, [1, 32, 136, 128]);  bmm_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_74: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_352, 1, 2);  view_default_352 = None\n",
      "        clone_default_14: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_74, memory_format = torch.contiguous_format);  transpose_int_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_353: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_14, [1, 136, 4096]);  clone_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant131 = self._param_constant131\n",
      "        t_default_101: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant131);  _param_constant131 = None\n",
      "        view_default_354: f32[136, 4096] = torch.ops.aten.view.default(view_default_353, [136, 4096]);  view_default_353 = None\n",
      "        mm_default_101: f32[136, 4096] = torch.ops.aten.mm.default(view_default_354, t_default_101);  view_default_354 = t_default_101 = None\n",
      "        view_default_355: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_101, [1, 136, 4096]);  mm_default_101 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_104: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_99, view_default_355);  add_tensor_99 = view_default_355 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_29: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_104, 2)\n",
      "        mean_dim_29: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_29, [-1], True);  pow_tensor_scalar_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_105: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_29, 1e-06);  mean_dim_29 = None\n",
      "        rsqrt_default_29: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_105);  add_tensor_105 = None\n",
      "        detach_default_44: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_29)\n",
      "        mul_tensor_132: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_104, rsqrt_default_29);  rsqrt_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant132 = self._param_constant132\n",
      "        mul_tensor_133: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant132, mul_tensor_132);  _param_constant132 = mul_tensor_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant133 = self._param_constant133\n",
      "        t_default_102: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant133);  _param_constant133 = None\n",
      "        view_default_356: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_133, [136, 4096])\n",
      "        mm_default_102: f32[136, 11008] = torch.ops.aten.mm.default(view_default_356, t_default_102);  view_default_356 = t_default_102 = None\n",
      "        view_default_357: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_102, [1, 136, 11008]);  mm_default_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_14: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_357);  view_default_357 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant134 = self._param_constant134\n",
      "        t_default_103: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant134);  _param_constant134 = None\n",
      "        view_default_358: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_133, [136, 4096]);  mul_tensor_133 = None\n",
      "        mm_default_103: f32[136, 11008] = torch.ops.aten.mm.default(view_default_358, t_default_103);  view_default_358 = t_default_103 = None\n",
      "        view_default_359: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_103, [1, 136, 11008]);  mm_default_103 = None\n",
      "        mul_tensor_134: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_14, view_default_359);  silu_default_14 = view_default_359 = None\n",
      "        _param_constant135 = self._param_constant135\n",
      "        t_default_104: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant135);  _param_constant135 = None\n",
      "        view_default_360: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_134, [136, 11008]);  mul_tensor_134 = None\n",
      "        mm_default_104: f32[136, 4096] = torch.ops.aten.mm.default(view_default_360, t_default_104);  view_default_360 = t_default_104 = None\n",
      "        view_default_361: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_104, [1, 136, 4096]);  mm_default_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_106: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_104, view_default_361);  add_tensor_104 = view_default_361 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_30: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_106, 2)\n",
      "        mean_dim_30: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_30, [-1], True);  pow_tensor_scalar_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_107: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_30, 1e-06);  mean_dim_30 = None\n",
      "        rsqrt_default_30: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_107);  add_tensor_107 = None\n",
      "        detach_default_45: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_30)\n",
      "        mul_tensor_135: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_106, rsqrt_default_30);  rsqrt_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant136 = self._param_constant136\n",
      "        mul_tensor_136: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant136, mul_tensor_135);  _param_constant136 = mul_tensor_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant137 = self._param_constant137\n",
      "        t_default_105: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant137);  _param_constant137 = None\n",
      "        view_default_362: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_136, [136, 4096])\n",
      "        mm_default_105: f32[136, 4096] = torch.ops.aten.mm.default(view_default_362, t_default_105);  view_default_362 = t_default_105 = None\n",
      "        view_default_363: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_105, [1, 136, 4096]);  mm_default_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant138 = self._param_constant138\n",
      "        t_default_106: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant138);  _param_constant138 = None\n",
      "        view_default_364: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_136, [136, 4096])\n",
      "        mm_default_106: f32[136, 4096] = torch.ops.aten.mm.default(view_default_364, t_default_106);  view_default_364 = t_default_106 = None\n",
      "        view_default_365: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_106, [1, 136, 4096]);  mm_default_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant139 = self._param_constant139\n",
      "        t_default_107: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant139);  _param_constant139 = None\n",
      "        view_default_366: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_136, [136, 4096]);  mul_tensor_136 = None\n",
      "        mm_default_107: f32[136, 4096] = torch.ops.aten.mm.default(view_default_366, t_default_107);  view_default_366 = t_default_107 = None\n",
      "        view_default_367: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_107, [1, 136, 4096]);  mm_default_107 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_368: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_363, [1, 136, 32, 128]);  view_default_363 = None\n",
      "        transpose_int_75: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_368, 1, 2);  view_default_368 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_369: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_365, [1, 136, 32, 128]);  view_default_365 = None\n",
      "        transpose_int_76: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_369, 1, 2);  view_default_369 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_370: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_367, [1, 136, 32, 128]);  view_default_367 = None\n",
      "        transpose_int_77: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_370, 1, 2);  view_default_370 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant30 = self._tensor_constant30\n",
      "        slice_tensor_154: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant30, 0, 0, 9223372036854775807);  _tensor_constant30 = None\n",
      "        slice_tensor_155: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_154, 1, 0, 9223372036854775807);  slice_tensor_154 = None\n",
      "        slice_tensor_156: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_155, 2, 0, 136);  slice_tensor_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant31 = self._tensor_constant31\n",
      "        slice_tensor_157: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant31, 0, 0, 9223372036854775807);  _tensor_constant31 = None\n",
      "        slice_tensor_158: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_157, 1, 0, 9223372036854775807);  slice_tensor_157 = None\n",
      "        slice_tensor_159: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_158, 2, 0, 136);  slice_tensor_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_60: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_156, 1);  slice_tensor_156 = None\n",
      "        squeeze_dim_61: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_60, 0);  squeeze_dim_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_62: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_159, 1);  slice_tensor_159 = None\n",
      "        squeeze_dim_63: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_62, 0);  squeeze_dim_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_30: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_61, [view_default]);  squeeze_dim_61 = None\n",
      "        unsqueeze_default_35: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_30, 1);  index_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_31: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_63, [view_default]);  squeeze_dim_63 = None\n",
      "        unsqueeze_default_36: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_31, 1);  index_tensor_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_137: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_75, unsqueeze_default_35)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_160: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_161: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 64, 9223372036854775807);  transpose_int_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_30: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_161);  slice_tensor_161 = None\n",
      "        cat_default_30: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_30, slice_tensor_160], -1);  neg_default_30 = slice_tensor_160 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_138: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_30, unsqueeze_default_36);  cat_default_30 = None\n",
      "        add_tensor_108: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_137, mul_tensor_138);  mul_tensor_137 = mul_tensor_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_139: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_76, unsqueeze_default_35);  unsqueeze_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_162: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_163: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 64, 9223372036854775807);  transpose_int_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_31: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_163);  slice_tensor_163 = None\n",
      "        cat_default_31: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_31, slice_tensor_162], -1);  neg_default_31 = slice_tensor_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_140: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_31, unsqueeze_default_36);  cat_default_31 = unsqueeze_default_36 = None\n",
      "        add_tensor_109: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_139, mul_tensor_140);  mul_tensor_139 = mul_tensor_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_78: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_109, 2, 3)\n",
      "        expand_default_62: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_108, [1, 32, 136, 128]);  add_tensor_108 = None\n",
      "        view_default_371: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_62, [32, 136, 128]);  expand_default_62 = None\n",
      "        expand_default_63: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_78, [1, 32, 128, 136]);  transpose_int_78 = None\n",
      "        view_default_372: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_63, [32, 128, 136]);  expand_default_63 = None\n",
      "        bmm_default_30: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_371, view_default_372);  view_default_371 = view_default_372 = None\n",
      "        view_default_373: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_30, [1, 32, 136, 136]);  bmm_default_30 = None\n",
      "        div_tensor_15: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_373, 11.313708498984761);  view_default_373 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_110: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_15, add_tensor_1);  div_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_15: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_110, -1, False);  add_tensor_110 = None\n",
      "        detach_default_46: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_15)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_64: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_15, [1, 32, 136, 136]);  _softmax_default_15 = None\n",
      "        view_default_374: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_64, [32, 136, 136]);  expand_default_64 = None\n",
      "        expand_default_65: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_77, [1, 32, 136, 128])\n",
      "        view_default_375: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_65, [32, 136, 128]);  expand_default_65 = None\n",
      "        bmm_default_31: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_374, view_default_375);  view_default_374 = view_default_375 = None\n",
      "        view_default_376: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_31, [1, 32, 136, 128]);  bmm_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_79: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_376, 1, 2);  view_default_376 = None\n",
      "        clone_default_15: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_79, memory_format = torch.contiguous_format);  transpose_int_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_377: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_15, [1, 136, 4096]);  clone_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant140 = self._param_constant140\n",
      "        t_default_108: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant140);  _param_constant140 = None\n",
      "        view_default_378: f32[136, 4096] = torch.ops.aten.view.default(view_default_377, [136, 4096]);  view_default_377 = None\n",
      "        mm_default_108: f32[136, 4096] = torch.ops.aten.mm.default(view_default_378, t_default_108);  view_default_378 = t_default_108 = None\n",
      "        view_default_379: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_108, [1, 136, 4096]);  mm_default_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_111: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_106, view_default_379);  add_tensor_106 = view_default_379 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_31: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_111, 2)\n",
      "        mean_dim_31: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_31, [-1], True);  pow_tensor_scalar_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_112: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_31, 1e-06);  mean_dim_31 = None\n",
      "        rsqrt_default_31: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_112);  add_tensor_112 = None\n",
      "        detach_default_47: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_31)\n",
      "        mul_tensor_141: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_111, rsqrt_default_31);  rsqrt_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant141 = self._param_constant141\n",
      "        mul_tensor_142: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant141, mul_tensor_141);  _param_constant141 = mul_tensor_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant142 = self._param_constant142\n",
      "        t_default_109: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant142);  _param_constant142 = None\n",
      "        view_default_380: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_142, [136, 4096])\n",
      "        mm_default_109: f32[136, 11008] = torch.ops.aten.mm.default(view_default_380, t_default_109);  view_default_380 = t_default_109 = None\n",
      "        view_default_381: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_109, [1, 136, 11008]);  mm_default_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_15: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_381);  view_default_381 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant143 = self._param_constant143\n",
      "        t_default_110: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant143);  _param_constant143 = None\n",
      "        view_default_382: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_142, [136, 4096]);  mul_tensor_142 = None\n",
      "        mm_default_110: f32[136, 11008] = torch.ops.aten.mm.default(view_default_382, t_default_110);  view_default_382 = t_default_110 = None\n",
      "        view_default_383: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_110, [1, 136, 11008]);  mm_default_110 = None\n",
      "        mul_tensor_143: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_15, view_default_383);  silu_default_15 = view_default_383 = None\n",
      "        _param_constant144 = self._param_constant144\n",
      "        t_default_111: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant144);  _param_constant144 = None\n",
      "        view_default_384: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_143, [136, 11008]);  mul_tensor_143 = None\n",
      "        mm_default_111: f32[136, 4096] = torch.ops.aten.mm.default(view_default_384, t_default_111);  view_default_384 = t_default_111 = None\n",
      "        view_default_385: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_111, [1, 136, 4096]);  mm_default_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_113: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_111, view_default_385);  add_tensor_111 = view_default_385 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_32: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_113, 2)\n",
      "        mean_dim_32: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_32, [-1], True);  pow_tensor_scalar_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_114: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_32, 1e-06);  mean_dim_32 = None\n",
      "        rsqrt_default_32: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_114);  add_tensor_114 = None\n",
      "        detach_default_48: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_32)\n",
      "        mul_tensor_144: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_113, rsqrt_default_32);  rsqrt_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant145 = self._param_constant145\n",
      "        mul_tensor_145: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant145, mul_tensor_144);  _param_constant145 = mul_tensor_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant146 = self._param_constant146\n",
      "        t_default_112: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant146);  _param_constant146 = None\n",
      "        view_default_386: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_145, [136, 4096])\n",
      "        mm_default_112: f32[136, 4096] = torch.ops.aten.mm.default(view_default_386, t_default_112);  view_default_386 = t_default_112 = None\n",
      "        view_default_387: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_112, [1, 136, 4096]);  mm_default_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant147 = self._param_constant147\n",
      "        t_default_113: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant147);  _param_constant147 = None\n",
      "        view_default_388: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_145, [136, 4096])\n",
      "        mm_default_113: f32[136, 4096] = torch.ops.aten.mm.default(view_default_388, t_default_113);  view_default_388 = t_default_113 = None\n",
      "        view_default_389: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_113, [1, 136, 4096]);  mm_default_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant148 = self._param_constant148\n",
      "        t_default_114: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant148);  _param_constant148 = None\n",
      "        view_default_390: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_145, [136, 4096]);  mul_tensor_145 = None\n",
      "        mm_default_114: f32[136, 4096] = torch.ops.aten.mm.default(view_default_390, t_default_114);  view_default_390 = t_default_114 = None\n",
      "        view_default_391: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_114, [1, 136, 4096]);  mm_default_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_392: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_387, [1, 136, 32, 128]);  view_default_387 = None\n",
      "        transpose_int_80: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_392, 1, 2);  view_default_392 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_393: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_389, [1, 136, 32, 128]);  view_default_389 = None\n",
      "        transpose_int_81: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_393, 1, 2);  view_default_393 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_394: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_391, [1, 136, 32, 128]);  view_default_391 = None\n",
      "        transpose_int_82: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_394, 1, 2);  view_default_394 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant32 = self._tensor_constant32\n",
      "        slice_tensor_164: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant32, 0, 0, 9223372036854775807);  _tensor_constant32 = None\n",
      "        slice_tensor_165: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_164, 1, 0, 9223372036854775807);  slice_tensor_164 = None\n",
      "        slice_tensor_166: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_165, 2, 0, 136);  slice_tensor_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant33 = self._tensor_constant33\n",
      "        slice_tensor_167: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant33, 0, 0, 9223372036854775807);  _tensor_constant33 = None\n",
      "        slice_tensor_168: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_167, 1, 0, 9223372036854775807);  slice_tensor_167 = None\n",
      "        slice_tensor_169: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_168, 2, 0, 136);  slice_tensor_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_64: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_166, 1);  slice_tensor_166 = None\n",
      "        squeeze_dim_65: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_64, 0);  squeeze_dim_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_66: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_169, 1);  slice_tensor_169 = None\n",
      "        squeeze_dim_67: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_66, 0);  squeeze_dim_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_32: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_65, [view_default]);  squeeze_dim_65 = None\n",
      "        unsqueeze_default_37: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_32, 1);  index_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_33: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_67, [view_default]);  squeeze_dim_67 = None\n",
      "        unsqueeze_default_38: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_33, 1);  index_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_146: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_80, unsqueeze_default_37)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_170: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_171: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 64, 9223372036854775807);  transpose_int_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_32: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_171);  slice_tensor_171 = None\n",
      "        cat_default_32: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_32, slice_tensor_170], -1);  neg_default_32 = slice_tensor_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_147: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_32, unsqueeze_default_38);  cat_default_32 = None\n",
      "        add_tensor_115: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_146, mul_tensor_147);  mul_tensor_146 = mul_tensor_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_148: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_81, unsqueeze_default_37);  unsqueeze_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_172: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_173: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 64, 9223372036854775807);  transpose_int_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_33: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_173);  slice_tensor_173 = None\n",
      "        cat_default_33: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_33, slice_tensor_172], -1);  neg_default_33 = slice_tensor_172 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_149: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_33, unsqueeze_default_38);  cat_default_33 = unsqueeze_default_38 = None\n",
      "        add_tensor_116: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_148, mul_tensor_149);  mul_tensor_148 = mul_tensor_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_83: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_116, 2, 3)\n",
      "        expand_default_66: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_115, [1, 32, 136, 128]);  add_tensor_115 = None\n",
      "        view_default_395: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_66, [32, 136, 128]);  expand_default_66 = None\n",
      "        expand_default_67: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_83, [1, 32, 128, 136]);  transpose_int_83 = None\n",
      "        view_default_396: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_67, [32, 128, 136]);  expand_default_67 = None\n",
      "        bmm_default_32: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_395, view_default_396);  view_default_395 = view_default_396 = None\n",
      "        view_default_397: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_32, [1, 32, 136, 136]);  bmm_default_32 = None\n",
      "        div_tensor_16: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_397, 11.313708498984761);  view_default_397 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_117: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_16, add_tensor_1);  div_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_16: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_117, -1, False);  add_tensor_117 = None\n",
      "        detach_default_49: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_16)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_68: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_16, [1, 32, 136, 136]);  _softmax_default_16 = None\n",
      "        view_default_398: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_68, [32, 136, 136]);  expand_default_68 = None\n",
      "        expand_default_69: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_82, [1, 32, 136, 128])\n",
      "        view_default_399: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_69, [32, 136, 128]);  expand_default_69 = None\n",
      "        bmm_default_33: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_398, view_default_399);  view_default_398 = view_default_399 = None\n",
      "        view_default_400: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_33, [1, 32, 136, 128]);  bmm_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_84: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_400, 1, 2);  view_default_400 = None\n",
      "        clone_default_16: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_84, memory_format = torch.contiguous_format);  transpose_int_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_401: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_16, [1, 136, 4096]);  clone_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant149 = self._param_constant149\n",
      "        t_default_115: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant149);  _param_constant149 = None\n",
      "        view_default_402: f32[136, 4096] = torch.ops.aten.view.default(view_default_401, [136, 4096]);  view_default_401 = None\n",
      "        mm_default_115: f32[136, 4096] = torch.ops.aten.mm.default(view_default_402, t_default_115);  view_default_402 = t_default_115 = None\n",
      "        view_default_403: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_115, [1, 136, 4096]);  mm_default_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_118: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_113, view_default_403);  add_tensor_113 = view_default_403 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_33: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_118, 2)\n",
      "        mean_dim_33: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_33, [-1], True);  pow_tensor_scalar_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_119: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_33, 1e-06);  mean_dim_33 = None\n",
      "        rsqrt_default_33: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_119);  add_tensor_119 = None\n",
      "        detach_default_50: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_33)\n",
      "        mul_tensor_150: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_118, rsqrt_default_33);  rsqrt_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant150 = self._param_constant150\n",
      "        mul_tensor_151: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant150, mul_tensor_150);  _param_constant150 = mul_tensor_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant151 = self._param_constant151\n",
      "        t_default_116: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant151);  _param_constant151 = None\n",
      "        view_default_404: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_151, [136, 4096])\n",
      "        mm_default_116: f32[136, 11008] = torch.ops.aten.mm.default(view_default_404, t_default_116);  view_default_404 = t_default_116 = None\n",
      "        view_default_405: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_116, [1, 136, 11008]);  mm_default_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_16: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_405);  view_default_405 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant152 = self._param_constant152\n",
      "        t_default_117: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant152);  _param_constant152 = None\n",
      "        view_default_406: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_151, [136, 4096]);  mul_tensor_151 = None\n",
      "        mm_default_117: f32[136, 11008] = torch.ops.aten.mm.default(view_default_406, t_default_117);  view_default_406 = t_default_117 = None\n",
      "        view_default_407: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_117, [1, 136, 11008]);  mm_default_117 = None\n",
      "        mul_tensor_152: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_16, view_default_407);  silu_default_16 = view_default_407 = None\n",
      "        _param_constant153 = self._param_constant153\n",
      "        t_default_118: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant153);  _param_constant153 = None\n",
      "        view_default_408: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_152, [136, 11008]);  mul_tensor_152 = None\n",
      "        mm_default_118: f32[136, 4096] = torch.ops.aten.mm.default(view_default_408, t_default_118);  view_default_408 = t_default_118 = None\n",
      "        view_default_409: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_118, [1, 136, 4096]);  mm_default_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_120: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_118, view_default_409);  add_tensor_118 = view_default_409 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_34: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_120, 2)\n",
      "        mean_dim_34: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_34, [-1], True);  pow_tensor_scalar_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_121: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_34, 1e-06);  mean_dim_34 = None\n",
      "        rsqrt_default_34: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_121);  add_tensor_121 = None\n",
      "        detach_default_51: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_34)\n",
      "        mul_tensor_153: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_120, rsqrt_default_34);  rsqrt_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant154 = self._param_constant154\n",
      "        mul_tensor_154: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant154, mul_tensor_153);  _param_constant154 = mul_tensor_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant155 = self._param_constant155\n",
      "        t_default_119: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant155);  _param_constant155 = None\n",
      "        view_default_410: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_154, [136, 4096])\n",
      "        mm_default_119: f32[136, 4096] = torch.ops.aten.mm.default(view_default_410, t_default_119);  view_default_410 = t_default_119 = None\n",
      "        view_default_411: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_119, [1, 136, 4096]);  mm_default_119 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant156 = self._param_constant156\n",
      "        t_default_120: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant156);  _param_constant156 = None\n",
      "        view_default_412: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_154, [136, 4096])\n",
      "        mm_default_120: f32[136, 4096] = torch.ops.aten.mm.default(view_default_412, t_default_120);  view_default_412 = t_default_120 = None\n",
      "        view_default_413: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_120, [1, 136, 4096]);  mm_default_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant157 = self._param_constant157\n",
      "        t_default_121: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant157);  _param_constant157 = None\n",
      "        view_default_414: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_154, [136, 4096]);  mul_tensor_154 = None\n",
      "        mm_default_121: f32[136, 4096] = torch.ops.aten.mm.default(view_default_414, t_default_121);  view_default_414 = t_default_121 = None\n",
      "        view_default_415: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_121, [1, 136, 4096]);  mm_default_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_416: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_411, [1, 136, 32, 128]);  view_default_411 = None\n",
      "        transpose_int_85: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_416, 1, 2);  view_default_416 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_417: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_413, [1, 136, 32, 128]);  view_default_413 = None\n",
      "        transpose_int_86: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_417, 1, 2);  view_default_417 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_418: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_415, [1, 136, 32, 128]);  view_default_415 = None\n",
      "        transpose_int_87: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_418, 1, 2);  view_default_418 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant34 = self._tensor_constant34\n",
      "        slice_tensor_174: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant34, 0, 0, 9223372036854775807);  _tensor_constant34 = None\n",
      "        slice_tensor_175: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_174, 1, 0, 9223372036854775807);  slice_tensor_174 = None\n",
      "        slice_tensor_176: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_175, 2, 0, 136);  slice_tensor_175 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant35 = self._tensor_constant35\n",
      "        slice_tensor_177: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant35, 0, 0, 9223372036854775807);  _tensor_constant35 = None\n",
      "        slice_tensor_178: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_177, 1, 0, 9223372036854775807);  slice_tensor_177 = None\n",
      "        slice_tensor_179: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_178, 2, 0, 136);  slice_tensor_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_68: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_176, 1);  slice_tensor_176 = None\n",
      "        squeeze_dim_69: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_68, 0);  squeeze_dim_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_70: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_179, 1);  slice_tensor_179 = None\n",
      "        squeeze_dim_71: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_70, 0);  squeeze_dim_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_34: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_69, [view_default]);  squeeze_dim_69 = None\n",
      "        unsqueeze_default_39: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_34, 1);  index_tensor_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_35: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_71, [view_default]);  squeeze_dim_71 = None\n",
      "        unsqueeze_default_40: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_35, 1);  index_tensor_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_155: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_85, unsqueeze_default_39)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_180: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_181: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 64, 9223372036854775807);  transpose_int_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_34: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_181);  slice_tensor_181 = None\n",
      "        cat_default_34: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_34, slice_tensor_180], -1);  neg_default_34 = slice_tensor_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_156: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_34, unsqueeze_default_40);  cat_default_34 = None\n",
      "        add_tensor_122: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_155, mul_tensor_156);  mul_tensor_155 = mul_tensor_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_157: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_86, unsqueeze_default_39);  unsqueeze_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_182: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_183: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 64, 9223372036854775807);  transpose_int_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_35: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_183);  slice_tensor_183 = None\n",
      "        cat_default_35: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_35, slice_tensor_182], -1);  neg_default_35 = slice_tensor_182 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_158: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_35, unsqueeze_default_40);  cat_default_35 = unsqueeze_default_40 = None\n",
      "        add_tensor_123: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_157, mul_tensor_158);  mul_tensor_157 = mul_tensor_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_88: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_123, 2, 3)\n",
      "        expand_default_70: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_122, [1, 32, 136, 128]);  add_tensor_122 = None\n",
      "        view_default_419: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_70, [32, 136, 128]);  expand_default_70 = None\n",
      "        expand_default_71: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_88, [1, 32, 128, 136]);  transpose_int_88 = None\n",
      "        view_default_420: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_71, [32, 128, 136]);  expand_default_71 = None\n",
      "        bmm_default_34: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_419, view_default_420);  view_default_419 = view_default_420 = None\n",
      "        view_default_421: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_34, [1, 32, 136, 136]);  bmm_default_34 = None\n",
      "        div_tensor_17: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_421, 11.313708498984761);  view_default_421 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_124: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_17, add_tensor_1);  div_tensor_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_17: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_124, -1, False);  add_tensor_124 = None\n",
      "        detach_default_52: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_17)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_72: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_17, [1, 32, 136, 136]);  _softmax_default_17 = None\n",
      "        view_default_422: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_72, [32, 136, 136]);  expand_default_72 = None\n",
      "        expand_default_73: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_87, [1, 32, 136, 128])\n",
      "        view_default_423: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_73, [32, 136, 128]);  expand_default_73 = None\n",
      "        bmm_default_35: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_422, view_default_423);  view_default_422 = view_default_423 = None\n",
      "        view_default_424: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_35, [1, 32, 136, 128]);  bmm_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_89: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_424, 1, 2);  view_default_424 = None\n",
      "        clone_default_17: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_89, memory_format = torch.contiguous_format);  transpose_int_89 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_425: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_17, [1, 136, 4096]);  clone_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant158 = self._param_constant158\n",
      "        t_default_122: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant158);  _param_constant158 = None\n",
      "        view_default_426: f32[136, 4096] = torch.ops.aten.view.default(view_default_425, [136, 4096]);  view_default_425 = None\n",
      "        mm_default_122: f32[136, 4096] = torch.ops.aten.mm.default(view_default_426, t_default_122);  view_default_426 = t_default_122 = None\n",
      "        view_default_427: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_122, [1, 136, 4096]);  mm_default_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_125: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_120, view_default_427);  add_tensor_120 = view_default_427 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_35: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_125, 2)\n",
      "        mean_dim_35: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_35, [-1], True);  pow_tensor_scalar_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_126: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_35, 1e-06);  mean_dim_35 = None\n",
      "        rsqrt_default_35: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_126);  add_tensor_126 = None\n",
      "        detach_default_53: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_35)\n",
      "        mul_tensor_159: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_125, rsqrt_default_35);  rsqrt_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant159 = self._param_constant159\n",
      "        mul_tensor_160: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant159, mul_tensor_159);  _param_constant159 = mul_tensor_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant160 = self._param_constant160\n",
      "        t_default_123: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant160);  _param_constant160 = None\n",
      "        view_default_428: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_160, [136, 4096])\n",
      "        mm_default_123: f32[136, 11008] = torch.ops.aten.mm.default(view_default_428, t_default_123);  view_default_428 = t_default_123 = None\n",
      "        view_default_429: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_123, [1, 136, 11008]);  mm_default_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_17: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_429);  view_default_429 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant161 = self._param_constant161\n",
      "        t_default_124: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant161);  _param_constant161 = None\n",
      "        view_default_430: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_160, [136, 4096]);  mul_tensor_160 = None\n",
      "        mm_default_124: f32[136, 11008] = torch.ops.aten.mm.default(view_default_430, t_default_124);  view_default_430 = t_default_124 = None\n",
      "        view_default_431: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_124, [1, 136, 11008]);  mm_default_124 = None\n",
      "        mul_tensor_161: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_17, view_default_431);  silu_default_17 = view_default_431 = None\n",
      "        _param_constant162 = self._param_constant162\n",
      "        t_default_125: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant162);  _param_constant162 = None\n",
      "        view_default_432: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_161, [136, 11008]);  mul_tensor_161 = None\n",
      "        mm_default_125: f32[136, 4096] = torch.ops.aten.mm.default(view_default_432, t_default_125);  view_default_432 = t_default_125 = None\n",
      "        view_default_433: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_125, [1, 136, 4096]);  mm_default_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_127: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_125, view_default_433);  add_tensor_125 = view_default_433 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_36: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_127, 2)\n",
      "        mean_dim_36: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_36, [-1], True);  pow_tensor_scalar_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_128: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_36, 1e-06);  mean_dim_36 = None\n",
      "        rsqrt_default_36: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_128);  add_tensor_128 = None\n",
      "        detach_default_54: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_36)\n",
      "        mul_tensor_162: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_127, rsqrt_default_36);  rsqrt_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant163 = self._param_constant163\n",
      "        mul_tensor_163: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant163, mul_tensor_162);  _param_constant163 = mul_tensor_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant164 = self._param_constant164\n",
      "        t_default_126: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant164);  _param_constant164 = None\n",
      "        view_default_434: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_163, [136, 4096])\n",
      "        mm_default_126: f32[136, 4096] = torch.ops.aten.mm.default(view_default_434, t_default_126);  view_default_434 = t_default_126 = None\n",
      "        view_default_435: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_126, [1, 136, 4096]);  mm_default_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant165 = self._param_constant165\n",
      "        t_default_127: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant165);  _param_constant165 = None\n",
      "        view_default_436: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_163, [136, 4096])\n",
      "        mm_default_127: f32[136, 4096] = torch.ops.aten.mm.default(view_default_436, t_default_127);  view_default_436 = t_default_127 = None\n",
      "        view_default_437: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_127, [1, 136, 4096]);  mm_default_127 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant166 = self._param_constant166\n",
      "        t_default_128: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant166);  _param_constant166 = None\n",
      "        view_default_438: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_163, [136, 4096]);  mul_tensor_163 = None\n",
      "        mm_default_128: f32[136, 4096] = torch.ops.aten.mm.default(view_default_438, t_default_128);  view_default_438 = t_default_128 = None\n",
      "        view_default_439: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_128, [1, 136, 4096]);  mm_default_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_440: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_435, [1, 136, 32, 128]);  view_default_435 = None\n",
      "        transpose_int_90: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_440, 1, 2);  view_default_440 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_441: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_437, [1, 136, 32, 128]);  view_default_437 = None\n",
      "        transpose_int_91: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_441, 1, 2);  view_default_441 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_442: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_439, [1, 136, 32, 128]);  view_default_439 = None\n",
      "        transpose_int_92: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_442, 1, 2);  view_default_442 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant36 = self._tensor_constant36\n",
      "        slice_tensor_184: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant36, 0, 0, 9223372036854775807);  _tensor_constant36 = None\n",
      "        slice_tensor_185: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_184, 1, 0, 9223372036854775807);  slice_tensor_184 = None\n",
      "        slice_tensor_186: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_185, 2, 0, 136);  slice_tensor_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant37 = self._tensor_constant37\n",
      "        slice_tensor_187: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant37, 0, 0, 9223372036854775807);  _tensor_constant37 = None\n",
      "        slice_tensor_188: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_187, 1, 0, 9223372036854775807);  slice_tensor_187 = None\n",
      "        slice_tensor_189: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_188, 2, 0, 136);  slice_tensor_188 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_72: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_186, 1);  slice_tensor_186 = None\n",
      "        squeeze_dim_73: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_72, 0);  squeeze_dim_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_74: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_189, 1);  slice_tensor_189 = None\n",
      "        squeeze_dim_75: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_74, 0);  squeeze_dim_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_36: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_73, [view_default]);  squeeze_dim_73 = None\n",
      "        unsqueeze_default_41: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_36, 1);  index_tensor_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_37: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_75, [view_default]);  squeeze_dim_75 = None\n",
      "        unsqueeze_default_42: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_37, 1);  index_tensor_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_164: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_90, unsqueeze_default_41)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_190: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_191: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 64, 9223372036854775807);  transpose_int_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_36: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_191);  slice_tensor_191 = None\n",
      "        cat_default_36: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_36, slice_tensor_190], -1);  neg_default_36 = slice_tensor_190 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_165: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_36, unsqueeze_default_42);  cat_default_36 = None\n",
      "        add_tensor_129: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_164, mul_tensor_165);  mul_tensor_164 = mul_tensor_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_166: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_91, unsqueeze_default_41);  unsqueeze_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_192: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_193: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 64, 9223372036854775807);  transpose_int_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_37: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_193);  slice_tensor_193 = None\n",
      "        cat_default_37: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_37, slice_tensor_192], -1);  neg_default_37 = slice_tensor_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_167: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_37, unsqueeze_default_42);  cat_default_37 = unsqueeze_default_42 = None\n",
      "        add_tensor_130: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_166, mul_tensor_167);  mul_tensor_166 = mul_tensor_167 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_93: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_130, 2, 3)\n",
      "        expand_default_74: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_129, [1, 32, 136, 128]);  add_tensor_129 = None\n",
      "        view_default_443: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_74, [32, 136, 128]);  expand_default_74 = None\n",
      "        expand_default_75: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_93, [1, 32, 128, 136]);  transpose_int_93 = None\n",
      "        view_default_444: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_75, [32, 128, 136]);  expand_default_75 = None\n",
      "        bmm_default_36: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_443, view_default_444);  view_default_443 = view_default_444 = None\n",
      "        view_default_445: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_36, [1, 32, 136, 136]);  bmm_default_36 = None\n",
      "        div_tensor_18: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_445, 11.313708498984761);  view_default_445 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_131: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_18, add_tensor_1);  div_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_18: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_131, -1, False);  add_tensor_131 = None\n",
      "        detach_default_55: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_18)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_76: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_18, [1, 32, 136, 136]);  _softmax_default_18 = None\n",
      "        view_default_446: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_76, [32, 136, 136]);  expand_default_76 = None\n",
      "        expand_default_77: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_92, [1, 32, 136, 128])\n",
      "        view_default_447: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_77, [32, 136, 128]);  expand_default_77 = None\n",
      "        bmm_default_37: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_446, view_default_447);  view_default_446 = view_default_447 = None\n",
      "        view_default_448: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_37, [1, 32, 136, 128]);  bmm_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_94: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_448, 1, 2);  view_default_448 = None\n",
      "        clone_default_18: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_94, memory_format = torch.contiguous_format);  transpose_int_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_449: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_18, [1, 136, 4096]);  clone_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant167 = self._param_constant167\n",
      "        t_default_129: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant167);  _param_constant167 = None\n",
      "        view_default_450: f32[136, 4096] = torch.ops.aten.view.default(view_default_449, [136, 4096]);  view_default_449 = None\n",
      "        mm_default_129: f32[136, 4096] = torch.ops.aten.mm.default(view_default_450, t_default_129);  view_default_450 = t_default_129 = None\n",
      "        view_default_451: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_129, [1, 136, 4096]);  mm_default_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_132: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_127, view_default_451);  add_tensor_127 = view_default_451 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_37: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_132, 2)\n",
      "        mean_dim_37: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_37, [-1], True);  pow_tensor_scalar_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_133: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_37, 1e-06);  mean_dim_37 = None\n",
      "        rsqrt_default_37: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_133);  add_tensor_133 = None\n",
      "        detach_default_56: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_37)\n",
      "        mul_tensor_168: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_132, rsqrt_default_37);  rsqrt_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant168 = self._param_constant168\n",
      "        mul_tensor_169: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant168, mul_tensor_168);  _param_constant168 = mul_tensor_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant169 = self._param_constant169\n",
      "        t_default_130: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant169);  _param_constant169 = None\n",
      "        view_default_452: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_169, [136, 4096])\n",
      "        mm_default_130: f32[136, 11008] = torch.ops.aten.mm.default(view_default_452, t_default_130);  view_default_452 = t_default_130 = None\n",
      "        view_default_453: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_130, [1, 136, 11008]);  mm_default_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_18: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_453);  view_default_453 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant170 = self._param_constant170\n",
      "        t_default_131: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant170);  _param_constant170 = None\n",
      "        view_default_454: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_169, [136, 4096]);  mul_tensor_169 = None\n",
      "        mm_default_131: f32[136, 11008] = torch.ops.aten.mm.default(view_default_454, t_default_131);  view_default_454 = t_default_131 = None\n",
      "        view_default_455: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_131, [1, 136, 11008]);  mm_default_131 = None\n",
      "        mul_tensor_170: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_18, view_default_455);  silu_default_18 = view_default_455 = None\n",
      "        _param_constant171 = self._param_constant171\n",
      "        t_default_132: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant171);  _param_constant171 = None\n",
      "        view_default_456: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_170, [136, 11008]);  mul_tensor_170 = None\n",
      "        mm_default_132: f32[136, 4096] = torch.ops.aten.mm.default(view_default_456, t_default_132);  view_default_456 = t_default_132 = None\n",
      "        view_default_457: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_132, [1, 136, 4096]);  mm_default_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_134: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_132, view_default_457);  add_tensor_132 = view_default_457 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_38: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_134, 2)\n",
      "        mean_dim_38: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_38, [-1], True);  pow_tensor_scalar_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_135: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_38, 1e-06);  mean_dim_38 = None\n",
      "        rsqrt_default_38: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_135);  add_tensor_135 = None\n",
      "        detach_default_57: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_38)\n",
      "        mul_tensor_171: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_134, rsqrt_default_38);  rsqrt_default_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant172 = self._param_constant172\n",
      "        mul_tensor_172: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant172, mul_tensor_171);  _param_constant172 = mul_tensor_171 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant173 = self._param_constant173\n",
      "        t_default_133: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant173);  _param_constant173 = None\n",
      "        view_default_458: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_172, [136, 4096])\n",
      "        mm_default_133: f32[136, 4096] = torch.ops.aten.mm.default(view_default_458, t_default_133);  view_default_458 = t_default_133 = None\n",
      "        view_default_459: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_133, [1, 136, 4096]);  mm_default_133 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant174 = self._param_constant174\n",
      "        t_default_134: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant174);  _param_constant174 = None\n",
      "        view_default_460: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_172, [136, 4096])\n",
      "        mm_default_134: f32[136, 4096] = torch.ops.aten.mm.default(view_default_460, t_default_134);  view_default_460 = t_default_134 = None\n",
      "        view_default_461: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_134, [1, 136, 4096]);  mm_default_134 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant175 = self._param_constant175\n",
      "        t_default_135: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant175);  _param_constant175 = None\n",
      "        view_default_462: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_172, [136, 4096]);  mul_tensor_172 = None\n",
      "        mm_default_135: f32[136, 4096] = torch.ops.aten.mm.default(view_default_462, t_default_135);  view_default_462 = t_default_135 = None\n",
      "        view_default_463: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_135, [1, 136, 4096]);  mm_default_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_464: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_459, [1, 136, 32, 128]);  view_default_459 = None\n",
      "        transpose_int_95: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_464, 1, 2);  view_default_464 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_465: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_461, [1, 136, 32, 128]);  view_default_461 = None\n",
      "        transpose_int_96: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_465, 1, 2);  view_default_465 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_466: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_463, [1, 136, 32, 128]);  view_default_463 = None\n",
      "        transpose_int_97: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_466, 1, 2);  view_default_466 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant38 = self._tensor_constant38\n",
      "        slice_tensor_194: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant38, 0, 0, 9223372036854775807);  _tensor_constant38 = None\n",
      "        slice_tensor_195: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_194, 1, 0, 9223372036854775807);  slice_tensor_194 = None\n",
      "        slice_tensor_196: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_195, 2, 0, 136);  slice_tensor_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant39 = self._tensor_constant39\n",
      "        slice_tensor_197: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant39, 0, 0, 9223372036854775807);  _tensor_constant39 = None\n",
      "        slice_tensor_198: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_197, 1, 0, 9223372036854775807);  slice_tensor_197 = None\n",
      "        slice_tensor_199: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_198, 2, 0, 136);  slice_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_76: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_196, 1);  slice_tensor_196 = None\n",
      "        squeeze_dim_77: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_76, 0);  squeeze_dim_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_78: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_199, 1);  slice_tensor_199 = None\n",
      "        squeeze_dim_79: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_78, 0);  squeeze_dim_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_38: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_77, [view_default]);  squeeze_dim_77 = None\n",
      "        unsqueeze_default_43: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_38, 1);  index_tensor_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_39: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_79, [view_default]);  squeeze_dim_79 = None\n",
      "        unsqueeze_default_44: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_39, 1);  index_tensor_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_173: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_95, unsqueeze_default_43)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_200: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_201: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 64, 9223372036854775807);  transpose_int_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_38: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_201);  slice_tensor_201 = None\n",
      "        cat_default_38: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_38, slice_tensor_200], -1);  neg_default_38 = slice_tensor_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_174: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_38, unsqueeze_default_44);  cat_default_38 = None\n",
      "        add_tensor_136: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_173, mul_tensor_174);  mul_tensor_173 = mul_tensor_174 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_175: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_96, unsqueeze_default_43);  unsqueeze_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_202: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_203: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 64, 9223372036854775807);  transpose_int_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_39: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_203);  slice_tensor_203 = None\n",
      "        cat_default_39: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_39, slice_tensor_202], -1);  neg_default_39 = slice_tensor_202 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_176: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_39, unsqueeze_default_44);  cat_default_39 = unsqueeze_default_44 = None\n",
      "        add_tensor_137: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_175, mul_tensor_176);  mul_tensor_175 = mul_tensor_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_98: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_137, 2, 3)\n",
      "        expand_default_78: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_136, [1, 32, 136, 128]);  add_tensor_136 = None\n",
      "        view_default_467: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_78, [32, 136, 128]);  expand_default_78 = None\n",
      "        expand_default_79: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_98, [1, 32, 128, 136]);  transpose_int_98 = None\n",
      "        view_default_468: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_79, [32, 128, 136]);  expand_default_79 = None\n",
      "        bmm_default_38: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_467, view_default_468);  view_default_467 = view_default_468 = None\n",
      "        view_default_469: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_38, [1, 32, 136, 136]);  bmm_default_38 = None\n",
      "        div_tensor_19: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_469, 11.313708498984761);  view_default_469 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_138: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_19, add_tensor_1);  div_tensor_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_19: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_138, -1, False);  add_tensor_138 = None\n",
      "        detach_default_58: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_19)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_80: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_19, [1, 32, 136, 136]);  _softmax_default_19 = None\n",
      "        view_default_470: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_80, [32, 136, 136]);  expand_default_80 = None\n",
      "        expand_default_81: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_97, [1, 32, 136, 128])\n",
      "        view_default_471: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_81, [32, 136, 128]);  expand_default_81 = None\n",
      "        bmm_default_39: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_470, view_default_471);  view_default_470 = view_default_471 = None\n",
      "        view_default_472: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_39, [1, 32, 136, 128]);  bmm_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_99: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_472, 1, 2);  view_default_472 = None\n",
      "        clone_default_19: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_99, memory_format = torch.contiguous_format);  transpose_int_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_473: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_19, [1, 136, 4096]);  clone_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant176 = self._param_constant176\n",
      "        t_default_136: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant176);  _param_constant176 = None\n",
      "        view_default_474: f32[136, 4096] = torch.ops.aten.view.default(view_default_473, [136, 4096]);  view_default_473 = None\n",
      "        mm_default_136: f32[136, 4096] = torch.ops.aten.mm.default(view_default_474, t_default_136);  view_default_474 = t_default_136 = None\n",
      "        view_default_475: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_136, [1, 136, 4096]);  mm_default_136 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_139: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_134, view_default_475);  add_tensor_134 = view_default_475 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_39: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_139, 2)\n",
      "        mean_dim_39: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_39, [-1], True);  pow_tensor_scalar_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_140: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_39, 1e-06);  mean_dim_39 = None\n",
      "        rsqrt_default_39: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_140);  add_tensor_140 = None\n",
      "        detach_default_59: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_39)\n",
      "        mul_tensor_177: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_139, rsqrt_default_39);  rsqrt_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant177 = self._param_constant177\n",
      "        mul_tensor_178: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant177, mul_tensor_177);  _param_constant177 = mul_tensor_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant178 = self._param_constant178\n",
      "        t_default_137: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant178);  _param_constant178 = None\n",
      "        view_default_476: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_178, [136, 4096])\n",
      "        mm_default_137: f32[136, 11008] = torch.ops.aten.mm.default(view_default_476, t_default_137);  view_default_476 = t_default_137 = None\n",
      "        view_default_477: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_137, [1, 136, 11008]);  mm_default_137 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_19: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_477);  view_default_477 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant179 = self._param_constant179\n",
      "        t_default_138: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant179);  _param_constant179 = None\n",
      "        view_default_478: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_178, [136, 4096]);  mul_tensor_178 = None\n",
      "        mm_default_138: f32[136, 11008] = torch.ops.aten.mm.default(view_default_478, t_default_138);  view_default_478 = t_default_138 = None\n",
      "        view_default_479: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_138, [1, 136, 11008]);  mm_default_138 = None\n",
      "        mul_tensor_179: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_19, view_default_479);  silu_default_19 = view_default_479 = None\n",
      "        _param_constant180 = self._param_constant180\n",
      "        t_default_139: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant180);  _param_constant180 = None\n",
      "        view_default_480: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_179, [136, 11008]);  mul_tensor_179 = None\n",
      "        mm_default_139: f32[136, 4096] = torch.ops.aten.mm.default(view_default_480, t_default_139);  view_default_480 = t_default_139 = None\n",
      "        view_default_481: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_139, [1, 136, 4096]);  mm_default_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_141: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_139, view_default_481);  add_tensor_139 = view_default_481 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_40: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_141, 2)\n",
      "        mean_dim_40: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_40, [-1], True);  pow_tensor_scalar_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_142: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_40, 1e-06);  mean_dim_40 = None\n",
      "        rsqrt_default_40: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_142);  add_tensor_142 = None\n",
      "        detach_default_60: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_40)\n",
      "        mul_tensor_180: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_141, rsqrt_default_40);  rsqrt_default_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant181 = self._param_constant181\n",
      "        mul_tensor_181: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant181, mul_tensor_180);  _param_constant181 = mul_tensor_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant182 = self._param_constant182\n",
      "        t_default_140: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant182);  _param_constant182 = None\n",
      "        view_default_482: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_181, [136, 4096])\n",
      "        mm_default_140: f32[136, 4096] = torch.ops.aten.mm.default(view_default_482, t_default_140);  view_default_482 = t_default_140 = None\n",
      "        view_default_483: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_140, [1, 136, 4096]);  mm_default_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant183 = self._param_constant183\n",
      "        t_default_141: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant183);  _param_constant183 = None\n",
      "        view_default_484: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_181, [136, 4096])\n",
      "        mm_default_141: f32[136, 4096] = torch.ops.aten.mm.default(view_default_484, t_default_141);  view_default_484 = t_default_141 = None\n",
      "        view_default_485: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_141, [1, 136, 4096]);  mm_default_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant184 = self._param_constant184\n",
      "        t_default_142: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant184);  _param_constant184 = None\n",
      "        view_default_486: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_181, [136, 4096]);  mul_tensor_181 = None\n",
      "        mm_default_142: f32[136, 4096] = torch.ops.aten.mm.default(view_default_486, t_default_142);  view_default_486 = t_default_142 = None\n",
      "        view_default_487: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_142, [1, 136, 4096]);  mm_default_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_488: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_483, [1, 136, 32, 128]);  view_default_483 = None\n",
      "        transpose_int_100: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_488, 1, 2);  view_default_488 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_489: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_485, [1, 136, 32, 128]);  view_default_485 = None\n",
      "        transpose_int_101: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_489, 1, 2);  view_default_489 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_490: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_487, [1, 136, 32, 128]);  view_default_487 = None\n",
      "        transpose_int_102: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_490, 1, 2);  view_default_490 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant40 = self._tensor_constant40\n",
      "        slice_tensor_204: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant40, 0, 0, 9223372036854775807);  _tensor_constant40 = None\n",
      "        slice_tensor_205: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_204, 1, 0, 9223372036854775807);  slice_tensor_204 = None\n",
      "        slice_tensor_206: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_205, 2, 0, 136);  slice_tensor_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant41 = self._tensor_constant41\n",
      "        slice_tensor_207: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant41, 0, 0, 9223372036854775807);  _tensor_constant41 = None\n",
      "        slice_tensor_208: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_207, 1, 0, 9223372036854775807);  slice_tensor_207 = None\n",
      "        slice_tensor_209: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_208, 2, 0, 136);  slice_tensor_208 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_80: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_206, 1);  slice_tensor_206 = None\n",
      "        squeeze_dim_81: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_80, 0);  squeeze_dim_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_82: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_209, 1);  slice_tensor_209 = None\n",
      "        squeeze_dim_83: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_82, 0);  squeeze_dim_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_40: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_81, [view_default]);  squeeze_dim_81 = None\n",
      "        unsqueeze_default_45: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_40, 1);  index_tensor_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_41: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_83, [view_default]);  squeeze_dim_83 = None\n",
      "        unsqueeze_default_46: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_41, 1);  index_tensor_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_182: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_100, unsqueeze_default_45)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_210: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_211: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 64, 9223372036854775807);  transpose_int_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_40: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_211);  slice_tensor_211 = None\n",
      "        cat_default_40: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_40, slice_tensor_210], -1);  neg_default_40 = slice_tensor_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_183: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_40, unsqueeze_default_46);  cat_default_40 = None\n",
      "        add_tensor_143: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_182, mul_tensor_183);  mul_tensor_182 = mul_tensor_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_184: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_101, unsqueeze_default_45);  unsqueeze_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_212: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_213: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 64, 9223372036854775807);  transpose_int_101 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_41: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_213);  slice_tensor_213 = None\n",
      "        cat_default_41: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_41, slice_tensor_212], -1);  neg_default_41 = slice_tensor_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_185: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_41, unsqueeze_default_46);  cat_default_41 = unsqueeze_default_46 = None\n",
      "        add_tensor_144: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_184, mul_tensor_185);  mul_tensor_184 = mul_tensor_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_103: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_144, 2, 3)\n",
      "        expand_default_82: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_143, [1, 32, 136, 128]);  add_tensor_143 = None\n",
      "        view_default_491: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_82, [32, 136, 128]);  expand_default_82 = None\n",
      "        expand_default_83: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_103, [1, 32, 128, 136]);  transpose_int_103 = None\n",
      "        view_default_492: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_83, [32, 128, 136]);  expand_default_83 = None\n",
      "        bmm_default_40: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_491, view_default_492);  view_default_491 = view_default_492 = None\n",
      "        view_default_493: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_40, [1, 32, 136, 136]);  bmm_default_40 = None\n",
      "        div_tensor_20: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_493, 11.313708498984761);  view_default_493 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_145: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_20, add_tensor_1);  div_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_20: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_145, -1, False);  add_tensor_145 = None\n",
      "        detach_default_61: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_20)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_84: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_20, [1, 32, 136, 136]);  _softmax_default_20 = None\n",
      "        view_default_494: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_84, [32, 136, 136]);  expand_default_84 = None\n",
      "        expand_default_85: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_102, [1, 32, 136, 128])\n",
      "        view_default_495: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_85, [32, 136, 128]);  expand_default_85 = None\n",
      "        bmm_default_41: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_494, view_default_495);  view_default_494 = view_default_495 = None\n",
      "        view_default_496: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_41, [1, 32, 136, 128]);  bmm_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_104: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_496, 1, 2);  view_default_496 = None\n",
      "        clone_default_20: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_104, memory_format = torch.contiguous_format);  transpose_int_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_497: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_20, [1, 136, 4096]);  clone_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant185 = self._param_constant185\n",
      "        t_default_143: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant185);  _param_constant185 = None\n",
      "        view_default_498: f32[136, 4096] = torch.ops.aten.view.default(view_default_497, [136, 4096]);  view_default_497 = None\n",
      "        mm_default_143: f32[136, 4096] = torch.ops.aten.mm.default(view_default_498, t_default_143);  view_default_498 = t_default_143 = None\n",
      "        view_default_499: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_143, [1, 136, 4096]);  mm_default_143 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_146: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_141, view_default_499);  add_tensor_141 = view_default_499 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_41: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_146, 2)\n",
      "        mean_dim_41: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_41, [-1], True);  pow_tensor_scalar_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_147: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_41, 1e-06);  mean_dim_41 = None\n",
      "        rsqrt_default_41: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_147);  add_tensor_147 = None\n",
      "        detach_default_62: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_41)\n",
      "        mul_tensor_186: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_146, rsqrt_default_41);  rsqrt_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant186 = self._param_constant186\n",
      "        mul_tensor_187: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant186, mul_tensor_186);  _param_constant186 = mul_tensor_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant187 = self._param_constant187\n",
      "        t_default_144: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant187);  _param_constant187 = None\n",
      "        view_default_500: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_187, [136, 4096])\n",
      "        mm_default_144: f32[136, 11008] = torch.ops.aten.mm.default(view_default_500, t_default_144);  view_default_500 = t_default_144 = None\n",
      "        view_default_501: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_144, [1, 136, 11008]);  mm_default_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_20: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_501);  view_default_501 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant188 = self._param_constant188\n",
      "        t_default_145: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant188);  _param_constant188 = None\n",
      "        view_default_502: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_187, [136, 4096]);  mul_tensor_187 = None\n",
      "        mm_default_145: f32[136, 11008] = torch.ops.aten.mm.default(view_default_502, t_default_145);  view_default_502 = t_default_145 = None\n",
      "        view_default_503: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_145, [1, 136, 11008]);  mm_default_145 = None\n",
      "        mul_tensor_188: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_20, view_default_503);  silu_default_20 = view_default_503 = None\n",
      "        _param_constant189 = self._param_constant189\n",
      "        t_default_146: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant189);  _param_constant189 = None\n",
      "        view_default_504: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_188, [136, 11008]);  mul_tensor_188 = None\n",
      "        mm_default_146: f32[136, 4096] = torch.ops.aten.mm.default(view_default_504, t_default_146);  view_default_504 = t_default_146 = None\n",
      "        view_default_505: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_146, [1, 136, 4096]);  mm_default_146 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_148: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_146, view_default_505);  add_tensor_146 = view_default_505 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_42: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_148, 2)\n",
      "        mean_dim_42: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_42, [-1], True);  pow_tensor_scalar_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_149: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_42, 1e-06);  mean_dim_42 = None\n",
      "        rsqrt_default_42: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_149);  add_tensor_149 = None\n",
      "        detach_default_63: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_42)\n",
      "        mul_tensor_189: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_148, rsqrt_default_42);  rsqrt_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant190 = self._param_constant190\n",
      "        mul_tensor_190: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant190, mul_tensor_189);  _param_constant190 = mul_tensor_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant191 = self._param_constant191\n",
      "        t_default_147: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant191);  _param_constant191 = None\n",
      "        view_default_506: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_190, [136, 4096])\n",
      "        mm_default_147: f32[136, 4096] = torch.ops.aten.mm.default(view_default_506, t_default_147);  view_default_506 = t_default_147 = None\n",
      "        view_default_507: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_147, [1, 136, 4096]);  mm_default_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant192 = self._param_constant192\n",
      "        t_default_148: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant192);  _param_constant192 = None\n",
      "        view_default_508: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_190, [136, 4096])\n",
      "        mm_default_148: f32[136, 4096] = torch.ops.aten.mm.default(view_default_508, t_default_148);  view_default_508 = t_default_148 = None\n",
      "        view_default_509: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_148, [1, 136, 4096]);  mm_default_148 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant193 = self._param_constant193\n",
      "        t_default_149: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant193);  _param_constant193 = None\n",
      "        view_default_510: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_190, [136, 4096]);  mul_tensor_190 = None\n",
      "        mm_default_149: f32[136, 4096] = torch.ops.aten.mm.default(view_default_510, t_default_149);  view_default_510 = t_default_149 = None\n",
      "        view_default_511: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_149, [1, 136, 4096]);  mm_default_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_512: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_507, [1, 136, 32, 128]);  view_default_507 = None\n",
      "        transpose_int_105: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_512, 1, 2);  view_default_512 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_513: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_509, [1, 136, 32, 128]);  view_default_509 = None\n",
      "        transpose_int_106: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_513, 1, 2);  view_default_513 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_514: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_511, [1, 136, 32, 128]);  view_default_511 = None\n",
      "        transpose_int_107: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_514, 1, 2);  view_default_514 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant42 = self._tensor_constant42\n",
      "        slice_tensor_214: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant42, 0, 0, 9223372036854775807);  _tensor_constant42 = None\n",
      "        slice_tensor_215: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_214, 1, 0, 9223372036854775807);  slice_tensor_214 = None\n",
      "        slice_tensor_216: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_215, 2, 0, 136);  slice_tensor_215 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant43 = self._tensor_constant43\n",
      "        slice_tensor_217: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant43, 0, 0, 9223372036854775807);  _tensor_constant43 = None\n",
      "        slice_tensor_218: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_217, 1, 0, 9223372036854775807);  slice_tensor_217 = None\n",
      "        slice_tensor_219: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_218, 2, 0, 136);  slice_tensor_218 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_84: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_216, 1);  slice_tensor_216 = None\n",
      "        squeeze_dim_85: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_84, 0);  squeeze_dim_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_86: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_219, 1);  slice_tensor_219 = None\n",
      "        squeeze_dim_87: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_86, 0);  squeeze_dim_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_42: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_85, [view_default]);  squeeze_dim_85 = None\n",
      "        unsqueeze_default_47: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_42, 1);  index_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_43: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_87, [view_default]);  squeeze_dim_87 = None\n",
      "        unsqueeze_default_48: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_43, 1);  index_tensor_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_191: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_105, unsqueeze_default_47)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_220: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_221: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 64, 9223372036854775807);  transpose_int_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_42: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_221);  slice_tensor_221 = None\n",
      "        cat_default_42: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_42, slice_tensor_220], -1);  neg_default_42 = slice_tensor_220 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_192: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_42, unsqueeze_default_48);  cat_default_42 = None\n",
      "        add_tensor_150: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_191, mul_tensor_192);  mul_tensor_191 = mul_tensor_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_193: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_106, unsqueeze_default_47);  unsqueeze_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_222: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_223: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 64, 9223372036854775807);  transpose_int_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_43: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_223);  slice_tensor_223 = None\n",
      "        cat_default_43: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_43, slice_tensor_222], -1);  neg_default_43 = slice_tensor_222 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_194: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_43, unsqueeze_default_48);  cat_default_43 = unsqueeze_default_48 = None\n",
      "        add_tensor_151: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_193, mul_tensor_194);  mul_tensor_193 = mul_tensor_194 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_108: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_151, 2, 3)\n",
      "        expand_default_86: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_150, [1, 32, 136, 128]);  add_tensor_150 = None\n",
      "        view_default_515: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_86, [32, 136, 128]);  expand_default_86 = None\n",
      "        expand_default_87: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_108, [1, 32, 128, 136]);  transpose_int_108 = None\n",
      "        view_default_516: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_87, [32, 128, 136]);  expand_default_87 = None\n",
      "        bmm_default_42: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_515, view_default_516);  view_default_515 = view_default_516 = None\n",
      "        view_default_517: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_42, [1, 32, 136, 136]);  bmm_default_42 = None\n",
      "        div_tensor_21: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_517, 11.313708498984761);  view_default_517 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_152: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_21, add_tensor_1);  div_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_21: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_152, -1, False);  add_tensor_152 = None\n",
      "        detach_default_64: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_88: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_21, [1, 32, 136, 136]);  _softmax_default_21 = None\n",
      "        view_default_518: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_88, [32, 136, 136]);  expand_default_88 = None\n",
      "        expand_default_89: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_107, [1, 32, 136, 128])\n",
      "        view_default_519: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_89, [32, 136, 128]);  expand_default_89 = None\n",
      "        bmm_default_43: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_518, view_default_519);  view_default_518 = view_default_519 = None\n",
      "        view_default_520: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_43, [1, 32, 136, 128]);  bmm_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_109: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_520, 1, 2);  view_default_520 = None\n",
      "        clone_default_21: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_109, memory_format = torch.contiguous_format);  transpose_int_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_521: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_21, [1, 136, 4096]);  clone_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant194 = self._param_constant194\n",
      "        t_default_150: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant194);  _param_constant194 = None\n",
      "        view_default_522: f32[136, 4096] = torch.ops.aten.view.default(view_default_521, [136, 4096]);  view_default_521 = None\n",
      "        mm_default_150: f32[136, 4096] = torch.ops.aten.mm.default(view_default_522, t_default_150);  view_default_522 = t_default_150 = None\n",
      "        view_default_523: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_150, [1, 136, 4096]);  mm_default_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_153: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_148, view_default_523);  add_tensor_148 = view_default_523 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_43: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_153, 2)\n",
      "        mean_dim_43: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_43, [-1], True);  pow_tensor_scalar_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_154: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_43, 1e-06);  mean_dim_43 = None\n",
      "        rsqrt_default_43: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_154);  add_tensor_154 = None\n",
      "        detach_default_65: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_43)\n",
      "        mul_tensor_195: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_153, rsqrt_default_43);  rsqrt_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant195 = self._param_constant195\n",
      "        mul_tensor_196: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant195, mul_tensor_195);  _param_constant195 = mul_tensor_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant196 = self._param_constant196\n",
      "        t_default_151: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant196);  _param_constant196 = None\n",
      "        view_default_524: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_196, [136, 4096])\n",
      "        mm_default_151: f32[136, 11008] = torch.ops.aten.mm.default(view_default_524, t_default_151);  view_default_524 = t_default_151 = None\n",
      "        view_default_525: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_151, [1, 136, 11008]);  mm_default_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_21: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_525);  view_default_525 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant197 = self._param_constant197\n",
      "        t_default_152: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant197);  _param_constant197 = None\n",
      "        view_default_526: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_196, [136, 4096]);  mul_tensor_196 = None\n",
      "        mm_default_152: f32[136, 11008] = torch.ops.aten.mm.default(view_default_526, t_default_152);  view_default_526 = t_default_152 = None\n",
      "        view_default_527: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_152, [1, 136, 11008]);  mm_default_152 = None\n",
      "        mul_tensor_197: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_21, view_default_527);  silu_default_21 = view_default_527 = None\n",
      "        _param_constant198 = self._param_constant198\n",
      "        t_default_153: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant198);  _param_constant198 = None\n",
      "        view_default_528: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_197, [136, 11008]);  mul_tensor_197 = None\n",
      "        mm_default_153: f32[136, 4096] = torch.ops.aten.mm.default(view_default_528, t_default_153);  view_default_528 = t_default_153 = None\n",
      "        view_default_529: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_153, [1, 136, 4096]);  mm_default_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_155: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_153, view_default_529);  add_tensor_153 = view_default_529 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_44: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_155, 2)\n",
      "        mean_dim_44: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_44, [-1], True);  pow_tensor_scalar_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_156: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_44, 1e-06);  mean_dim_44 = None\n",
      "        rsqrt_default_44: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_156);  add_tensor_156 = None\n",
      "        detach_default_66: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_44)\n",
      "        mul_tensor_198: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_155, rsqrt_default_44);  rsqrt_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant199 = self._param_constant199\n",
      "        mul_tensor_199: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant199, mul_tensor_198);  _param_constant199 = mul_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant200 = self._param_constant200\n",
      "        t_default_154: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant200);  _param_constant200 = None\n",
      "        view_default_530: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_199, [136, 4096])\n",
      "        mm_default_154: f32[136, 4096] = torch.ops.aten.mm.default(view_default_530, t_default_154);  view_default_530 = t_default_154 = None\n",
      "        view_default_531: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_154, [1, 136, 4096]);  mm_default_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant201 = self._param_constant201\n",
      "        t_default_155: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant201);  _param_constant201 = None\n",
      "        view_default_532: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_199, [136, 4096])\n",
      "        mm_default_155: f32[136, 4096] = torch.ops.aten.mm.default(view_default_532, t_default_155);  view_default_532 = t_default_155 = None\n",
      "        view_default_533: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_155, [1, 136, 4096]);  mm_default_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant202 = self._param_constant202\n",
      "        t_default_156: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant202);  _param_constant202 = None\n",
      "        view_default_534: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_199, [136, 4096]);  mul_tensor_199 = None\n",
      "        mm_default_156: f32[136, 4096] = torch.ops.aten.mm.default(view_default_534, t_default_156);  view_default_534 = t_default_156 = None\n",
      "        view_default_535: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_156, [1, 136, 4096]);  mm_default_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_536: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_531, [1, 136, 32, 128]);  view_default_531 = None\n",
      "        transpose_int_110: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_536, 1, 2);  view_default_536 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_537: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_533, [1, 136, 32, 128]);  view_default_533 = None\n",
      "        transpose_int_111: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_537, 1, 2);  view_default_537 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_538: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_535, [1, 136, 32, 128]);  view_default_535 = None\n",
      "        transpose_int_112: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_538, 1, 2);  view_default_538 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant44 = self._tensor_constant44\n",
      "        slice_tensor_224: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant44, 0, 0, 9223372036854775807);  _tensor_constant44 = None\n",
      "        slice_tensor_225: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_224, 1, 0, 9223372036854775807);  slice_tensor_224 = None\n",
      "        slice_tensor_226: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_225, 2, 0, 136);  slice_tensor_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant45 = self._tensor_constant45\n",
      "        slice_tensor_227: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant45, 0, 0, 9223372036854775807);  _tensor_constant45 = None\n",
      "        slice_tensor_228: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_227, 1, 0, 9223372036854775807);  slice_tensor_227 = None\n",
      "        slice_tensor_229: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_228, 2, 0, 136);  slice_tensor_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_88: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_226, 1);  slice_tensor_226 = None\n",
      "        squeeze_dim_89: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_88, 0);  squeeze_dim_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_90: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_229, 1);  slice_tensor_229 = None\n",
      "        squeeze_dim_91: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_90, 0);  squeeze_dim_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_44: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_89, [view_default]);  squeeze_dim_89 = None\n",
      "        unsqueeze_default_49: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_44, 1);  index_tensor_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_45: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_91, [view_default]);  squeeze_dim_91 = None\n",
      "        unsqueeze_default_50: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_45, 1);  index_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_200: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_110, unsqueeze_default_49)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_230: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_231: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 64, 9223372036854775807);  transpose_int_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_44: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_231);  slice_tensor_231 = None\n",
      "        cat_default_44: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_44, slice_tensor_230], -1);  neg_default_44 = slice_tensor_230 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_201: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_44, unsqueeze_default_50);  cat_default_44 = None\n",
      "        add_tensor_157: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_200, mul_tensor_201);  mul_tensor_200 = mul_tensor_201 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_202: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_111, unsqueeze_default_49);  unsqueeze_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_232: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_233: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 64, 9223372036854775807);  transpose_int_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_45: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_233);  slice_tensor_233 = None\n",
      "        cat_default_45: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_45, slice_tensor_232], -1);  neg_default_45 = slice_tensor_232 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_203: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_45, unsqueeze_default_50);  cat_default_45 = unsqueeze_default_50 = None\n",
      "        add_tensor_158: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_202, mul_tensor_203);  mul_tensor_202 = mul_tensor_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_113: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_158, 2, 3)\n",
      "        expand_default_90: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_157, [1, 32, 136, 128]);  add_tensor_157 = None\n",
      "        view_default_539: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_90, [32, 136, 128]);  expand_default_90 = None\n",
      "        expand_default_91: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_113, [1, 32, 128, 136]);  transpose_int_113 = None\n",
      "        view_default_540: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_91, [32, 128, 136]);  expand_default_91 = None\n",
      "        bmm_default_44: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_539, view_default_540);  view_default_539 = view_default_540 = None\n",
      "        view_default_541: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_44, [1, 32, 136, 136]);  bmm_default_44 = None\n",
      "        div_tensor_22: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_541, 11.313708498984761);  view_default_541 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_159: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_22, add_tensor_1);  div_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_22: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_159, -1, False);  add_tensor_159 = None\n",
      "        detach_default_67: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_22)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_92: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_22, [1, 32, 136, 136]);  _softmax_default_22 = None\n",
      "        view_default_542: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_92, [32, 136, 136]);  expand_default_92 = None\n",
      "        expand_default_93: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_112, [1, 32, 136, 128])\n",
      "        view_default_543: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_93, [32, 136, 128]);  expand_default_93 = None\n",
      "        bmm_default_45: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_542, view_default_543);  view_default_542 = view_default_543 = None\n",
      "        view_default_544: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_45, [1, 32, 136, 128]);  bmm_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_114: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_544, 1, 2);  view_default_544 = None\n",
      "        clone_default_22: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_114, memory_format = torch.contiguous_format);  transpose_int_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_545: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_22, [1, 136, 4096]);  clone_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant203 = self._param_constant203\n",
      "        t_default_157: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant203);  _param_constant203 = None\n",
      "        view_default_546: f32[136, 4096] = torch.ops.aten.view.default(view_default_545, [136, 4096]);  view_default_545 = None\n",
      "        mm_default_157: f32[136, 4096] = torch.ops.aten.mm.default(view_default_546, t_default_157);  view_default_546 = t_default_157 = None\n",
      "        view_default_547: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_157, [1, 136, 4096]);  mm_default_157 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_160: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_155, view_default_547);  add_tensor_155 = view_default_547 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_45: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_160, 2)\n",
      "        mean_dim_45: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_45, [-1], True);  pow_tensor_scalar_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_161: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_45, 1e-06);  mean_dim_45 = None\n",
      "        rsqrt_default_45: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_161);  add_tensor_161 = None\n",
      "        detach_default_68: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_45)\n",
      "        mul_tensor_204: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_160, rsqrt_default_45);  rsqrt_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant204 = self._param_constant204\n",
      "        mul_tensor_205: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant204, mul_tensor_204);  _param_constant204 = mul_tensor_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant205 = self._param_constant205\n",
      "        t_default_158: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant205);  _param_constant205 = None\n",
      "        view_default_548: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_205, [136, 4096])\n",
      "        mm_default_158: f32[136, 11008] = torch.ops.aten.mm.default(view_default_548, t_default_158);  view_default_548 = t_default_158 = None\n",
      "        view_default_549: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_158, [1, 136, 11008]);  mm_default_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_22: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_549);  view_default_549 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant206 = self._param_constant206\n",
      "        t_default_159: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant206);  _param_constant206 = None\n",
      "        view_default_550: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_205, [136, 4096]);  mul_tensor_205 = None\n",
      "        mm_default_159: f32[136, 11008] = torch.ops.aten.mm.default(view_default_550, t_default_159);  view_default_550 = t_default_159 = None\n",
      "        view_default_551: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_159, [1, 136, 11008]);  mm_default_159 = None\n",
      "        mul_tensor_206: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_22, view_default_551);  silu_default_22 = view_default_551 = None\n",
      "        _param_constant207 = self._param_constant207\n",
      "        t_default_160: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant207);  _param_constant207 = None\n",
      "        view_default_552: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_206, [136, 11008]);  mul_tensor_206 = None\n",
      "        mm_default_160: f32[136, 4096] = torch.ops.aten.mm.default(view_default_552, t_default_160);  view_default_552 = t_default_160 = None\n",
      "        view_default_553: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_160, [1, 136, 4096]);  mm_default_160 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_162: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_160, view_default_553);  add_tensor_160 = view_default_553 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_46: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_162, 2)\n",
      "        mean_dim_46: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_46, [-1], True);  pow_tensor_scalar_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_163: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_46, 1e-06);  mean_dim_46 = None\n",
      "        rsqrt_default_46: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_163);  add_tensor_163 = None\n",
      "        detach_default_69: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_46)\n",
      "        mul_tensor_207: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_162, rsqrt_default_46);  rsqrt_default_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant208 = self._param_constant208\n",
      "        mul_tensor_208: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant208, mul_tensor_207);  _param_constant208 = mul_tensor_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant209 = self._param_constant209\n",
      "        t_default_161: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant209);  _param_constant209 = None\n",
      "        view_default_554: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_208, [136, 4096])\n",
      "        mm_default_161: f32[136, 4096] = torch.ops.aten.mm.default(view_default_554, t_default_161);  view_default_554 = t_default_161 = None\n",
      "        view_default_555: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_161, [1, 136, 4096]);  mm_default_161 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant210 = self._param_constant210\n",
      "        t_default_162: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant210);  _param_constant210 = None\n",
      "        view_default_556: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_208, [136, 4096])\n",
      "        mm_default_162: f32[136, 4096] = torch.ops.aten.mm.default(view_default_556, t_default_162);  view_default_556 = t_default_162 = None\n",
      "        view_default_557: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_162, [1, 136, 4096]);  mm_default_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant211 = self._param_constant211\n",
      "        t_default_163: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant211);  _param_constant211 = None\n",
      "        view_default_558: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_208, [136, 4096]);  mul_tensor_208 = None\n",
      "        mm_default_163: f32[136, 4096] = torch.ops.aten.mm.default(view_default_558, t_default_163);  view_default_558 = t_default_163 = None\n",
      "        view_default_559: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_163, [1, 136, 4096]);  mm_default_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_560: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_555, [1, 136, 32, 128]);  view_default_555 = None\n",
      "        transpose_int_115: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_560, 1, 2);  view_default_560 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_561: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_557, [1, 136, 32, 128]);  view_default_557 = None\n",
      "        transpose_int_116: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_561, 1, 2);  view_default_561 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_562: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_559, [1, 136, 32, 128]);  view_default_559 = None\n",
      "        transpose_int_117: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_562, 1, 2);  view_default_562 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant46 = self._tensor_constant46\n",
      "        slice_tensor_234: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant46, 0, 0, 9223372036854775807);  _tensor_constant46 = None\n",
      "        slice_tensor_235: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_234, 1, 0, 9223372036854775807);  slice_tensor_234 = None\n",
      "        slice_tensor_236: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_235, 2, 0, 136);  slice_tensor_235 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant47 = self._tensor_constant47\n",
      "        slice_tensor_237: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant47, 0, 0, 9223372036854775807);  _tensor_constant47 = None\n",
      "        slice_tensor_238: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_237, 1, 0, 9223372036854775807);  slice_tensor_237 = None\n",
      "        slice_tensor_239: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_238, 2, 0, 136);  slice_tensor_238 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_92: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_236, 1);  slice_tensor_236 = None\n",
      "        squeeze_dim_93: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_92, 0);  squeeze_dim_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_94: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_239, 1);  slice_tensor_239 = None\n",
      "        squeeze_dim_95: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_94, 0);  squeeze_dim_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_46: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_93, [view_default]);  squeeze_dim_93 = None\n",
      "        unsqueeze_default_51: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_46, 1);  index_tensor_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_47: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_95, [view_default]);  squeeze_dim_95 = None\n",
      "        unsqueeze_default_52: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_47, 1);  index_tensor_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_209: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_115, unsqueeze_default_51)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_240: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_241: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 64, 9223372036854775807);  transpose_int_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_46: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_241);  slice_tensor_241 = None\n",
      "        cat_default_46: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_46, slice_tensor_240], -1);  neg_default_46 = slice_tensor_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_210: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_46, unsqueeze_default_52);  cat_default_46 = None\n",
      "        add_tensor_164: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_209, mul_tensor_210);  mul_tensor_209 = mul_tensor_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_211: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_116, unsqueeze_default_51);  unsqueeze_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_242: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_243: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 64, 9223372036854775807);  transpose_int_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_47: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_243);  slice_tensor_243 = None\n",
      "        cat_default_47: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_47, slice_tensor_242], -1);  neg_default_47 = slice_tensor_242 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_212: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_47, unsqueeze_default_52);  cat_default_47 = unsqueeze_default_52 = None\n",
      "        add_tensor_165: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_211, mul_tensor_212);  mul_tensor_211 = mul_tensor_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_118: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_165, 2, 3)\n",
      "        expand_default_94: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_164, [1, 32, 136, 128]);  add_tensor_164 = None\n",
      "        view_default_563: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_94, [32, 136, 128]);  expand_default_94 = None\n",
      "        expand_default_95: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_118, [1, 32, 128, 136]);  transpose_int_118 = None\n",
      "        view_default_564: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_95, [32, 128, 136]);  expand_default_95 = None\n",
      "        bmm_default_46: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_563, view_default_564);  view_default_563 = view_default_564 = None\n",
      "        view_default_565: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_46, [1, 32, 136, 136]);  bmm_default_46 = None\n",
      "        div_tensor_23: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_565, 11.313708498984761);  view_default_565 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_166: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_23, add_tensor_1);  div_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_23: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_166, -1, False);  add_tensor_166 = None\n",
      "        detach_default_70: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_23)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_96: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_23, [1, 32, 136, 136]);  _softmax_default_23 = None\n",
      "        view_default_566: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_96, [32, 136, 136]);  expand_default_96 = None\n",
      "        expand_default_97: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_117, [1, 32, 136, 128])\n",
      "        view_default_567: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_97, [32, 136, 128]);  expand_default_97 = None\n",
      "        bmm_default_47: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_566, view_default_567);  view_default_566 = view_default_567 = None\n",
      "        view_default_568: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_47, [1, 32, 136, 128]);  bmm_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_119: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_568, 1, 2);  view_default_568 = None\n",
      "        clone_default_23: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_119, memory_format = torch.contiguous_format);  transpose_int_119 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_569: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_23, [1, 136, 4096]);  clone_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant212 = self._param_constant212\n",
      "        t_default_164: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant212);  _param_constant212 = None\n",
      "        view_default_570: f32[136, 4096] = torch.ops.aten.view.default(view_default_569, [136, 4096]);  view_default_569 = None\n",
      "        mm_default_164: f32[136, 4096] = torch.ops.aten.mm.default(view_default_570, t_default_164);  view_default_570 = t_default_164 = None\n",
      "        view_default_571: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_164, [1, 136, 4096]);  mm_default_164 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_167: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_162, view_default_571);  add_tensor_162 = view_default_571 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_47: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_167, 2)\n",
      "        mean_dim_47: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_47, [-1], True);  pow_tensor_scalar_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_168: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_47, 1e-06);  mean_dim_47 = None\n",
      "        rsqrt_default_47: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_168);  add_tensor_168 = None\n",
      "        detach_default_71: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_47)\n",
      "        mul_tensor_213: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_167, rsqrt_default_47);  rsqrt_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant213 = self._param_constant213\n",
      "        mul_tensor_214: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant213, mul_tensor_213);  _param_constant213 = mul_tensor_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant214 = self._param_constant214\n",
      "        t_default_165: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant214);  _param_constant214 = None\n",
      "        view_default_572: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_214, [136, 4096])\n",
      "        mm_default_165: f32[136, 11008] = torch.ops.aten.mm.default(view_default_572, t_default_165);  view_default_572 = t_default_165 = None\n",
      "        view_default_573: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_165, [1, 136, 11008]);  mm_default_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_23: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_573);  view_default_573 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant215 = self._param_constant215\n",
      "        t_default_166: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant215);  _param_constant215 = None\n",
      "        view_default_574: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_214, [136, 4096]);  mul_tensor_214 = None\n",
      "        mm_default_166: f32[136, 11008] = torch.ops.aten.mm.default(view_default_574, t_default_166);  view_default_574 = t_default_166 = None\n",
      "        view_default_575: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_166, [1, 136, 11008]);  mm_default_166 = None\n",
      "        mul_tensor_215: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_23, view_default_575);  silu_default_23 = view_default_575 = None\n",
      "        _param_constant216 = self._param_constant216\n",
      "        t_default_167: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant216);  _param_constant216 = None\n",
      "        view_default_576: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_215, [136, 11008]);  mul_tensor_215 = None\n",
      "        mm_default_167: f32[136, 4096] = torch.ops.aten.mm.default(view_default_576, t_default_167);  view_default_576 = t_default_167 = None\n",
      "        view_default_577: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_167, [1, 136, 4096]);  mm_default_167 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_169: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_167, view_default_577);  add_tensor_167 = view_default_577 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_48: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_169, 2)\n",
      "        mean_dim_48: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_48, [-1], True);  pow_tensor_scalar_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_170: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_48, 1e-06);  mean_dim_48 = None\n",
      "        rsqrt_default_48: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_170);  add_tensor_170 = None\n",
      "        detach_default_72: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_48)\n",
      "        mul_tensor_216: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_169, rsqrt_default_48);  rsqrt_default_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant217 = self._param_constant217\n",
      "        mul_tensor_217: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant217, mul_tensor_216);  _param_constant217 = mul_tensor_216 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant218 = self._param_constant218\n",
      "        t_default_168: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant218);  _param_constant218 = None\n",
      "        view_default_578: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_217, [136, 4096])\n",
      "        mm_default_168: f32[136, 4096] = torch.ops.aten.mm.default(view_default_578, t_default_168);  view_default_578 = t_default_168 = None\n",
      "        view_default_579: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_168, [1, 136, 4096]);  mm_default_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant219 = self._param_constant219\n",
      "        t_default_169: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant219);  _param_constant219 = None\n",
      "        view_default_580: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_217, [136, 4096])\n",
      "        mm_default_169: f32[136, 4096] = torch.ops.aten.mm.default(view_default_580, t_default_169);  view_default_580 = t_default_169 = None\n",
      "        view_default_581: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_169, [1, 136, 4096]);  mm_default_169 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant220 = self._param_constant220\n",
      "        t_default_170: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant220);  _param_constant220 = None\n",
      "        view_default_582: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_217, [136, 4096]);  mul_tensor_217 = None\n",
      "        mm_default_170: f32[136, 4096] = torch.ops.aten.mm.default(view_default_582, t_default_170);  view_default_582 = t_default_170 = None\n",
      "        view_default_583: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_170, [1, 136, 4096]);  mm_default_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_584: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_579, [1, 136, 32, 128]);  view_default_579 = None\n",
      "        transpose_int_120: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_584, 1, 2);  view_default_584 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_585: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_581, [1, 136, 32, 128]);  view_default_581 = None\n",
      "        transpose_int_121: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_585, 1, 2);  view_default_585 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_586: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_583, [1, 136, 32, 128]);  view_default_583 = None\n",
      "        transpose_int_122: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_586, 1, 2);  view_default_586 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant48 = self._tensor_constant48\n",
      "        slice_tensor_244: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant48, 0, 0, 9223372036854775807);  _tensor_constant48 = None\n",
      "        slice_tensor_245: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_244, 1, 0, 9223372036854775807);  slice_tensor_244 = None\n",
      "        slice_tensor_246: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_245, 2, 0, 136);  slice_tensor_245 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant49 = self._tensor_constant49\n",
      "        slice_tensor_247: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant49, 0, 0, 9223372036854775807);  _tensor_constant49 = None\n",
      "        slice_tensor_248: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_247, 1, 0, 9223372036854775807);  slice_tensor_247 = None\n",
      "        slice_tensor_249: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_248, 2, 0, 136);  slice_tensor_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_96: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_246, 1);  slice_tensor_246 = None\n",
      "        squeeze_dim_97: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_96, 0);  squeeze_dim_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_98: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_249, 1);  slice_tensor_249 = None\n",
      "        squeeze_dim_99: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_98, 0);  squeeze_dim_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_48: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_97, [view_default]);  squeeze_dim_97 = None\n",
      "        unsqueeze_default_53: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_48, 1);  index_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_49: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_99, [view_default]);  squeeze_dim_99 = None\n",
      "        unsqueeze_default_54: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_49, 1);  index_tensor_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_218: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_120, unsqueeze_default_53)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_250: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_251: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 64, 9223372036854775807);  transpose_int_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_48: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_251);  slice_tensor_251 = None\n",
      "        cat_default_48: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_48, slice_tensor_250], -1);  neg_default_48 = slice_tensor_250 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_219: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_48, unsqueeze_default_54);  cat_default_48 = None\n",
      "        add_tensor_171: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_218, mul_tensor_219);  mul_tensor_218 = mul_tensor_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_220: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_121, unsqueeze_default_53);  unsqueeze_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_252: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_253: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 64, 9223372036854775807);  transpose_int_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_49: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_253);  slice_tensor_253 = None\n",
      "        cat_default_49: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_49, slice_tensor_252], -1);  neg_default_49 = slice_tensor_252 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_221: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_49, unsqueeze_default_54);  cat_default_49 = unsqueeze_default_54 = None\n",
      "        add_tensor_172: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_220, mul_tensor_221);  mul_tensor_220 = mul_tensor_221 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_123: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_172, 2, 3)\n",
      "        expand_default_98: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_171, [1, 32, 136, 128]);  add_tensor_171 = None\n",
      "        view_default_587: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_98, [32, 136, 128]);  expand_default_98 = None\n",
      "        expand_default_99: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_123, [1, 32, 128, 136]);  transpose_int_123 = None\n",
      "        view_default_588: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_99, [32, 128, 136]);  expand_default_99 = None\n",
      "        bmm_default_48: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_587, view_default_588);  view_default_587 = view_default_588 = None\n",
      "        view_default_589: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_48, [1, 32, 136, 136]);  bmm_default_48 = None\n",
      "        div_tensor_24: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_589, 11.313708498984761);  view_default_589 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_173: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_24, add_tensor_1);  div_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_24: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_173, -1, False);  add_tensor_173 = None\n",
      "        detach_default_73: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_24)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_100: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_24, [1, 32, 136, 136]);  _softmax_default_24 = None\n",
      "        view_default_590: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_100, [32, 136, 136]);  expand_default_100 = None\n",
      "        expand_default_101: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_122, [1, 32, 136, 128])\n",
      "        view_default_591: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_101, [32, 136, 128]);  expand_default_101 = None\n",
      "        bmm_default_49: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_590, view_default_591);  view_default_590 = view_default_591 = None\n",
      "        view_default_592: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_49, [1, 32, 136, 128]);  bmm_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_124: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_592, 1, 2);  view_default_592 = None\n",
      "        clone_default_24: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_124, memory_format = torch.contiguous_format);  transpose_int_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_593: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_24, [1, 136, 4096]);  clone_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant221 = self._param_constant221\n",
      "        t_default_171: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant221);  _param_constant221 = None\n",
      "        view_default_594: f32[136, 4096] = torch.ops.aten.view.default(view_default_593, [136, 4096]);  view_default_593 = None\n",
      "        mm_default_171: f32[136, 4096] = torch.ops.aten.mm.default(view_default_594, t_default_171);  view_default_594 = t_default_171 = None\n",
      "        view_default_595: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_171, [1, 136, 4096]);  mm_default_171 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_174: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_169, view_default_595);  add_tensor_169 = view_default_595 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_49: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_174, 2)\n",
      "        mean_dim_49: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_49, [-1], True);  pow_tensor_scalar_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_175: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_49, 1e-06);  mean_dim_49 = None\n",
      "        rsqrt_default_49: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_175);  add_tensor_175 = None\n",
      "        detach_default_74: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_49)\n",
      "        mul_tensor_222: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_174, rsqrt_default_49);  rsqrt_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant222 = self._param_constant222\n",
      "        mul_tensor_223: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant222, mul_tensor_222);  _param_constant222 = mul_tensor_222 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant223 = self._param_constant223\n",
      "        t_default_172: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant223);  _param_constant223 = None\n",
      "        view_default_596: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_223, [136, 4096])\n",
      "        mm_default_172: f32[136, 11008] = torch.ops.aten.mm.default(view_default_596, t_default_172);  view_default_596 = t_default_172 = None\n",
      "        view_default_597: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_172, [1, 136, 11008]);  mm_default_172 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_24: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_597);  view_default_597 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant224 = self._param_constant224\n",
      "        t_default_173: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant224);  _param_constant224 = None\n",
      "        view_default_598: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_223, [136, 4096]);  mul_tensor_223 = None\n",
      "        mm_default_173: f32[136, 11008] = torch.ops.aten.mm.default(view_default_598, t_default_173);  view_default_598 = t_default_173 = None\n",
      "        view_default_599: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_173, [1, 136, 11008]);  mm_default_173 = None\n",
      "        mul_tensor_224: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_24, view_default_599);  silu_default_24 = view_default_599 = None\n",
      "        _param_constant225 = self._param_constant225\n",
      "        t_default_174: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant225);  _param_constant225 = None\n",
      "        view_default_600: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_224, [136, 11008]);  mul_tensor_224 = None\n",
      "        mm_default_174: f32[136, 4096] = torch.ops.aten.mm.default(view_default_600, t_default_174);  view_default_600 = t_default_174 = None\n",
      "        view_default_601: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_174, [1, 136, 4096]);  mm_default_174 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_176: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_174, view_default_601);  add_tensor_174 = view_default_601 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_50: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_176, 2)\n",
      "        mean_dim_50: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_50, [-1], True);  pow_tensor_scalar_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_177: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_50, 1e-06);  mean_dim_50 = None\n",
      "        rsqrt_default_50: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_177);  add_tensor_177 = None\n",
      "        detach_default_75: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_50)\n",
      "        mul_tensor_225: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_176, rsqrt_default_50);  rsqrt_default_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant226 = self._param_constant226\n",
      "        mul_tensor_226: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant226, mul_tensor_225);  _param_constant226 = mul_tensor_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant227 = self._param_constant227\n",
      "        t_default_175: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant227);  _param_constant227 = None\n",
      "        view_default_602: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_226, [136, 4096])\n",
      "        mm_default_175: f32[136, 4096] = torch.ops.aten.mm.default(view_default_602, t_default_175);  view_default_602 = t_default_175 = None\n",
      "        view_default_603: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_175, [1, 136, 4096]);  mm_default_175 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant228 = self._param_constant228\n",
      "        t_default_176: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant228);  _param_constant228 = None\n",
      "        view_default_604: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_226, [136, 4096])\n",
      "        mm_default_176: f32[136, 4096] = torch.ops.aten.mm.default(view_default_604, t_default_176);  view_default_604 = t_default_176 = None\n",
      "        view_default_605: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_176, [1, 136, 4096]);  mm_default_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant229 = self._param_constant229\n",
      "        t_default_177: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant229);  _param_constant229 = None\n",
      "        view_default_606: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_226, [136, 4096]);  mul_tensor_226 = None\n",
      "        mm_default_177: f32[136, 4096] = torch.ops.aten.mm.default(view_default_606, t_default_177);  view_default_606 = t_default_177 = None\n",
      "        view_default_607: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_177, [1, 136, 4096]);  mm_default_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_608: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_603, [1, 136, 32, 128]);  view_default_603 = None\n",
      "        transpose_int_125: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_608, 1, 2);  view_default_608 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_609: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_605, [1, 136, 32, 128]);  view_default_605 = None\n",
      "        transpose_int_126: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_609, 1, 2);  view_default_609 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_610: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_607, [1, 136, 32, 128]);  view_default_607 = None\n",
      "        transpose_int_127: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_610, 1, 2);  view_default_610 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant50 = self._tensor_constant50\n",
      "        slice_tensor_254: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant50, 0, 0, 9223372036854775807);  _tensor_constant50 = None\n",
      "        slice_tensor_255: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_254, 1, 0, 9223372036854775807);  slice_tensor_254 = None\n",
      "        slice_tensor_256: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_255, 2, 0, 136);  slice_tensor_255 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant51 = self._tensor_constant51\n",
      "        slice_tensor_257: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant51, 0, 0, 9223372036854775807);  _tensor_constant51 = None\n",
      "        slice_tensor_258: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_257, 1, 0, 9223372036854775807);  slice_tensor_257 = None\n",
      "        slice_tensor_259: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_258, 2, 0, 136);  slice_tensor_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_100: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_256, 1);  slice_tensor_256 = None\n",
      "        squeeze_dim_101: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_100, 0);  squeeze_dim_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_102: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_259, 1);  slice_tensor_259 = None\n",
      "        squeeze_dim_103: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_102, 0);  squeeze_dim_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_50: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_101, [view_default]);  squeeze_dim_101 = None\n",
      "        unsqueeze_default_55: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_50, 1);  index_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_51: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_103, [view_default]);  squeeze_dim_103 = None\n",
      "        unsqueeze_default_56: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_51, 1);  index_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_227: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_125, unsqueeze_default_55)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_260: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_261: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 64, 9223372036854775807);  transpose_int_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_50: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_261);  slice_tensor_261 = None\n",
      "        cat_default_50: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_50, slice_tensor_260], -1);  neg_default_50 = slice_tensor_260 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_228: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_50, unsqueeze_default_56);  cat_default_50 = None\n",
      "        add_tensor_178: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_227, mul_tensor_228);  mul_tensor_227 = mul_tensor_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_229: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_126, unsqueeze_default_55);  unsqueeze_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_262: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_263: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 64, 9223372036854775807);  transpose_int_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_51: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_263);  slice_tensor_263 = None\n",
      "        cat_default_51: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_51, slice_tensor_262], -1);  neg_default_51 = slice_tensor_262 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_230: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_51, unsqueeze_default_56);  cat_default_51 = unsqueeze_default_56 = None\n",
      "        add_tensor_179: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_229, mul_tensor_230);  mul_tensor_229 = mul_tensor_230 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_128: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_179, 2, 3)\n",
      "        expand_default_102: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_178, [1, 32, 136, 128]);  add_tensor_178 = None\n",
      "        view_default_611: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_102, [32, 136, 128]);  expand_default_102 = None\n",
      "        expand_default_103: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_128, [1, 32, 128, 136]);  transpose_int_128 = None\n",
      "        view_default_612: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_103, [32, 128, 136]);  expand_default_103 = None\n",
      "        bmm_default_50: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_611, view_default_612);  view_default_611 = view_default_612 = None\n",
      "        view_default_613: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_50, [1, 32, 136, 136]);  bmm_default_50 = None\n",
      "        div_tensor_25: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_613, 11.313708498984761);  view_default_613 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_180: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_25, add_tensor_1);  div_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_25: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_180, -1, False);  add_tensor_180 = None\n",
      "        detach_default_76: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_25)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_104: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_25, [1, 32, 136, 136]);  _softmax_default_25 = None\n",
      "        view_default_614: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_104, [32, 136, 136]);  expand_default_104 = None\n",
      "        expand_default_105: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_127, [1, 32, 136, 128])\n",
      "        view_default_615: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_105, [32, 136, 128]);  expand_default_105 = None\n",
      "        bmm_default_51: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_614, view_default_615);  view_default_614 = view_default_615 = None\n",
      "        view_default_616: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_51, [1, 32, 136, 128]);  bmm_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_129: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_616, 1, 2);  view_default_616 = None\n",
      "        clone_default_25: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_129, memory_format = torch.contiguous_format);  transpose_int_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_617: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_25, [1, 136, 4096]);  clone_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant230 = self._param_constant230\n",
      "        t_default_178: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant230);  _param_constant230 = None\n",
      "        view_default_618: f32[136, 4096] = torch.ops.aten.view.default(view_default_617, [136, 4096]);  view_default_617 = None\n",
      "        mm_default_178: f32[136, 4096] = torch.ops.aten.mm.default(view_default_618, t_default_178);  view_default_618 = t_default_178 = None\n",
      "        view_default_619: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_178, [1, 136, 4096]);  mm_default_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_181: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_176, view_default_619);  add_tensor_176 = view_default_619 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_51: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_181, 2)\n",
      "        mean_dim_51: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_51, [-1], True);  pow_tensor_scalar_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_182: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_51, 1e-06);  mean_dim_51 = None\n",
      "        rsqrt_default_51: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_182);  add_tensor_182 = None\n",
      "        detach_default_77: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_51)\n",
      "        mul_tensor_231: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_181, rsqrt_default_51);  rsqrt_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant231 = self._param_constant231\n",
      "        mul_tensor_232: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant231, mul_tensor_231);  _param_constant231 = mul_tensor_231 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant232 = self._param_constant232\n",
      "        t_default_179: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant232);  _param_constant232 = None\n",
      "        view_default_620: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_232, [136, 4096])\n",
      "        mm_default_179: f32[136, 11008] = torch.ops.aten.mm.default(view_default_620, t_default_179);  view_default_620 = t_default_179 = None\n",
      "        view_default_621: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_179, [1, 136, 11008]);  mm_default_179 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_25: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_621);  view_default_621 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant233 = self._param_constant233\n",
      "        t_default_180: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant233);  _param_constant233 = None\n",
      "        view_default_622: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_232, [136, 4096]);  mul_tensor_232 = None\n",
      "        mm_default_180: f32[136, 11008] = torch.ops.aten.mm.default(view_default_622, t_default_180);  view_default_622 = t_default_180 = None\n",
      "        view_default_623: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_180, [1, 136, 11008]);  mm_default_180 = None\n",
      "        mul_tensor_233: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_25, view_default_623);  silu_default_25 = view_default_623 = None\n",
      "        _param_constant234 = self._param_constant234\n",
      "        t_default_181: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant234);  _param_constant234 = None\n",
      "        view_default_624: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_233, [136, 11008]);  mul_tensor_233 = None\n",
      "        mm_default_181: f32[136, 4096] = torch.ops.aten.mm.default(view_default_624, t_default_181);  view_default_624 = t_default_181 = None\n",
      "        view_default_625: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_181, [1, 136, 4096]);  mm_default_181 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_183: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_181, view_default_625);  add_tensor_181 = view_default_625 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_52: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_183, 2)\n",
      "        mean_dim_52: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_52, [-1], True);  pow_tensor_scalar_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_184: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_52, 1e-06);  mean_dim_52 = None\n",
      "        rsqrt_default_52: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_184);  add_tensor_184 = None\n",
      "        detach_default_78: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_52)\n",
      "        mul_tensor_234: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_183, rsqrt_default_52);  rsqrt_default_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant235 = self._param_constant235\n",
      "        mul_tensor_235: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant235, mul_tensor_234);  _param_constant235 = mul_tensor_234 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant236 = self._param_constant236\n",
      "        t_default_182: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant236);  _param_constant236 = None\n",
      "        view_default_626: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_235, [136, 4096])\n",
      "        mm_default_182: f32[136, 4096] = torch.ops.aten.mm.default(view_default_626, t_default_182);  view_default_626 = t_default_182 = None\n",
      "        view_default_627: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_182, [1, 136, 4096]);  mm_default_182 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant237 = self._param_constant237\n",
      "        t_default_183: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant237);  _param_constant237 = None\n",
      "        view_default_628: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_235, [136, 4096])\n",
      "        mm_default_183: f32[136, 4096] = torch.ops.aten.mm.default(view_default_628, t_default_183);  view_default_628 = t_default_183 = None\n",
      "        view_default_629: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_183, [1, 136, 4096]);  mm_default_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant238 = self._param_constant238\n",
      "        t_default_184: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant238);  _param_constant238 = None\n",
      "        view_default_630: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_235, [136, 4096]);  mul_tensor_235 = None\n",
      "        mm_default_184: f32[136, 4096] = torch.ops.aten.mm.default(view_default_630, t_default_184);  view_default_630 = t_default_184 = None\n",
      "        view_default_631: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_184, [1, 136, 4096]);  mm_default_184 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_632: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_627, [1, 136, 32, 128]);  view_default_627 = None\n",
      "        transpose_int_130: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_632, 1, 2);  view_default_632 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_633: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_629, [1, 136, 32, 128]);  view_default_629 = None\n",
      "        transpose_int_131: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_633, 1, 2);  view_default_633 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_634: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_631, [1, 136, 32, 128]);  view_default_631 = None\n",
      "        transpose_int_132: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_634, 1, 2);  view_default_634 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant52 = self._tensor_constant52\n",
      "        slice_tensor_264: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant52, 0, 0, 9223372036854775807);  _tensor_constant52 = None\n",
      "        slice_tensor_265: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_264, 1, 0, 9223372036854775807);  slice_tensor_264 = None\n",
      "        slice_tensor_266: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_265, 2, 0, 136);  slice_tensor_265 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant53 = self._tensor_constant53\n",
      "        slice_tensor_267: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant53, 0, 0, 9223372036854775807);  _tensor_constant53 = None\n",
      "        slice_tensor_268: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_267, 1, 0, 9223372036854775807);  slice_tensor_267 = None\n",
      "        slice_tensor_269: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_268, 2, 0, 136);  slice_tensor_268 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_104: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_266, 1);  slice_tensor_266 = None\n",
      "        squeeze_dim_105: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_104, 0);  squeeze_dim_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_106: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_269, 1);  slice_tensor_269 = None\n",
      "        squeeze_dim_107: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_106, 0);  squeeze_dim_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_52: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_105, [view_default]);  squeeze_dim_105 = None\n",
      "        unsqueeze_default_57: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_52, 1);  index_tensor_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_53: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_107, [view_default]);  squeeze_dim_107 = None\n",
      "        unsqueeze_default_58: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_53, 1);  index_tensor_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_236: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_130, unsqueeze_default_57)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_270: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_271: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 64, 9223372036854775807);  transpose_int_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_52: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_271);  slice_tensor_271 = None\n",
      "        cat_default_52: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_52, slice_tensor_270], -1);  neg_default_52 = slice_tensor_270 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_237: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_52, unsqueeze_default_58);  cat_default_52 = None\n",
      "        add_tensor_185: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_236, mul_tensor_237);  mul_tensor_236 = mul_tensor_237 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_238: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_131, unsqueeze_default_57);  unsqueeze_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_272: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_273: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 64, 9223372036854775807);  transpose_int_131 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_53: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_273);  slice_tensor_273 = None\n",
      "        cat_default_53: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_53, slice_tensor_272], -1);  neg_default_53 = slice_tensor_272 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_239: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_53, unsqueeze_default_58);  cat_default_53 = unsqueeze_default_58 = None\n",
      "        add_tensor_186: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_238, mul_tensor_239);  mul_tensor_238 = mul_tensor_239 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_133: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_186, 2, 3)\n",
      "        expand_default_106: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_185, [1, 32, 136, 128]);  add_tensor_185 = None\n",
      "        view_default_635: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_106, [32, 136, 128]);  expand_default_106 = None\n",
      "        expand_default_107: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_133, [1, 32, 128, 136]);  transpose_int_133 = None\n",
      "        view_default_636: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_107, [32, 128, 136]);  expand_default_107 = None\n",
      "        bmm_default_52: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_635, view_default_636);  view_default_635 = view_default_636 = None\n",
      "        view_default_637: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_52, [1, 32, 136, 136]);  bmm_default_52 = None\n",
      "        div_tensor_26: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_637, 11.313708498984761);  view_default_637 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_187: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_26, add_tensor_1);  div_tensor_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_26: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_187, -1, False);  add_tensor_187 = None\n",
      "        detach_default_79: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_26)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_108: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_26, [1, 32, 136, 136]);  _softmax_default_26 = None\n",
      "        view_default_638: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_108, [32, 136, 136]);  expand_default_108 = None\n",
      "        expand_default_109: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_132, [1, 32, 136, 128])\n",
      "        view_default_639: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_109, [32, 136, 128]);  expand_default_109 = None\n",
      "        bmm_default_53: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_638, view_default_639);  view_default_638 = view_default_639 = None\n",
      "        view_default_640: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_53, [1, 32, 136, 128]);  bmm_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_134: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_640, 1, 2);  view_default_640 = None\n",
      "        clone_default_26: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_134, memory_format = torch.contiguous_format);  transpose_int_134 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_641: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_26, [1, 136, 4096]);  clone_default_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant239 = self._param_constant239\n",
      "        t_default_185: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant239);  _param_constant239 = None\n",
      "        view_default_642: f32[136, 4096] = torch.ops.aten.view.default(view_default_641, [136, 4096]);  view_default_641 = None\n",
      "        mm_default_185: f32[136, 4096] = torch.ops.aten.mm.default(view_default_642, t_default_185);  view_default_642 = t_default_185 = None\n",
      "        view_default_643: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_185, [1, 136, 4096]);  mm_default_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_188: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_183, view_default_643);  add_tensor_183 = view_default_643 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_53: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_188, 2)\n",
      "        mean_dim_53: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_53, [-1], True);  pow_tensor_scalar_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_189: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_53, 1e-06);  mean_dim_53 = None\n",
      "        rsqrt_default_53: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_189);  add_tensor_189 = None\n",
      "        detach_default_80: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_53)\n",
      "        mul_tensor_240: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_188, rsqrt_default_53);  rsqrt_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant240 = self._param_constant240\n",
      "        mul_tensor_241: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant240, mul_tensor_240);  _param_constant240 = mul_tensor_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant241 = self._param_constant241\n",
      "        t_default_186: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant241);  _param_constant241 = None\n",
      "        view_default_644: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_241, [136, 4096])\n",
      "        mm_default_186: f32[136, 11008] = torch.ops.aten.mm.default(view_default_644, t_default_186);  view_default_644 = t_default_186 = None\n",
      "        view_default_645: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_186, [1, 136, 11008]);  mm_default_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_26: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_645);  view_default_645 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant242 = self._param_constant242\n",
      "        t_default_187: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant242);  _param_constant242 = None\n",
      "        view_default_646: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_241, [136, 4096]);  mul_tensor_241 = None\n",
      "        mm_default_187: f32[136, 11008] = torch.ops.aten.mm.default(view_default_646, t_default_187);  view_default_646 = t_default_187 = None\n",
      "        view_default_647: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_187, [1, 136, 11008]);  mm_default_187 = None\n",
      "        mul_tensor_242: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_26, view_default_647);  silu_default_26 = view_default_647 = None\n",
      "        _param_constant243 = self._param_constant243\n",
      "        t_default_188: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant243);  _param_constant243 = None\n",
      "        view_default_648: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_242, [136, 11008]);  mul_tensor_242 = None\n",
      "        mm_default_188: f32[136, 4096] = torch.ops.aten.mm.default(view_default_648, t_default_188);  view_default_648 = t_default_188 = None\n",
      "        view_default_649: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_188, [1, 136, 4096]);  mm_default_188 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_190: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_188, view_default_649);  add_tensor_188 = view_default_649 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_54: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_190, 2)\n",
      "        mean_dim_54: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_54, [-1], True);  pow_tensor_scalar_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_191: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_54, 1e-06);  mean_dim_54 = None\n",
      "        rsqrt_default_54: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_191);  add_tensor_191 = None\n",
      "        detach_default_81: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_54)\n",
      "        mul_tensor_243: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_190, rsqrt_default_54);  rsqrt_default_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant244 = self._param_constant244\n",
      "        mul_tensor_244: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant244, mul_tensor_243);  _param_constant244 = mul_tensor_243 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant245 = self._param_constant245\n",
      "        t_default_189: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant245);  _param_constant245 = None\n",
      "        view_default_650: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_244, [136, 4096])\n",
      "        mm_default_189: f32[136, 4096] = torch.ops.aten.mm.default(view_default_650, t_default_189);  view_default_650 = t_default_189 = None\n",
      "        view_default_651: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_189, [1, 136, 4096]);  mm_default_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant246 = self._param_constant246\n",
      "        t_default_190: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant246);  _param_constant246 = None\n",
      "        view_default_652: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_244, [136, 4096])\n",
      "        mm_default_190: f32[136, 4096] = torch.ops.aten.mm.default(view_default_652, t_default_190);  view_default_652 = t_default_190 = None\n",
      "        view_default_653: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_190, [1, 136, 4096]);  mm_default_190 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant247 = self._param_constant247\n",
      "        t_default_191: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant247);  _param_constant247 = None\n",
      "        view_default_654: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_244, [136, 4096]);  mul_tensor_244 = None\n",
      "        mm_default_191: f32[136, 4096] = torch.ops.aten.mm.default(view_default_654, t_default_191);  view_default_654 = t_default_191 = None\n",
      "        view_default_655: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_191, [1, 136, 4096]);  mm_default_191 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_656: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_651, [1, 136, 32, 128]);  view_default_651 = None\n",
      "        transpose_int_135: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_656, 1, 2);  view_default_656 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_657: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_653, [1, 136, 32, 128]);  view_default_653 = None\n",
      "        transpose_int_136: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_657, 1, 2);  view_default_657 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_658: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_655, [1, 136, 32, 128]);  view_default_655 = None\n",
      "        transpose_int_137: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_658, 1, 2);  view_default_658 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant54 = self._tensor_constant54\n",
      "        slice_tensor_274: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant54, 0, 0, 9223372036854775807);  _tensor_constant54 = None\n",
      "        slice_tensor_275: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_274, 1, 0, 9223372036854775807);  slice_tensor_274 = None\n",
      "        slice_tensor_276: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_275, 2, 0, 136);  slice_tensor_275 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant55 = self._tensor_constant55\n",
      "        slice_tensor_277: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant55, 0, 0, 9223372036854775807);  _tensor_constant55 = None\n",
      "        slice_tensor_278: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_277, 1, 0, 9223372036854775807);  slice_tensor_277 = None\n",
      "        slice_tensor_279: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_278, 2, 0, 136);  slice_tensor_278 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_108: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_276, 1);  slice_tensor_276 = None\n",
      "        squeeze_dim_109: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_108, 0);  squeeze_dim_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_110: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_279, 1);  slice_tensor_279 = None\n",
      "        squeeze_dim_111: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_110, 0);  squeeze_dim_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_54: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_109, [view_default]);  squeeze_dim_109 = None\n",
      "        unsqueeze_default_59: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_54, 1);  index_tensor_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_55: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_111, [view_default]);  squeeze_dim_111 = None\n",
      "        unsqueeze_default_60: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_55, 1);  index_tensor_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_245: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_135, unsqueeze_default_59)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_280: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_281: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 64, 9223372036854775807);  transpose_int_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_54: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_281);  slice_tensor_281 = None\n",
      "        cat_default_54: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_54, slice_tensor_280], -1);  neg_default_54 = slice_tensor_280 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_246: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_54, unsqueeze_default_60);  cat_default_54 = None\n",
      "        add_tensor_192: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_245, mul_tensor_246);  mul_tensor_245 = mul_tensor_246 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_247: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_136, unsqueeze_default_59);  unsqueeze_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_282: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_283: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 64, 9223372036854775807);  transpose_int_136 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_55: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_283);  slice_tensor_283 = None\n",
      "        cat_default_55: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_55, slice_tensor_282], -1);  neg_default_55 = slice_tensor_282 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_248: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_55, unsqueeze_default_60);  cat_default_55 = unsqueeze_default_60 = None\n",
      "        add_tensor_193: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_247, mul_tensor_248);  mul_tensor_247 = mul_tensor_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_138: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_193, 2, 3)\n",
      "        expand_default_110: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_192, [1, 32, 136, 128]);  add_tensor_192 = None\n",
      "        view_default_659: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_110, [32, 136, 128]);  expand_default_110 = None\n",
      "        expand_default_111: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_138, [1, 32, 128, 136]);  transpose_int_138 = None\n",
      "        view_default_660: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_111, [32, 128, 136]);  expand_default_111 = None\n",
      "        bmm_default_54: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_659, view_default_660);  view_default_659 = view_default_660 = None\n",
      "        view_default_661: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_54, [1, 32, 136, 136]);  bmm_default_54 = None\n",
      "        div_tensor_27: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_661, 11.313708498984761);  view_default_661 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_194: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_27, add_tensor_1);  div_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_27: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_194, -1, False);  add_tensor_194 = None\n",
      "        detach_default_82: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_27)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_112: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_27, [1, 32, 136, 136]);  _softmax_default_27 = None\n",
      "        view_default_662: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_112, [32, 136, 136]);  expand_default_112 = None\n",
      "        expand_default_113: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_137, [1, 32, 136, 128])\n",
      "        view_default_663: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_113, [32, 136, 128]);  expand_default_113 = None\n",
      "        bmm_default_55: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_662, view_default_663);  view_default_662 = view_default_663 = None\n",
      "        view_default_664: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_55, [1, 32, 136, 128]);  bmm_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_139: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_664, 1, 2);  view_default_664 = None\n",
      "        clone_default_27: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_139, memory_format = torch.contiguous_format);  transpose_int_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_665: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_27, [1, 136, 4096]);  clone_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant248 = self._param_constant248\n",
      "        t_default_192: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant248);  _param_constant248 = None\n",
      "        view_default_666: f32[136, 4096] = torch.ops.aten.view.default(view_default_665, [136, 4096]);  view_default_665 = None\n",
      "        mm_default_192: f32[136, 4096] = torch.ops.aten.mm.default(view_default_666, t_default_192);  view_default_666 = t_default_192 = None\n",
      "        view_default_667: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_192, [1, 136, 4096]);  mm_default_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_195: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_190, view_default_667);  add_tensor_190 = view_default_667 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_55: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_195, 2)\n",
      "        mean_dim_55: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_55, [-1], True);  pow_tensor_scalar_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_196: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_55, 1e-06);  mean_dim_55 = None\n",
      "        rsqrt_default_55: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_196);  add_tensor_196 = None\n",
      "        detach_default_83: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_55)\n",
      "        mul_tensor_249: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_195, rsqrt_default_55);  rsqrt_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant249 = self._param_constant249\n",
      "        mul_tensor_250: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant249, mul_tensor_249);  _param_constant249 = mul_tensor_249 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant250 = self._param_constant250\n",
      "        t_default_193: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant250);  _param_constant250 = None\n",
      "        view_default_668: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_250, [136, 4096])\n",
      "        mm_default_193: f32[136, 11008] = torch.ops.aten.mm.default(view_default_668, t_default_193);  view_default_668 = t_default_193 = None\n",
      "        view_default_669: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_193, [1, 136, 11008]);  mm_default_193 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_27: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_669);  view_default_669 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant251 = self._param_constant251\n",
      "        t_default_194: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant251);  _param_constant251 = None\n",
      "        view_default_670: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_250, [136, 4096]);  mul_tensor_250 = None\n",
      "        mm_default_194: f32[136, 11008] = torch.ops.aten.mm.default(view_default_670, t_default_194);  view_default_670 = t_default_194 = None\n",
      "        view_default_671: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_194, [1, 136, 11008]);  mm_default_194 = None\n",
      "        mul_tensor_251: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_27, view_default_671);  silu_default_27 = view_default_671 = None\n",
      "        _param_constant252 = self._param_constant252\n",
      "        t_default_195: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant252);  _param_constant252 = None\n",
      "        view_default_672: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_251, [136, 11008]);  mul_tensor_251 = None\n",
      "        mm_default_195: f32[136, 4096] = torch.ops.aten.mm.default(view_default_672, t_default_195);  view_default_672 = t_default_195 = None\n",
      "        view_default_673: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_195, [1, 136, 4096]);  mm_default_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_197: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_195, view_default_673);  add_tensor_195 = view_default_673 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_56: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_197, 2)\n",
      "        mean_dim_56: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_56, [-1], True);  pow_tensor_scalar_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_198: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_56, 1e-06);  mean_dim_56 = None\n",
      "        rsqrt_default_56: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_198);  add_tensor_198 = None\n",
      "        detach_default_84: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_56)\n",
      "        mul_tensor_252: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_197, rsqrt_default_56);  rsqrt_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant253 = self._param_constant253\n",
      "        mul_tensor_253: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant253, mul_tensor_252);  _param_constant253 = mul_tensor_252 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant254 = self._param_constant254\n",
      "        t_default_196: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant254);  _param_constant254 = None\n",
      "        view_default_674: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_253, [136, 4096])\n",
      "        mm_default_196: f32[136, 4096] = torch.ops.aten.mm.default(view_default_674, t_default_196);  view_default_674 = t_default_196 = None\n",
      "        view_default_675: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_196, [1, 136, 4096]);  mm_default_196 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant255 = self._param_constant255\n",
      "        t_default_197: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant255);  _param_constant255 = None\n",
      "        view_default_676: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_253, [136, 4096])\n",
      "        mm_default_197: f32[136, 4096] = torch.ops.aten.mm.default(view_default_676, t_default_197);  view_default_676 = t_default_197 = None\n",
      "        view_default_677: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_197, [1, 136, 4096]);  mm_default_197 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant256 = self._param_constant256\n",
      "        t_default_198: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant256);  _param_constant256 = None\n",
      "        view_default_678: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_253, [136, 4096]);  mul_tensor_253 = None\n",
      "        mm_default_198: f32[136, 4096] = torch.ops.aten.mm.default(view_default_678, t_default_198);  view_default_678 = t_default_198 = None\n",
      "        view_default_679: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_198, [1, 136, 4096]);  mm_default_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_680: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_675, [1, 136, 32, 128]);  view_default_675 = None\n",
      "        transpose_int_140: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_680, 1, 2);  view_default_680 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_681: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_677, [1, 136, 32, 128]);  view_default_677 = None\n",
      "        transpose_int_141: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_681, 1, 2);  view_default_681 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_682: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_679, [1, 136, 32, 128]);  view_default_679 = None\n",
      "        transpose_int_142: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_682, 1, 2);  view_default_682 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant56 = self._tensor_constant56\n",
      "        slice_tensor_284: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant56, 0, 0, 9223372036854775807);  _tensor_constant56 = None\n",
      "        slice_tensor_285: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_284, 1, 0, 9223372036854775807);  slice_tensor_284 = None\n",
      "        slice_tensor_286: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_285, 2, 0, 136);  slice_tensor_285 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant57 = self._tensor_constant57\n",
      "        slice_tensor_287: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant57, 0, 0, 9223372036854775807);  _tensor_constant57 = None\n",
      "        slice_tensor_288: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_287, 1, 0, 9223372036854775807);  slice_tensor_287 = None\n",
      "        slice_tensor_289: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_288, 2, 0, 136);  slice_tensor_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_112: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_286, 1);  slice_tensor_286 = None\n",
      "        squeeze_dim_113: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_112, 0);  squeeze_dim_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_114: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_289, 1);  slice_tensor_289 = None\n",
      "        squeeze_dim_115: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_114, 0);  squeeze_dim_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_56: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_113, [view_default]);  squeeze_dim_113 = None\n",
      "        unsqueeze_default_61: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_56, 1);  index_tensor_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_57: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_115, [view_default]);  squeeze_dim_115 = None\n",
      "        unsqueeze_default_62: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_57, 1);  index_tensor_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_254: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_140, unsqueeze_default_61)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_290: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_291: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 64, 9223372036854775807);  transpose_int_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_56: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_291);  slice_tensor_291 = None\n",
      "        cat_default_56: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_56, slice_tensor_290], -1);  neg_default_56 = slice_tensor_290 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_255: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_56, unsqueeze_default_62);  cat_default_56 = None\n",
      "        add_tensor_199: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_254, mul_tensor_255);  mul_tensor_254 = mul_tensor_255 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_256: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_141, unsqueeze_default_61);  unsqueeze_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_292: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_293: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 64, 9223372036854775807);  transpose_int_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_57: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_293);  slice_tensor_293 = None\n",
      "        cat_default_57: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_57, slice_tensor_292], -1);  neg_default_57 = slice_tensor_292 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_257: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_57, unsqueeze_default_62);  cat_default_57 = unsqueeze_default_62 = None\n",
      "        add_tensor_200: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_256, mul_tensor_257);  mul_tensor_256 = mul_tensor_257 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_143: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_200, 2, 3)\n",
      "        expand_default_114: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_199, [1, 32, 136, 128]);  add_tensor_199 = None\n",
      "        view_default_683: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_114, [32, 136, 128]);  expand_default_114 = None\n",
      "        expand_default_115: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_143, [1, 32, 128, 136]);  transpose_int_143 = None\n",
      "        view_default_684: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_115, [32, 128, 136]);  expand_default_115 = None\n",
      "        bmm_default_56: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_683, view_default_684);  view_default_683 = view_default_684 = None\n",
      "        view_default_685: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_56, [1, 32, 136, 136]);  bmm_default_56 = None\n",
      "        div_tensor_28: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_685, 11.313708498984761);  view_default_685 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_201: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_28, add_tensor_1);  div_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_28: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_201, -1, False);  add_tensor_201 = None\n",
      "        detach_default_85: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_28)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_116: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_28, [1, 32, 136, 136]);  _softmax_default_28 = None\n",
      "        view_default_686: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_116, [32, 136, 136]);  expand_default_116 = None\n",
      "        expand_default_117: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_142, [1, 32, 136, 128])\n",
      "        view_default_687: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_117, [32, 136, 128]);  expand_default_117 = None\n",
      "        bmm_default_57: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_686, view_default_687);  view_default_686 = view_default_687 = None\n",
      "        view_default_688: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_57, [1, 32, 136, 128]);  bmm_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_144: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_688, 1, 2);  view_default_688 = None\n",
      "        clone_default_28: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_144, memory_format = torch.contiguous_format);  transpose_int_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_689: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_28, [1, 136, 4096]);  clone_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant257 = self._param_constant257\n",
      "        t_default_199: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant257);  _param_constant257 = None\n",
      "        view_default_690: f32[136, 4096] = torch.ops.aten.view.default(view_default_689, [136, 4096]);  view_default_689 = None\n",
      "        mm_default_199: f32[136, 4096] = torch.ops.aten.mm.default(view_default_690, t_default_199);  view_default_690 = t_default_199 = None\n",
      "        view_default_691: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_199, [1, 136, 4096]);  mm_default_199 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_202: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_197, view_default_691);  add_tensor_197 = view_default_691 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_57: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_202, 2)\n",
      "        mean_dim_57: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_57, [-1], True);  pow_tensor_scalar_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_203: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_57, 1e-06);  mean_dim_57 = None\n",
      "        rsqrt_default_57: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_203);  add_tensor_203 = None\n",
      "        detach_default_86: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_57)\n",
      "        mul_tensor_258: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_202, rsqrt_default_57);  rsqrt_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant258 = self._param_constant258\n",
      "        mul_tensor_259: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant258, mul_tensor_258);  _param_constant258 = mul_tensor_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant259 = self._param_constant259\n",
      "        t_default_200: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant259);  _param_constant259 = None\n",
      "        view_default_692: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_259, [136, 4096])\n",
      "        mm_default_200: f32[136, 11008] = torch.ops.aten.mm.default(view_default_692, t_default_200);  view_default_692 = t_default_200 = None\n",
      "        view_default_693: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_200, [1, 136, 11008]);  mm_default_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_28: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_693);  view_default_693 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant260 = self._param_constant260\n",
      "        t_default_201: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant260);  _param_constant260 = None\n",
      "        view_default_694: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_259, [136, 4096]);  mul_tensor_259 = None\n",
      "        mm_default_201: f32[136, 11008] = torch.ops.aten.mm.default(view_default_694, t_default_201);  view_default_694 = t_default_201 = None\n",
      "        view_default_695: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_201, [1, 136, 11008]);  mm_default_201 = None\n",
      "        mul_tensor_260: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_28, view_default_695);  silu_default_28 = view_default_695 = None\n",
      "        _param_constant261 = self._param_constant261\n",
      "        t_default_202: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant261);  _param_constant261 = None\n",
      "        view_default_696: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_260, [136, 11008]);  mul_tensor_260 = None\n",
      "        mm_default_202: f32[136, 4096] = torch.ops.aten.mm.default(view_default_696, t_default_202);  view_default_696 = t_default_202 = None\n",
      "        view_default_697: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_202, [1, 136, 4096]);  mm_default_202 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_204: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_202, view_default_697);  add_tensor_202 = view_default_697 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_58: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_204, 2)\n",
      "        mean_dim_58: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_58, [-1], True);  pow_tensor_scalar_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_205: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_58, 1e-06);  mean_dim_58 = None\n",
      "        rsqrt_default_58: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_205);  add_tensor_205 = None\n",
      "        detach_default_87: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_58)\n",
      "        mul_tensor_261: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_204, rsqrt_default_58);  rsqrt_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant262 = self._param_constant262\n",
      "        mul_tensor_262: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant262, mul_tensor_261);  _param_constant262 = mul_tensor_261 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant263 = self._param_constant263\n",
      "        t_default_203: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant263);  _param_constant263 = None\n",
      "        view_default_698: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_262, [136, 4096])\n",
      "        mm_default_203: f32[136, 4096] = torch.ops.aten.mm.default(view_default_698, t_default_203);  view_default_698 = t_default_203 = None\n",
      "        view_default_699: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_203, [1, 136, 4096]);  mm_default_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant264 = self._param_constant264\n",
      "        t_default_204: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant264);  _param_constant264 = None\n",
      "        view_default_700: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_262, [136, 4096])\n",
      "        mm_default_204: f32[136, 4096] = torch.ops.aten.mm.default(view_default_700, t_default_204);  view_default_700 = t_default_204 = None\n",
      "        view_default_701: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_204, [1, 136, 4096]);  mm_default_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant265 = self._param_constant265\n",
      "        t_default_205: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant265);  _param_constant265 = None\n",
      "        view_default_702: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_262, [136, 4096]);  mul_tensor_262 = None\n",
      "        mm_default_205: f32[136, 4096] = torch.ops.aten.mm.default(view_default_702, t_default_205);  view_default_702 = t_default_205 = None\n",
      "        view_default_703: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_205, [1, 136, 4096]);  mm_default_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_704: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_699, [1, 136, 32, 128]);  view_default_699 = None\n",
      "        transpose_int_145: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_704, 1, 2);  view_default_704 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_705: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_701, [1, 136, 32, 128]);  view_default_701 = None\n",
      "        transpose_int_146: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_705, 1, 2);  view_default_705 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_706: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_703, [1, 136, 32, 128]);  view_default_703 = None\n",
      "        transpose_int_147: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_706, 1, 2);  view_default_706 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant58 = self._tensor_constant58\n",
      "        slice_tensor_294: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant58, 0, 0, 9223372036854775807);  _tensor_constant58 = None\n",
      "        slice_tensor_295: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_294, 1, 0, 9223372036854775807);  slice_tensor_294 = None\n",
      "        slice_tensor_296: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_295, 2, 0, 136);  slice_tensor_295 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant59 = self._tensor_constant59\n",
      "        slice_tensor_297: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant59, 0, 0, 9223372036854775807);  _tensor_constant59 = None\n",
      "        slice_tensor_298: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_297, 1, 0, 9223372036854775807);  slice_tensor_297 = None\n",
      "        slice_tensor_299: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_298, 2, 0, 136);  slice_tensor_298 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_116: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_296, 1);  slice_tensor_296 = None\n",
      "        squeeze_dim_117: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_116, 0);  squeeze_dim_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_118: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_299, 1);  slice_tensor_299 = None\n",
      "        squeeze_dim_119: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_118, 0);  squeeze_dim_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_58: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_117, [view_default]);  squeeze_dim_117 = None\n",
      "        unsqueeze_default_63: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_58, 1);  index_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_59: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_119, [view_default]);  squeeze_dim_119 = None\n",
      "        unsqueeze_default_64: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_59, 1);  index_tensor_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_263: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_145, unsqueeze_default_63)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_300: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_301: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 64, 9223372036854775807);  transpose_int_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_58: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_301);  slice_tensor_301 = None\n",
      "        cat_default_58: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_58, slice_tensor_300], -1);  neg_default_58 = slice_tensor_300 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_264: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_58, unsqueeze_default_64);  cat_default_58 = None\n",
      "        add_tensor_206: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_263, mul_tensor_264);  mul_tensor_263 = mul_tensor_264 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_265: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_146, unsqueeze_default_63);  unsqueeze_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_302: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_303: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 64, 9223372036854775807);  transpose_int_146 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_59: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_303);  slice_tensor_303 = None\n",
      "        cat_default_59: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_59, slice_tensor_302], -1);  neg_default_59 = slice_tensor_302 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_266: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_59, unsqueeze_default_64);  cat_default_59 = unsqueeze_default_64 = None\n",
      "        add_tensor_207: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_265, mul_tensor_266);  mul_tensor_265 = mul_tensor_266 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_148: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_207, 2, 3)\n",
      "        expand_default_118: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_206, [1, 32, 136, 128]);  add_tensor_206 = None\n",
      "        view_default_707: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_118, [32, 136, 128]);  expand_default_118 = None\n",
      "        expand_default_119: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_148, [1, 32, 128, 136]);  transpose_int_148 = None\n",
      "        view_default_708: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_119, [32, 128, 136]);  expand_default_119 = None\n",
      "        bmm_default_58: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_707, view_default_708);  view_default_707 = view_default_708 = None\n",
      "        view_default_709: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_58, [1, 32, 136, 136]);  bmm_default_58 = None\n",
      "        div_tensor_29: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_709, 11.313708498984761);  view_default_709 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_208: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_29, add_tensor_1);  div_tensor_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_29: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_208, -1, False);  add_tensor_208 = None\n",
      "        detach_default_88: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_29)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_120: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_29, [1, 32, 136, 136]);  _softmax_default_29 = None\n",
      "        view_default_710: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_120, [32, 136, 136]);  expand_default_120 = None\n",
      "        expand_default_121: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_147, [1, 32, 136, 128])\n",
      "        view_default_711: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_121, [32, 136, 128]);  expand_default_121 = None\n",
      "        bmm_default_59: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_710, view_default_711);  view_default_710 = view_default_711 = None\n",
      "        view_default_712: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_59, [1, 32, 136, 128]);  bmm_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_149: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_712, 1, 2);  view_default_712 = None\n",
      "        clone_default_29: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_149, memory_format = torch.contiguous_format);  transpose_int_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_713: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_29, [1, 136, 4096]);  clone_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant266 = self._param_constant266\n",
      "        t_default_206: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant266);  _param_constant266 = None\n",
      "        view_default_714: f32[136, 4096] = torch.ops.aten.view.default(view_default_713, [136, 4096]);  view_default_713 = None\n",
      "        mm_default_206: f32[136, 4096] = torch.ops.aten.mm.default(view_default_714, t_default_206);  view_default_714 = t_default_206 = None\n",
      "        view_default_715: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_206, [1, 136, 4096]);  mm_default_206 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_209: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_204, view_default_715);  add_tensor_204 = view_default_715 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_59: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_209, 2)\n",
      "        mean_dim_59: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_59, [-1], True);  pow_tensor_scalar_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_210: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_59, 1e-06);  mean_dim_59 = None\n",
      "        rsqrt_default_59: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_210);  add_tensor_210 = None\n",
      "        detach_default_89: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_59)\n",
      "        mul_tensor_267: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_209, rsqrt_default_59);  rsqrt_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant267 = self._param_constant267\n",
      "        mul_tensor_268: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant267, mul_tensor_267);  _param_constant267 = mul_tensor_267 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant268 = self._param_constant268\n",
      "        t_default_207: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant268);  _param_constant268 = None\n",
      "        view_default_716: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_268, [136, 4096])\n",
      "        mm_default_207: f32[136, 11008] = torch.ops.aten.mm.default(view_default_716, t_default_207);  view_default_716 = t_default_207 = None\n",
      "        view_default_717: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_207, [1, 136, 11008]);  mm_default_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_29: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_717);  view_default_717 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant269 = self._param_constant269\n",
      "        t_default_208: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant269);  _param_constant269 = None\n",
      "        view_default_718: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_268, [136, 4096]);  mul_tensor_268 = None\n",
      "        mm_default_208: f32[136, 11008] = torch.ops.aten.mm.default(view_default_718, t_default_208);  view_default_718 = t_default_208 = None\n",
      "        view_default_719: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_208, [1, 136, 11008]);  mm_default_208 = None\n",
      "        mul_tensor_269: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_29, view_default_719);  silu_default_29 = view_default_719 = None\n",
      "        _param_constant270 = self._param_constant270\n",
      "        t_default_209: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant270);  _param_constant270 = None\n",
      "        view_default_720: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_269, [136, 11008]);  mul_tensor_269 = None\n",
      "        mm_default_209: f32[136, 4096] = torch.ops.aten.mm.default(view_default_720, t_default_209);  view_default_720 = t_default_209 = None\n",
      "        view_default_721: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_209, [1, 136, 4096]);  mm_default_209 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_211: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_209, view_default_721);  add_tensor_209 = view_default_721 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_60: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_211, 2)\n",
      "        mean_dim_60: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_60, [-1], True);  pow_tensor_scalar_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_212: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_60, 1e-06);  mean_dim_60 = None\n",
      "        rsqrt_default_60: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_212);  add_tensor_212 = None\n",
      "        detach_default_90: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_60)\n",
      "        mul_tensor_270: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_211, rsqrt_default_60);  rsqrt_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant271 = self._param_constant271\n",
      "        mul_tensor_271: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant271, mul_tensor_270);  _param_constant271 = mul_tensor_270 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant272 = self._param_constant272\n",
      "        t_default_210: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant272);  _param_constant272 = None\n",
      "        view_default_722: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_271, [136, 4096])\n",
      "        mm_default_210: f32[136, 4096] = torch.ops.aten.mm.default(view_default_722, t_default_210);  view_default_722 = t_default_210 = None\n",
      "        view_default_723: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_210, [1, 136, 4096]);  mm_default_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant273 = self._param_constant273\n",
      "        t_default_211: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant273);  _param_constant273 = None\n",
      "        view_default_724: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_271, [136, 4096])\n",
      "        mm_default_211: f32[136, 4096] = torch.ops.aten.mm.default(view_default_724, t_default_211);  view_default_724 = t_default_211 = None\n",
      "        view_default_725: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_211, [1, 136, 4096]);  mm_default_211 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant274 = self._param_constant274\n",
      "        t_default_212: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant274);  _param_constant274 = None\n",
      "        view_default_726: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_271, [136, 4096]);  mul_tensor_271 = None\n",
      "        mm_default_212: f32[136, 4096] = torch.ops.aten.mm.default(view_default_726, t_default_212);  view_default_726 = t_default_212 = None\n",
      "        view_default_727: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_212, [1, 136, 4096]);  mm_default_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_728: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_723, [1, 136, 32, 128]);  view_default_723 = None\n",
      "        transpose_int_150: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_728, 1, 2);  view_default_728 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_729: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_725, [1, 136, 32, 128]);  view_default_725 = None\n",
      "        transpose_int_151: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_729, 1, 2);  view_default_729 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_730: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_727, [1, 136, 32, 128]);  view_default_727 = None\n",
      "        transpose_int_152: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_730, 1, 2);  view_default_730 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant60 = self._tensor_constant60\n",
      "        slice_tensor_304: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant60, 0, 0, 9223372036854775807);  _tensor_constant60 = None\n",
      "        slice_tensor_305: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_304, 1, 0, 9223372036854775807);  slice_tensor_304 = None\n",
      "        slice_tensor_306: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_305, 2, 0, 136);  slice_tensor_305 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant61 = self._tensor_constant61\n",
      "        slice_tensor_307: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant61, 0, 0, 9223372036854775807);  _tensor_constant61 = None\n",
      "        slice_tensor_308: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_307, 1, 0, 9223372036854775807);  slice_tensor_307 = None\n",
      "        slice_tensor_309: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_308, 2, 0, 136);  slice_tensor_308 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_120: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_306, 1);  slice_tensor_306 = None\n",
      "        squeeze_dim_121: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_120, 0);  squeeze_dim_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_122: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_309, 1);  slice_tensor_309 = None\n",
      "        squeeze_dim_123: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_122, 0);  squeeze_dim_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_60: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_121, [view_default]);  squeeze_dim_121 = None\n",
      "        unsqueeze_default_65: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_60, 1);  index_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_61: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_123, [view_default]);  squeeze_dim_123 = None\n",
      "        unsqueeze_default_66: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_61, 1);  index_tensor_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_272: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_150, unsqueeze_default_65)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_310: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_311: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 64, 9223372036854775807);  transpose_int_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_60: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_311);  slice_tensor_311 = None\n",
      "        cat_default_60: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_60, slice_tensor_310], -1);  neg_default_60 = slice_tensor_310 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_273: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_60, unsqueeze_default_66);  cat_default_60 = None\n",
      "        add_tensor_213: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_272, mul_tensor_273);  mul_tensor_272 = mul_tensor_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_274: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_151, unsqueeze_default_65);  unsqueeze_default_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_312: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_313: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 64, 9223372036854775807);  transpose_int_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_61: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_313);  slice_tensor_313 = None\n",
      "        cat_default_61: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_61, slice_tensor_312], -1);  neg_default_61 = slice_tensor_312 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_275: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_61, unsqueeze_default_66);  cat_default_61 = unsqueeze_default_66 = None\n",
      "        add_tensor_214: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_274, mul_tensor_275);  mul_tensor_274 = mul_tensor_275 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_153: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_214, 2, 3)\n",
      "        expand_default_122: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_213, [1, 32, 136, 128]);  add_tensor_213 = None\n",
      "        view_default_731: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_122, [32, 136, 128]);  expand_default_122 = None\n",
      "        expand_default_123: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_153, [1, 32, 128, 136]);  transpose_int_153 = None\n",
      "        view_default_732: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_123, [32, 128, 136]);  expand_default_123 = None\n",
      "        bmm_default_60: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_731, view_default_732);  view_default_731 = view_default_732 = None\n",
      "        view_default_733: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_60, [1, 32, 136, 136]);  bmm_default_60 = None\n",
      "        div_tensor_30: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_733, 11.313708498984761);  view_default_733 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_215: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_30, add_tensor_1);  div_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_30: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_215, -1, False);  add_tensor_215 = None\n",
      "        detach_default_91: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_30)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_124: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_30, [1, 32, 136, 136]);  _softmax_default_30 = None\n",
      "        view_default_734: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_124, [32, 136, 136]);  expand_default_124 = None\n",
      "        expand_default_125: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_152, [1, 32, 136, 128])\n",
      "        view_default_735: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_125, [32, 136, 128]);  expand_default_125 = None\n",
      "        bmm_default_61: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_734, view_default_735);  view_default_734 = view_default_735 = None\n",
      "        view_default_736: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_61, [1, 32, 136, 128]);  bmm_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_154: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_736, 1, 2);  view_default_736 = None\n",
      "        clone_default_30: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_154, memory_format = torch.contiguous_format);  transpose_int_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_737: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_30, [1, 136, 4096]);  clone_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant275 = self._param_constant275\n",
      "        t_default_213: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant275);  _param_constant275 = None\n",
      "        view_default_738: f32[136, 4096] = torch.ops.aten.view.default(view_default_737, [136, 4096]);  view_default_737 = None\n",
      "        mm_default_213: f32[136, 4096] = torch.ops.aten.mm.default(view_default_738, t_default_213);  view_default_738 = t_default_213 = None\n",
      "        view_default_739: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_213, [1, 136, 4096]);  mm_default_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_216: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_211, view_default_739);  add_tensor_211 = view_default_739 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_61: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_216, 2)\n",
      "        mean_dim_61: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_61, [-1], True);  pow_tensor_scalar_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_217: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_61, 1e-06);  mean_dim_61 = None\n",
      "        rsqrt_default_61: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_217);  add_tensor_217 = None\n",
      "        detach_default_92: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_61)\n",
      "        mul_tensor_276: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_216, rsqrt_default_61);  rsqrt_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant276 = self._param_constant276\n",
      "        mul_tensor_277: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant276, mul_tensor_276);  _param_constant276 = mul_tensor_276 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant277 = self._param_constant277\n",
      "        t_default_214: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant277);  _param_constant277 = None\n",
      "        view_default_740: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_277, [136, 4096])\n",
      "        mm_default_214: f32[136, 11008] = torch.ops.aten.mm.default(view_default_740, t_default_214);  view_default_740 = t_default_214 = None\n",
      "        view_default_741: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_214, [1, 136, 11008]);  mm_default_214 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_30: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_741);  view_default_741 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant278 = self._param_constant278\n",
      "        t_default_215: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant278);  _param_constant278 = None\n",
      "        view_default_742: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_277, [136, 4096]);  mul_tensor_277 = None\n",
      "        mm_default_215: f32[136, 11008] = torch.ops.aten.mm.default(view_default_742, t_default_215);  view_default_742 = t_default_215 = None\n",
      "        view_default_743: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_215, [1, 136, 11008]);  mm_default_215 = None\n",
      "        mul_tensor_278: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_30, view_default_743);  silu_default_30 = view_default_743 = None\n",
      "        _param_constant279 = self._param_constant279\n",
      "        t_default_216: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant279);  _param_constant279 = None\n",
      "        view_default_744: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_278, [136, 11008]);  mul_tensor_278 = None\n",
      "        mm_default_216: f32[136, 4096] = torch.ops.aten.mm.default(view_default_744, t_default_216);  view_default_744 = t_default_216 = None\n",
      "        view_default_745: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_216, [1, 136, 4096]);  mm_default_216 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_218: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_216, view_default_745);  add_tensor_216 = view_default_745 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_62: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_218, 2)\n",
      "        mean_dim_62: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_62, [-1], True);  pow_tensor_scalar_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_219: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_62, 1e-06);  mean_dim_62 = None\n",
      "        rsqrt_default_62: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_219);  add_tensor_219 = None\n",
      "        detach_default_93: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_62)\n",
      "        mul_tensor_279: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_218, rsqrt_default_62);  rsqrt_default_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant280 = self._param_constant280\n",
      "        mul_tensor_280: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant280, mul_tensor_279);  _param_constant280 = mul_tensor_279 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant281 = self._param_constant281\n",
      "        t_default_217: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant281);  _param_constant281 = None\n",
      "        view_default_746: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_280, [136, 4096])\n",
      "        mm_default_217: f32[136, 4096] = torch.ops.aten.mm.default(view_default_746, t_default_217);  view_default_746 = t_default_217 = None\n",
      "        view_default_747: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_217, [1, 136, 4096]);  mm_default_217 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant282 = self._param_constant282\n",
      "        t_default_218: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant282);  _param_constant282 = None\n",
      "        view_default_748: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_280, [136, 4096])\n",
      "        mm_default_218: f32[136, 4096] = torch.ops.aten.mm.default(view_default_748, t_default_218);  view_default_748 = t_default_218 = None\n",
      "        view_default_749: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_218, [1, 136, 4096]);  mm_default_218 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant283 = self._param_constant283\n",
      "        t_default_219: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant283);  _param_constant283 = None\n",
      "        view_default_750: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_280, [136, 4096]);  mul_tensor_280 = None\n",
      "        mm_default_219: f32[136, 4096] = torch.ops.aten.mm.default(view_default_750, t_default_219);  view_default_750 = t_default_219 = None\n",
      "        view_default_751: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_219, [1, 136, 4096]);  mm_default_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_752: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_747, [1, 136, 32, 128]);  view_default_747 = None\n",
      "        transpose_int_155: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_752, 1, 2);  view_default_752 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_753: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_749, [1, 136, 32, 128]);  view_default_749 = None\n",
      "        transpose_int_156: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_753, 1, 2);  view_default_753 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_754: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_751, [1, 136, 32, 128]);  view_default_751 = None\n",
      "        transpose_int_157: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_754, 1, 2);  view_default_754 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant62 = self._tensor_constant62\n",
      "        slice_tensor_314: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant62, 0, 0, 9223372036854775807);  _tensor_constant62 = None\n",
      "        slice_tensor_315: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_314, 1, 0, 9223372036854775807);  slice_tensor_314 = None\n",
      "        slice_tensor_316: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_315, 2, 0, 136);  slice_tensor_315 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant63 = self._tensor_constant63\n",
      "        slice_tensor_317: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant63, 0, 0, 9223372036854775807);  _tensor_constant63 = None\n",
      "        slice_tensor_318: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_317, 1, 0, 9223372036854775807);  slice_tensor_317 = None\n",
      "        slice_tensor_319: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_318, 2, 0, 136);  slice_tensor_318 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_124: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_316, 1);  slice_tensor_316 = None\n",
      "        squeeze_dim_125: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_124, 0);  squeeze_dim_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_126: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_319, 1);  slice_tensor_319 = None\n",
      "        squeeze_dim_127: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_126, 0);  squeeze_dim_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_62: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_125, [view_default]);  squeeze_dim_125 = None\n",
      "        unsqueeze_default_67: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_62, 1);  index_tensor_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_63: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_127, [view_default]);  squeeze_dim_127 = view_default = None\n",
      "        unsqueeze_default_68: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_63, 1);  index_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_281: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_155, unsqueeze_default_67)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_320: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_321: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 64, 9223372036854775807);  transpose_int_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_62: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_321);  slice_tensor_321 = None\n",
      "        cat_default_62: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_62, slice_tensor_320], -1);  neg_default_62 = slice_tensor_320 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_282: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_62, unsqueeze_default_68);  cat_default_62 = None\n",
      "        add_tensor_220: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_281, mul_tensor_282);  mul_tensor_281 = mul_tensor_282 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_283: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_156, unsqueeze_default_67);  unsqueeze_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_322: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_323: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 64, 9223372036854775807);  transpose_int_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_63: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_323);  slice_tensor_323 = None\n",
      "        cat_default_63: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_63, slice_tensor_322], -1);  neg_default_63 = slice_tensor_322 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_284: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_63, unsqueeze_default_68);  cat_default_63 = unsqueeze_default_68 = None\n",
      "        add_tensor_221: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_283, mul_tensor_284);  mul_tensor_283 = mul_tensor_284 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_158: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_221, 2, 3)\n",
      "        expand_default_126: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_220, [1, 32, 136, 128]);  add_tensor_220 = None\n",
      "        view_default_755: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_126, [32, 136, 128]);  expand_default_126 = None\n",
      "        expand_default_127: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_158, [1, 32, 128, 136]);  transpose_int_158 = None\n",
      "        view_default_756: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_127, [32, 128, 136]);  expand_default_127 = None\n",
      "        bmm_default_62: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_755, view_default_756);  view_default_755 = view_default_756 = None\n",
      "        view_default_757: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_62, [1, 32, 136, 136]);  bmm_default_62 = None\n",
      "        div_tensor_31: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_757, 11.313708498984761);  view_default_757 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_222: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_31, add_tensor_1);  div_tensor_31 = add_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_31: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_222, -1, False);  add_tensor_222 = None\n",
      "        detach_default_94: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_31)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_128: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_31, [1, 32, 136, 136]);  _softmax_default_31 = None\n",
      "        view_default_758: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_128, [32, 136, 136]);  expand_default_128 = None\n",
      "        expand_default_129: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_157, [1, 32, 136, 128])\n",
      "        view_default_759: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_129, [32, 136, 128]);  expand_default_129 = None\n",
      "        bmm_default_63: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_758, view_default_759);  view_default_758 = view_default_759 = None\n",
      "        view_default_760: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_63, [1, 32, 136, 128]);  bmm_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_159: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_760, 1, 2);  view_default_760 = None\n",
      "        clone_default_31: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_159, memory_format = torch.contiguous_format);  transpose_int_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_761: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_31, [1, 136, 4096]);  clone_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant284 = self._param_constant284\n",
      "        t_default_220: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant284);  _param_constant284 = None\n",
      "        view_default_762: f32[136, 4096] = torch.ops.aten.view.default(view_default_761, [136, 4096]);  view_default_761 = None\n",
      "        mm_default_220: f32[136, 4096] = torch.ops.aten.mm.default(view_default_762, t_default_220);  view_default_762 = t_default_220 = None\n",
      "        view_default_763: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_220, [1, 136, 4096]);  mm_default_220 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_223: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_218, view_default_763);  add_tensor_218 = view_default_763 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_63: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_223, 2)\n",
      "        mean_dim_63: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_63, [-1], True);  pow_tensor_scalar_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_224: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_63, 1e-06);  mean_dim_63 = None\n",
      "        rsqrt_default_63: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_224);  add_tensor_224 = None\n",
      "        detach_default_95: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_63)\n",
      "        mul_tensor_285: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_223, rsqrt_default_63);  rsqrt_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant285 = self._param_constant285\n",
      "        mul_tensor_286: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant285, mul_tensor_285);  _param_constant285 = mul_tensor_285 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant286 = self._param_constant286\n",
      "        t_default_221: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant286);  _param_constant286 = None\n",
      "        view_default_764: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_286, [136, 4096])\n",
      "        mm_default_221: f32[136, 11008] = torch.ops.aten.mm.default(view_default_764, t_default_221);  view_default_764 = t_default_221 = None\n",
      "        view_default_765: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_221, [1, 136, 11008]);  mm_default_221 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_31: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_765);  view_default_765 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant287 = self._param_constant287\n",
      "        t_default_222: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant287);  _param_constant287 = None\n",
      "        view_default_766: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_286, [136, 4096]);  mul_tensor_286 = None\n",
      "        mm_default_222: f32[136, 11008] = torch.ops.aten.mm.default(view_default_766, t_default_222);  view_default_766 = t_default_222 = None\n",
      "        view_default_767: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_222, [1, 136, 11008]);  mm_default_222 = None\n",
      "        mul_tensor_287: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_31, view_default_767);  silu_default_31 = view_default_767 = None\n",
      "        _param_constant288 = self._param_constant288\n",
      "        t_default_223: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant288);  _param_constant288 = None\n",
      "        view_default_768: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_287, [136, 11008]);  mul_tensor_287 = None\n",
      "        mm_default_223: f32[136, 4096] = torch.ops.aten.mm.default(view_default_768, t_default_223);  view_default_768 = t_default_223 = None\n",
      "        view_default_769: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_223, [1, 136, 4096]);  mm_default_223 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_225: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_223, view_default_769);  add_tensor_223 = view_default_769 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_64: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_225, 2)\n",
      "        mean_dim_64: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_64, [-1], True);  pow_tensor_scalar_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_226: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_64, 1e-06);  mean_dim_64 = None\n",
      "        rsqrt_default_64: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_226);  add_tensor_226 = None\n",
      "        detach_default_96: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_64)\n",
      "        mul_tensor_288: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_225, rsqrt_default_64);  add_tensor_225 = rsqrt_default_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant289 = self._param_constant289\n",
      "        mul_tensor_289: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant289, mul_tensor_288);  _param_constant289 = mul_tensor_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:838, code: logits = self.lm_head(hidden_states)\n",
      "        _param_constant290 = self._param_constant290\n",
      "        t_default_224: f32[4096, 32000] = torch.ops.aten.t.default(_param_constant290);  _param_constant290 = None\n",
      "        view_default_770: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_289, [136, 4096]);  mul_tensor_289 = None\n",
      "        mm_default_224: f32[136, 32000] = torch.ops.aten.mm.default(view_default_770, t_default_224);  view_default_770 = t_default_224 = None\n",
      "        view_default_771: f32[1, 136, 32000] = torch.ops.aten.view.default(mm_default_224, [1, 136, 32000]);  mm_default_224 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_2064425/3922580714.py:36, code: token1 = torch.argmax(result.logits[:, -1, :], dim=1)\n",
      "        slice_tensor_324: f32[1, 136, 32000] = torch.ops.aten.slice.Tensor(view_default_771, 0, 0, 9223372036854775807);  view_default_771 = None\n",
      "        select_int: f32[1, 32000] = torch.ops.aten.select.int(slice_tensor_324, 1, -1);  slice_tensor_324 = None\n",
      "        slice_tensor_325: f32[1, 32000] = torch.ops.aten.slice.Tensor(select_int, 1, 0, 9223372036854775807);  select_int = None\n",
      "        argmax_default: i64[1] = torch.ops.aten.argmax.default(slice_tensor_325, 1);  slice_tensor_325 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_2064425/3922580714.py:37, code: token1 = token1[None, :]\n",
      "        unsqueeze_default_69: i64[1, 1] = torch.ops.aten.unsqueeze.default(argmax_default, 0);  argmax_default = None\n",
      "        slice_tensor_326: i64[1, 1] = torch.ops.aten.slice.Tensor(unsqueeze_default_69, 1, 0, 9223372036854775807);  unsqueeze_default_69 = None\n",
      "        return pytree.tree_unflatten([slice_tensor_326, add_tensor_4, transpose_int_2, add_tensor_11, transpose_int_7, add_tensor_18, transpose_int_12, add_tensor_25, transpose_int_17, add_tensor_32, transpose_int_22, add_tensor_39, transpose_int_27, add_tensor_46, transpose_int_32, add_tensor_53, transpose_int_37, add_tensor_60, transpose_int_42, add_tensor_67, transpose_int_47, add_tensor_74, transpose_int_52, add_tensor_81, transpose_int_57, add_tensor_88, transpose_int_62, add_tensor_95, transpose_int_67, add_tensor_102, transpose_int_72, add_tensor_109, transpose_int_77, add_tensor_116, transpose_int_82, add_tensor_123, transpose_int_87, add_tensor_130, transpose_int_92, add_tensor_137, transpose_int_97, add_tensor_144, transpose_int_102, add_tensor_151, transpose_int_107, add_tensor_158, transpose_int_112, add_tensor_165, transpose_int_117, add_tensor_172, transpose_int_122, add_tensor_179, transpose_int_127, add_tensor_186, transpose_int_132, add_tensor_193, transpose_int_137, add_tensor_200, transpose_int_142, add_tensor_207, transpose_int_147, add_tensor_214, transpose_int_152, add_tensor_221, transpose_int_157], self._out_spec)\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"class GraphModule(torch.nn.Module):\\n    def forward(self, input_ids):\\n        arg0: i64[1, 136], = fx_pytree.tree_flatten_spec(([input_ids], {}), self._in_spec)\\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:654, code: position_ids = torch.arange(\\n        arange_start: i64[136] = torch.ops.aten.arange.start(0, 136, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:657, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\\n        unsqueeze_default: i64[1, 136] = torch.ops.aten.unsqueeze.default(arange_start, 0);  arange_start = None\\n        view_default: i64[1, 136] = torch.ops.aten.view.default(unsqueeze_default, [-1, 136]);  unsqueeze_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:662, code: inputs_embeds = self.embed_tokens(input_ids)\\n        _param_constant0 = self._param_constant0\\n        embedding_default: f32[1, 136, 4096] = torch.ops.aten.embedding.default(_param_constant0, arg0);  _param_constant0 = arg0 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:665, code: attention_mask = torch.ones(\\n        ones_default: b8[1, 136] = torch.ops.aten.ones.default([1, 136], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:50, code: mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\\n        full_default: f32[136, 136] = torch.ops.aten.full.default([136, 136], -3.4028234663852886e+38, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:51, code: mask_cond = torch.arange(mask.size(-1), device=device)\\n        arange_default: i64[136] = torch.ops.aten.arange.default(136, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:52, code: mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\\n        add_tensor: i64[136] = torch.ops.aten.add.Tensor(arange_default, 1)\\n        view_default_1: i64[136, 1] = torch.ops.aten.view.default(add_tensor, [136, 1]);  add_tensor = None\\n        lt_tensor: b8[136, 136] = torch.ops.aten.lt.Tensor(arange_default, view_default_1);  arange_default = view_default_1 = None\\n        masked_fill__scalar: f32[136, 136] = torch.ops.aten.masked_fill_.Scalar(full_default, lt_tensor, 0);  full_default = lt_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:57, code: return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\\n        unsqueeze_default_1: f32[1, 136, 136] = torch.ops.aten.unsqueeze.default(masked_fill__scalar, 0);  masked_fill__scalar = None\\n        unsqueeze_default_2: f32[1, 1, 136, 136] = torch.ops.aten.unsqueeze.default(unsqueeze_default_1, 1);  unsqueeze_default_1 = None\\n        slice_tensor: f32[1, 1, 136, 136] = torch.ops.aten.slice.Tensor(unsqueeze_default_2, 2, 0, 9223372036854775807);  unsqueeze_default_2 = None\\n        slice_tensor_1: f32[1, 1, 136, 136] = torch.ops.aten.slice.Tensor(slice_tensor, 3, 0, 9223372036854775807);  slice_tensor = None\\n        expand_default: f32[1, 1, 136, 136] = torch.ops.aten.expand.default(slice_tensor_1, [1, 1, 136, 136]);  slice_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:68, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\\n        slice_tensor_2: b8[1, 136] = torch.ops.aten.slice.Tensor(ones_default, 0, 0, 9223372036854775807);  ones_default = None\\n        unsqueeze_default_3: b8[1, 1, 136] = torch.ops.aten.unsqueeze.default(slice_tensor_2, 1);  slice_tensor_2 = None\\n        unsqueeze_default_4: b8[1, 1, 1, 136] = torch.ops.aten.unsqueeze.default(unsqueeze_default_3, 2);  unsqueeze_default_3 = None\\n        slice_tensor_3: b8[1, 1, 1, 136] = torch.ops.aten.slice.Tensor(unsqueeze_default_4, 3, 0, 9223372036854775807);  unsqueeze_default_4 = None\\n        expand_default_1: b8[1, 1, 136, 136] = torch.ops.aten.expand.default(slice_tensor_3, [1, 1, 136, 136]);  slice_tensor_3 = None\\n        _to_copy_default: f32[1, 1, 136, 136] = torch.ops.aten._to_copy.default(expand_default_1, dtype = torch.float32);  expand_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70, code: inverted_mask = 1.0 - expanded_mask\\n        rsub_scalar: f32[1, 1, 136, 136] = torch.ops.aten.rsub.Scalar(_to_copy_default, 1.0);  _to_copy_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\\n        _to_copy_default_1: b8[1, 1, 136, 136] = torch.ops.aten._to_copy.default(rsub_scalar, dtype = torch.bool)\\n        masked_fill_scalar: f32[1, 1, 136, 136] = torch.ops.aten.masked_fill.Scalar(rsub_scalar, _to_copy_default_1, -3.4028234663852886e+38);  rsub_scalar = _to_copy_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:609, code: expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\\n        add_tensor_1: f32[1, 1, 136, 136] = torch.ops.aten.add.Tensor(masked_fill_scalar, expand_default);  masked_fill_scalar = expand_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(embedding_default, 2)\\n        mean_dim: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar, [-1], True);  pow_tensor_scalar = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_2: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim, 1e-06);  mean_dim = None\\n        rsqrt_default: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_2);  add_tensor_2 = None\\n        detach_default: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default)\\n        mul_tensor: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(embedding_default, rsqrt_default);  rsqrt_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant1 = self._param_constant1\\n        mul_tensor_1: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant1, mul_tensor);  _param_constant1 = mul_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant2 = self._param_constant2\\n        t_default: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant2);  _param_constant2 = None\\n        view_default_2: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_1, [136, 4096])\\n        mm_default: f32[136, 4096] = torch.ops.aten.mm.default(view_default_2, t_default);  view_default_2 = t_default = None\\n        view_default_3: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default, [1, 136, 4096]);  mm_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant3 = self._param_constant3\\n        t_default_1: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant3);  _param_constant3 = None\\n        view_default_4: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_1, [136, 4096])\\n        mm_default_1: f32[136, 4096] = torch.ops.aten.mm.default(view_default_4, t_default_1);  view_default_4 = t_default_1 = None\\n        view_default_5: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_1, [1, 136, 4096]);  mm_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant4 = self._param_constant4\\n        t_default_2: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant4);  _param_constant4 = None\\n        view_default_6: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_1, [136, 4096]);  mul_tensor_1 = None\\n        mm_default_2: f32[136, 4096] = torch.ops.aten.mm.default(view_default_6, t_default_2);  view_default_6 = t_default_2 = None\\n        view_default_7: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_2, [1, 136, 4096]);  mm_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_8: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_3, [1, 136, 32, 128]);  view_default_3 = None\\n        transpose_int: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_8, 1, 2);  view_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_9: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_5, [1, 136, 32, 128]);  view_default_5 = None\\n        transpose_int_1: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_9, 1, 2);  view_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_10: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_7, [1, 136, 32, 128]);  view_default_7 = None\\n        transpose_int_2: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_10, 1, 2);  view_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant0 = self._tensor_constant0\\n        slice_tensor_4: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant0, 0, 0, 9223372036854775807);  _tensor_constant0 = None\\n        slice_tensor_5: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_4, 1, 0, 9223372036854775807);  slice_tensor_4 = None\\n        slice_tensor_6: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_5, 2, 0, 136);  slice_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant1 = self._tensor_constant1\\n        slice_tensor_7: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant1, 0, 0, 9223372036854775807);  _tensor_constant1 = None\\n        slice_tensor_8: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_7, 1, 0, 9223372036854775807);  slice_tensor_7 = None\\n        slice_tensor_9: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_8, 2, 0, 136);  slice_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_6, 1);  slice_tensor_6 = None\\n        squeeze_dim_1: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim, 0);  squeeze_dim = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_2: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_9, 1);  slice_tensor_9 = None\\n        squeeze_dim_3: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_2, 0);  squeeze_dim_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_1, [view_default]);  squeeze_dim_1 = None\\n        unsqueeze_default_5: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor, 1);  index_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_1: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_3, [view_default]);  squeeze_dim_3 = None\\n        unsqueeze_default_6: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_1, 1);  index_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_2: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int, unsqueeze_default_5)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_10: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_11: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 64, 9223372036854775807);  transpose_int = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_11);  slice_tensor_11 = None\\n        cat_default: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default, slice_tensor_10], -1);  neg_default = slice_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_3: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default, unsqueeze_default_6);  cat_default = None\\n        add_tensor_3: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_2, mul_tensor_3);  mul_tensor_2 = mul_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_4: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_1, unsqueeze_default_5);  unsqueeze_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_12: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_13: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 64, 9223372036854775807);  transpose_int_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_1: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_13);  slice_tensor_13 = None\\n        cat_default_1: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_1, slice_tensor_12], -1);  neg_default_1 = slice_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_5: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_1, unsqueeze_default_6);  cat_default_1 = unsqueeze_default_6 = None\\n        add_tensor_4: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_4, mul_tensor_5);  mul_tensor_4 = mul_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_3: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_4, 2, 3)\\n        expand_default_2: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_3, [1, 32, 136, 128]);  add_tensor_3 = None\\n        view_default_11: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_2, [32, 136, 128]);  expand_default_2 = None\\n        expand_default_3: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_3, [1, 32, 128, 136]);  transpose_int_3 = None\\n        view_default_12: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_3, [32, 128, 136]);  expand_default_3 = None\\n        bmm_default: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_11, view_default_12);  view_default_11 = view_default_12 = None\\n        view_default_13: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default, [1, 32, 136, 136]);  bmm_default = None\\n        div_tensor: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_13, 11.313708498984761);  view_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_5: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor, add_tensor_1);  div_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_5, -1, False);  add_tensor_5 = None\\n        detach_default_1: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_4: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default, [1, 32, 136, 136]);  _softmax_default = None\\n        view_default_14: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_4, [32, 136, 136]);  expand_default_4 = None\\n        expand_default_5: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_2, [1, 32, 136, 128])\\n        view_default_15: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_5, [32, 136, 128]);  expand_default_5 = None\\n        bmm_default_1: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_14, view_default_15);  view_default_14 = view_default_15 = None\\n        view_default_16: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_1, [1, 32, 136, 128]);  bmm_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_4: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_16, 1, 2);  view_default_16 = None\\n        clone_default: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_4, memory_format = torch.contiguous_format);  transpose_int_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_17: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default, [1, 136, 4096]);  clone_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant5 = self._param_constant5\\n        t_default_3: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant5);  _param_constant5 = None\\n        view_default_18: f32[136, 4096] = torch.ops.aten.view.default(view_default_17, [136, 4096]);  view_default_17 = None\\n        mm_default_3: f32[136, 4096] = torch.ops.aten.mm.default(view_default_18, t_default_3);  view_default_18 = t_default_3 = None\\n        view_default_19: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_3, [1, 136, 4096]);  mm_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_6: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(embedding_default, view_default_19);  embedding_default = view_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_1: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_6, 2)\\n        mean_dim_1: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_1, [-1], True);  pow_tensor_scalar_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_7: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_1, 1e-06);  mean_dim_1 = None\\n        rsqrt_default_1: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_7);  add_tensor_7 = None\\n        detach_default_2: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_1)\\n        mul_tensor_6: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_6, rsqrt_default_1);  rsqrt_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant6 = self._param_constant6\\n        mul_tensor_7: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant6, mul_tensor_6);  _param_constant6 = mul_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant7 = self._param_constant7\\n        t_default_4: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant7);  _param_constant7 = None\\n        view_default_20: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_7, [136, 4096])\\n        mm_default_4: f32[136, 11008] = torch.ops.aten.mm.default(view_default_20, t_default_4);  view_default_20 = t_default_4 = None\\n        view_default_21: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_4, [1, 136, 11008]);  mm_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_21);  view_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant8 = self._param_constant8\\n        t_default_5: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant8);  _param_constant8 = None\\n        view_default_22: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_7, [136, 4096]);  mul_tensor_7 = None\\n        mm_default_5: f32[136, 11008] = torch.ops.aten.mm.default(view_default_22, t_default_5);  view_default_22 = t_default_5 = None\\n        view_default_23: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_5, [1, 136, 11008]);  mm_default_5 = None\\n        mul_tensor_8: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default, view_default_23);  silu_default = view_default_23 = None\\n        _param_constant9 = self._param_constant9\\n        t_default_6: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant9);  _param_constant9 = None\\n        view_default_24: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_8, [136, 11008]);  mul_tensor_8 = None\\n        mm_default_6: f32[136, 4096] = torch.ops.aten.mm.default(view_default_24, t_default_6);  view_default_24 = t_default_6 = None\\n        view_default_25: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_6, [1, 136, 4096]);  mm_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_8: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_6, view_default_25);  add_tensor_6 = view_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_2: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_8, 2)\\n        mean_dim_2: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_2, [-1], True);  pow_tensor_scalar_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_9: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_2, 1e-06);  mean_dim_2 = None\\n        rsqrt_default_2: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_9);  add_tensor_9 = None\\n        detach_default_3: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_2)\\n        mul_tensor_9: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_8, rsqrt_default_2);  rsqrt_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant10 = self._param_constant10\\n        mul_tensor_10: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant10, mul_tensor_9);  _param_constant10 = mul_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant11 = self._param_constant11\\n        t_default_7: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant11);  _param_constant11 = None\\n        view_default_26: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_10, [136, 4096])\\n        mm_default_7: f32[136, 4096] = torch.ops.aten.mm.default(view_default_26, t_default_7);  view_default_26 = t_default_7 = None\\n        view_default_27: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_7, [1, 136, 4096]);  mm_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant12 = self._param_constant12\\n        t_default_8: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant12);  _param_constant12 = None\\n        view_default_28: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_10, [136, 4096])\\n        mm_default_8: f32[136, 4096] = torch.ops.aten.mm.default(view_default_28, t_default_8);  view_default_28 = t_default_8 = None\\n        view_default_29: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_8, [1, 136, 4096]);  mm_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant13 = self._param_constant13\\n        t_default_9: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant13);  _param_constant13 = None\\n        view_default_30: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_10, [136, 4096]);  mul_tensor_10 = None\\n        mm_default_9: f32[136, 4096] = torch.ops.aten.mm.default(view_default_30, t_default_9);  view_default_30 = t_default_9 = None\\n        view_default_31: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_9, [1, 136, 4096]);  mm_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_32: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_27, [1, 136, 32, 128]);  view_default_27 = None\\n        transpose_int_5: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_32, 1, 2);  view_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_33: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_29, [1, 136, 32, 128]);  view_default_29 = None\\n        transpose_int_6: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_33, 1, 2);  view_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_34: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_31, [1, 136, 32, 128]);  view_default_31 = None\\n        transpose_int_7: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_34, 1, 2);  view_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant2 = self._tensor_constant2\\n        slice_tensor_14: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant2, 0, 0, 9223372036854775807);  _tensor_constant2 = None\\n        slice_tensor_15: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_14, 1, 0, 9223372036854775807);  slice_tensor_14 = None\\n        slice_tensor_16: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_15, 2, 0, 136);  slice_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant3 = self._tensor_constant3\\n        slice_tensor_17: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant3, 0, 0, 9223372036854775807);  _tensor_constant3 = None\\n        slice_tensor_18: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_17, 1, 0, 9223372036854775807);  slice_tensor_17 = None\\n        slice_tensor_19: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_18, 2, 0, 136);  slice_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_4: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_16, 1);  slice_tensor_16 = None\\n        squeeze_dim_5: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_4, 0);  squeeze_dim_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_6: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_19, 1);  slice_tensor_19 = None\\n        squeeze_dim_7: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_6, 0);  squeeze_dim_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_2: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_5, [view_default]);  squeeze_dim_5 = None\\n        unsqueeze_default_7: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_2, 1);  index_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_3: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_7, [view_default]);  squeeze_dim_7 = None\\n        unsqueeze_default_8: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_3, 1);  index_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_11: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_5, unsqueeze_default_7)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_20: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_21: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 64, 9223372036854775807);  transpose_int_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_2: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_21);  slice_tensor_21 = None\\n        cat_default_2: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_2, slice_tensor_20], -1);  neg_default_2 = slice_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_12: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_2, unsqueeze_default_8);  cat_default_2 = None\\n        add_tensor_10: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_11, mul_tensor_12);  mul_tensor_11 = mul_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_13: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_6, unsqueeze_default_7);  unsqueeze_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_22: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_23: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 64, 9223372036854775807);  transpose_int_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_3: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_23);  slice_tensor_23 = None\\n        cat_default_3: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_3, slice_tensor_22], -1);  neg_default_3 = slice_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_14: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_3, unsqueeze_default_8);  cat_default_3 = unsqueeze_default_8 = None\\n        add_tensor_11: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_13, mul_tensor_14);  mul_tensor_13 = mul_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_8: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_11, 2, 3)\\n        expand_default_6: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_10, [1, 32, 136, 128]);  add_tensor_10 = None\\n        view_default_35: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_6, [32, 136, 128]);  expand_default_6 = None\\n        expand_default_7: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_8, [1, 32, 128, 136]);  transpose_int_8 = None\\n        view_default_36: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_7, [32, 128, 136]);  expand_default_7 = None\\n        bmm_default_2: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_35, view_default_36);  view_default_35 = view_default_36 = None\\n        view_default_37: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_2, [1, 32, 136, 136]);  bmm_default_2 = None\\n        div_tensor_1: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_37, 11.313708498984761);  view_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_12: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_1, add_tensor_1);  div_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_1: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_12, -1, False);  add_tensor_12 = None\\n        detach_default_4: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_1)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_8: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_1, [1, 32, 136, 136]);  _softmax_default_1 = None\\n        view_default_38: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_8, [32, 136, 136]);  expand_default_8 = None\\n        expand_default_9: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_7, [1, 32, 136, 128])\\n        view_default_39: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_9, [32, 136, 128]);  expand_default_9 = None\\n        bmm_default_3: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_38, view_default_39);  view_default_38 = view_default_39 = None\\n        view_default_40: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_3, [1, 32, 136, 128]);  bmm_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_9: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_40, 1, 2);  view_default_40 = None\\n        clone_default_1: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_9, memory_format = torch.contiguous_format);  transpose_int_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_41: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_1, [1, 136, 4096]);  clone_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant14 = self._param_constant14\\n        t_default_10: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant14);  _param_constant14 = None\\n        view_default_42: f32[136, 4096] = torch.ops.aten.view.default(view_default_41, [136, 4096]);  view_default_41 = None\\n        mm_default_10: f32[136, 4096] = torch.ops.aten.mm.default(view_default_42, t_default_10);  view_default_42 = t_default_10 = None\\n        view_default_43: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_10, [1, 136, 4096]);  mm_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_13: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_8, view_default_43);  add_tensor_8 = view_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_3: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_13, 2)\\n        mean_dim_3: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_3, [-1], True);  pow_tensor_scalar_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_14: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_3, 1e-06);  mean_dim_3 = None\\n        rsqrt_default_3: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_14);  add_tensor_14 = None\\n        detach_default_5: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_3)\\n        mul_tensor_15: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_13, rsqrt_default_3);  rsqrt_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant15 = self._param_constant15\\n        mul_tensor_16: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant15, mul_tensor_15);  _param_constant15 = mul_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant16 = self._param_constant16\\n        t_default_11: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant16);  _param_constant16 = None\\n        view_default_44: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_16, [136, 4096])\\n        mm_default_11: f32[136, 11008] = torch.ops.aten.mm.default(view_default_44, t_default_11);  view_default_44 = t_default_11 = None\\n        view_default_45: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_11, [1, 136, 11008]);  mm_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_1: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_45);  view_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant17 = self._param_constant17\\n        t_default_12: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant17);  _param_constant17 = None\\n        view_default_46: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_16, [136, 4096]);  mul_tensor_16 = None\\n        mm_default_12: f32[136, 11008] = torch.ops.aten.mm.default(view_default_46, t_default_12);  view_default_46 = t_default_12 = None\\n        view_default_47: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_12, [1, 136, 11008]);  mm_default_12 = None\\n        mul_tensor_17: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_1, view_default_47);  silu_default_1 = view_default_47 = None\\n        _param_constant18 = self._param_constant18\\n        t_default_13: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant18);  _param_constant18 = None\\n        view_default_48: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_17, [136, 11008]);  mul_tensor_17 = None\\n        mm_default_13: f32[136, 4096] = torch.ops.aten.mm.default(view_default_48, t_default_13);  view_default_48 = t_default_13 = None\\n        view_default_49: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_13, [1, 136, 4096]);  mm_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_15: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_13, view_default_49);  add_tensor_13 = view_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_4: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_15, 2)\\n        mean_dim_4: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_4, [-1], True);  pow_tensor_scalar_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_16: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_4, 1e-06);  mean_dim_4 = None\\n        rsqrt_default_4: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_16);  add_tensor_16 = None\\n        detach_default_6: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_4)\\n        mul_tensor_18: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_15, rsqrt_default_4);  rsqrt_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant19 = self._param_constant19\\n        mul_tensor_19: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant19, mul_tensor_18);  _param_constant19 = mul_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant20 = self._param_constant20\\n        t_default_14: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant20);  _param_constant20 = None\\n        view_default_50: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_19, [136, 4096])\\n        mm_default_14: f32[136, 4096] = torch.ops.aten.mm.default(view_default_50, t_default_14);  view_default_50 = t_default_14 = None\\n        view_default_51: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_14, [1, 136, 4096]);  mm_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant21 = self._param_constant21\\n        t_default_15: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant21);  _param_constant21 = None\\n        view_default_52: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_19, [136, 4096])\\n        mm_default_15: f32[136, 4096] = torch.ops.aten.mm.default(view_default_52, t_default_15);  view_default_52 = t_default_15 = None\\n        view_default_53: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_15, [1, 136, 4096]);  mm_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant22 = self._param_constant22\\n        t_default_16: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant22);  _param_constant22 = None\\n        view_default_54: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_19, [136, 4096]);  mul_tensor_19 = None\\n        mm_default_16: f32[136, 4096] = torch.ops.aten.mm.default(view_default_54, t_default_16);  view_default_54 = t_default_16 = None\\n        view_default_55: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_16, [1, 136, 4096]);  mm_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_56: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_51, [1, 136, 32, 128]);  view_default_51 = None\\n        transpose_int_10: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_56, 1, 2);  view_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_57: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_53, [1, 136, 32, 128]);  view_default_53 = None\\n        transpose_int_11: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_57, 1, 2);  view_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_58: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_55, [1, 136, 32, 128]);  view_default_55 = None\\n        transpose_int_12: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_58, 1, 2);  view_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant4 = self._tensor_constant4\\n        slice_tensor_24: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant4, 0, 0, 9223372036854775807);  _tensor_constant4 = None\\n        slice_tensor_25: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_24, 1, 0, 9223372036854775807);  slice_tensor_24 = None\\n        slice_tensor_26: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_25, 2, 0, 136);  slice_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant5 = self._tensor_constant5\\n        slice_tensor_27: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant5, 0, 0, 9223372036854775807);  _tensor_constant5 = None\\n        slice_tensor_28: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_27, 1, 0, 9223372036854775807);  slice_tensor_27 = None\\n        slice_tensor_29: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_28, 2, 0, 136);  slice_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_8: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_26, 1);  slice_tensor_26 = None\\n        squeeze_dim_9: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_8, 0);  squeeze_dim_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_10: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_29, 1);  slice_tensor_29 = None\\n        squeeze_dim_11: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_10, 0);  squeeze_dim_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_4: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_9, [view_default]);  squeeze_dim_9 = None\\n        unsqueeze_default_9: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_4, 1);  index_tensor_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_5: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_11, [view_default]);  squeeze_dim_11 = None\\n        unsqueeze_default_10: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_5, 1);  index_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_20: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_10, unsqueeze_default_9)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_30: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_31: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 64, 9223372036854775807);  transpose_int_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_4: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_31);  slice_tensor_31 = None\\n        cat_default_4: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_4, slice_tensor_30], -1);  neg_default_4 = slice_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_21: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_4, unsqueeze_default_10);  cat_default_4 = None\\n        add_tensor_17: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_20, mul_tensor_21);  mul_tensor_20 = mul_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_22: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_11, unsqueeze_default_9);  unsqueeze_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_32: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_33: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 64, 9223372036854775807);  transpose_int_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_5: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_33);  slice_tensor_33 = None\\n        cat_default_5: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_5, slice_tensor_32], -1);  neg_default_5 = slice_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_23: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_5, unsqueeze_default_10);  cat_default_5 = unsqueeze_default_10 = None\\n        add_tensor_18: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_22, mul_tensor_23);  mul_tensor_22 = mul_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_13: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_18, 2, 3)\\n        expand_default_10: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_17, [1, 32, 136, 128]);  add_tensor_17 = None\\n        view_default_59: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_10, [32, 136, 128]);  expand_default_10 = None\\n        expand_default_11: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_13, [1, 32, 128, 136]);  transpose_int_13 = None\\n        view_default_60: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_11, [32, 128, 136]);  expand_default_11 = None\\n        bmm_default_4: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_59, view_default_60);  view_default_59 = view_default_60 = None\\n        view_default_61: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_4, [1, 32, 136, 136]);  bmm_default_4 = None\\n        div_tensor_2: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_61, 11.313708498984761);  view_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_19: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_2, add_tensor_1);  div_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_2: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_19, -1, False);  add_tensor_19 = None\\n        detach_default_7: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_2)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_12: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_2, [1, 32, 136, 136]);  _softmax_default_2 = None\\n        view_default_62: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_12, [32, 136, 136]);  expand_default_12 = None\\n        expand_default_13: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_12, [1, 32, 136, 128])\\n        view_default_63: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_13, [32, 136, 128]);  expand_default_13 = None\\n        bmm_default_5: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_62, view_default_63);  view_default_62 = view_default_63 = None\\n        view_default_64: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_5, [1, 32, 136, 128]);  bmm_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_14: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_64, 1, 2);  view_default_64 = None\\n        clone_default_2: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_14, memory_format = torch.contiguous_format);  transpose_int_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_65: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_2, [1, 136, 4096]);  clone_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant23 = self._param_constant23\\n        t_default_17: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant23);  _param_constant23 = None\\n        view_default_66: f32[136, 4096] = torch.ops.aten.view.default(view_default_65, [136, 4096]);  view_default_65 = None\\n        mm_default_17: f32[136, 4096] = torch.ops.aten.mm.default(view_default_66, t_default_17);  view_default_66 = t_default_17 = None\\n        view_default_67: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_17, [1, 136, 4096]);  mm_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_20: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_15, view_default_67);  add_tensor_15 = view_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_5: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_20, 2)\\n        mean_dim_5: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_5, [-1], True);  pow_tensor_scalar_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_21: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_5, 1e-06);  mean_dim_5 = None\\n        rsqrt_default_5: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_21);  add_tensor_21 = None\\n        detach_default_8: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_5)\\n        mul_tensor_24: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_20, rsqrt_default_5);  rsqrt_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant24 = self._param_constant24\\n        mul_tensor_25: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant24, mul_tensor_24);  _param_constant24 = mul_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant25 = self._param_constant25\\n        t_default_18: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant25);  _param_constant25 = None\\n        view_default_68: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_25, [136, 4096])\\n        mm_default_18: f32[136, 11008] = torch.ops.aten.mm.default(view_default_68, t_default_18);  view_default_68 = t_default_18 = None\\n        view_default_69: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_18, [1, 136, 11008]);  mm_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_2: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_69);  view_default_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant26 = self._param_constant26\\n        t_default_19: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant26);  _param_constant26 = None\\n        view_default_70: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_25, [136, 4096]);  mul_tensor_25 = None\\n        mm_default_19: f32[136, 11008] = torch.ops.aten.mm.default(view_default_70, t_default_19);  view_default_70 = t_default_19 = None\\n        view_default_71: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_19, [1, 136, 11008]);  mm_default_19 = None\\n        mul_tensor_26: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_2, view_default_71);  silu_default_2 = view_default_71 = None\\n        _param_constant27 = self._param_constant27\\n        t_default_20: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant27);  _param_constant27 = None\\n        view_default_72: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_26, [136, 11008]);  mul_tensor_26 = None\\n        mm_default_20: f32[136, 4096] = torch.ops.aten.mm.default(view_default_72, t_default_20);  view_default_72 = t_default_20 = None\\n        view_default_73: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_20, [1, 136, 4096]);  mm_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_22: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_20, view_default_73);  add_tensor_20 = view_default_73 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_6: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_22, 2)\\n        mean_dim_6: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_6, [-1], True);  pow_tensor_scalar_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_23: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_6, 1e-06);  mean_dim_6 = None\\n        rsqrt_default_6: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_23);  add_tensor_23 = None\\n        detach_default_9: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_6)\\n        mul_tensor_27: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_22, rsqrt_default_6);  rsqrt_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant28 = self._param_constant28\\n        mul_tensor_28: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant28, mul_tensor_27);  _param_constant28 = mul_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant29 = self._param_constant29\\n        t_default_21: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant29);  _param_constant29 = None\\n        view_default_74: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_28, [136, 4096])\\n        mm_default_21: f32[136, 4096] = torch.ops.aten.mm.default(view_default_74, t_default_21);  view_default_74 = t_default_21 = None\\n        view_default_75: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_21, [1, 136, 4096]);  mm_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant30 = self._param_constant30\\n        t_default_22: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant30);  _param_constant30 = None\\n        view_default_76: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_28, [136, 4096])\\n        mm_default_22: f32[136, 4096] = torch.ops.aten.mm.default(view_default_76, t_default_22);  view_default_76 = t_default_22 = None\\n        view_default_77: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_22, [1, 136, 4096]);  mm_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant31 = self._param_constant31\\n        t_default_23: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant31);  _param_constant31 = None\\n        view_default_78: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_28, [136, 4096]);  mul_tensor_28 = None\\n        mm_default_23: f32[136, 4096] = torch.ops.aten.mm.default(view_default_78, t_default_23);  view_default_78 = t_default_23 = None\\n        view_default_79: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_23, [1, 136, 4096]);  mm_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_80: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_75, [1, 136, 32, 128]);  view_default_75 = None\\n        transpose_int_15: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_80, 1, 2);  view_default_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_81: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_77, [1, 136, 32, 128]);  view_default_77 = None\\n        transpose_int_16: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_81, 1, 2);  view_default_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_82: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_79, [1, 136, 32, 128]);  view_default_79 = None\\n        transpose_int_17: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_82, 1, 2);  view_default_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant6 = self._tensor_constant6\\n        slice_tensor_34: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant6, 0, 0, 9223372036854775807);  _tensor_constant6 = None\\n        slice_tensor_35: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_34, 1, 0, 9223372036854775807);  slice_tensor_34 = None\\n        slice_tensor_36: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_35, 2, 0, 136);  slice_tensor_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant7 = self._tensor_constant7\\n        slice_tensor_37: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant7, 0, 0, 9223372036854775807);  _tensor_constant7 = None\\n        slice_tensor_38: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_37, 1, 0, 9223372036854775807);  slice_tensor_37 = None\\n        slice_tensor_39: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_38, 2, 0, 136);  slice_tensor_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_12: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_36, 1);  slice_tensor_36 = None\\n        squeeze_dim_13: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_12, 0);  squeeze_dim_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_14: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_39, 1);  slice_tensor_39 = None\\n        squeeze_dim_15: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_14, 0);  squeeze_dim_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_6: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_13, [view_default]);  squeeze_dim_13 = None\\n        unsqueeze_default_11: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_6, 1);  index_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_7: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_15, [view_default]);  squeeze_dim_15 = None\\n        unsqueeze_default_12: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_7, 1);  index_tensor_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_29: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_15, unsqueeze_default_11)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_40: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_41: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 64, 9223372036854775807);  transpose_int_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_6: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_41);  slice_tensor_41 = None\\n        cat_default_6: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_6, slice_tensor_40], -1);  neg_default_6 = slice_tensor_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_30: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_6, unsqueeze_default_12);  cat_default_6 = None\\n        add_tensor_24: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_29, mul_tensor_30);  mul_tensor_29 = mul_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_31: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_16, unsqueeze_default_11);  unsqueeze_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_42: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_43: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 64, 9223372036854775807);  transpose_int_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_7: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_43);  slice_tensor_43 = None\\n        cat_default_7: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_7, slice_tensor_42], -1);  neg_default_7 = slice_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_32: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_7, unsqueeze_default_12);  cat_default_7 = unsqueeze_default_12 = None\\n        add_tensor_25: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_31, mul_tensor_32);  mul_tensor_31 = mul_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_18: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_25, 2, 3)\\n        expand_default_14: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_24, [1, 32, 136, 128]);  add_tensor_24 = None\\n        view_default_83: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_14, [32, 136, 128]);  expand_default_14 = None\\n        expand_default_15: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_18, [1, 32, 128, 136]);  transpose_int_18 = None\\n        view_default_84: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_15, [32, 128, 136]);  expand_default_15 = None\\n        bmm_default_6: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_83, view_default_84);  view_default_83 = view_default_84 = None\\n        view_default_85: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_6, [1, 32, 136, 136]);  bmm_default_6 = None\\n        div_tensor_3: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_85, 11.313708498984761);  view_default_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_26: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_3, add_tensor_1);  div_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_3: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_26, -1, False);  add_tensor_26 = None\\n        detach_default_10: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_3)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_16: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_3, [1, 32, 136, 136]);  _softmax_default_3 = None\\n        view_default_86: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_16, [32, 136, 136]);  expand_default_16 = None\\n        expand_default_17: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_17, [1, 32, 136, 128])\\n        view_default_87: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_17, [32, 136, 128]);  expand_default_17 = None\\n        bmm_default_7: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_86, view_default_87);  view_default_86 = view_default_87 = None\\n        view_default_88: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_7, [1, 32, 136, 128]);  bmm_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_19: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_88, 1, 2);  view_default_88 = None\\n        clone_default_3: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_19, memory_format = torch.contiguous_format);  transpose_int_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_89: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_3, [1, 136, 4096]);  clone_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant32 = self._param_constant32\\n        t_default_24: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant32);  _param_constant32 = None\\n        view_default_90: f32[136, 4096] = torch.ops.aten.view.default(view_default_89, [136, 4096]);  view_default_89 = None\\n        mm_default_24: f32[136, 4096] = torch.ops.aten.mm.default(view_default_90, t_default_24);  view_default_90 = t_default_24 = None\\n        view_default_91: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_24, [1, 136, 4096]);  mm_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_27: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_22, view_default_91);  add_tensor_22 = view_default_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_7: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_27, 2)\\n        mean_dim_7: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_7, [-1], True);  pow_tensor_scalar_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_28: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_7, 1e-06);  mean_dim_7 = None\\n        rsqrt_default_7: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_28);  add_tensor_28 = None\\n        detach_default_11: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_7)\\n        mul_tensor_33: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_27, rsqrt_default_7);  rsqrt_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant33 = self._param_constant33\\n        mul_tensor_34: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant33, mul_tensor_33);  _param_constant33 = mul_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant34 = self._param_constant34\\n        t_default_25: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant34);  _param_constant34 = None\\n        view_default_92: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_34, [136, 4096])\\n        mm_default_25: f32[136, 11008] = torch.ops.aten.mm.default(view_default_92, t_default_25);  view_default_92 = t_default_25 = None\\n        view_default_93: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_25, [1, 136, 11008]);  mm_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_3: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_93);  view_default_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant35 = self._param_constant35\\n        t_default_26: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant35);  _param_constant35 = None\\n        view_default_94: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_34, [136, 4096]);  mul_tensor_34 = None\\n        mm_default_26: f32[136, 11008] = torch.ops.aten.mm.default(view_default_94, t_default_26);  view_default_94 = t_default_26 = None\\n        view_default_95: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_26, [1, 136, 11008]);  mm_default_26 = None\\n        mul_tensor_35: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_3, view_default_95);  silu_default_3 = view_default_95 = None\\n        _param_constant36 = self._param_constant36\\n        t_default_27: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant36);  _param_constant36 = None\\n        view_default_96: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_35, [136, 11008]);  mul_tensor_35 = None\\n        mm_default_27: f32[136, 4096] = torch.ops.aten.mm.default(view_default_96, t_default_27);  view_default_96 = t_default_27 = None\\n        view_default_97: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_27, [1, 136, 4096]);  mm_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_29: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_27, view_default_97);  add_tensor_27 = view_default_97 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_8: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_29, 2)\\n        mean_dim_8: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_8, [-1], True);  pow_tensor_scalar_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_30: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_8, 1e-06);  mean_dim_8 = None\\n        rsqrt_default_8: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_30);  add_tensor_30 = None\\n        detach_default_12: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_8)\\n        mul_tensor_36: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_29, rsqrt_default_8);  rsqrt_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant37 = self._param_constant37\\n        mul_tensor_37: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant37, mul_tensor_36);  _param_constant37 = mul_tensor_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant38 = self._param_constant38\\n        t_default_28: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant38);  _param_constant38 = None\\n        view_default_98: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_37, [136, 4096])\\n        mm_default_28: f32[136, 4096] = torch.ops.aten.mm.default(view_default_98, t_default_28);  view_default_98 = t_default_28 = None\\n        view_default_99: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_28, [1, 136, 4096]);  mm_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant39 = self._param_constant39\\n        t_default_29: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant39);  _param_constant39 = None\\n        view_default_100: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_37, [136, 4096])\\n        mm_default_29: f32[136, 4096] = torch.ops.aten.mm.default(view_default_100, t_default_29);  view_default_100 = t_default_29 = None\\n        view_default_101: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_29, [1, 136, 4096]);  mm_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant40 = self._param_constant40\\n        t_default_30: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant40);  _param_constant40 = None\\n        view_default_102: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_37, [136, 4096]);  mul_tensor_37 = None\\n        mm_default_30: f32[136, 4096] = torch.ops.aten.mm.default(view_default_102, t_default_30);  view_default_102 = t_default_30 = None\\n        view_default_103: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_30, [1, 136, 4096]);  mm_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_104: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_99, [1, 136, 32, 128]);  view_default_99 = None\\n        transpose_int_20: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_104, 1, 2);  view_default_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_105: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_101, [1, 136, 32, 128]);  view_default_101 = None\\n        transpose_int_21: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_105, 1, 2);  view_default_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_106: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_103, [1, 136, 32, 128]);  view_default_103 = None\\n        transpose_int_22: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_106, 1, 2);  view_default_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant8 = self._tensor_constant8\\n        slice_tensor_44: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant8, 0, 0, 9223372036854775807);  _tensor_constant8 = None\\n        slice_tensor_45: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_44, 1, 0, 9223372036854775807);  slice_tensor_44 = None\\n        slice_tensor_46: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_45, 2, 0, 136);  slice_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant9 = self._tensor_constant9\\n        slice_tensor_47: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant9, 0, 0, 9223372036854775807);  _tensor_constant9 = None\\n        slice_tensor_48: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_47, 1, 0, 9223372036854775807);  slice_tensor_47 = None\\n        slice_tensor_49: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_48, 2, 0, 136);  slice_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_16: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_46, 1);  slice_tensor_46 = None\\n        squeeze_dim_17: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_16, 0);  squeeze_dim_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_18: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_49, 1);  slice_tensor_49 = None\\n        squeeze_dim_19: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_18, 0);  squeeze_dim_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_8: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_17, [view_default]);  squeeze_dim_17 = None\\n        unsqueeze_default_13: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_8, 1);  index_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_9: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_19, [view_default]);  squeeze_dim_19 = None\\n        unsqueeze_default_14: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_9, 1);  index_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_38: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_20, unsqueeze_default_13)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_50: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_51: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 64, 9223372036854775807);  transpose_int_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_8: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_51);  slice_tensor_51 = None\\n        cat_default_8: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_8, slice_tensor_50], -1);  neg_default_8 = slice_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_39: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_8, unsqueeze_default_14);  cat_default_8 = None\\n        add_tensor_31: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_38, mul_tensor_39);  mul_tensor_38 = mul_tensor_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_40: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_21, unsqueeze_default_13);  unsqueeze_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_52: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_53: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 64, 9223372036854775807);  transpose_int_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_9: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_53);  slice_tensor_53 = None\\n        cat_default_9: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_9, slice_tensor_52], -1);  neg_default_9 = slice_tensor_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_41: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_9, unsqueeze_default_14);  cat_default_9 = unsqueeze_default_14 = None\\n        add_tensor_32: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_40, mul_tensor_41);  mul_tensor_40 = mul_tensor_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_23: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_32, 2, 3)\\n        expand_default_18: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_31, [1, 32, 136, 128]);  add_tensor_31 = None\\n        view_default_107: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_18, [32, 136, 128]);  expand_default_18 = None\\n        expand_default_19: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_23, [1, 32, 128, 136]);  transpose_int_23 = None\\n        view_default_108: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_19, [32, 128, 136]);  expand_default_19 = None\\n        bmm_default_8: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_107, view_default_108);  view_default_107 = view_default_108 = None\\n        view_default_109: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_8, [1, 32, 136, 136]);  bmm_default_8 = None\\n        div_tensor_4: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_109, 11.313708498984761);  view_default_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_33: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_4, add_tensor_1);  div_tensor_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_4: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_33, -1, False);  add_tensor_33 = None\\n        detach_default_13: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_4)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_20: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_4, [1, 32, 136, 136]);  _softmax_default_4 = None\\n        view_default_110: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_20, [32, 136, 136]);  expand_default_20 = None\\n        expand_default_21: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_22, [1, 32, 136, 128])\\n        view_default_111: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_21, [32, 136, 128]);  expand_default_21 = None\\n        bmm_default_9: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_110, view_default_111);  view_default_110 = view_default_111 = None\\n        view_default_112: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_9, [1, 32, 136, 128]);  bmm_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_24: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_112, 1, 2);  view_default_112 = None\\n        clone_default_4: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_24, memory_format = torch.contiguous_format);  transpose_int_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_113: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_4, [1, 136, 4096]);  clone_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant41 = self._param_constant41\\n        t_default_31: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant41);  _param_constant41 = None\\n        view_default_114: f32[136, 4096] = torch.ops.aten.view.default(view_default_113, [136, 4096]);  view_default_113 = None\\n        mm_default_31: f32[136, 4096] = torch.ops.aten.mm.default(view_default_114, t_default_31);  view_default_114 = t_default_31 = None\\n        view_default_115: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_31, [1, 136, 4096]);  mm_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_34: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_29, view_default_115);  add_tensor_29 = view_default_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_9: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_34, 2)\\n        mean_dim_9: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_9, [-1], True);  pow_tensor_scalar_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_35: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_9, 1e-06);  mean_dim_9 = None\\n        rsqrt_default_9: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_35);  add_tensor_35 = None\\n        detach_default_14: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_9)\\n        mul_tensor_42: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_34, rsqrt_default_9);  rsqrt_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant42 = self._param_constant42\\n        mul_tensor_43: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant42, mul_tensor_42);  _param_constant42 = mul_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant43 = self._param_constant43\\n        t_default_32: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant43);  _param_constant43 = None\\n        view_default_116: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_43, [136, 4096])\\n        mm_default_32: f32[136, 11008] = torch.ops.aten.mm.default(view_default_116, t_default_32);  view_default_116 = t_default_32 = None\\n        view_default_117: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_32, [1, 136, 11008]);  mm_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_4: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_117);  view_default_117 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant44 = self._param_constant44\\n        t_default_33: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant44);  _param_constant44 = None\\n        view_default_118: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_43, [136, 4096]);  mul_tensor_43 = None\\n        mm_default_33: f32[136, 11008] = torch.ops.aten.mm.default(view_default_118, t_default_33);  view_default_118 = t_default_33 = None\\n        view_default_119: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_33, [1, 136, 11008]);  mm_default_33 = None\\n        mul_tensor_44: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_4, view_default_119);  silu_default_4 = view_default_119 = None\\n        _param_constant45 = self._param_constant45\\n        t_default_34: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant45);  _param_constant45 = None\\n        view_default_120: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_44, [136, 11008]);  mul_tensor_44 = None\\n        mm_default_34: f32[136, 4096] = torch.ops.aten.mm.default(view_default_120, t_default_34);  view_default_120 = t_default_34 = None\\n        view_default_121: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_34, [1, 136, 4096]);  mm_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_36: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_34, view_default_121);  add_tensor_34 = view_default_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_10: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_36, 2)\\n        mean_dim_10: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_10, [-1], True);  pow_tensor_scalar_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_37: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_10, 1e-06);  mean_dim_10 = None\\n        rsqrt_default_10: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_37);  add_tensor_37 = None\\n        detach_default_15: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_10)\\n        mul_tensor_45: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_36, rsqrt_default_10);  rsqrt_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant46 = self._param_constant46\\n        mul_tensor_46: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant46, mul_tensor_45);  _param_constant46 = mul_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant47 = self._param_constant47\\n        t_default_35: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant47);  _param_constant47 = None\\n        view_default_122: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_46, [136, 4096])\\n        mm_default_35: f32[136, 4096] = torch.ops.aten.mm.default(view_default_122, t_default_35);  view_default_122 = t_default_35 = None\\n        view_default_123: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_35, [1, 136, 4096]);  mm_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant48 = self._param_constant48\\n        t_default_36: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant48);  _param_constant48 = None\\n        view_default_124: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_46, [136, 4096])\\n        mm_default_36: f32[136, 4096] = torch.ops.aten.mm.default(view_default_124, t_default_36);  view_default_124 = t_default_36 = None\\n        view_default_125: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_36, [1, 136, 4096]);  mm_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant49 = self._param_constant49\\n        t_default_37: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant49);  _param_constant49 = None\\n        view_default_126: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_46, [136, 4096]);  mul_tensor_46 = None\\n        mm_default_37: f32[136, 4096] = torch.ops.aten.mm.default(view_default_126, t_default_37);  view_default_126 = t_default_37 = None\\n        view_default_127: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_37, [1, 136, 4096]);  mm_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_128: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_123, [1, 136, 32, 128]);  view_default_123 = None\\n        transpose_int_25: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_128, 1, 2);  view_default_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_129: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_125, [1, 136, 32, 128]);  view_default_125 = None\\n        transpose_int_26: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_129, 1, 2);  view_default_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_130: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_127, [1, 136, 32, 128]);  view_default_127 = None\\n        transpose_int_27: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_130, 1, 2);  view_default_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant10 = self._tensor_constant10\\n        slice_tensor_54: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant10, 0, 0, 9223372036854775807);  _tensor_constant10 = None\\n        slice_tensor_55: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_54, 1, 0, 9223372036854775807);  slice_tensor_54 = None\\n        slice_tensor_56: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_55, 2, 0, 136);  slice_tensor_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant11 = self._tensor_constant11\\n        slice_tensor_57: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant11, 0, 0, 9223372036854775807);  _tensor_constant11 = None\\n        slice_tensor_58: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_57, 1, 0, 9223372036854775807);  slice_tensor_57 = None\\n        slice_tensor_59: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_58, 2, 0, 136);  slice_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_20: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_56, 1);  slice_tensor_56 = None\\n        squeeze_dim_21: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_20, 0);  squeeze_dim_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_22: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_59, 1);  slice_tensor_59 = None\\n        squeeze_dim_23: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_22, 0);  squeeze_dim_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_10: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_21, [view_default]);  squeeze_dim_21 = None\\n        unsqueeze_default_15: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_10, 1);  index_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_11: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_23, [view_default]);  squeeze_dim_23 = None\\n        unsqueeze_default_16: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_11, 1);  index_tensor_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_47: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_25, unsqueeze_default_15)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_60: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_61: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 64, 9223372036854775807);  transpose_int_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_10: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_61);  slice_tensor_61 = None\\n        cat_default_10: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_10, slice_tensor_60], -1);  neg_default_10 = slice_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_48: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_10, unsqueeze_default_16);  cat_default_10 = None\\n        add_tensor_38: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_47, mul_tensor_48);  mul_tensor_47 = mul_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_49: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_26, unsqueeze_default_15);  unsqueeze_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_62: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_63: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 64, 9223372036854775807);  transpose_int_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_11: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_63);  slice_tensor_63 = None\\n        cat_default_11: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_11, slice_tensor_62], -1);  neg_default_11 = slice_tensor_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_50: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_11, unsqueeze_default_16);  cat_default_11 = unsqueeze_default_16 = None\\n        add_tensor_39: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_49, mul_tensor_50);  mul_tensor_49 = mul_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_28: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_39, 2, 3)\\n        expand_default_22: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_38, [1, 32, 136, 128]);  add_tensor_38 = None\\n        view_default_131: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_22, [32, 136, 128]);  expand_default_22 = None\\n        expand_default_23: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_28, [1, 32, 128, 136]);  transpose_int_28 = None\\n        view_default_132: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_23, [32, 128, 136]);  expand_default_23 = None\\n        bmm_default_10: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_131, view_default_132);  view_default_131 = view_default_132 = None\\n        view_default_133: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_10, [1, 32, 136, 136]);  bmm_default_10 = None\\n        div_tensor_5: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_133, 11.313708498984761);  view_default_133 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_40: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_5, add_tensor_1);  div_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_5: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_40, -1, False);  add_tensor_40 = None\\n        detach_default_16: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_5)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_24: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_5, [1, 32, 136, 136]);  _softmax_default_5 = None\\n        view_default_134: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_24, [32, 136, 136]);  expand_default_24 = None\\n        expand_default_25: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_27, [1, 32, 136, 128])\\n        view_default_135: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_25, [32, 136, 128]);  expand_default_25 = None\\n        bmm_default_11: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_134, view_default_135);  view_default_134 = view_default_135 = None\\n        view_default_136: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_11, [1, 32, 136, 128]);  bmm_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_29: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_136, 1, 2);  view_default_136 = None\\n        clone_default_5: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_29, memory_format = torch.contiguous_format);  transpose_int_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_137: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_5, [1, 136, 4096]);  clone_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant50 = self._param_constant50\\n        t_default_38: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant50);  _param_constant50 = None\\n        view_default_138: f32[136, 4096] = torch.ops.aten.view.default(view_default_137, [136, 4096]);  view_default_137 = None\\n        mm_default_38: f32[136, 4096] = torch.ops.aten.mm.default(view_default_138, t_default_38);  view_default_138 = t_default_38 = None\\n        view_default_139: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_38, [1, 136, 4096]);  mm_default_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_41: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_36, view_default_139);  add_tensor_36 = view_default_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_11: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_41, 2)\\n        mean_dim_11: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_11, [-1], True);  pow_tensor_scalar_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_42: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_11, 1e-06);  mean_dim_11 = None\\n        rsqrt_default_11: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_42);  add_tensor_42 = None\\n        detach_default_17: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_11)\\n        mul_tensor_51: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_41, rsqrt_default_11);  rsqrt_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant51 = self._param_constant51\\n        mul_tensor_52: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant51, mul_tensor_51);  _param_constant51 = mul_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant52 = self._param_constant52\\n        t_default_39: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant52);  _param_constant52 = None\\n        view_default_140: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_52, [136, 4096])\\n        mm_default_39: f32[136, 11008] = torch.ops.aten.mm.default(view_default_140, t_default_39);  view_default_140 = t_default_39 = None\\n        view_default_141: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_39, [1, 136, 11008]);  mm_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_5: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_141);  view_default_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant53 = self._param_constant53\\n        t_default_40: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant53);  _param_constant53 = None\\n        view_default_142: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_52, [136, 4096]);  mul_tensor_52 = None\\n        mm_default_40: f32[136, 11008] = torch.ops.aten.mm.default(view_default_142, t_default_40);  view_default_142 = t_default_40 = None\\n        view_default_143: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_40, [1, 136, 11008]);  mm_default_40 = None\\n        mul_tensor_53: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_5, view_default_143);  silu_default_5 = view_default_143 = None\\n        _param_constant54 = self._param_constant54\\n        t_default_41: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant54);  _param_constant54 = None\\n        view_default_144: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_53, [136, 11008]);  mul_tensor_53 = None\\n        mm_default_41: f32[136, 4096] = torch.ops.aten.mm.default(view_default_144, t_default_41);  view_default_144 = t_default_41 = None\\n        view_default_145: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_41, [1, 136, 4096]);  mm_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_43: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_41, view_default_145);  add_tensor_41 = view_default_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_12: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_43, 2)\\n        mean_dim_12: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_12, [-1], True);  pow_tensor_scalar_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_44: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_12, 1e-06);  mean_dim_12 = None\\n        rsqrt_default_12: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_44);  add_tensor_44 = None\\n        detach_default_18: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_12)\\n        mul_tensor_54: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_43, rsqrt_default_12);  rsqrt_default_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant55 = self._param_constant55\\n        mul_tensor_55: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant55, mul_tensor_54);  _param_constant55 = mul_tensor_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant56 = self._param_constant56\\n        t_default_42: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant56);  _param_constant56 = None\\n        view_default_146: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_55, [136, 4096])\\n        mm_default_42: f32[136, 4096] = torch.ops.aten.mm.default(view_default_146, t_default_42);  view_default_146 = t_default_42 = None\\n        view_default_147: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_42, [1, 136, 4096]);  mm_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant57 = self._param_constant57\\n        t_default_43: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant57);  _param_constant57 = None\\n        view_default_148: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_55, [136, 4096])\\n        mm_default_43: f32[136, 4096] = torch.ops.aten.mm.default(view_default_148, t_default_43);  view_default_148 = t_default_43 = None\\n        view_default_149: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_43, [1, 136, 4096]);  mm_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant58 = self._param_constant58\\n        t_default_44: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant58);  _param_constant58 = None\\n        view_default_150: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_55, [136, 4096]);  mul_tensor_55 = None\\n        mm_default_44: f32[136, 4096] = torch.ops.aten.mm.default(view_default_150, t_default_44);  view_default_150 = t_default_44 = None\\n        view_default_151: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_44, [1, 136, 4096]);  mm_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_152: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_147, [1, 136, 32, 128]);  view_default_147 = None\\n        transpose_int_30: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_152, 1, 2);  view_default_152 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_153: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_149, [1, 136, 32, 128]);  view_default_149 = None\\n        transpose_int_31: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_153, 1, 2);  view_default_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_154: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_151, [1, 136, 32, 128]);  view_default_151 = None\\n        transpose_int_32: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_154, 1, 2);  view_default_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant12 = self._tensor_constant12\\n        slice_tensor_64: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant12, 0, 0, 9223372036854775807);  _tensor_constant12 = None\\n        slice_tensor_65: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_64, 1, 0, 9223372036854775807);  slice_tensor_64 = None\\n        slice_tensor_66: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_65, 2, 0, 136);  slice_tensor_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant13 = self._tensor_constant13\\n        slice_tensor_67: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant13, 0, 0, 9223372036854775807);  _tensor_constant13 = None\\n        slice_tensor_68: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_67, 1, 0, 9223372036854775807);  slice_tensor_67 = None\\n        slice_tensor_69: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_68, 2, 0, 136);  slice_tensor_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_24: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_66, 1);  slice_tensor_66 = None\\n        squeeze_dim_25: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_24, 0);  squeeze_dim_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_26: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_69, 1);  slice_tensor_69 = None\\n        squeeze_dim_27: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_26, 0);  squeeze_dim_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_12: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_25, [view_default]);  squeeze_dim_25 = None\\n        unsqueeze_default_17: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_12, 1);  index_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_13: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_27, [view_default]);  squeeze_dim_27 = None\\n        unsqueeze_default_18: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_13, 1);  index_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_56: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_30, unsqueeze_default_17)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_70: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_71: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 64, 9223372036854775807);  transpose_int_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_12: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_71);  slice_tensor_71 = None\\n        cat_default_12: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_12, slice_tensor_70], -1);  neg_default_12 = slice_tensor_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_57: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_12, unsqueeze_default_18);  cat_default_12 = None\\n        add_tensor_45: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_56, mul_tensor_57);  mul_tensor_56 = mul_tensor_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_58: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_31, unsqueeze_default_17);  unsqueeze_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_72: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_73: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 64, 9223372036854775807);  transpose_int_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_13: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_73);  slice_tensor_73 = None\\n        cat_default_13: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_13, slice_tensor_72], -1);  neg_default_13 = slice_tensor_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_59: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_13, unsqueeze_default_18);  cat_default_13 = unsqueeze_default_18 = None\\n        add_tensor_46: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_58, mul_tensor_59);  mul_tensor_58 = mul_tensor_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_33: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_46, 2, 3)\\n        expand_default_26: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_45, [1, 32, 136, 128]);  add_tensor_45 = None\\n        view_default_155: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_26, [32, 136, 128]);  expand_default_26 = None\\n        expand_default_27: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_33, [1, 32, 128, 136]);  transpose_int_33 = None\\n        view_default_156: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_27, [32, 128, 136]);  expand_default_27 = None\\n        bmm_default_12: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_155, view_default_156);  view_default_155 = view_default_156 = None\\n        view_default_157: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_12, [1, 32, 136, 136]);  bmm_default_12 = None\\n        div_tensor_6: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_157, 11.313708498984761);  view_default_157 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_47: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_6, add_tensor_1);  div_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_6: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_47, -1, False);  add_tensor_47 = None\\n        detach_default_19: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_6)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_28: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_6, [1, 32, 136, 136]);  _softmax_default_6 = None\\n        view_default_158: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_28, [32, 136, 136]);  expand_default_28 = None\\n        expand_default_29: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_32, [1, 32, 136, 128])\\n        view_default_159: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_29, [32, 136, 128]);  expand_default_29 = None\\n        bmm_default_13: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_158, view_default_159);  view_default_158 = view_default_159 = None\\n        view_default_160: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_13, [1, 32, 136, 128]);  bmm_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_34: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_160, 1, 2);  view_default_160 = None\\n        clone_default_6: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_34, memory_format = torch.contiguous_format);  transpose_int_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_161: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_6, [1, 136, 4096]);  clone_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant59 = self._param_constant59\\n        t_default_45: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant59);  _param_constant59 = None\\n        view_default_162: f32[136, 4096] = torch.ops.aten.view.default(view_default_161, [136, 4096]);  view_default_161 = None\\n        mm_default_45: f32[136, 4096] = torch.ops.aten.mm.default(view_default_162, t_default_45);  view_default_162 = t_default_45 = None\\n        view_default_163: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_45, [1, 136, 4096]);  mm_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_48: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_43, view_default_163);  add_tensor_43 = view_default_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_13: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_48, 2)\\n        mean_dim_13: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_13, [-1], True);  pow_tensor_scalar_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_49: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_13, 1e-06);  mean_dim_13 = None\\n        rsqrt_default_13: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_49);  add_tensor_49 = None\\n        detach_default_20: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_13)\\n        mul_tensor_60: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_48, rsqrt_default_13);  rsqrt_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant60 = self._param_constant60\\n        mul_tensor_61: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant60, mul_tensor_60);  _param_constant60 = mul_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant61 = self._param_constant61\\n        t_default_46: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant61);  _param_constant61 = None\\n        view_default_164: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_61, [136, 4096])\\n        mm_default_46: f32[136, 11008] = torch.ops.aten.mm.default(view_default_164, t_default_46);  view_default_164 = t_default_46 = None\\n        view_default_165: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_46, [1, 136, 11008]);  mm_default_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_6: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_165);  view_default_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant62 = self._param_constant62\\n        t_default_47: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant62);  _param_constant62 = None\\n        view_default_166: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_61, [136, 4096]);  mul_tensor_61 = None\\n        mm_default_47: f32[136, 11008] = torch.ops.aten.mm.default(view_default_166, t_default_47);  view_default_166 = t_default_47 = None\\n        view_default_167: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_47, [1, 136, 11008]);  mm_default_47 = None\\n        mul_tensor_62: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_6, view_default_167);  silu_default_6 = view_default_167 = None\\n        _param_constant63 = self._param_constant63\\n        t_default_48: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant63);  _param_constant63 = None\\n        view_default_168: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_62, [136, 11008]);  mul_tensor_62 = None\\n        mm_default_48: f32[136, 4096] = torch.ops.aten.mm.default(view_default_168, t_default_48);  view_default_168 = t_default_48 = None\\n        view_default_169: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_48, [1, 136, 4096]);  mm_default_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_50: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_48, view_default_169);  add_tensor_48 = view_default_169 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_14: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_50, 2)\\n        mean_dim_14: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_14, [-1], True);  pow_tensor_scalar_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_51: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_14, 1e-06);  mean_dim_14 = None\\n        rsqrt_default_14: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_51);  add_tensor_51 = None\\n        detach_default_21: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_14)\\n        mul_tensor_63: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_50, rsqrt_default_14);  rsqrt_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant64 = self._param_constant64\\n        mul_tensor_64: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant64, mul_tensor_63);  _param_constant64 = mul_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant65 = self._param_constant65\\n        t_default_49: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant65);  _param_constant65 = None\\n        view_default_170: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_64, [136, 4096])\\n        mm_default_49: f32[136, 4096] = torch.ops.aten.mm.default(view_default_170, t_default_49);  view_default_170 = t_default_49 = None\\n        view_default_171: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_49, [1, 136, 4096]);  mm_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant66 = self._param_constant66\\n        t_default_50: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant66);  _param_constant66 = None\\n        view_default_172: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_64, [136, 4096])\\n        mm_default_50: f32[136, 4096] = torch.ops.aten.mm.default(view_default_172, t_default_50);  view_default_172 = t_default_50 = None\\n        view_default_173: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_50, [1, 136, 4096]);  mm_default_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant67 = self._param_constant67\\n        t_default_51: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant67);  _param_constant67 = None\\n        view_default_174: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_64, [136, 4096]);  mul_tensor_64 = None\\n        mm_default_51: f32[136, 4096] = torch.ops.aten.mm.default(view_default_174, t_default_51);  view_default_174 = t_default_51 = None\\n        view_default_175: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_51, [1, 136, 4096]);  mm_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_176: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_171, [1, 136, 32, 128]);  view_default_171 = None\\n        transpose_int_35: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_176, 1, 2);  view_default_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_177: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_173, [1, 136, 32, 128]);  view_default_173 = None\\n        transpose_int_36: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_177, 1, 2);  view_default_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_178: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_175, [1, 136, 32, 128]);  view_default_175 = None\\n        transpose_int_37: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_178, 1, 2);  view_default_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant14 = self._tensor_constant14\\n        slice_tensor_74: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant14, 0, 0, 9223372036854775807);  _tensor_constant14 = None\\n        slice_tensor_75: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_74, 1, 0, 9223372036854775807);  slice_tensor_74 = None\\n        slice_tensor_76: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_75, 2, 0, 136);  slice_tensor_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant15 = self._tensor_constant15\\n        slice_tensor_77: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant15, 0, 0, 9223372036854775807);  _tensor_constant15 = None\\n        slice_tensor_78: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_77, 1, 0, 9223372036854775807);  slice_tensor_77 = None\\n        slice_tensor_79: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_78, 2, 0, 136);  slice_tensor_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_28: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_76, 1);  slice_tensor_76 = None\\n        squeeze_dim_29: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_28, 0);  squeeze_dim_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_30: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_79, 1);  slice_tensor_79 = None\\n        squeeze_dim_31: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_30, 0);  squeeze_dim_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_14: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_29, [view_default]);  squeeze_dim_29 = None\\n        unsqueeze_default_19: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_14, 1);  index_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_15: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_31, [view_default]);  squeeze_dim_31 = None\\n        unsqueeze_default_20: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_15, 1);  index_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_65: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_35, unsqueeze_default_19)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_80: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_81: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 64, 9223372036854775807);  transpose_int_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_14: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_81);  slice_tensor_81 = None\\n        cat_default_14: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_14, slice_tensor_80], -1);  neg_default_14 = slice_tensor_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_66: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_14, unsqueeze_default_20);  cat_default_14 = None\\n        add_tensor_52: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_65, mul_tensor_66);  mul_tensor_65 = mul_tensor_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_67: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_36, unsqueeze_default_19);  unsqueeze_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_82: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_83: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 64, 9223372036854775807);  transpose_int_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_15: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_83);  slice_tensor_83 = None\\n        cat_default_15: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_15, slice_tensor_82], -1);  neg_default_15 = slice_tensor_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_68: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_15, unsqueeze_default_20);  cat_default_15 = unsqueeze_default_20 = None\\n        add_tensor_53: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_67, mul_tensor_68);  mul_tensor_67 = mul_tensor_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_38: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_53, 2, 3)\\n        expand_default_30: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_52, [1, 32, 136, 128]);  add_tensor_52 = None\\n        view_default_179: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_30, [32, 136, 128]);  expand_default_30 = None\\n        expand_default_31: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_38, [1, 32, 128, 136]);  transpose_int_38 = None\\n        view_default_180: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_31, [32, 128, 136]);  expand_default_31 = None\\n        bmm_default_14: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_179, view_default_180);  view_default_179 = view_default_180 = None\\n        view_default_181: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_14, [1, 32, 136, 136]);  bmm_default_14 = None\\n        div_tensor_7: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_181, 11.313708498984761);  view_default_181 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_54: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_7, add_tensor_1);  div_tensor_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_7: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_54, -1, False);  add_tensor_54 = None\\n        detach_default_22: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_7)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_32: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_7, [1, 32, 136, 136]);  _softmax_default_7 = None\\n        view_default_182: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_32, [32, 136, 136]);  expand_default_32 = None\\n        expand_default_33: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_37, [1, 32, 136, 128])\\n        view_default_183: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_33, [32, 136, 128]);  expand_default_33 = None\\n        bmm_default_15: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_182, view_default_183);  view_default_182 = view_default_183 = None\\n        view_default_184: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_15, [1, 32, 136, 128]);  bmm_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_39: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_184, 1, 2);  view_default_184 = None\\n        clone_default_7: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_39, memory_format = torch.contiguous_format);  transpose_int_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_185: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_7, [1, 136, 4096]);  clone_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant68 = self._param_constant68\\n        t_default_52: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant68);  _param_constant68 = None\\n        view_default_186: f32[136, 4096] = torch.ops.aten.view.default(view_default_185, [136, 4096]);  view_default_185 = None\\n        mm_default_52: f32[136, 4096] = torch.ops.aten.mm.default(view_default_186, t_default_52);  view_default_186 = t_default_52 = None\\n        view_default_187: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_52, [1, 136, 4096]);  mm_default_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_55: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_50, view_default_187);  add_tensor_50 = view_default_187 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_15: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_55, 2)\\n        mean_dim_15: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_15, [-1], True);  pow_tensor_scalar_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_56: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_15, 1e-06);  mean_dim_15 = None\\n        rsqrt_default_15: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_56);  add_tensor_56 = None\\n        detach_default_23: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_15)\\n        mul_tensor_69: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_55, rsqrt_default_15);  rsqrt_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant69 = self._param_constant69\\n        mul_tensor_70: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant69, mul_tensor_69);  _param_constant69 = mul_tensor_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant70 = self._param_constant70\\n        t_default_53: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant70);  _param_constant70 = None\\n        view_default_188: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_70, [136, 4096])\\n        mm_default_53: f32[136, 11008] = torch.ops.aten.mm.default(view_default_188, t_default_53);  view_default_188 = t_default_53 = None\\n        view_default_189: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_53, [1, 136, 11008]);  mm_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_7: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_189);  view_default_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant71 = self._param_constant71\\n        t_default_54: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant71);  _param_constant71 = None\\n        view_default_190: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_70, [136, 4096]);  mul_tensor_70 = None\\n        mm_default_54: f32[136, 11008] = torch.ops.aten.mm.default(view_default_190, t_default_54);  view_default_190 = t_default_54 = None\\n        view_default_191: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_54, [1, 136, 11008]);  mm_default_54 = None\\n        mul_tensor_71: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_7, view_default_191);  silu_default_7 = view_default_191 = None\\n        _param_constant72 = self._param_constant72\\n        t_default_55: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant72);  _param_constant72 = None\\n        view_default_192: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_71, [136, 11008]);  mul_tensor_71 = None\\n        mm_default_55: f32[136, 4096] = torch.ops.aten.mm.default(view_default_192, t_default_55);  view_default_192 = t_default_55 = None\\n        view_default_193: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_55, [1, 136, 4096]);  mm_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_57: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_55, view_default_193);  add_tensor_55 = view_default_193 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_16: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_57, 2)\\n        mean_dim_16: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_16, [-1], True);  pow_tensor_scalar_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_58: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_16, 1e-06);  mean_dim_16 = None\\n        rsqrt_default_16: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_58);  add_tensor_58 = None\\n        detach_default_24: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_16)\\n        mul_tensor_72: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_57, rsqrt_default_16);  rsqrt_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant73 = self._param_constant73\\n        mul_tensor_73: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant73, mul_tensor_72);  _param_constant73 = mul_tensor_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant74 = self._param_constant74\\n        t_default_56: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant74);  _param_constant74 = None\\n        view_default_194: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_73, [136, 4096])\\n        mm_default_56: f32[136, 4096] = torch.ops.aten.mm.default(view_default_194, t_default_56);  view_default_194 = t_default_56 = None\\n        view_default_195: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_56, [1, 136, 4096]);  mm_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant75 = self._param_constant75\\n        t_default_57: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant75);  _param_constant75 = None\\n        view_default_196: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_73, [136, 4096])\\n        mm_default_57: f32[136, 4096] = torch.ops.aten.mm.default(view_default_196, t_default_57);  view_default_196 = t_default_57 = None\\n        view_default_197: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_57, [1, 136, 4096]);  mm_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant76 = self._param_constant76\\n        t_default_58: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant76);  _param_constant76 = None\\n        view_default_198: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_73, [136, 4096]);  mul_tensor_73 = None\\n        mm_default_58: f32[136, 4096] = torch.ops.aten.mm.default(view_default_198, t_default_58);  view_default_198 = t_default_58 = None\\n        view_default_199: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_58, [1, 136, 4096]);  mm_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_200: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_195, [1, 136, 32, 128]);  view_default_195 = None\\n        transpose_int_40: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_200, 1, 2);  view_default_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_201: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_197, [1, 136, 32, 128]);  view_default_197 = None\\n        transpose_int_41: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_201, 1, 2);  view_default_201 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_202: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_199, [1, 136, 32, 128]);  view_default_199 = None\\n        transpose_int_42: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_202, 1, 2);  view_default_202 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant16 = self._tensor_constant16\\n        slice_tensor_84: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant16, 0, 0, 9223372036854775807);  _tensor_constant16 = None\\n        slice_tensor_85: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_84, 1, 0, 9223372036854775807);  slice_tensor_84 = None\\n        slice_tensor_86: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_85, 2, 0, 136);  slice_tensor_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant17 = self._tensor_constant17\\n        slice_tensor_87: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant17, 0, 0, 9223372036854775807);  _tensor_constant17 = None\\n        slice_tensor_88: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_87, 1, 0, 9223372036854775807);  slice_tensor_87 = None\\n        slice_tensor_89: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_88, 2, 0, 136);  slice_tensor_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_32: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_86, 1);  slice_tensor_86 = None\\n        squeeze_dim_33: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_32, 0);  squeeze_dim_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_34: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_89, 1);  slice_tensor_89 = None\\n        squeeze_dim_35: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_34, 0);  squeeze_dim_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_16: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_33, [view_default]);  squeeze_dim_33 = None\\n        unsqueeze_default_21: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_16, 1);  index_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_17: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_35, [view_default]);  squeeze_dim_35 = None\\n        unsqueeze_default_22: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_17, 1);  index_tensor_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_74: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_40, unsqueeze_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_90: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_91: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 64, 9223372036854775807);  transpose_int_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_16: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_91);  slice_tensor_91 = None\\n        cat_default_16: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_16, slice_tensor_90], -1);  neg_default_16 = slice_tensor_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_75: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_16, unsqueeze_default_22);  cat_default_16 = None\\n        add_tensor_59: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_74, mul_tensor_75);  mul_tensor_74 = mul_tensor_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_76: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_41, unsqueeze_default_21);  unsqueeze_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_92: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_93: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 64, 9223372036854775807);  transpose_int_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_17: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_93);  slice_tensor_93 = None\\n        cat_default_17: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_17, slice_tensor_92], -1);  neg_default_17 = slice_tensor_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_77: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_17, unsqueeze_default_22);  cat_default_17 = unsqueeze_default_22 = None\\n        add_tensor_60: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_76, mul_tensor_77);  mul_tensor_76 = mul_tensor_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_43: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_60, 2, 3)\\n        expand_default_34: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_59, [1, 32, 136, 128]);  add_tensor_59 = None\\n        view_default_203: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_34, [32, 136, 128]);  expand_default_34 = None\\n        expand_default_35: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_43, [1, 32, 128, 136]);  transpose_int_43 = None\\n        view_default_204: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_35, [32, 128, 136]);  expand_default_35 = None\\n        bmm_default_16: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_203, view_default_204);  view_default_203 = view_default_204 = None\\n        view_default_205: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_16, [1, 32, 136, 136]);  bmm_default_16 = None\\n        div_tensor_8: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_205, 11.313708498984761);  view_default_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_61: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_8, add_tensor_1);  div_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_8: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_61, -1, False);  add_tensor_61 = None\\n        detach_default_25: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_8)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_36: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_8, [1, 32, 136, 136]);  _softmax_default_8 = None\\n        view_default_206: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_36, [32, 136, 136]);  expand_default_36 = None\\n        expand_default_37: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_42, [1, 32, 136, 128])\\n        view_default_207: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_37, [32, 136, 128]);  expand_default_37 = None\\n        bmm_default_17: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_206, view_default_207);  view_default_206 = view_default_207 = None\\n        view_default_208: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_17, [1, 32, 136, 128]);  bmm_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_44: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_208, 1, 2);  view_default_208 = None\\n        clone_default_8: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_44, memory_format = torch.contiguous_format);  transpose_int_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_209: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_8, [1, 136, 4096]);  clone_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant77 = self._param_constant77\\n        t_default_59: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant77);  _param_constant77 = None\\n        view_default_210: f32[136, 4096] = torch.ops.aten.view.default(view_default_209, [136, 4096]);  view_default_209 = None\\n        mm_default_59: f32[136, 4096] = torch.ops.aten.mm.default(view_default_210, t_default_59);  view_default_210 = t_default_59 = None\\n        view_default_211: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_59, [1, 136, 4096]);  mm_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_62: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_57, view_default_211);  add_tensor_57 = view_default_211 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_17: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_62, 2)\\n        mean_dim_17: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_17, [-1], True);  pow_tensor_scalar_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_63: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_17, 1e-06);  mean_dim_17 = None\\n        rsqrt_default_17: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_63);  add_tensor_63 = None\\n        detach_default_26: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_17)\\n        mul_tensor_78: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_62, rsqrt_default_17);  rsqrt_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant78 = self._param_constant78\\n        mul_tensor_79: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant78, mul_tensor_78);  _param_constant78 = mul_tensor_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant79 = self._param_constant79\\n        t_default_60: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant79);  _param_constant79 = None\\n        view_default_212: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_79, [136, 4096])\\n        mm_default_60: f32[136, 11008] = torch.ops.aten.mm.default(view_default_212, t_default_60);  view_default_212 = t_default_60 = None\\n        view_default_213: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_60, [1, 136, 11008]);  mm_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_8: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_213);  view_default_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant80 = self._param_constant80\\n        t_default_61: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant80);  _param_constant80 = None\\n        view_default_214: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_79, [136, 4096]);  mul_tensor_79 = None\\n        mm_default_61: f32[136, 11008] = torch.ops.aten.mm.default(view_default_214, t_default_61);  view_default_214 = t_default_61 = None\\n        view_default_215: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_61, [1, 136, 11008]);  mm_default_61 = None\\n        mul_tensor_80: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_8, view_default_215);  silu_default_8 = view_default_215 = None\\n        _param_constant81 = self._param_constant81\\n        t_default_62: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant81);  _param_constant81 = None\\n        view_default_216: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_80, [136, 11008]);  mul_tensor_80 = None\\n        mm_default_62: f32[136, 4096] = torch.ops.aten.mm.default(view_default_216, t_default_62);  view_default_216 = t_default_62 = None\\n        view_default_217: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_62, [1, 136, 4096]);  mm_default_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_64: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_62, view_default_217);  add_tensor_62 = view_default_217 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_18: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_64, 2)\\n        mean_dim_18: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_18, [-1], True);  pow_tensor_scalar_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_65: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_18, 1e-06);  mean_dim_18 = None\\n        rsqrt_default_18: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_65);  add_tensor_65 = None\\n        detach_default_27: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_18)\\n        mul_tensor_81: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_64, rsqrt_default_18);  rsqrt_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant82 = self._param_constant82\\n        mul_tensor_82: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant82, mul_tensor_81);  _param_constant82 = mul_tensor_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant83 = self._param_constant83\\n        t_default_63: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant83);  _param_constant83 = None\\n        view_default_218: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_82, [136, 4096])\\n        mm_default_63: f32[136, 4096] = torch.ops.aten.mm.default(view_default_218, t_default_63);  view_default_218 = t_default_63 = None\\n        view_default_219: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_63, [1, 136, 4096]);  mm_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant84 = self._param_constant84\\n        t_default_64: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant84);  _param_constant84 = None\\n        view_default_220: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_82, [136, 4096])\\n        mm_default_64: f32[136, 4096] = torch.ops.aten.mm.default(view_default_220, t_default_64);  view_default_220 = t_default_64 = None\\n        view_default_221: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_64, [1, 136, 4096]);  mm_default_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant85 = self._param_constant85\\n        t_default_65: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant85);  _param_constant85 = None\\n        view_default_222: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_82, [136, 4096]);  mul_tensor_82 = None\\n        mm_default_65: f32[136, 4096] = torch.ops.aten.mm.default(view_default_222, t_default_65);  view_default_222 = t_default_65 = None\\n        view_default_223: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_65, [1, 136, 4096]);  mm_default_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_224: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_219, [1, 136, 32, 128]);  view_default_219 = None\\n        transpose_int_45: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_224, 1, 2);  view_default_224 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_225: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_221, [1, 136, 32, 128]);  view_default_221 = None\\n        transpose_int_46: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_225, 1, 2);  view_default_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_226: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_223, [1, 136, 32, 128]);  view_default_223 = None\\n        transpose_int_47: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_226, 1, 2);  view_default_226 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant18 = self._tensor_constant18\\n        slice_tensor_94: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant18, 0, 0, 9223372036854775807);  _tensor_constant18 = None\\n        slice_tensor_95: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_94, 1, 0, 9223372036854775807);  slice_tensor_94 = None\\n        slice_tensor_96: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_95, 2, 0, 136);  slice_tensor_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant19 = self._tensor_constant19\\n        slice_tensor_97: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant19, 0, 0, 9223372036854775807);  _tensor_constant19 = None\\n        slice_tensor_98: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_97, 1, 0, 9223372036854775807);  slice_tensor_97 = None\\n        slice_tensor_99: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_98, 2, 0, 136);  slice_tensor_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_36: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_96, 1);  slice_tensor_96 = None\\n        squeeze_dim_37: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_36, 0);  squeeze_dim_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_38: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_99, 1);  slice_tensor_99 = None\\n        squeeze_dim_39: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_38, 0);  squeeze_dim_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_18: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_37, [view_default]);  squeeze_dim_37 = None\\n        unsqueeze_default_23: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_18, 1);  index_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_19: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_39, [view_default]);  squeeze_dim_39 = None\\n        unsqueeze_default_24: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_19, 1);  index_tensor_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_83: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_45, unsqueeze_default_23)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_100: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_101: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 64, 9223372036854775807);  transpose_int_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_18: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_101);  slice_tensor_101 = None\\n        cat_default_18: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_18, slice_tensor_100], -1);  neg_default_18 = slice_tensor_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_84: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_18, unsqueeze_default_24);  cat_default_18 = None\\n        add_tensor_66: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_83, mul_tensor_84);  mul_tensor_83 = mul_tensor_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_85: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_46, unsqueeze_default_23);  unsqueeze_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_102: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_103: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 64, 9223372036854775807);  transpose_int_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_19: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_103);  slice_tensor_103 = None\\n        cat_default_19: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_19, slice_tensor_102], -1);  neg_default_19 = slice_tensor_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_86: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_19, unsqueeze_default_24);  cat_default_19 = unsqueeze_default_24 = None\\n        add_tensor_67: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_85, mul_tensor_86);  mul_tensor_85 = mul_tensor_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_48: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_67, 2, 3)\\n        expand_default_38: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_66, [1, 32, 136, 128]);  add_tensor_66 = None\\n        view_default_227: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_38, [32, 136, 128]);  expand_default_38 = None\\n        expand_default_39: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_48, [1, 32, 128, 136]);  transpose_int_48 = None\\n        view_default_228: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_39, [32, 128, 136]);  expand_default_39 = None\\n        bmm_default_18: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_227, view_default_228);  view_default_227 = view_default_228 = None\\n        view_default_229: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_18, [1, 32, 136, 136]);  bmm_default_18 = None\\n        div_tensor_9: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_229, 11.313708498984761);  view_default_229 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_68: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_9, add_tensor_1);  div_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_9: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_68, -1, False);  add_tensor_68 = None\\n        detach_default_28: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_9)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_40: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_9, [1, 32, 136, 136]);  _softmax_default_9 = None\\n        view_default_230: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_40, [32, 136, 136]);  expand_default_40 = None\\n        expand_default_41: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_47, [1, 32, 136, 128])\\n        view_default_231: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_41, [32, 136, 128]);  expand_default_41 = None\\n        bmm_default_19: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_230, view_default_231);  view_default_230 = view_default_231 = None\\n        view_default_232: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_19, [1, 32, 136, 128]);  bmm_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_49: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_232, 1, 2);  view_default_232 = None\\n        clone_default_9: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_49, memory_format = torch.contiguous_format);  transpose_int_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_233: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_9, [1, 136, 4096]);  clone_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant86 = self._param_constant86\\n        t_default_66: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant86);  _param_constant86 = None\\n        view_default_234: f32[136, 4096] = torch.ops.aten.view.default(view_default_233, [136, 4096]);  view_default_233 = None\\n        mm_default_66: f32[136, 4096] = torch.ops.aten.mm.default(view_default_234, t_default_66);  view_default_234 = t_default_66 = None\\n        view_default_235: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_66, [1, 136, 4096]);  mm_default_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_69: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_64, view_default_235);  add_tensor_64 = view_default_235 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_19: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_69, 2)\\n        mean_dim_19: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_19, [-1], True);  pow_tensor_scalar_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_70: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_19, 1e-06);  mean_dim_19 = None\\n        rsqrt_default_19: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_70);  add_tensor_70 = None\\n        detach_default_29: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_19)\\n        mul_tensor_87: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_69, rsqrt_default_19);  rsqrt_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant87 = self._param_constant87\\n        mul_tensor_88: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant87, mul_tensor_87);  _param_constant87 = mul_tensor_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant88 = self._param_constant88\\n        t_default_67: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant88);  _param_constant88 = None\\n        view_default_236: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_88, [136, 4096])\\n        mm_default_67: f32[136, 11008] = torch.ops.aten.mm.default(view_default_236, t_default_67);  view_default_236 = t_default_67 = None\\n        view_default_237: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_67, [1, 136, 11008]);  mm_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_9: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_237);  view_default_237 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant89 = self._param_constant89\\n        t_default_68: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant89);  _param_constant89 = None\\n        view_default_238: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_88, [136, 4096]);  mul_tensor_88 = None\\n        mm_default_68: f32[136, 11008] = torch.ops.aten.mm.default(view_default_238, t_default_68);  view_default_238 = t_default_68 = None\\n        view_default_239: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_68, [1, 136, 11008]);  mm_default_68 = None\\n        mul_tensor_89: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_9, view_default_239);  silu_default_9 = view_default_239 = None\\n        _param_constant90 = self._param_constant90\\n        t_default_69: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant90);  _param_constant90 = None\\n        view_default_240: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_89, [136, 11008]);  mul_tensor_89 = None\\n        mm_default_69: f32[136, 4096] = torch.ops.aten.mm.default(view_default_240, t_default_69);  view_default_240 = t_default_69 = None\\n        view_default_241: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_69, [1, 136, 4096]);  mm_default_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_71: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_69, view_default_241);  add_tensor_69 = view_default_241 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_20: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_71, 2)\\n        mean_dim_20: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_20, [-1], True);  pow_tensor_scalar_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_72: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_20, 1e-06);  mean_dim_20 = None\\n        rsqrt_default_20: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_72);  add_tensor_72 = None\\n        detach_default_30: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_20)\\n        mul_tensor_90: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_71, rsqrt_default_20);  rsqrt_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant91 = self._param_constant91\\n        mul_tensor_91: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant91, mul_tensor_90);  _param_constant91 = mul_tensor_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant92 = self._param_constant92\\n        t_default_70: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant92);  _param_constant92 = None\\n        view_default_242: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_91, [136, 4096])\\n        mm_default_70: f32[136, 4096] = torch.ops.aten.mm.default(view_default_242, t_default_70);  view_default_242 = t_default_70 = None\\n        view_default_243: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_70, [1, 136, 4096]);  mm_default_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant93 = self._param_constant93\\n        t_default_71: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant93);  _param_constant93 = None\\n        view_default_244: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_91, [136, 4096])\\n        mm_default_71: f32[136, 4096] = torch.ops.aten.mm.default(view_default_244, t_default_71);  view_default_244 = t_default_71 = None\\n        view_default_245: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_71, [1, 136, 4096]);  mm_default_71 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant94 = self._param_constant94\\n        t_default_72: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant94);  _param_constant94 = None\\n        view_default_246: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_91, [136, 4096]);  mul_tensor_91 = None\\n        mm_default_72: f32[136, 4096] = torch.ops.aten.mm.default(view_default_246, t_default_72);  view_default_246 = t_default_72 = None\\n        view_default_247: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_72, [1, 136, 4096]);  mm_default_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_248: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_243, [1, 136, 32, 128]);  view_default_243 = None\\n        transpose_int_50: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_248, 1, 2);  view_default_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_249: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_245, [1, 136, 32, 128]);  view_default_245 = None\\n        transpose_int_51: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_249, 1, 2);  view_default_249 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_250: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_247, [1, 136, 32, 128]);  view_default_247 = None\\n        transpose_int_52: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_250, 1, 2);  view_default_250 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant20 = self._tensor_constant20\\n        slice_tensor_104: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant20, 0, 0, 9223372036854775807);  _tensor_constant20 = None\\n        slice_tensor_105: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_104, 1, 0, 9223372036854775807);  slice_tensor_104 = None\\n        slice_tensor_106: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_105, 2, 0, 136);  slice_tensor_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant21 = self._tensor_constant21\\n        slice_tensor_107: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant21, 0, 0, 9223372036854775807);  _tensor_constant21 = None\\n        slice_tensor_108: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_107, 1, 0, 9223372036854775807);  slice_tensor_107 = None\\n        slice_tensor_109: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_108, 2, 0, 136);  slice_tensor_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_40: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_106, 1);  slice_tensor_106 = None\\n        squeeze_dim_41: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_40, 0);  squeeze_dim_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_42: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_109, 1);  slice_tensor_109 = None\\n        squeeze_dim_43: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_42, 0);  squeeze_dim_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_20: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_41, [view_default]);  squeeze_dim_41 = None\\n        unsqueeze_default_25: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_20, 1);  index_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_21: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_43, [view_default]);  squeeze_dim_43 = None\\n        unsqueeze_default_26: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_21, 1);  index_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_92: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_50, unsqueeze_default_25)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_110: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_111: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 64, 9223372036854775807);  transpose_int_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_20: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_111);  slice_tensor_111 = None\\n        cat_default_20: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_20, slice_tensor_110], -1);  neg_default_20 = slice_tensor_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_93: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_20, unsqueeze_default_26);  cat_default_20 = None\\n        add_tensor_73: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_92, mul_tensor_93);  mul_tensor_92 = mul_tensor_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_94: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_51, unsqueeze_default_25);  unsqueeze_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_112: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_113: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 64, 9223372036854775807);  transpose_int_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_21: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_113);  slice_tensor_113 = None\\n        cat_default_21: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_21, slice_tensor_112], -1);  neg_default_21 = slice_tensor_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_95: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_21, unsqueeze_default_26);  cat_default_21 = unsqueeze_default_26 = None\\n        add_tensor_74: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_94, mul_tensor_95);  mul_tensor_94 = mul_tensor_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_53: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_74, 2, 3)\\n        expand_default_42: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_73, [1, 32, 136, 128]);  add_tensor_73 = None\\n        view_default_251: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_42, [32, 136, 128]);  expand_default_42 = None\\n        expand_default_43: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_53, [1, 32, 128, 136]);  transpose_int_53 = None\\n        view_default_252: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_43, [32, 128, 136]);  expand_default_43 = None\\n        bmm_default_20: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_251, view_default_252);  view_default_251 = view_default_252 = None\\n        view_default_253: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_20, [1, 32, 136, 136]);  bmm_default_20 = None\\n        div_tensor_10: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_253, 11.313708498984761);  view_default_253 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_75: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_10, add_tensor_1);  div_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_10: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_75, -1, False);  add_tensor_75 = None\\n        detach_default_31: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_10)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_44: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_10, [1, 32, 136, 136]);  _softmax_default_10 = None\\n        view_default_254: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_44, [32, 136, 136]);  expand_default_44 = None\\n        expand_default_45: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_52, [1, 32, 136, 128])\\n        view_default_255: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_45, [32, 136, 128]);  expand_default_45 = None\\n        bmm_default_21: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_254, view_default_255);  view_default_254 = view_default_255 = None\\n        view_default_256: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_21, [1, 32, 136, 128]);  bmm_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_54: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_256, 1, 2);  view_default_256 = None\\n        clone_default_10: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_54, memory_format = torch.contiguous_format);  transpose_int_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_257: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_10, [1, 136, 4096]);  clone_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant95 = self._param_constant95\\n        t_default_73: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant95);  _param_constant95 = None\\n        view_default_258: f32[136, 4096] = torch.ops.aten.view.default(view_default_257, [136, 4096]);  view_default_257 = None\\n        mm_default_73: f32[136, 4096] = torch.ops.aten.mm.default(view_default_258, t_default_73);  view_default_258 = t_default_73 = None\\n        view_default_259: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_73, [1, 136, 4096]);  mm_default_73 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_76: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_71, view_default_259);  add_tensor_71 = view_default_259 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_21: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_76, 2)\\n        mean_dim_21: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_21, [-1], True);  pow_tensor_scalar_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_77: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_21, 1e-06);  mean_dim_21 = None\\n        rsqrt_default_21: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_77);  add_tensor_77 = None\\n        detach_default_32: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_21)\\n        mul_tensor_96: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_76, rsqrt_default_21);  rsqrt_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant96 = self._param_constant96\\n        mul_tensor_97: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant96, mul_tensor_96);  _param_constant96 = mul_tensor_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant97 = self._param_constant97\\n        t_default_74: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant97);  _param_constant97 = None\\n        view_default_260: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_97, [136, 4096])\\n        mm_default_74: f32[136, 11008] = torch.ops.aten.mm.default(view_default_260, t_default_74);  view_default_260 = t_default_74 = None\\n        view_default_261: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_74, [1, 136, 11008]);  mm_default_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_10: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_261);  view_default_261 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant98 = self._param_constant98\\n        t_default_75: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant98);  _param_constant98 = None\\n        view_default_262: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_97, [136, 4096]);  mul_tensor_97 = None\\n        mm_default_75: f32[136, 11008] = torch.ops.aten.mm.default(view_default_262, t_default_75);  view_default_262 = t_default_75 = None\\n        view_default_263: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_75, [1, 136, 11008]);  mm_default_75 = None\\n        mul_tensor_98: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_10, view_default_263);  silu_default_10 = view_default_263 = None\\n        _param_constant99 = self._param_constant99\\n        t_default_76: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant99);  _param_constant99 = None\\n        view_default_264: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_98, [136, 11008]);  mul_tensor_98 = None\\n        mm_default_76: f32[136, 4096] = torch.ops.aten.mm.default(view_default_264, t_default_76);  view_default_264 = t_default_76 = None\\n        view_default_265: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_76, [1, 136, 4096]);  mm_default_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_78: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_76, view_default_265);  add_tensor_76 = view_default_265 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_22: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_78, 2)\\n        mean_dim_22: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_22, [-1], True);  pow_tensor_scalar_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_79: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_22, 1e-06);  mean_dim_22 = None\\n        rsqrt_default_22: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_79);  add_tensor_79 = None\\n        detach_default_33: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_22)\\n        mul_tensor_99: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_78, rsqrt_default_22);  rsqrt_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant100 = self._param_constant100\\n        mul_tensor_100: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant100, mul_tensor_99);  _param_constant100 = mul_tensor_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant101 = self._param_constant101\\n        t_default_77: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant101);  _param_constant101 = None\\n        view_default_266: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_100, [136, 4096])\\n        mm_default_77: f32[136, 4096] = torch.ops.aten.mm.default(view_default_266, t_default_77);  view_default_266 = t_default_77 = None\\n        view_default_267: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_77, [1, 136, 4096]);  mm_default_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant102 = self._param_constant102\\n        t_default_78: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant102);  _param_constant102 = None\\n        view_default_268: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_100, [136, 4096])\\n        mm_default_78: f32[136, 4096] = torch.ops.aten.mm.default(view_default_268, t_default_78);  view_default_268 = t_default_78 = None\\n        view_default_269: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_78, [1, 136, 4096]);  mm_default_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant103 = self._param_constant103\\n        t_default_79: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant103);  _param_constant103 = None\\n        view_default_270: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_100, [136, 4096]);  mul_tensor_100 = None\\n        mm_default_79: f32[136, 4096] = torch.ops.aten.mm.default(view_default_270, t_default_79);  view_default_270 = t_default_79 = None\\n        view_default_271: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_79, [1, 136, 4096]);  mm_default_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_272: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_267, [1, 136, 32, 128]);  view_default_267 = None\\n        transpose_int_55: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_272, 1, 2);  view_default_272 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_273: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_269, [1, 136, 32, 128]);  view_default_269 = None\\n        transpose_int_56: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_273, 1, 2);  view_default_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_274: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_271, [1, 136, 32, 128]);  view_default_271 = None\\n        transpose_int_57: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_274, 1, 2);  view_default_274 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant22 = self._tensor_constant22\\n        slice_tensor_114: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant22, 0, 0, 9223372036854775807);  _tensor_constant22 = None\\n        slice_tensor_115: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_114, 1, 0, 9223372036854775807);  slice_tensor_114 = None\\n        slice_tensor_116: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_115, 2, 0, 136);  slice_tensor_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant23 = self._tensor_constant23\\n        slice_tensor_117: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant23, 0, 0, 9223372036854775807);  _tensor_constant23 = None\\n        slice_tensor_118: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_117, 1, 0, 9223372036854775807);  slice_tensor_117 = None\\n        slice_tensor_119: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_118, 2, 0, 136);  slice_tensor_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_44: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_116, 1);  slice_tensor_116 = None\\n        squeeze_dim_45: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_44, 0);  squeeze_dim_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_46: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_119, 1);  slice_tensor_119 = None\\n        squeeze_dim_47: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_46, 0);  squeeze_dim_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_22: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_45, [view_default]);  squeeze_dim_45 = None\\n        unsqueeze_default_27: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_22, 1);  index_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_23: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_47, [view_default]);  squeeze_dim_47 = None\\n        unsqueeze_default_28: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_23, 1);  index_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_101: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_55, unsqueeze_default_27)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_120: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_121: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 64, 9223372036854775807);  transpose_int_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_22: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_121);  slice_tensor_121 = None\\n        cat_default_22: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_22, slice_tensor_120], -1);  neg_default_22 = slice_tensor_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_102: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_22, unsqueeze_default_28);  cat_default_22 = None\\n        add_tensor_80: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_101, mul_tensor_102);  mul_tensor_101 = mul_tensor_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_103: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_56, unsqueeze_default_27);  unsqueeze_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_122: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_123: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 64, 9223372036854775807);  transpose_int_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_23: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_123);  slice_tensor_123 = None\\n        cat_default_23: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_23, slice_tensor_122], -1);  neg_default_23 = slice_tensor_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_104: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_23, unsqueeze_default_28);  cat_default_23 = unsqueeze_default_28 = None\\n        add_tensor_81: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_103, mul_tensor_104);  mul_tensor_103 = mul_tensor_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_58: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_81, 2, 3)\\n        expand_default_46: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_80, [1, 32, 136, 128]);  add_tensor_80 = None\\n        view_default_275: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_46, [32, 136, 128]);  expand_default_46 = None\\n        expand_default_47: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_58, [1, 32, 128, 136]);  transpose_int_58 = None\\n        view_default_276: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_47, [32, 128, 136]);  expand_default_47 = None\\n        bmm_default_22: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_275, view_default_276);  view_default_275 = view_default_276 = None\\n        view_default_277: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_22, [1, 32, 136, 136]);  bmm_default_22 = None\\n        div_tensor_11: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_277, 11.313708498984761);  view_default_277 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_82: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_11, add_tensor_1);  div_tensor_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_11: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_82, -1, False);  add_tensor_82 = None\\n        detach_default_34: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_11)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_48: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_11, [1, 32, 136, 136]);  _softmax_default_11 = None\\n        view_default_278: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_48, [32, 136, 136]);  expand_default_48 = None\\n        expand_default_49: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_57, [1, 32, 136, 128])\\n        view_default_279: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_49, [32, 136, 128]);  expand_default_49 = None\\n        bmm_default_23: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_278, view_default_279);  view_default_278 = view_default_279 = None\\n        view_default_280: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_23, [1, 32, 136, 128]);  bmm_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_59: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_280, 1, 2);  view_default_280 = None\\n        clone_default_11: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_59, memory_format = torch.contiguous_format);  transpose_int_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_281: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_11, [1, 136, 4096]);  clone_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant104 = self._param_constant104\\n        t_default_80: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant104);  _param_constant104 = None\\n        view_default_282: f32[136, 4096] = torch.ops.aten.view.default(view_default_281, [136, 4096]);  view_default_281 = None\\n        mm_default_80: f32[136, 4096] = torch.ops.aten.mm.default(view_default_282, t_default_80);  view_default_282 = t_default_80 = None\\n        view_default_283: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_80, [1, 136, 4096]);  mm_default_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_83: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_78, view_default_283);  add_tensor_78 = view_default_283 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_23: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_83, 2)\\n        mean_dim_23: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_23, [-1], True);  pow_tensor_scalar_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_84: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_23, 1e-06);  mean_dim_23 = None\\n        rsqrt_default_23: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_84);  add_tensor_84 = None\\n        detach_default_35: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_23)\\n        mul_tensor_105: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_83, rsqrt_default_23);  rsqrt_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant105 = self._param_constant105\\n        mul_tensor_106: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant105, mul_tensor_105);  _param_constant105 = mul_tensor_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant106 = self._param_constant106\\n        t_default_81: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant106);  _param_constant106 = None\\n        view_default_284: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_106, [136, 4096])\\n        mm_default_81: f32[136, 11008] = torch.ops.aten.mm.default(view_default_284, t_default_81);  view_default_284 = t_default_81 = None\\n        view_default_285: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_81, [1, 136, 11008]);  mm_default_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_11: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_285);  view_default_285 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant107 = self._param_constant107\\n        t_default_82: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant107);  _param_constant107 = None\\n        view_default_286: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_106, [136, 4096]);  mul_tensor_106 = None\\n        mm_default_82: f32[136, 11008] = torch.ops.aten.mm.default(view_default_286, t_default_82);  view_default_286 = t_default_82 = None\\n        view_default_287: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_82, [1, 136, 11008]);  mm_default_82 = None\\n        mul_tensor_107: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_11, view_default_287);  silu_default_11 = view_default_287 = None\\n        _param_constant108 = self._param_constant108\\n        t_default_83: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant108);  _param_constant108 = None\\n        view_default_288: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_107, [136, 11008]);  mul_tensor_107 = None\\n        mm_default_83: f32[136, 4096] = torch.ops.aten.mm.default(view_default_288, t_default_83);  view_default_288 = t_default_83 = None\\n        view_default_289: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_83, [1, 136, 4096]);  mm_default_83 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_85: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_83, view_default_289);  add_tensor_83 = view_default_289 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_24: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_85, 2)\\n        mean_dim_24: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_24, [-1], True);  pow_tensor_scalar_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_86: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_24, 1e-06);  mean_dim_24 = None\\n        rsqrt_default_24: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_86);  add_tensor_86 = None\\n        detach_default_36: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_24)\\n        mul_tensor_108: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_85, rsqrt_default_24);  rsqrt_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant109 = self._param_constant109\\n        mul_tensor_109: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant109, mul_tensor_108);  _param_constant109 = mul_tensor_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant110 = self._param_constant110\\n        t_default_84: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant110);  _param_constant110 = None\\n        view_default_290: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_109, [136, 4096])\\n        mm_default_84: f32[136, 4096] = torch.ops.aten.mm.default(view_default_290, t_default_84);  view_default_290 = t_default_84 = None\\n        view_default_291: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_84, [1, 136, 4096]);  mm_default_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant111 = self._param_constant111\\n        t_default_85: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant111);  _param_constant111 = None\\n        view_default_292: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_109, [136, 4096])\\n        mm_default_85: f32[136, 4096] = torch.ops.aten.mm.default(view_default_292, t_default_85);  view_default_292 = t_default_85 = None\\n        view_default_293: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_85, [1, 136, 4096]);  mm_default_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant112 = self._param_constant112\\n        t_default_86: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant112);  _param_constant112 = None\\n        view_default_294: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_109, [136, 4096]);  mul_tensor_109 = None\\n        mm_default_86: f32[136, 4096] = torch.ops.aten.mm.default(view_default_294, t_default_86);  view_default_294 = t_default_86 = None\\n        view_default_295: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_86, [1, 136, 4096]);  mm_default_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_296: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_291, [1, 136, 32, 128]);  view_default_291 = None\\n        transpose_int_60: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_296, 1, 2);  view_default_296 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_297: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_293, [1, 136, 32, 128]);  view_default_293 = None\\n        transpose_int_61: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_297, 1, 2);  view_default_297 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_298: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_295, [1, 136, 32, 128]);  view_default_295 = None\\n        transpose_int_62: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_298, 1, 2);  view_default_298 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant24 = self._tensor_constant24\\n        slice_tensor_124: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant24, 0, 0, 9223372036854775807);  _tensor_constant24 = None\\n        slice_tensor_125: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_124, 1, 0, 9223372036854775807);  slice_tensor_124 = None\\n        slice_tensor_126: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_125, 2, 0, 136);  slice_tensor_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant25 = self._tensor_constant25\\n        slice_tensor_127: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant25, 0, 0, 9223372036854775807);  _tensor_constant25 = None\\n        slice_tensor_128: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_127, 1, 0, 9223372036854775807);  slice_tensor_127 = None\\n        slice_tensor_129: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_128, 2, 0, 136);  slice_tensor_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_48: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_126, 1);  slice_tensor_126 = None\\n        squeeze_dim_49: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_48, 0);  squeeze_dim_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_50: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_129, 1);  slice_tensor_129 = None\\n        squeeze_dim_51: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_50, 0);  squeeze_dim_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_24: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_49, [view_default]);  squeeze_dim_49 = None\\n        unsqueeze_default_29: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_24, 1);  index_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_25: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_51, [view_default]);  squeeze_dim_51 = None\\n        unsqueeze_default_30: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_25, 1);  index_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_110: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_60, unsqueeze_default_29)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_130: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_131: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 64, 9223372036854775807);  transpose_int_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_24: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_131);  slice_tensor_131 = None\\n        cat_default_24: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_24, slice_tensor_130], -1);  neg_default_24 = slice_tensor_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_111: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_24, unsqueeze_default_30);  cat_default_24 = None\\n        add_tensor_87: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_110, mul_tensor_111);  mul_tensor_110 = mul_tensor_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_112: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_61, unsqueeze_default_29);  unsqueeze_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_132: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_133: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 64, 9223372036854775807);  transpose_int_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_25: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_133);  slice_tensor_133 = None\\n        cat_default_25: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_25, slice_tensor_132], -1);  neg_default_25 = slice_tensor_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_113: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_25, unsqueeze_default_30);  cat_default_25 = unsqueeze_default_30 = None\\n        add_tensor_88: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_112, mul_tensor_113);  mul_tensor_112 = mul_tensor_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_63: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_88, 2, 3)\\n        expand_default_50: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_87, [1, 32, 136, 128]);  add_tensor_87 = None\\n        view_default_299: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_50, [32, 136, 128]);  expand_default_50 = None\\n        expand_default_51: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_63, [1, 32, 128, 136]);  transpose_int_63 = None\\n        view_default_300: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_51, [32, 128, 136]);  expand_default_51 = None\\n        bmm_default_24: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_299, view_default_300);  view_default_299 = view_default_300 = None\\n        view_default_301: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_24, [1, 32, 136, 136]);  bmm_default_24 = None\\n        div_tensor_12: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_301, 11.313708498984761);  view_default_301 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_89: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_12, add_tensor_1);  div_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_12: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_89, -1, False);  add_tensor_89 = None\\n        detach_default_37: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_12)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_52: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_12, [1, 32, 136, 136]);  _softmax_default_12 = None\\n        view_default_302: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_52, [32, 136, 136]);  expand_default_52 = None\\n        expand_default_53: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_62, [1, 32, 136, 128])\\n        view_default_303: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_53, [32, 136, 128]);  expand_default_53 = None\\n        bmm_default_25: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_302, view_default_303);  view_default_302 = view_default_303 = None\\n        view_default_304: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_25, [1, 32, 136, 128]);  bmm_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_64: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_304, 1, 2);  view_default_304 = None\\n        clone_default_12: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_64, memory_format = torch.contiguous_format);  transpose_int_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_305: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_12, [1, 136, 4096]);  clone_default_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant113 = self._param_constant113\\n        t_default_87: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant113);  _param_constant113 = None\\n        view_default_306: f32[136, 4096] = torch.ops.aten.view.default(view_default_305, [136, 4096]);  view_default_305 = None\\n        mm_default_87: f32[136, 4096] = torch.ops.aten.mm.default(view_default_306, t_default_87);  view_default_306 = t_default_87 = None\\n        view_default_307: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_87, [1, 136, 4096]);  mm_default_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_90: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_85, view_default_307);  add_tensor_85 = view_default_307 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_25: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_90, 2)\\n        mean_dim_25: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_25, [-1], True);  pow_tensor_scalar_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_91: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_25, 1e-06);  mean_dim_25 = None\\n        rsqrt_default_25: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_91);  add_tensor_91 = None\\n        detach_default_38: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_25)\\n        mul_tensor_114: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_90, rsqrt_default_25);  rsqrt_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant114 = self._param_constant114\\n        mul_tensor_115: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant114, mul_tensor_114);  _param_constant114 = mul_tensor_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant115 = self._param_constant115\\n        t_default_88: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant115);  _param_constant115 = None\\n        view_default_308: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_115, [136, 4096])\\n        mm_default_88: f32[136, 11008] = torch.ops.aten.mm.default(view_default_308, t_default_88);  view_default_308 = t_default_88 = None\\n        view_default_309: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_88, [1, 136, 11008]);  mm_default_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_12: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_309);  view_default_309 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant116 = self._param_constant116\\n        t_default_89: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant116);  _param_constant116 = None\\n        view_default_310: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_115, [136, 4096]);  mul_tensor_115 = None\\n        mm_default_89: f32[136, 11008] = torch.ops.aten.mm.default(view_default_310, t_default_89);  view_default_310 = t_default_89 = None\\n        view_default_311: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_89, [1, 136, 11008]);  mm_default_89 = None\\n        mul_tensor_116: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_12, view_default_311);  silu_default_12 = view_default_311 = None\\n        _param_constant117 = self._param_constant117\\n        t_default_90: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant117);  _param_constant117 = None\\n        view_default_312: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_116, [136, 11008]);  mul_tensor_116 = None\\n        mm_default_90: f32[136, 4096] = torch.ops.aten.mm.default(view_default_312, t_default_90);  view_default_312 = t_default_90 = None\\n        view_default_313: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_90, [1, 136, 4096]);  mm_default_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_92: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_90, view_default_313);  add_tensor_90 = view_default_313 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_26: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_92, 2)\\n        mean_dim_26: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_26, [-1], True);  pow_tensor_scalar_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_93: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_26, 1e-06);  mean_dim_26 = None\\n        rsqrt_default_26: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_93);  add_tensor_93 = None\\n        detach_default_39: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_26)\\n        mul_tensor_117: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_92, rsqrt_default_26);  rsqrt_default_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant118 = self._param_constant118\\n        mul_tensor_118: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant118, mul_tensor_117);  _param_constant118 = mul_tensor_117 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant119 = self._param_constant119\\n        t_default_91: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant119);  _param_constant119 = None\\n        view_default_314: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_118, [136, 4096])\\n        mm_default_91: f32[136, 4096] = torch.ops.aten.mm.default(view_default_314, t_default_91);  view_default_314 = t_default_91 = None\\n        view_default_315: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_91, [1, 136, 4096]);  mm_default_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant120 = self._param_constant120\\n        t_default_92: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant120);  _param_constant120 = None\\n        view_default_316: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_118, [136, 4096])\\n        mm_default_92: f32[136, 4096] = torch.ops.aten.mm.default(view_default_316, t_default_92);  view_default_316 = t_default_92 = None\\n        view_default_317: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_92, [1, 136, 4096]);  mm_default_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant121 = self._param_constant121\\n        t_default_93: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant121);  _param_constant121 = None\\n        view_default_318: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_118, [136, 4096]);  mul_tensor_118 = None\\n        mm_default_93: f32[136, 4096] = torch.ops.aten.mm.default(view_default_318, t_default_93);  view_default_318 = t_default_93 = None\\n        view_default_319: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_93, [1, 136, 4096]);  mm_default_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_320: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_315, [1, 136, 32, 128]);  view_default_315 = None\\n        transpose_int_65: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_320, 1, 2);  view_default_320 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_321: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_317, [1, 136, 32, 128]);  view_default_317 = None\\n        transpose_int_66: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_321, 1, 2);  view_default_321 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_322: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_319, [1, 136, 32, 128]);  view_default_319 = None\\n        transpose_int_67: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_322, 1, 2);  view_default_322 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant26 = self._tensor_constant26\\n        slice_tensor_134: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant26, 0, 0, 9223372036854775807);  _tensor_constant26 = None\\n        slice_tensor_135: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_134, 1, 0, 9223372036854775807);  slice_tensor_134 = None\\n        slice_tensor_136: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_135, 2, 0, 136);  slice_tensor_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant27 = self._tensor_constant27\\n        slice_tensor_137: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant27, 0, 0, 9223372036854775807);  _tensor_constant27 = None\\n        slice_tensor_138: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_137, 1, 0, 9223372036854775807);  slice_tensor_137 = None\\n        slice_tensor_139: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_138, 2, 0, 136);  slice_tensor_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_52: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_136, 1);  slice_tensor_136 = None\\n        squeeze_dim_53: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_52, 0);  squeeze_dim_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_54: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_139, 1);  slice_tensor_139 = None\\n        squeeze_dim_55: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_54, 0);  squeeze_dim_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_26: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_53, [view_default]);  squeeze_dim_53 = None\\n        unsqueeze_default_31: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_26, 1);  index_tensor_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_27: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_55, [view_default]);  squeeze_dim_55 = None\\n        unsqueeze_default_32: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_27, 1);  index_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_119: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_65, unsqueeze_default_31)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_140: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_141: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 64, 9223372036854775807);  transpose_int_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_26: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_141);  slice_tensor_141 = None\\n        cat_default_26: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_26, slice_tensor_140], -1);  neg_default_26 = slice_tensor_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_120: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_26, unsqueeze_default_32);  cat_default_26 = None\\n        add_tensor_94: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_119, mul_tensor_120);  mul_tensor_119 = mul_tensor_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_121: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_66, unsqueeze_default_31);  unsqueeze_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_142: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_143: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 64, 9223372036854775807);  transpose_int_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_27: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_143);  slice_tensor_143 = None\\n        cat_default_27: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_27, slice_tensor_142], -1);  neg_default_27 = slice_tensor_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_122: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_27, unsqueeze_default_32);  cat_default_27 = unsqueeze_default_32 = None\\n        add_tensor_95: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_121, mul_tensor_122);  mul_tensor_121 = mul_tensor_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_68: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_95, 2, 3)\\n        expand_default_54: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_94, [1, 32, 136, 128]);  add_tensor_94 = None\\n        view_default_323: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_54, [32, 136, 128]);  expand_default_54 = None\\n        expand_default_55: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_68, [1, 32, 128, 136]);  transpose_int_68 = None\\n        view_default_324: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_55, [32, 128, 136]);  expand_default_55 = None\\n        bmm_default_26: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_323, view_default_324);  view_default_323 = view_default_324 = None\\n        view_default_325: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_26, [1, 32, 136, 136]);  bmm_default_26 = None\\n        div_tensor_13: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_325, 11.313708498984761);  view_default_325 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_96: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_13, add_tensor_1);  div_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_13: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_96, -1, False);  add_tensor_96 = None\\n        detach_default_40: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_13)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_56: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_13, [1, 32, 136, 136]);  _softmax_default_13 = None\\n        view_default_326: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_56, [32, 136, 136]);  expand_default_56 = None\\n        expand_default_57: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_67, [1, 32, 136, 128])\\n        view_default_327: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_57, [32, 136, 128]);  expand_default_57 = None\\n        bmm_default_27: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_326, view_default_327);  view_default_326 = view_default_327 = None\\n        view_default_328: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_27, [1, 32, 136, 128]);  bmm_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_69: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_328, 1, 2);  view_default_328 = None\\n        clone_default_13: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_69, memory_format = torch.contiguous_format);  transpose_int_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_329: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_13, [1, 136, 4096]);  clone_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant122 = self._param_constant122\\n        t_default_94: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant122);  _param_constant122 = None\\n        view_default_330: f32[136, 4096] = torch.ops.aten.view.default(view_default_329, [136, 4096]);  view_default_329 = None\\n        mm_default_94: f32[136, 4096] = torch.ops.aten.mm.default(view_default_330, t_default_94);  view_default_330 = t_default_94 = None\\n        view_default_331: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_94, [1, 136, 4096]);  mm_default_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_97: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_92, view_default_331);  add_tensor_92 = view_default_331 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_27: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_97, 2)\\n        mean_dim_27: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_27, [-1], True);  pow_tensor_scalar_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_98: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_27, 1e-06);  mean_dim_27 = None\\n        rsqrt_default_27: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_98);  add_tensor_98 = None\\n        detach_default_41: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_27)\\n        mul_tensor_123: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_97, rsqrt_default_27);  rsqrt_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant123 = self._param_constant123\\n        mul_tensor_124: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant123, mul_tensor_123);  _param_constant123 = mul_tensor_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant124 = self._param_constant124\\n        t_default_95: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant124);  _param_constant124 = None\\n        view_default_332: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_124, [136, 4096])\\n        mm_default_95: f32[136, 11008] = torch.ops.aten.mm.default(view_default_332, t_default_95);  view_default_332 = t_default_95 = None\\n        view_default_333: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_95, [1, 136, 11008]);  mm_default_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_13: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_333);  view_default_333 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant125 = self._param_constant125\\n        t_default_96: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant125);  _param_constant125 = None\\n        view_default_334: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_124, [136, 4096]);  mul_tensor_124 = None\\n        mm_default_96: f32[136, 11008] = torch.ops.aten.mm.default(view_default_334, t_default_96);  view_default_334 = t_default_96 = None\\n        view_default_335: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_96, [1, 136, 11008]);  mm_default_96 = None\\n        mul_tensor_125: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_13, view_default_335);  silu_default_13 = view_default_335 = None\\n        _param_constant126 = self._param_constant126\\n        t_default_97: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant126);  _param_constant126 = None\\n        view_default_336: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_125, [136, 11008]);  mul_tensor_125 = None\\n        mm_default_97: f32[136, 4096] = torch.ops.aten.mm.default(view_default_336, t_default_97);  view_default_336 = t_default_97 = None\\n        view_default_337: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_97, [1, 136, 4096]);  mm_default_97 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_99: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_97, view_default_337);  add_tensor_97 = view_default_337 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_28: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_99, 2)\\n        mean_dim_28: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_28, [-1], True);  pow_tensor_scalar_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_100: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_28, 1e-06);  mean_dim_28 = None\\n        rsqrt_default_28: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_100);  add_tensor_100 = None\\n        detach_default_42: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_28)\\n        mul_tensor_126: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_99, rsqrt_default_28);  rsqrt_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant127 = self._param_constant127\\n        mul_tensor_127: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant127, mul_tensor_126);  _param_constant127 = mul_tensor_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant128 = self._param_constant128\\n        t_default_98: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant128);  _param_constant128 = None\\n        view_default_338: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_127, [136, 4096])\\n        mm_default_98: f32[136, 4096] = torch.ops.aten.mm.default(view_default_338, t_default_98);  view_default_338 = t_default_98 = None\\n        view_default_339: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_98, [1, 136, 4096]);  mm_default_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant129 = self._param_constant129\\n        t_default_99: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant129);  _param_constant129 = None\\n        view_default_340: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_127, [136, 4096])\\n        mm_default_99: f32[136, 4096] = torch.ops.aten.mm.default(view_default_340, t_default_99);  view_default_340 = t_default_99 = None\\n        view_default_341: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_99, [1, 136, 4096]);  mm_default_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant130 = self._param_constant130\\n        t_default_100: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant130);  _param_constant130 = None\\n        view_default_342: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_127, [136, 4096]);  mul_tensor_127 = None\\n        mm_default_100: f32[136, 4096] = torch.ops.aten.mm.default(view_default_342, t_default_100);  view_default_342 = t_default_100 = None\\n        view_default_343: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_100, [1, 136, 4096]);  mm_default_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_344: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_339, [1, 136, 32, 128]);  view_default_339 = None\\n        transpose_int_70: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_344, 1, 2);  view_default_344 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_345: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_341, [1, 136, 32, 128]);  view_default_341 = None\\n        transpose_int_71: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_345, 1, 2);  view_default_345 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_346: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_343, [1, 136, 32, 128]);  view_default_343 = None\\n        transpose_int_72: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_346, 1, 2);  view_default_346 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant28 = self._tensor_constant28\\n        slice_tensor_144: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant28, 0, 0, 9223372036854775807);  _tensor_constant28 = None\\n        slice_tensor_145: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_144, 1, 0, 9223372036854775807);  slice_tensor_144 = None\\n        slice_tensor_146: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_145, 2, 0, 136);  slice_tensor_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant29 = self._tensor_constant29\\n        slice_tensor_147: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant29, 0, 0, 9223372036854775807);  _tensor_constant29 = None\\n        slice_tensor_148: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_147, 1, 0, 9223372036854775807);  slice_tensor_147 = None\\n        slice_tensor_149: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_148, 2, 0, 136);  slice_tensor_148 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_56: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_146, 1);  slice_tensor_146 = None\\n        squeeze_dim_57: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_56, 0);  squeeze_dim_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_58: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_149, 1);  slice_tensor_149 = None\\n        squeeze_dim_59: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_58, 0);  squeeze_dim_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_28: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_57, [view_default]);  squeeze_dim_57 = None\\n        unsqueeze_default_33: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_28, 1);  index_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_29: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_59, [view_default]);  squeeze_dim_59 = None\\n        unsqueeze_default_34: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_29, 1);  index_tensor_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_128: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_70, unsqueeze_default_33)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_150: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_151: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 64, 9223372036854775807);  transpose_int_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_28: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_151);  slice_tensor_151 = None\\n        cat_default_28: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_28, slice_tensor_150], -1);  neg_default_28 = slice_tensor_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_129: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_28, unsqueeze_default_34);  cat_default_28 = None\\n        add_tensor_101: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_128, mul_tensor_129);  mul_tensor_128 = mul_tensor_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_130: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_71, unsqueeze_default_33);  unsqueeze_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_152: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_153: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 64, 9223372036854775807);  transpose_int_71 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_29: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_153);  slice_tensor_153 = None\\n        cat_default_29: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_29, slice_tensor_152], -1);  neg_default_29 = slice_tensor_152 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_131: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_29, unsqueeze_default_34);  cat_default_29 = unsqueeze_default_34 = None\\n        add_tensor_102: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_130, mul_tensor_131);  mul_tensor_130 = mul_tensor_131 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_73: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_102, 2, 3)\\n        expand_default_58: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_101, [1, 32, 136, 128]);  add_tensor_101 = None\\n        view_default_347: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_58, [32, 136, 128]);  expand_default_58 = None\\n        expand_default_59: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_73, [1, 32, 128, 136]);  transpose_int_73 = None\\n        view_default_348: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_59, [32, 128, 136]);  expand_default_59 = None\\n        bmm_default_28: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_347, view_default_348);  view_default_347 = view_default_348 = None\\n        view_default_349: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_28, [1, 32, 136, 136]);  bmm_default_28 = None\\n        div_tensor_14: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_349, 11.313708498984761);  view_default_349 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_103: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_14, add_tensor_1);  div_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_14: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_103, -1, False);  add_tensor_103 = None\\n        detach_default_43: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_14)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_60: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_14, [1, 32, 136, 136]);  _softmax_default_14 = None\\n        view_default_350: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_60, [32, 136, 136]);  expand_default_60 = None\\n        expand_default_61: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_72, [1, 32, 136, 128])\\n        view_default_351: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_61, [32, 136, 128]);  expand_default_61 = None\\n        bmm_default_29: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_350, view_default_351);  view_default_350 = view_default_351 = None\\n        view_default_352: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_29, [1, 32, 136, 128]);  bmm_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_74: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_352, 1, 2);  view_default_352 = None\\n        clone_default_14: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_74, memory_format = torch.contiguous_format);  transpose_int_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_353: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_14, [1, 136, 4096]);  clone_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant131 = self._param_constant131\\n        t_default_101: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant131);  _param_constant131 = None\\n        view_default_354: f32[136, 4096] = torch.ops.aten.view.default(view_default_353, [136, 4096]);  view_default_353 = None\\n        mm_default_101: f32[136, 4096] = torch.ops.aten.mm.default(view_default_354, t_default_101);  view_default_354 = t_default_101 = None\\n        view_default_355: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_101, [1, 136, 4096]);  mm_default_101 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_104: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_99, view_default_355);  add_tensor_99 = view_default_355 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_29: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_104, 2)\\n        mean_dim_29: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_29, [-1], True);  pow_tensor_scalar_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_105: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_29, 1e-06);  mean_dim_29 = None\\n        rsqrt_default_29: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_105);  add_tensor_105 = None\\n        detach_default_44: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_29)\\n        mul_tensor_132: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_104, rsqrt_default_29);  rsqrt_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant132 = self._param_constant132\\n        mul_tensor_133: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant132, mul_tensor_132);  _param_constant132 = mul_tensor_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant133 = self._param_constant133\\n        t_default_102: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant133);  _param_constant133 = None\\n        view_default_356: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_133, [136, 4096])\\n        mm_default_102: f32[136, 11008] = torch.ops.aten.mm.default(view_default_356, t_default_102);  view_default_356 = t_default_102 = None\\n        view_default_357: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_102, [1, 136, 11008]);  mm_default_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_14: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_357);  view_default_357 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant134 = self._param_constant134\\n        t_default_103: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant134);  _param_constant134 = None\\n        view_default_358: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_133, [136, 4096]);  mul_tensor_133 = None\\n        mm_default_103: f32[136, 11008] = torch.ops.aten.mm.default(view_default_358, t_default_103);  view_default_358 = t_default_103 = None\\n        view_default_359: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_103, [1, 136, 11008]);  mm_default_103 = None\\n        mul_tensor_134: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_14, view_default_359);  silu_default_14 = view_default_359 = None\\n        _param_constant135 = self._param_constant135\\n        t_default_104: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant135);  _param_constant135 = None\\n        view_default_360: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_134, [136, 11008]);  mul_tensor_134 = None\\n        mm_default_104: f32[136, 4096] = torch.ops.aten.mm.default(view_default_360, t_default_104);  view_default_360 = t_default_104 = None\\n        view_default_361: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_104, [1, 136, 4096]);  mm_default_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_106: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_104, view_default_361);  add_tensor_104 = view_default_361 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_30: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_106, 2)\\n        mean_dim_30: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_30, [-1], True);  pow_tensor_scalar_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_107: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_30, 1e-06);  mean_dim_30 = None\\n        rsqrt_default_30: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_107);  add_tensor_107 = None\\n        detach_default_45: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_30)\\n        mul_tensor_135: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_106, rsqrt_default_30);  rsqrt_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant136 = self._param_constant136\\n        mul_tensor_136: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant136, mul_tensor_135);  _param_constant136 = mul_tensor_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant137 = self._param_constant137\\n        t_default_105: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant137);  _param_constant137 = None\\n        view_default_362: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_136, [136, 4096])\\n        mm_default_105: f32[136, 4096] = torch.ops.aten.mm.default(view_default_362, t_default_105);  view_default_362 = t_default_105 = None\\n        view_default_363: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_105, [1, 136, 4096]);  mm_default_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant138 = self._param_constant138\\n        t_default_106: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant138);  _param_constant138 = None\\n        view_default_364: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_136, [136, 4096])\\n        mm_default_106: f32[136, 4096] = torch.ops.aten.mm.default(view_default_364, t_default_106);  view_default_364 = t_default_106 = None\\n        view_default_365: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_106, [1, 136, 4096]);  mm_default_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant139 = self._param_constant139\\n        t_default_107: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant139);  _param_constant139 = None\\n        view_default_366: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_136, [136, 4096]);  mul_tensor_136 = None\\n        mm_default_107: f32[136, 4096] = torch.ops.aten.mm.default(view_default_366, t_default_107);  view_default_366 = t_default_107 = None\\n        view_default_367: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_107, [1, 136, 4096]);  mm_default_107 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_368: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_363, [1, 136, 32, 128]);  view_default_363 = None\\n        transpose_int_75: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_368, 1, 2);  view_default_368 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_369: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_365, [1, 136, 32, 128]);  view_default_365 = None\\n        transpose_int_76: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_369, 1, 2);  view_default_369 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_370: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_367, [1, 136, 32, 128]);  view_default_367 = None\\n        transpose_int_77: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_370, 1, 2);  view_default_370 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant30 = self._tensor_constant30\\n        slice_tensor_154: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant30, 0, 0, 9223372036854775807);  _tensor_constant30 = None\\n        slice_tensor_155: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_154, 1, 0, 9223372036854775807);  slice_tensor_154 = None\\n        slice_tensor_156: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_155, 2, 0, 136);  slice_tensor_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant31 = self._tensor_constant31\\n        slice_tensor_157: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant31, 0, 0, 9223372036854775807);  _tensor_constant31 = None\\n        slice_tensor_158: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_157, 1, 0, 9223372036854775807);  slice_tensor_157 = None\\n        slice_tensor_159: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_158, 2, 0, 136);  slice_tensor_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_60: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_156, 1);  slice_tensor_156 = None\\n        squeeze_dim_61: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_60, 0);  squeeze_dim_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_62: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_159, 1);  slice_tensor_159 = None\\n        squeeze_dim_63: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_62, 0);  squeeze_dim_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_30: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_61, [view_default]);  squeeze_dim_61 = None\\n        unsqueeze_default_35: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_30, 1);  index_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_31: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_63, [view_default]);  squeeze_dim_63 = None\\n        unsqueeze_default_36: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_31, 1);  index_tensor_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_137: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_75, unsqueeze_default_35)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_160: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_161: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 64, 9223372036854775807);  transpose_int_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_30: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_161);  slice_tensor_161 = None\\n        cat_default_30: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_30, slice_tensor_160], -1);  neg_default_30 = slice_tensor_160 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_138: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_30, unsqueeze_default_36);  cat_default_30 = None\\n        add_tensor_108: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_137, mul_tensor_138);  mul_tensor_137 = mul_tensor_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_139: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_76, unsqueeze_default_35);  unsqueeze_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_162: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_163: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 64, 9223372036854775807);  transpose_int_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_31: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_163);  slice_tensor_163 = None\\n        cat_default_31: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_31, slice_tensor_162], -1);  neg_default_31 = slice_tensor_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_140: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_31, unsqueeze_default_36);  cat_default_31 = unsqueeze_default_36 = None\\n        add_tensor_109: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_139, mul_tensor_140);  mul_tensor_139 = mul_tensor_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_78: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_109, 2, 3)\\n        expand_default_62: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_108, [1, 32, 136, 128]);  add_tensor_108 = None\\n        view_default_371: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_62, [32, 136, 128]);  expand_default_62 = None\\n        expand_default_63: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_78, [1, 32, 128, 136]);  transpose_int_78 = None\\n        view_default_372: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_63, [32, 128, 136]);  expand_default_63 = None\\n        bmm_default_30: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_371, view_default_372);  view_default_371 = view_default_372 = None\\n        view_default_373: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_30, [1, 32, 136, 136]);  bmm_default_30 = None\\n        div_tensor_15: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_373, 11.313708498984761);  view_default_373 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_110: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_15, add_tensor_1);  div_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_15: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_110, -1, False);  add_tensor_110 = None\\n        detach_default_46: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_15)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_64: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_15, [1, 32, 136, 136]);  _softmax_default_15 = None\\n        view_default_374: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_64, [32, 136, 136]);  expand_default_64 = None\\n        expand_default_65: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_77, [1, 32, 136, 128])\\n        view_default_375: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_65, [32, 136, 128]);  expand_default_65 = None\\n        bmm_default_31: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_374, view_default_375);  view_default_374 = view_default_375 = None\\n        view_default_376: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_31, [1, 32, 136, 128]);  bmm_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_79: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_376, 1, 2);  view_default_376 = None\\n        clone_default_15: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_79, memory_format = torch.contiguous_format);  transpose_int_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_377: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_15, [1, 136, 4096]);  clone_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant140 = self._param_constant140\\n        t_default_108: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant140);  _param_constant140 = None\\n        view_default_378: f32[136, 4096] = torch.ops.aten.view.default(view_default_377, [136, 4096]);  view_default_377 = None\\n        mm_default_108: f32[136, 4096] = torch.ops.aten.mm.default(view_default_378, t_default_108);  view_default_378 = t_default_108 = None\\n        view_default_379: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_108, [1, 136, 4096]);  mm_default_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_111: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_106, view_default_379);  add_tensor_106 = view_default_379 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_31: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_111, 2)\\n        mean_dim_31: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_31, [-1], True);  pow_tensor_scalar_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_112: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_31, 1e-06);  mean_dim_31 = None\\n        rsqrt_default_31: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_112);  add_tensor_112 = None\\n        detach_default_47: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_31)\\n        mul_tensor_141: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_111, rsqrt_default_31);  rsqrt_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant141 = self._param_constant141\\n        mul_tensor_142: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant141, mul_tensor_141);  _param_constant141 = mul_tensor_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant142 = self._param_constant142\\n        t_default_109: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant142);  _param_constant142 = None\\n        view_default_380: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_142, [136, 4096])\\n        mm_default_109: f32[136, 11008] = torch.ops.aten.mm.default(view_default_380, t_default_109);  view_default_380 = t_default_109 = None\\n        view_default_381: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_109, [1, 136, 11008]);  mm_default_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_15: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_381);  view_default_381 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant143 = self._param_constant143\\n        t_default_110: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant143);  _param_constant143 = None\\n        view_default_382: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_142, [136, 4096]);  mul_tensor_142 = None\\n        mm_default_110: f32[136, 11008] = torch.ops.aten.mm.default(view_default_382, t_default_110);  view_default_382 = t_default_110 = None\\n        view_default_383: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_110, [1, 136, 11008]);  mm_default_110 = None\\n        mul_tensor_143: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_15, view_default_383);  silu_default_15 = view_default_383 = None\\n        _param_constant144 = self._param_constant144\\n        t_default_111: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant144);  _param_constant144 = None\\n        view_default_384: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_143, [136, 11008]);  mul_tensor_143 = None\\n        mm_default_111: f32[136, 4096] = torch.ops.aten.mm.default(view_default_384, t_default_111);  view_default_384 = t_default_111 = None\\n        view_default_385: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_111, [1, 136, 4096]);  mm_default_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_113: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_111, view_default_385);  add_tensor_111 = view_default_385 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_32: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_113, 2)\\n        mean_dim_32: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_32, [-1], True);  pow_tensor_scalar_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_114: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_32, 1e-06);  mean_dim_32 = None\\n        rsqrt_default_32: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_114);  add_tensor_114 = None\\n        detach_default_48: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_32)\\n        mul_tensor_144: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_113, rsqrt_default_32);  rsqrt_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant145 = self._param_constant145\\n        mul_tensor_145: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant145, mul_tensor_144);  _param_constant145 = mul_tensor_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant146 = self._param_constant146\\n        t_default_112: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant146);  _param_constant146 = None\\n        view_default_386: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_145, [136, 4096])\\n        mm_default_112: f32[136, 4096] = torch.ops.aten.mm.default(view_default_386, t_default_112);  view_default_386 = t_default_112 = None\\n        view_default_387: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_112, [1, 136, 4096]);  mm_default_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant147 = self._param_constant147\\n        t_default_113: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant147);  _param_constant147 = None\\n        view_default_388: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_145, [136, 4096])\\n        mm_default_113: f32[136, 4096] = torch.ops.aten.mm.default(view_default_388, t_default_113);  view_default_388 = t_default_113 = None\\n        view_default_389: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_113, [1, 136, 4096]);  mm_default_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant148 = self._param_constant148\\n        t_default_114: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant148);  _param_constant148 = None\\n        view_default_390: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_145, [136, 4096]);  mul_tensor_145 = None\\n        mm_default_114: f32[136, 4096] = torch.ops.aten.mm.default(view_default_390, t_default_114);  view_default_390 = t_default_114 = None\\n        view_default_391: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_114, [1, 136, 4096]);  mm_default_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_392: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_387, [1, 136, 32, 128]);  view_default_387 = None\\n        transpose_int_80: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_392, 1, 2);  view_default_392 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_393: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_389, [1, 136, 32, 128]);  view_default_389 = None\\n        transpose_int_81: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_393, 1, 2);  view_default_393 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_394: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_391, [1, 136, 32, 128]);  view_default_391 = None\\n        transpose_int_82: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_394, 1, 2);  view_default_394 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant32 = self._tensor_constant32\\n        slice_tensor_164: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant32, 0, 0, 9223372036854775807);  _tensor_constant32 = None\\n        slice_tensor_165: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_164, 1, 0, 9223372036854775807);  slice_tensor_164 = None\\n        slice_tensor_166: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_165, 2, 0, 136);  slice_tensor_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant33 = self._tensor_constant33\\n        slice_tensor_167: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant33, 0, 0, 9223372036854775807);  _tensor_constant33 = None\\n        slice_tensor_168: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_167, 1, 0, 9223372036854775807);  slice_tensor_167 = None\\n        slice_tensor_169: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_168, 2, 0, 136);  slice_tensor_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_64: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_166, 1);  slice_tensor_166 = None\\n        squeeze_dim_65: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_64, 0);  squeeze_dim_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_66: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_169, 1);  slice_tensor_169 = None\\n        squeeze_dim_67: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_66, 0);  squeeze_dim_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_32: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_65, [view_default]);  squeeze_dim_65 = None\\n        unsqueeze_default_37: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_32, 1);  index_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_33: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_67, [view_default]);  squeeze_dim_67 = None\\n        unsqueeze_default_38: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_33, 1);  index_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_146: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_80, unsqueeze_default_37)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_170: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_171: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 64, 9223372036854775807);  transpose_int_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_32: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_171);  slice_tensor_171 = None\\n        cat_default_32: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_32, slice_tensor_170], -1);  neg_default_32 = slice_tensor_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_147: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_32, unsqueeze_default_38);  cat_default_32 = None\\n        add_tensor_115: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_146, mul_tensor_147);  mul_tensor_146 = mul_tensor_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_148: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_81, unsqueeze_default_37);  unsqueeze_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_172: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_173: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 64, 9223372036854775807);  transpose_int_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_33: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_173);  slice_tensor_173 = None\\n        cat_default_33: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_33, slice_tensor_172], -1);  neg_default_33 = slice_tensor_172 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_149: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_33, unsqueeze_default_38);  cat_default_33 = unsqueeze_default_38 = None\\n        add_tensor_116: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_148, mul_tensor_149);  mul_tensor_148 = mul_tensor_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_83: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_116, 2, 3)\\n        expand_default_66: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_115, [1, 32, 136, 128]);  add_tensor_115 = None\\n        view_default_395: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_66, [32, 136, 128]);  expand_default_66 = None\\n        expand_default_67: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_83, [1, 32, 128, 136]);  transpose_int_83 = None\\n        view_default_396: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_67, [32, 128, 136]);  expand_default_67 = None\\n        bmm_default_32: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_395, view_default_396);  view_default_395 = view_default_396 = None\\n        view_default_397: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_32, [1, 32, 136, 136]);  bmm_default_32 = None\\n        div_tensor_16: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_397, 11.313708498984761);  view_default_397 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_117: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_16, add_tensor_1);  div_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_16: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_117, -1, False);  add_tensor_117 = None\\n        detach_default_49: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_16)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_68: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_16, [1, 32, 136, 136]);  _softmax_default_16 = None\\n        view_default_398: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_68, [32, 136, 136]);  expand_default_68 = None\\n        expand_default_69: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_82, [1, 32, 136, 128])\\n        view_default_399: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_69, [32, 136, 128]);  expand_default_69 = None\\n        bmm_default_33: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_398, view_default_399);  view_default_398 = view_default_399 = None\\n        view_default_400: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_33, [1, 32, 136, 128]);  bmm_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_84: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_400, 1, 2);  view_default_400 = None\\n        clone_default_16: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_84, memory_format = torch.contiguous_format);  transpose_int_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_401: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_16, [1, 136, 4096]);  clone_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant149 = self._param_constant149\\n        t_default_115: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant149);  _param_constant149 = None\\n        view_default_402: f32[136, 4096] = torch.ops.aten.view.default(view_default_401, [136, 4096]);  view_default_401 = None\\n        mm_default_115: f32[136, 4096] = torch.ops.aten.mm.default(view_default_402, t_default_115);  view_default_402 = t_default_115 = None\\n        view_default_403: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_115, [1, 136, 4096]);  mm_default_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_118: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_113, view_default_403);  add_tensor_113 = view_default_403 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_33: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_118, 2)\\n        mean_dim_33: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_33, [-1], True);  pow_tensor_scalar_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_119: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_33, 1e-06);  mean_dim_33 = None\\n        rsqrt_default_33: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_119);  add_tensor_119 = None\\n        detach_default_50: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_33)\\n        mul_tensor_150: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_118, rsqrt_default_33);  rsqrt_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant150 = self._param_constant150\\n        mul_tensor_151: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant150, mul_tensor_150);  _param_constant150 = mul_tensor_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant151 = self._param_constant151\\n        t_default_116: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant151);  _param_constant151 = None\\n        view_default_404: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_151, [136, 4096])\\n        mm_default_116: f32[136, 11008] = torch.ops.aten.mm.default(view_default_404, t_default_116);  view_default_404 = t_default_116 = None\\n        view_default_405: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_116, [1, 136, 11008]);  mm_default_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_16: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_405);  view_default_405 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant152 = self._param_constant152\\n        t_default_117: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant152);  _param_constant152 = None\\n        view_default_406: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_151, [136, 4096]);  mul_tensor_151 = None\\n        mm_default_117: f32[136, 11008] = torch.ops.aten.mm.default(view_default_406, t_default_117);  view_default_406 = t_default_117 = None\\n        view_default_407: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_117, [1, 136, 11008]);  mm_default_117 = None\\n        mul_tensor_152: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_16, view_default_407);  silu_default_16 = view_default_407 = None\\n        _param_constant153 = self._param_constant153\\n        t_default_118: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant153);  _param_constant153 = None\\n        view_default_408: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_152, [136, 11008]);  mul_tensor_152 = None\\n        mm_default_118: f32[136, 4096] = torch.ops.aten.mm.default(view_default_408, t_default_118);  view_default_408 = t_default_118 = None\\n        view_default_409: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_118, [1, 136, 4096]);  mm_default_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_120: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_118, view_default_409);  add_tensor_118 = view_default_409 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_34: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_120, 2)\\n        mean_dim_34: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_34, [-1], True);  pow_tensor_scalar_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_121: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_34, 1e-06);  mean_dim_34 = None\\n        rsqrt_default_34: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_121);  add_tensor_121 = None\\n        detach_default_51: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_34)\\n        mul_tensor_153: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_120, rsqrt_default_34);  rsqrt_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant154 = self._param_constant154\\n        mul_tensor_154: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant154, mul_tensor_153);  _param_constant154 = mul_tensor_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant155 = self._param_constant155\\n        t_default_119: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant155);  _param_constant155 = None\\n        view_default_410: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_154, [136, 4096])\\n        mm_default_119: f32[136, 4096] = torch.ops.aten.mm.default(view_default_410, t_default_119);  view_default_410 = t_default_119 = None\\n        view_default_411: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_119, [1, 136, 4096]);  mm_default_119 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant156 = self._param_constant156\\n        t_default_120: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant156);  _param_constant156 = None\\n        view_default_412: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_154, [136, 4096])\\n        mm_default_120: f32[136, 4096] = torch.ops.aten.mm.default(view_default_412, t_default_120);  view_default_412 = t_default_120 = None\\n        view_default_413: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_120, [1, 136, 4096]);  mm_default_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant157 = self._param_constant157\\n        t_default_121: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant157);  _param_constant157 = None\\n        view_default_414: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_154, [136, 4096]);  mul_tensor_154 = None\\n        mm_default_121: f32[136, 4096] = torch.ops.aten.mm.default(view_default_414, t_default_121);  view_default_414 = t_default_121 = None\\n        view_default_415: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_121, [1, 136, 4096]);  mm_default_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_416: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_411, [1, 136, 32, 128]);  view_default_411 = None\\n        transpose_int_85: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_416, 1, 2);  view_default_416 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_417: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_413, [1, 136, 32, 128]);  view_default_413 = None\\n        transpose_int_86: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_417, 1, 2);  view_default_417 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_418: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_415, [1, 136, 32, 128]);  view_default_415 = None\\n        transpose_int_87: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_418, 1, 2);  view_default_418 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant34 = self._tensor_constant34\\n        slice_tensor_174: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant34, 0, 0, 9223372036854775807);  _tensor_constant34 = None\\n        slice_tensor_175: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_174, 1, 0, 9223372036854775807);  slice_tensor_174 = None\\n        slice_tensor_176: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_175, 2, 0, 136);  slice_tensor_175 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant35 = self._tensor_constant35\\n        slice_tensor_177: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant35, 0, 0, 9223372036854775807);  _tensor_constant35 = None\\n        slice_tensor_178: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_177, 1, 0, 9223372036854775807);  slice_tensor_177 = None\\n        slice_tensor_179: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_178, 2, 0, 136);  slice_tensor_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_68: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_176, 1);  slice_tensor_176 = None\\n        squeeze_dim_69: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_68, 0);  squeeze_dim_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_70: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_179, 1);  slice_tensor_179 = None\\n        squeeze_dim_71: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_70, 0);  squeeze_dim_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_34: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_69, [view_default]);  squeeze_dim_69 = None\\n        unsqueeze_default_39: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_34, 1);  index_tensor_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_35: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_71, [view_default]);  squeeze_dim_71 = None\\n        unsqueeze_default_40: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_35, 1);  index_tensor_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_155: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_85, unsqueeze_default_39)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_180: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_181: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 64, 9223372036854775807);  transpose_int_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_34: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_181);  slice_tensor_181 = None\\n        cat_default_34: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_34, slice_tensor_180], -1);  neg_default_34 = slice_tensor_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_156: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_34, unsqueeze_default_40);  cat_default_34 = None\\n        add_tensor_122: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_155, mul_tensor_156);  mul_tensor_155 = mul_tensor_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_157: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_86, unsqueeze_default_39);  unsqueeze_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_182: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_183: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 64, 9223372036854775807);  transpose_int_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_35: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_183);  slice_tensor_183 = None\\n        cat_default_35: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_35, slice_tensor_182], -1);  neg_default_35 = slice_tensor_182 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_158: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_35, unsqueeze_default_40);  cat_default_35 = unsqueeze_default_40 = None\\n        add_tensor_123: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_157, mul_tensor_158);  mul_tensor_157 = mul_tensor_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_88: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_123, 2, 3)\\n        expand_default_70: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_122, [1, 32, 136, 128]);  add_tensor_122 = None\\n        view_default_419: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_70, [32, 136, 128]);  expand_default_70 = None\\n        expand_default_71: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_88, [1, 32, 128, 136]);  transpose_int_88 = None\\n        view_default_420: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_71, [32, 128, 136]);  expand_default_71 = None\\n        bmm_default_34: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_419, view_default_420);  view_default_419 = view_default_420 = None\\n        view_default_421: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_34, [1, 32, 136, 136]);  bmm_default_34 = None\\n        div_tensor_17: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_421, 11.313708498984761);  view_default_421 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_124: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_17, add_tensor_1);  div_tensor_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_17: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_124, -1, False);  add_tensor_124 = None\\n        detach_default_52: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_17)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_72: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_17, [1, 32, 136, 136]);  _softmax_default_17 = None\\n        view_default_422: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_72, [32, 136, 136]);  expand_default_72 = None\\n        expand_default_73: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_87, [1, 32, 136, 128])\\n        view_default_423: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_73, [32, 136, 128]);  expand_default_73 = None\\n        bmm_default_35: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_422, view_default_423);  view_default_422 = view_default_423 = None\\n        view_default_424: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_35, [1, 32, 136, 128]);  bmm_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_89: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_424, 1, 2);  view_default_424 = None\\n        clone_default_17: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_89, memory_format = torch.contiguous_format);  transpose_int_89 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_425: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_17, [1, 136, 4096]);  clone_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant158 = self._param_constant158\\n        t_default_122: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant158);  _param_constant158 = None\\n        view_default_426: f32[136, 4096] = torch.ops.aten.view.default(view_default_425, [136, 4096]);  view_default_425 = None\\n        mm_default_122: f32[136, 4096] = torch.ops.aten.mm.default(view_default_426, t_default_122);  view_default_426 = t_default_122 = None\\n        view_default_427: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_122, [1, 136, 4096]);  mm_default_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_125: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_120, view_default_427);  add_tensor_120 = view_default_427 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_35: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_125, 2)\\n        mean_dim_35: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_35, [-1], True);  pow_tensor_scalar_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_126: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_35, 1e-06);  mean_dim_35 = None\\n        rsqrt_default_35: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_126);  add_tensor_126 = None\\n        detach_default_53: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_35)\\n        mul_tensor_159: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_125, rsqrt_default_35);  rsqrt_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant159 = self._param_constant159\\n        mul_tensor_160: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant159, mul_tensor_159);  _param_constant159 = mul_tensor_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant160 = self._param_constant160\\n        t_default_123: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant160);  _param_constant160 = None\\n        view_default_428: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_160, [136, 4096])\\n        mm_default_123: f32[136, 11008] = torch.ops.aten.mm.default(view_default_428, t_default_123);  view_default_428 = t_default_123 = None\\n        view_default_429: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_123, [1, 136, 11008]);  mm_default_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_17: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_429);  view_default_429 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant161 = self._param_constant161\\n        t_default_124: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant161);  _param_constant161 = None\\n        view_default_430: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_160, [136, 4096]);  mul_tensor_160 = None\\n        mm_default_124: f32[136, 11008] = torch.ops.aten.mm.default(view_default_430, t_default_124);  view_default_430 = t_default_124 = None\\n        view_default_431: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_124, [1, 136, 11008]);  mm_default_124 = None\\n        mul_tensor_161: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_17, view_default_431);  silu_default_17 = view_default_431 = None\\n        _param_constant162 = self._param_constant162\\n        t_default_125: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant162);  _param_constant162 = None\\n        view_default_432: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_161, [136, 11008]);  mul_tensor_161 = None\\n        mm_default_125: f32[136, 4096] = torch.ops.aten.mm.default(view_default_432, t_default_125);  view_default_432 = t_default_125 = None\\n        view_default_433: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_125, [1, 136, 4096]);  mm_default_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_127: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_125, view_default_433);  add_tensor_125 = view_default_433 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_36: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_127, 2)\\n        mean_dim_36: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_36, [-1], True);  pow_tensor_scalar_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_128: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_36, 1e-06);  mean_dim_36 = None\\n        rsqrt_default_36: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_128);  add_tensor_128 = None\\n        detach_default_54: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_36)\\n        mul_tensor_162: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_127, rsqrt_default_36);  rsqrt_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant163 = self._param_constant163\\n        mul_tensor_163: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant163, mul_tensor_162);  _param_constant163 = mul_tensor_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant164 = self._param_constant164\\n        t_default_126: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant164);  _param_constant164 = None\\n        view_default_434: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_163, [136, 4096])\\n        mm_default_126: f32[136, 4096] = torch.ops.aten.mm.default(view_default_434, t_default_126);  view_default_434 = t_default_126 = None\\n        view_default_435: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_126, [1, 136, 4096]);  mm_default_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant165 = self._param_constant165\\n        t_default_127: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant165);  _param_constant165 = None\\n        view_default_436: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_163, [136, 4096])\\n        mm_default_127: f32[136, 4096] = torch.ops.aten.mm.default(view_default_436, t_default_127);  view_default_436 = t_default_127 = None\\n        view_default_437: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_127, [1, 136, 4096]);  mm_default_127 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant166 = self._param_constant166\\n        t_default_128: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant166);  _param_constant166 = None\\n        view_default_438: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_163, [136, 4096]);  mul_tensor_163 = None\\n        mm_default_128: f32[136, 4096] = torch.ops.aten.mm.default(view_default_438, t_default_128);  view_default_438 = t_default_128 = None\\n        view_default_439: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_128, [1, 136, 4096]);  mm_default_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_440: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_435, [1, 136, 32, 128]);  view_default_435 = None\\n        transpose_int_90: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_440, 1, 2);  view_default_440 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_441: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_437, [1, 136, 32, 128]);  view_default_437 = None\\n        transpose_int_91: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_441, 1, 2);  view_default_441 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_442: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_439, [1, 136, 32, 128]);  view_default_439 = None\\n        transpose_int_92: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_442, 1, 2);  view_default_442 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant36 = self._tensor_constant36\\n        slice_tensor_184: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant36, 0, 0, 9223372036854775807);  _tensor_constant36 = None\\n        slice_tensor_185: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_184, 1, 0, 9223372036854775807);  slice_tensor_184 = None\\n        slice_tensor_186: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_185, 2, 0, 136);  slice_tensor_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant37 = self._tensor_constant37\\n        slice_tensor_187: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant37, 0, 0, 9223372036854775807);  _tensor_constant37 = None\\n        slice_tensor_188: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_187, 1, 0, 9223372036854775807);  slice_tensor_187 = None\\n        slice_tensor_189: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_188, 2, 0, 136);  slice_tensor_188 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_72: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_186, 1);  slice_tensor_186 = None\\n        squeeze_dim_73: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_72, 0);  squeeze_dim_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_74: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_189, 1);  slice_tensor_189 = None\\n        squeeze_dim_75: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_74, 0);  squeeze_dim_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_36: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_73, [view_default]);  squeeze_dim_73 = None\\n        unsqueeze_default_41: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_36, 1);  index_tensor_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_37: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_75, [view_default]);  squeeze_dim_75 = None\\n        unsqueeze_default_42: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_37, 1);  index_tensor_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_164: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_90, unsqueeze_default_41)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_190: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_191: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 64, 9223372036854775807);  transpose_int_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_36: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_191);  slice_tensor_191 = None\\n        cat_default_36: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_36, slice_tensor_190], -1);  neg_default_36 = slice_tensor_190 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_165: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_36, unsqueeze_default_42);  cat_default_36 = None\\n        add_tensor_129: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_164, mul_tensor_165);  mul_tensor_164 = mul_tensor_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_166: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_91, unsqueeze_default_41);  unsqueeze_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_192: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_193: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 64, 9223372036854775807);  transpose_int_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_37: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_193);  slice_tensor_193 = None\\n        cat_default_37: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_37, slice_tensor_192], -1);  neg_default_37 = slice_tensor_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_167: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_37, unsqueeze_default_42);  cat_default_37 = unsqueeze_default_42 = None\\n        add_tensor_130: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_166, mul_tensor_167);  mul_tensor_166 = mul_tensor_167 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_93: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_130, 2, 3)\\n        expand_default_74: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_129, [1, 32, 136, 128]);  add_tensor_129 = None\\n        view_default_443: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_74, [32, 136, 128]);  expand_default_74 = None\\n        expand_default_75: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_93, [1, 32, 128, 136]);  transpose_int_93 = None\\n        view_default_444: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_75, [32, 128, 136]);  expand_default_75 = None\\n        bmm_default_36: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_443, view_default_444);  view_default_443 = view_default_444 = None\\n        view_default_445: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_36, [1, 32, 136, 136]);  bmm_default_36 = None\\n        div_tensor_18: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_445, 11.313708498984761);  view_default_445 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_131: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_18, add_tensor_1);  div_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_18: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_131, -1, False);  add_tensor_131 = None\\n        detach_default_55: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_18)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_76: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_18, [1, 32, 136, 136]);  _softmax_default_18 = None\\n        view_default_446: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_76, [32, 136, 136]);  expand_default_76 = None\\n        expand_default_77: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_92, [1, 32, 136, 128])\\n        view_default_447: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_77, [32, 136, 128]);  expand_default_77 = None\\n        bmm_default_37: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_446, view_default_447);  view_default_446 = view_default_447 = None\\n        view_default_448: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_37, [1, 32, 136, 128]);  bmm_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_94: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_448, 1, 2);  view_default_448 = None\\n        clone_default_18: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_94, memory_format = torch.contiguous_format);  transpose_int_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_449: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_18, [1, 136, 4096]);  clone_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant167 = self._param_constant167\\n        t_default_129: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant167);  _param_constant167 = None\\n        view_default_450: f32[136, 4096] = torch.ops.aten.view.default(view_default_449, [136, 4096]);  view_default_449 = None\\n        mm_default_129: f32[136, 4096] = torch.ops.aten.mm.default(view_default_450, t_default_129);  view_default_450 = t_default_129 = None\\n        view_default_451: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_129, [1, 136, 4096]);  mm_default_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_132: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_127, view_default_451);  add_tensor_127 = view_default_451 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_37: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_132, 2)\\n        mean_dim_37: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_37, [-1], True);  pow_tensor_scalar_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_133: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_37, 1e-06);  mean_dim_37 = None\\n        rsqrt_default_37: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_133);  add_tensor_133 = None\\n        detach_default_56: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_37)\\n        mul_tensor_168: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_132, rsqrt_default_37);  rsqrt_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant168 = self._param_constant168\\n        mul_tensor_169: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant168, mul_tensor_168);  _param_constant168 = mul_tensor_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant169 = self._param_constant169\\n        t_default_130: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant169);  _param_constant169 = None\\n        view_default_452: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_169, [136, 4096])\\n        mm_default_130: f32[136, 11008] = torch.ops.aten.mm.default(view_default_452, t_default_130);  view_default_452 = t_default_130 = None\\n        view_default_453: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_130, [1, 136, 11008]);  mm_default_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_18: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_453);  view_default_453 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant170 = self._param_constant170\\n        t_default_131: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant170);  _param_constant170 = None\\n        view_default_454: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_169, [136, 4096]);  mul_tensor_169 = None\\n        mm_default_131: f32[136, 11008] = torch.ops.aten.mm.default(view_default_454, t_default_131);  view_default_454 = t_default_131 = None\\n        view_default_455: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_131, [1, 136, 11008]);  mm_default_131 = None\\n        mul_tensor_170: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_18, view_default_455);  silu_default_18 = view_default_455 = None\\n        _param_constant171 = self._param_constant171\\n        t_default_132: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant171);  _param_constant171 = None\\n        view_default_456: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_170, [136, 11008]);  mul_tensor_170 = None\\n        mm_default_132: f32[136, 4096] = torch.ops.aten.mm.default(view_default_456, t_default_132);  view_default_456 = t_default_132 = None\\n        view_default_457: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_132, [1, 136, 4096]);  mm_default_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_134: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_132, view_default_457);  add_tensor_132 = view_default_457 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_38: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_134, 2)\\n        mean_dim_38: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_38, [-1], True);  pow_tensor_scalar_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_135: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_38, 1e-06);  mean_dim_38 = None\\n        rsqrt_default_38: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_135);  add_tensor_135 = None\\n        detach_default_57: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_38)\\n        mul_tensor_171: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_134, rsqrt_default_38);  rsqrt_default_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant172 = self._param_constant172\\n        mul_tensor_172: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant172, mul_tensor_171);  _param_constant172 = mul_tensor_171 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant173 = self._param_constant173\\n        t_default_133: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant173);  _param_constant173 = None\\n        view_default_458: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_172, [136, 4096])\\n        mm_default_133: f32[136, 4096] = torch.ops.aten.mm.default(view_default_458, t_default_133);  view_default_458 = t_default_133 = None\\n        view_default_459: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_133, [1, 136, 4096]);  mm_default_133 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant174 = self._param_constant174\\n        t_default_134: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant174);  _param_constant174 = None\\n        view_default_460: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_172, [136, 4096])\\n        mm_default_134: f32[136, 4096] = torch.ops.aten.mm.default(view_default_460, t_default_134);  view_default_460 = t_default_134 = None\\n        view_default_461: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_134, [1, 136, 4096]);  mm_default_134 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant175 = self._param_constant175\\n        t_default_135: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant175);  _param_constant175 = None\\n        view_default_462: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_172, [136, 4096]);  mul_tensor_172 = None\\n        mm_default_135: f32[136, 4096] = torch.ops.aten.mm.default(view_default_462, t_default_135);  view_default_462 = t_default_135 = None\\n        view_default_463: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_135, [1, 136, 4096]);  mm_default_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_464: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_459, [1, 136, 32, 128]);  view_default_459 = None\\n        transpose_int_95: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_464, 1, 2);  view_default_464 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_465: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_461, [1, 136, 32, 128]);  view_default_461 = None\\n        transpose_int_96: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_465, 1, 2);  view_default_465 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_466: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_463, [1, 136, 32, 128]);  view_default_463 = None\\n        transpose_int_97: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_466, 1, 2);  view_default_466 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant38 = self._tensor_constant38\\n        slice_tensor_194: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant38, 0, 0, 9223372036854775807);  _tensor_constant38 = None\\n        slice_tensor_195: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_194, 1, 0, 9223372036854775807);  slice_tensor_194 = None\\n        slice_tensor_196: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_195, 2, 0, 136);  slice_tensor_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant39 = self._tensor_constant39\\n        slice_tensor_197: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant39, 0, 0, 9223372036854775807);  _tensor_constant39 = None\\n        slice_tensor_198: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_197, 1, 0, 9223372036854775807);  slice_tensor_197 = None\\n        slice_tensor_199: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_198, 2, 0, 136);  slice_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_76: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_196, 1);  slice_tensor_196 = None\\n        squeeze_dim_77: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_76, 0);  squeeze_dim_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_78: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_199, 1);  slice_tensor_199 = None\\n        squeeze_dim_79: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_78, 0);  squeeze_dim_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_38: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_77, [view_default]);  squeeze_dim_77 = None\\n        unsqueeze_default_43: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_38, 1);  index_tensor_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_39: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_79, [view_default]);  squeeze_dim_79 = None\\n        unsqueeze_default_44: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_39, 1);  index_tensor_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_173: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_95, unsqueeze_default_43)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_200: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_201: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 64, 9223372036854775807);  transpose_int_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_38: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_201);  slice_tensor_201 = None\\n        cat_default_38: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_38, slice_tensor_200], -1);  neg_default_38 = slice_tensor_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_174: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_38, unsqueeze_default_44);  cat_default_38 = None\\n        add_tensor_136: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_173, mul_tensor_174);  mul_tensor_173 = mul_tensor_174 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_175: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_96, unsqueeze_default_43);  unsqueeze_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_202: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_203: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 64, 9223372036854775807);  transpose_int_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_39: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_203);  slice_tensor_203 = None\\n        cat_default_39: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_39, slice_tensor_202], -1);  neg_default_39 = slice_tensor_202 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_176: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_39, unsqueeze_default_44);  cat_default_39 = unsqueeze_default_44 = None\\n        add_tensor_137: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_175, mul_tensor_176);  mul_tensor_175 = mul_tensor_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_98: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_137, 2, 3)\\n        expand_default_78: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_136, [1, 32, 136, 128]);  add_tensor_136 = None\\n        view_default_467: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_78, [32, 136, 128]);  expand_default_78 = None\\n        expand_default_79: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_98, [1, 32, 128, 136]);  transpose_int_98 = None\\n        view_default_468: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_79, [32, 128, 136]);  expand_default_79 = None\\n        bmm_default_38: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_467, view_default_468);  view_default_467 = view_default_468 = None\\n        view_default_469: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_38, [1, 32, 136, 136]);  bmm_default_38 = None\\n        div_tensor_19: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_469, 11.313708498984761);  view_default_469 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_138: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_19, add_tensor_1);  div_tensor_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_19: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_138, -1, False);  add_tensor_138 = None\\n        detach_default_58: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_19)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_80: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_19, [1, 32, 136, 136]);  _softmax_default_19 = None\\n        view_default_470: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_80, [32, 136, 136]);  expand_default_80 = None\\n        expand_default_81: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_97, [1, 32, 136, 128])\\n        view_default_471: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_81, [32, 136, 128]);  expand_default_81 = None\\n        bmm_default_39: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_470, view_default_471);  view_default_470 = view_default_471 = None\\n        view_default_472: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_39, [1, 32, 136, 128]);  bmm_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_99: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_472, 1, 2);  view_default_472 = None\\n        clone_default_19: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_99, memory_format = torch.contiguous_format);  transpose_int_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_473: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_19, [1, 136, 4096]);  clone_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant176 = self._param_constant176\\n        t_default_136: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant176);  _param_constant176 = None\\n        view_default_474: f32[136, 4096] = torch.ops.aten.view.default(view_default_473, [136, 4096]);  view_default_473 = None\\n        mm_default_136: f32[136, 4096] = torch.ops.aten.mm.default(view_default_474, t_default_136);  view_default_474 = t_default_136 = None\\n        view_default_475: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_136, [1, 136, 4096]);  mm_default_136 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_139: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_134, view_default_475);  add_tensor_134 = view_default_475 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_39: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_139, 2)\\n        mean_dim_39: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_39, [-1], True);  pow_tensor_scalar_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_140: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_39, 1e-06);  mean_dim_39 = None\\n        rsqrt_default_39: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_140);  add_tensor_140 = None\\n        detach_default_59: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_39)\\n        mul_tensor_177: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_139, rsqrt_default_39);  rsqrt_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant177 = self._param_constant177\\n        mul_tensor_178: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant177, mul_tensor_177);  _param_constant177 = mul_tensor_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant178 = self._param_constant178\\n        t_default_137: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant178);  _param_constant178 = None\\n        view_default_476: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_178, [136, 4096])\\n        mm_default_137: f32[136, 11008] = torch.ops.aten.mm.default(view_default_476, t_default_137);  view_default_476 = t_default_137 = None\\n        view_default_477: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_137, [1, 136, 11008]);  mm_default_137 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_19: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_477);  view_default_477 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant179 = self._param_constant179\\n        t_default_138: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant179);  _param_constant179 = None\\n        view_default_478: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_178, [136, 4096]);  mul_tensor_178 = None\\n        mm_default_138: f32[136, 11008] = torch.ops.aten.mm.default(view_default_478, t_default_138);  view_default_478 = t_default_138 = None\\n        view_default_479: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_138, [1, 136, 11008]);  mm_default_138 = None\\n        mul_tensor_179: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_19, view_default_479);  silu_default_19 = view_default_479 = None\\n        _param_constant180 = self._param_constant180\\n        t_default_139: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant180);  _param_constant180 = None\\n        view_default_480: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_179, [136, 11008]);  mul_tensor_179 = None\\n        mm_default_139: f32[136, 4096] = torch.ops.aten.mm.default(view_default_480, t_default_139);  view_default_480 = t_default_139 = None\\n        view_default_481: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_139, [1, 136, 4096]);  mm_default_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_141: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_139, view_default_481);  add_tensor_139 = view_default_481 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_40: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_141, 2)\\n        mean_dim_40: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_40, [-1], True);  pow_tensor_scalar_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_142: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_40, 1e-06);  mean_dim_40 = None\\n        rsqrt_default_40: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_142);  add_tensor_142 = None\\n        detach_default_60: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_40)\\n        mul_tensor_180: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_141, rsqrt_default_40);  rsqrt_default_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant181 = self._param_constant181\\n        mul_tensor_181: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant181, mul_tensor_180);  _param_constant181 = mul_tensor_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant182 = self._param_constant182\\n        t_default_140: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant182);  _param_constant182 = None\\n        view_default_482: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_181, [136, 4096])\\n        mm_default_140: f32[136, 4096] = torch.ops.aten.mm.default(view_default_482, t_default_140);  view_default_482 = t_default_140 = None\\n        view_default_483: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_140, [1, 136, 4096]);  mm_default_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant183 = self._param_constant183\\n        t_default_141: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant183);  _param_constant183 = None\\n        view_default_484: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_181, [136, 4096])\\n        mm_default_141: f32[136, 4096] = torch.ops.aten.mm.default(view_default_484, t_default_141);  view_default_484 = t_default_141 = None\\n        view_default_485: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_141, [1, 136, 4096]);  mm_default_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant184 = self._param_constant184\\n        t_default_142: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant184);  _param_constant184 = None\\n        view_default_486: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_181, [136, 4096]);  mul_tensor_181 = None\\n        mm_default_142: f32[136, 4096] = torch.ops.aten.mm.default(view_default_486, t_default_142);  view_default_486 = t_default_142 = None\\n        view_default_487: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_142, [1, 136, 4096]);  mm_default_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_488: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_483, [1, 136, 32, 128]);  view_default_483 = None\\n        transpose_int_100: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_488, 1, 2);  view_default_488 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_489: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_485, [1, 136, 32, 128]);  view_default_485 = None\\n        transpose_int_101: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_489, 1, 2);  view_default_489 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_490: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_487, [1, 136, 32, 128]);  view_default_487 = None\\n        transpose_int_102: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_490, 1, 2);  view_default_490 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant40 = self._tensor_constant40\\n        slice_tensor_204: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant40, 0, 0, 9223372036854775807);  _tensor_constant40 = None\\n        slice_tensor_205: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_204, 1, 0, 9223372036854775807);  slice_tensor_204 = None\\n        slice_tensor_206: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_205, 2, 0, 136);  slice_tensor_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant41 = self._tensor_constant41\\n        slice_tensor_207: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant41, 0, 0, 9223372036854775807);  _tensor_constant41 = None\\n        slice_tensor_208: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_207, 1, 0, 9223372036854775807);  slice_tensor_207 = None\\n        slice_tensor_209: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_208, 2, 0, 136);  slice_tensor_208 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_80: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_206, 1);  slice_tensor_206 = None\\n        squeeze_dim_81: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_80, 0);  squeeze_dim_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_82: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_209, 1);  slice_tensor_209 = None\\n        squeeze_dim_83: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_82, 0);  squeeze_dim_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_40: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_81, [view_default]);  squeeze_dim_81 = None\\n        unsqueeze_default_45: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_40, 1);  index_tensor_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_41: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_83, [view_default]);  squeeze_dim_83 = None\\n        unsqueeze_default_46: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_41, 1);  index_tensor_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_182: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_100, unsqueeze_default_45)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_210: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_211: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 64, 9223372036854775807);  transpose_int_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_40: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_211);  slice_tensor_211 = None\\n        cat_default_40: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_40, slice_tensor_210], -1);  neg_default_40 = slice_tensor_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_183: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_40, unsqueeze_default_46);  cat_default_40 = None\\n        add_tensor_143: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_182, mul_tensor_183);  mul_tensor_182 = mul_tensor_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_184: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_101, unsqueeze_default_45);  unsqueeze_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_212: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_213: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 64, 9223372036854775807);  transpose_int_101 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_41: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_213);  slice_tensor_213 = None\\n        cat_default_41: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_41, slice_tensor_212], -1);  neg_default_41 = slice_tensor_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_185: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_41, unsqueeze_default_46);  cat_default_41 = unsqueeze_default_46 = None\\n        add_tensor_144: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_184, mul_tensor_185);  mul_tensor_184 = mul_tensor_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_103: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_144, 2, 3)\\n        expand_default_82: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_143, [1, 32, 136, 128]);  add_tensor_143 = None\\n        view_default_491: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_82, [32, 136, 128]);  expand_default_82 = None\\n        expand_default_83: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_103, [1, 32, 128, 136]);  transpose_int_103 = None\\n        view_default_492: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_83, [32, 128, 136]);  expand_default_83 = None\\n        bmm_default_40: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_491, view_default_492);  view_default_491 = view_default_492 = None\\n        view_default_493: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_40, [1, 32, 136, 136]);  bmm_default_40 = None\\n        div_tensor_20: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_493, 11.313708498984761);  view_default_493 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_145: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_20, add_tensor_1);  div_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_20: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_145, -1, False);  add_tensor_145 = None\\n        detach_default_61: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_20)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_84: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_20, [1, 32, 136, 136]);  _softmax_default_20 = None\\n        view_default_494: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_84, [32, 136, 136]);  expand_default_84 = None\\n        expand_default_85: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_102, [1, 32, 136, 128])\\n        view_default_495: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_85, [32, 136, 128]);  expand_default_85 = None\\n        bmm_default_41: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_494, view_default_495);  view_default_494 = view_default_495 = None\\n        view_default_496: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_41, [1, 32, 136, 128]);  bmm_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_104: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_496, 1, 2);  view_default_496 = None\\n        clone_default_20: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_104, memory_format = torch.contiguous_format);  transpose_int_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_497: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_20, [1, 136, 4096]);  clone_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant185 = self._param_constant185\\n        t_default_143: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant185);  _param_constant185 = None\\n        view_default_498: f32[136, 4096] = torch.ops.aten.view.default(view_default_497, [136, 4096]);  view_default_497 = None\\n        mm_default_143: f32[136, 4096] = torch.ops.aten.mm.default(view_default_498, t_default_143);  view_default_498 = t_default_143 = None\\n        view_default_499: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_143, [1, 136, 4096]);  mm_default_143 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_146: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_141, view_default_499);  add_tensor_141 = view_default_499 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_41: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_146, 2)\\n        mean_dim_41: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_41, [-1], True);  pow_tensor_scalar_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_147: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_41, 1e-06);  mean_dim_41 = None\\n        rsqrt_default_41: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_147);  add_tensor_147 = None\\n        detach_default_62: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_41)\\n        mul_tensor_186: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_146, rsqrt_default_41);  rsqrt_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant186 = self._param_constant186\\n        mul_tensor_187: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant186, mul_tensor_186);  _param_constant186 = mul_tensor_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant187 = self._param_constant187\\n        t_default_144: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant187);  _param_constant187 = None\\n        view_default_500: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_187, [136, 4096])\\n        mm_default_144: f32[136, 11008] = torch.ops.aten.mm.default(view_default_500, t_default_144);  view_default_500 = t_default_144 = None\\n        view_default_501: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_144, [1, 136, 11008]);  mm_default_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_20: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_501);  view_default_501 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant188 = self._param_constant188\\n        t_default_145: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant188);  _param_constant188 = None\\n        view_default_502: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_187, [136, 4096]);  mul_tensor_187 = None\\n        mm_default_145: f32[136, 11008] = torch.ops.aten.mm.default(view_default_502, t_default_145);  view_default_502 = t_default_145 = None\\n        view_default_503: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_145, [1, 136, 11008]);  mm_default_145 = None\\n        mul_tensor_188: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_20, view_default_503);  silu_default_20 = view_default_503 = None\\n        _param_constant189 = self._param_constant189\\n        t_default_146: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant189);  _param_constant189 = None\\n        view_default_504: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_188, [136, 11008]);  mul_tensor_188 = None\\n        mm_default_146: f32[136, 4096] = torch.ops.aten.mm.default(view_default_504, t_default_146);  view_default_504 = t_default_146 = None\\n        view_default_505: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_146, [1, 136, 4096]);  mm_default_146 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_148: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_146, view_default_505);  add_tensor_146 = view_default_505 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_42: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_148, 2)\\n        mean_dim_42: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_42, [-1], True);  pow_tensor_scalar_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_149: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_42, 1e-06);  mean_dim_42 = None\\n        rsqrt_default_42: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_149);  add_tensor_149 = None\\n        detach_default_63: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_42)\\n        mul_tensor_189: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_148, rsqrt_default_42);  rsqrt_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant190 = self._param_constant190\\n        mul_tensor_190: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant190, mul_tensor_189);  _param_constant190 = mul_tensor_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant191 = self._param_constant191\\n        t_default_147: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant191);  _param_constant191 = None\\n        view_default_506: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_190, [136, 4096])\\n        mm_default_147: f32[136, 4096] = torch.ops.aten.mm.default(view_default_506, t_default_147);  view_default_506 = t_default_147 = None\\n        view_default_507: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_147, [1, 136, 4096]);  mm_default_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant192 = self._param_constant192\\n        t_default_148: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant192);  _param_constant192 = None\\n        view_default_508: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_190, [136, 4096])\\n        mm_default_148: f32[136, 4096] = torch.ops.aten.mm.default(view_default_508, t_default_148);  view_default_508 = t_default_148 = None\\n        view_default_509: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_148, [1, 136, 4096]);  mm_default_148 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant193 = self._param_constant193\\n        t_default_149: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant193);  _param_constant193 = None\\n        view_default_510: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_190, [136, 4096]);  mul_tensor_190 = None\\n        mm_default_149: f32[136, 4096] = torch.ops.aten.mm.default(view_default_510, t_default_149);  view_default_510 = t_default_149 = None\\n        view_default_511: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_149, [1, 136, 4096]);  mm_default_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_512: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_507, [1, 136, 32, 128]);  view_default_507 = None\\n        transpose_int_105: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_512, 1, 2);  view_default_512 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_513: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_509, [1, 136, 32, 128]);  view_default_509 = None\\n        transpose_int_106: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_513, 1, 2);  view_default_513 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_514: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_511, [1, 136, 32, 128]);  view_default_511 = None\\n        transpose_int_107: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_514, 1, 2);  view_default_514 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant42 = self._tensor_constant42\\n        slice_tensor_214: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant42, 0, 0, 9223372036854775807);  _tensor_constant42 = None\\n        slice_tensor_215: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_214, 1, 0, 9223372036854775807);  slice_tensor_214 = None\\n        slice_tensor_216: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_215, 2, 0, 136);  slice_tensor_215 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant43 = self._tensor_constant43\\n        slice_tensor_217: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant43, 0, 0, 9223372036854775807);  _tensor_constant43 = None\\n        slice_tensor_218: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_217, 1, 0, 9223372036854775807);  slice_tensor_217 = None\\n        slice_tensor_219: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_218, 2, 0, 136);  slice_tensor_218 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_84: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_216, 1);  slice_tensor_216 = None\\n        squeeze_dim_85: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_84, 0);  squeeze_dim_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_86: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_219, 1);  slice_tensor_219 = None\\n        squeeze_dim_87: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_86, 0);  squeeze_dim_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_42: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_85, [view_default]);  squeeze_dim_85 = None\\n        unsqueeze_default_47: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_42, 1);  index_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_43: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_87, [view_default]);  squeeze_dim_87 = None\\n        unsqueeze_default_48: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_43, 1);  index_tensor_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_191: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_105, unsqueeze_default_47)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_220: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_221: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 64, 9223372036854775807);  transpose_int_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_42: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_221);  slice_tensor_221 = None\\n        cat_default_42: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_42, slice_tensor_220], -1);  neg_default_42 = slice_tensor_220 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_192: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_42, unsqueeze_default_48);  cat_default_42 = None\\n        add_tensor_150: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_191, mul_tensor_192);  mul_tensor_191 = mul_tensor_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_193: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_106, unsqueeze_default_47);  unsqueeze_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_222: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_223: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 64, 9223372036854775807);  transpose_int_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_43: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_223);  slice_tensor_223 = None\\n        cat_default_43: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_43, slice_tensor_222], -1);  neg_default_43 = slice_tensor_222 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_194: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_43, unsqueeze_default_48);  cat_default_43 = unsqueeze_default_48 = None\\n        add_tensor_151: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_193, mul_tensor_194);  mul_tensor_193 = mul_tensor_194 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_108: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_151, 2, 3)\\n        expand_default_86: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_150, [1, 32, 136, 128]);  add_tensor_150 = None\\n        view_default_515: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_86, [32, 136, 128]);  expand_default_86 = None\\n        expand_default_87: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_108, [1, 32, 128, 136]);  transpose_int_108 = None\\n        view_default_516: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_87, [32, 128, 136]);  expand_default_87 = None\\n        bmm_default_42: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_515, view_default_516);  view_default_515 = view_default_516 = None\\n        view_default_517: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_42, [1, 32, 136, 136]);  bmm_default_42 = None\\n        div_tensor_21: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_517, 11.313708498984761);  view_default_517 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_152: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_21, add_tensor_1);  div_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_21: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_152, -1, False);  add_tensor_152 = None\\n        detach_default_64: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_88: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_21, [1, 32, 136, 136]);  _softmax_default_21 = None\\n        view_default_518: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_88, [32, 136, 136]);  expand_default_88 = None\\n        expand_default_89: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_107, [1, 32, 136, 128])\\n        view_default_519: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_89, [32, 136, 128]);  expand_default_89 = None\\n        bmm_default_43: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_518, view_default_519);  view_default_518 = view_default_519 = None\\n        view_default_520: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_43, [1, 32, 136, 128]);  bmm_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_109: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_520, 1, 2);  view_default_520 = None\\n        clone_default_21: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_109, memory_format = torch.contiguous_format);  transpose_int_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_521: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_21, [1, 136, 4096]);  clone_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant194 = self._param_constant194\\n        t_default_150: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant194);  _param_constant194 = None\\n        view_default_522: f32[136, 4096] = torch.ops.aten.view.default(view_default_521, [136, 4096]);  view_default_521 = None\\n        mm_default_150: f32[136, 4096] = torch.ops.aten.mm.default(view_default_522, t_default_150);  view_default_522 = t_default_150 = None\\n        view_default_523: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_150, [1, 136, 4096]);  mm_default_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_153: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_148, view_default_523);  add_tensor_148 = view_default_523 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_43: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_153, 2)\\n        mean_dim_43: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_43, [-1], True);  pow_tensor_scalar_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_154: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_43, 1e-06);  mean_dim_43 = None\\n        rsqrt_default_43: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_154);  add_tensor_154 = None\\n        detach_default_65: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_43)\\n        mul_tensor_195: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_153, rsqrt_default_43);  rsqrt_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant195 = self._param_constant195\\n        mul_tensor_196: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant195, mul_tensor_195);  _param_constant195 = mul_tensor_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant196 = self._param_constant196\\n        t_default_151: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant196);  _param_constant196 = None\\n        view_default_524: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_196, [136, 4096])\\n        mm_default_151: f32[136, 11008] = torch.ops.aten.mm.default(view_default_524, t_default_151);  view_default_524 = t_default_151 = None\\n        view_default_525: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_151, [1, 136, 11008]);  mm_default_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_21: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_525);  view_default_525 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant197 = self._param_constant197\\n        t_default_152: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant197);  _param_constant197 = None\\n        view_default_526: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_196, [136, 4096]);  mul_tensor_196 = None\\n        mm_default_152: f32[136, 11008] = torch.ops.aten.mm.default(view_default_526, t_default_152);  view_default_526 = t_default_152 = None\\n        view_default_527: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_152, [1, 136, 11008]);  mm_default_152 = None\\n        mul_tensor_197: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_21, view_default_527);  silu_default_21 = view_default_527 = None\\n        _param_constant198 = self._param_constant198\\n        t_default_153: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant198);  _param_constant198 = None\\n        view_default_528: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_197, [136, 11008]);  mul_tensor_197 = None\\n        mm_default_153: f32[136, 4096] = torch.ops.aten.mm.default(view_default_528, t_default_153);  view_default_528 = t_default_153 = None\\n        view_default_529: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_153, [1, 136, 4096]);  mm_default_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_155: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_153, view_default_529);  add_tensor_153 = view_default_529 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_44: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_155, 2)\\n        mean_dim_44: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_44, [-1], True);  pow_tensor_scalar_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_156: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_44, 1e-06);  mean_dim_44 = None\\n        rsqrt_default_44: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_156);  add_tensor_156 = None\\n        detach_default_66: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_44)\\n        mul_tensor_198: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_155, rsqrt_default_44);  rsqrt_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant199 = self._param_constant199\\n        mul_tensor_199: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant199, mul_tensor_198);  _param_constant199 = mul_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant200 = self._param_constant200\\n        t_default_154: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant200);  _param_constant200 = None\\n        view_default_530: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_199, [136, 4096])\\n        mm_default_154: f32[136, 4096] = torch.ops.aten.mm.default(view_default_530, t_default_154);  view_default_530 = t_default_154 = None\\n        view_default_531: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_154, [1, 136, 4096]);  mm_default_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant201 = self._param_constant201\\n        t_default_155: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant201);  _param_constant201 = None\\n        view_default_532: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_199, [136, 4096])\\n        mm_default_155: f32[136, 4096] = torch.ops.aten.mm.default(view_default_532, t_default_155);  view_default_532 = t_default_155 = None\\n        view_default_533: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_155, [1, 136, 4096]);  mm_default_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant202 = self._param_constant202\\n        t_default_156: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant202);  _param_constant202 = None\\n        view_default_534: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_199, [136, 4096]);  mul_tensor_199 = None\\n        mm_default_156: f32[136, 4096] = torch.ops.aten.mm.default(view_default_534, t_default_156);  view_default_534 = t_default_156 = None\\n        view_default_535: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_156, [1, 136, 4096]);  mm_default_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_536: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_531, [1, 136, 32, 128]);  view_default_531 = None\\n        transpose_int_110: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_536, 1, 2);  view_default_536 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_537: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_533, [1, 136, 32, 128]);  view_default_533 = None\\n        transpose_int_111: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_537, 1, 2);  view_default_537 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_538: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_535, [1, 136, 32, 128]);  view_default_535 = None\\n        transpose_int_112: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_538, 1, 2);  view_default_538 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant44 = self._tensor_constant44\\n        slice_tensor_224: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant44, 0, 0, 9223372036854775807);  _tensor_constant44 = None\\n        slice_tensor_225: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_224, 1, 0, 9223372036854775807);  slice_tensor_224 = None\\n        slice_tensor_226: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_225, 2, 0, 136);  slice_tensor_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant45 = self._tensor_constant45\\n        slice_tensor_227: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant45, 0, 0, 9223372036854775807);  _tensor_constant45 = None\\n        slice_tensor_228: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_227, 1, 0, 9223372036854775807);  slice_tensor_227 = None\\n        slice_tensor_229: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_228, 2, 0, 136);  slice_tensor_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_88: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_226, 1);  slice_tensor_226 = None\\n        squeeze_dim_89: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_88, 0);  squeeze_dim_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_90: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_229, 1);  slice_tensor_229 = None\\n        squeeze_dim_91: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_90, 0);  squeeze_dim_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_44: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_89, [view_default]);  squeeze_dim_89 = None\\n        unsqueeze_default_49: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_44, 1);  index_tensor_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_45: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_91, [view_default]);  squeeze_dim_91 = None\\n        unsqueeze_default_50: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_45, 1);  index_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_200: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_110, unsqueeze_default_49)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_230: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_231: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 64, 9223372036854775807);  transpose_int_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_44: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_231);  slice_tensor_231 = None\\n        cat_default_44: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_44, slice_tensor_230], -1);  neg_default_44 = slice_tensor_230 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_201: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_44, unsqueeze_default_50);  cat_default_44 = None\\n        add_tensor_157: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_200, mul_tensor_201);  mul_tensor_200 = mul_tensor_201 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_202: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_111, unsqueeze_default_49);  unsqueeze_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_232: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_233: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 64, 9223372036854775807);  transpose_int_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_45: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_233);  slice_tensor_233 = None\\n        cat_default_45: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_45, slice_tensor_232], -1);  neg_default_45 = slice_tensor_232 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_203: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_45, unsqueeze_default_50);  cat_default_45 = unsqueeze_default_50 = None\\n        add_tensor_158: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_202, mul_tensor_203);  mul_tensor_202 = mul_tensor_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_113: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_158, 2, 3)\\n        expand_default_90: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_157, [1, 32, 136, 128]);  add_tensor_157 = None\\n        view_default_539: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_90, [32, 136, 128]);  expand_default_90 = None\\n        expand_default_91: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_113, [1, 32, 128, 136]);  transpose_int_113 = None\\n        view_default_540: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_91, [32, 128, 136]);  expand_default_91 = None\\n        bmm_default_44: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_539, view_default_540);  view_default_539 = view_default_540 = None\\n        view_default_541: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_44, [1, 32, 136, 136]);  bmm_default_44 = None\\n        div_tensor_22: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_541, 11.313708498984761);  view_default_541 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_159: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_22, add_tensor_1);  div_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_22: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_159, -1, False);  add_tensor_159 = None\\n        detach_default_67: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_22)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_92: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_22, [1, 32, 136, 136]);  _softmax_default_22 = None\\n        view_default_542: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_92, [32, 136, 136]);  expand_default_92 = None\\n        expand_default_93: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_112, [1, 32, 136, 128])\\n        view_default_543: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_93, [32, 136, 128]);  expand_default_93 = None\\n        bmm_default_45: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_542, view_default_543);  view_default_542 = view_default_543 = None\\n        view_default_544: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_45, [1, 32, 136, 128]);  bmm_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_114: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_544, 1, 2);  view_default_544 = None\\n        clone_default_22: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_114, memory_format = torch.contiguous_format);  transpose_int_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_545: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_22, [1, 136, 4096]);  clone_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant203 = self._param_constant203\\n        t_default_157: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant203);  _param_constant203 = None\\n        view_default_546: f32[136, 4096] = torch.ops.aten.view.default(view_default_545, [136, 4096]);  view_default_545 = None\\n        mm_default_157: f32[136, 4096] = torch.ops.aten.mm.default(view_default_546, t_default_157);  view_default_546 = t_default_157 = None\\n        view_default_547: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_157, [1, 136, 4096]);  mm_default_157 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_160: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_155, view_default_547);  add_tensor_155 = view_default_547 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_45: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_160, 2)\\n        mean_dim_45: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_45, [-1], True);  pow_tensor_scalar_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_161: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_45, 1e-06);  mean_dim_45 = None\\n        rsqrt_default_45: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_161);  add_tensor_161 = None\\n        detach_default_68: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_45)\\n        mul_tensor_204: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_160, rsqrt_default_45);  rsqrt_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant204 = self._param_constant204\\n        mul_tensor_205: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant204, mul_tensor_204);  _param_constant204 = mul_tensor_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant205 = self._param_constant205\\n        t_default_158: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant205);  _param_constant205 = None\\n        view_default_548: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_205, [136, 4096])\\n        mm_default_158: f32[136, 11008] = torch.ops.aten.mm.default(view_default_548, t_default_158);  view_default_548 = t_default_158 = None\\n        view_default_549: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_158, [1, 136, 11008]);  mm_default_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_22: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_549);  view_default_549 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant206 = self._param_constant206\\n        t_default_159: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant206);  _param_constant206 = None\\n        view_default_550: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_205, [136, 4096]);  mul_tensor_205 = None\\n        mm_default_159: f32[136, 11008] = torch.ops.aten.mm.default(view_default_550, t_default_159);  view_default_550 = t_default_159 = None\\n        view_default_551: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_159, [1, 136, 11008]);  mm_default_159 = None\\n        mul_tensor_206: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_22, view_default_551);  silu_default_22 = view_default_551 = None\\n        _param_constant207 = self._param_constant207\\n        t_default_160: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant207);  _param_constant207 = None\\n        view_default_552: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_206, [136, 11008]);  mul_tensor_206 = None\\n        mm_default_160: f32[136, 4096] = torch.ops.aten.mm.default(view_default_552, t_default_160);  view_default_552 = t_default_160 = None\\n        view_default_553: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_160, [1, 136, 4096]);  mm_default_160 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_162: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_160, view_default_553);  add_tensor_160 = view_default_553 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_46: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_162, 2)\\n        mean_dim_46: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_46, [-1], True);  pow_tensor_scalar_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_163: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_46, 1e-06);  mean_dim_46 = None\\n        rsqrt_default_46: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_163);  add_tensor_163 = None\\n        detach_default_69: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_46)\\n        mul_tensor_207: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_162, rsqrt_default_46);  rsqrt_default_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant208 = self._param_constant208\\n        mul_tensor_208: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant208, mul_tensor_207);  _param_constant208 = mul_tensor_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant209 = self._param_constant209\\n        t_default_161: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant209);  _param_constant209 = None\\n        view_default_554: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_208, [136, 4096])\\n        mm_default_161: f32[136, 4096] = torch.ops.aten.mm.default(view_default_554, t_default_161);  view_default_554 = t_default_161 = None\\n        view_default_555: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_161, [1, 136, 4096]);  mm_default_161 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant210 = self._param_constant210\\n        t_default_162: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant210);  _param_constant210 = None\\n        view_default_556: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_208, [136, 4096])\\n        mm_default_162: f32[136, 4096] = torch.ops.aten.mm.default(view_default_556, t_default_162);  view_default_556 = t_default_162 = None\\n        view_default_557: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_162, [1, 136, 4096]);  mm_default_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant211 = self._param_constant211\\n        t_default_163: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant211);  _param_constant211 = None\\n        view_default_558: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_208, [136, 4096]);  mul_tensor_208 = None\\n        mm_default_163: f32[136, 4096] = torch.ops.aten.mm.default(view_default_558, t_default_163);  view_default_558 = t_default_163 = None\\n        view_default_559: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_163, [1, 136, 4096]);  mm_default_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_560: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_555, [1, 136, 32, 128]);  view_default_555 = None\\n        transpose_int_115: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_560, 1, 2);  view_default_560 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_561: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_557, [1, 136, 32, 128]);  view_default_557 = None\\n        transpose_int_116: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_561, 1, 2);  view_default_561 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_562: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_559, [1, 136, 32, 128]);  view_default_559 = None\\n        transpose_int_117: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_562, 1, 2);  view_default_562 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant46 = self._tensor_constant46\\n        slice_tensor_234: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant46, 0, 0, 9223372036854775807);  _tensor_constant46 = None\\n        slice_tensor_235: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_234, 1, 0, 9223372036854775807);  slice_tensor_234 = None\\n        slice_tensor_236: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_235, 2, 0, 136);  slice_tensor_235 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant47 = self._tensor_constant47\\n        slice_tensor_237: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant47, 0, 0, 9223372036854775807);  _tensor_constant47 = None\\n        slice_tensor_238: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_237, 1, 0, 9223372036854775807);  slice_tensor_237 = None\\n        slice_tensor_239: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_238, 2, 0, 136);  slice_tensor_238 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_92: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_236, 1);  slice_tensor_236 = None\\n        squeeze_dim_93: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_92, 0);  squeeze_dim_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_94: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_239, 1);  slice_tensor_239 = None\\n        squeeze_dim_95: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_94, 0);  squeeze_dim_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_46: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_93, [view_default]);  squeeze_dim_93 = None\\n        unsqueeze_default_51: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_46, 1);  index_tensor_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_47: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_95, [view_default]);  squeeze_dim_95 = None\\n        unsqueeze_default_52: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_47, 1);  index_tensor_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_209: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_115, unsqueeze_default_51)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_240: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_241: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 64, 9223372036854775807);  transpose_int_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_46: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_241);  slice_tensor_241 = None\\n        cat_default_46: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_46, slice_tensor_240], -1);  neg_default_46 = slice_tensor_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_210: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_46, unsqueeze_default_52);  cat_default_46 = None\\n        add_tensor_164: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_209, mul_tensor_210);  mul_tensor_209 = mul_tensor_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_211: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_116, unsqueeze_default_51);  unsqueeze_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_242: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_243: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 64, 9223372036854775807);  transpose_int_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_47: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_243);  slice_tensor_243 = None\\n        cat_default_47: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_47, slice_tensor_242], -1);  neg_default_47 = slice_tensor_242 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_212: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_47, unsqueeze_default_52);  cat_default_47 = unsqueeze_default_52 = None\\n        add_tensor_165: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_211, mul_tensor_212);  mul_tensor_211 = mul_tensor_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_118: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_165, 2, 3)\\n        expand_default_94: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_164, [1, 32, 136, 128]);  add_tensor_164 = None\\n        view_default_563: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_94, [32, 136, 128]);  expand_default_94 = None\\n        expand_default_95: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_118, [1, 32, 128, 136]);  transpose_int_118 = None\\n        view_default_564: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_95, [32, 128, 136]);  expand_default_95 = None\\n        bmm_default_46: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_563, view_default_564);  view_default_563 = view_default_564 = None\\n        view_default_565: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_46, [1, 32, 136, 136]);  bmm_default_46 = None\\n        div_tensor_23: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_565, 11.313708498984761);  view_default_565 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_166: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_23, add_tensor_1);  div_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_23: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_166, -1, False);  add_tensor_166 = None\\n        detach_default_70: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_23)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_96: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_23, [1, 32, 136, 136]);  _softmax_default_23 = None\\n        view_default_566: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_96, [32, 136, 136]);  expand_default_96 = None\\n        expand_default_97: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_117, [1, 32, 136, 128])\\n        view_default_567: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_97, [32, 136, 128]);  expand_default_97 = None\\n        bmm_default_47: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_566, view_default_567);  view_default_566 = view_default_567 = None\\n        view_default_568: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_47, [1, 32, 136, 128]);  bmm_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_119: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_568, 1, 2);  view_default_568 = None\\n        clone_default_23: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_119, memory_format = torch.contiguous_format);  transpose_int_119 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_569: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_23, [1, 136, 4096]);  clone_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant212 = self._param_constant212\\n        t_default_164: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant212);  _param_constant212 = None\\n        view_default_570: f32[136, 4096] = torch.ops.aten.view.default(view_default_569, [136, 4096]);  view_default_569 = None\\n        mm_default_164: f32[136, 4096] = torch.ops.aten.mm.default(view_default_570, t_default_164);  view_default_570 = t_default_164 = None\\n        view_default_571: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_164, [1, 136, 4096]);  mm_default_164 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_167: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_162, view_default_571);  add_tensor_162 = view_default_571 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_47: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_167, 2)\\n        mean_dim_47: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_47, [-1], True);  pow_tensor_scalar_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_168: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_47, 1e-06);  mean_dim_47 = None\\n        rsqrt_default_47: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_168);  add_tensor_168 = None\\n        detach_default_71: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_47)\\n        mul_tensor_213: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_167, rsqrt_default_47);  rsqrt_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant213 = self._param_constant213\\n        mul_tensor_214: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant213, mul_tensor_213);  _param_constant213 = mul_tensor_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant214 = self._param_constant214\\n        t_default_165: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant214);  _param_constant214 = None\\n        view_default_572: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_214, [136, 4096])\\n        mm_default_165: f32[136, 11008] = torch.ops.aten.mm.default(view_default_572, t_default_165);  view_default_572 = t_default_165 = None\\n        view_default_573: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_165, [1, 136, 11008]);  mm_default_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_23: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_573);  view_default_573 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant215 = self._param_constant215\\n        t_default_166: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant215);  _param_constant215 = None\\n        view_default_574: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_214, [136, 4096]);  mul_tensor_214 = None\\n        mm_default_166: f32[136, 11008] = torch.ops.aten.mm.default(view_default_574, t_default_166);  view_default_574 = t_default_166 = None\\n        view_default_575: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_166, [1, 136, 11008]);  mm_default_166 = None\\n        mul_tensor_215: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_23, view_default_575);  silu_default_23 = view_default_575 = None\\n        _param_constant216 = self._param_constant216\\n        t_default_167: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant216);  _param_constant216 = None\\n        view_default_576: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_215, [136, 11008]);  mul_tensor_215 = None\\n        mm_default_167: f32[136, 4096] = torch.ops.aten.mm.default(view_default_576, t_default_167);  view_default_576 = t_default_167 = None\\n        view_default_577: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_167, [1, 136, 4096]);  mm_default_167 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_169: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_167, view_default_577);  add_tensor_167 = view_default_577 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_48: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_169, 2)\\n        mean_dim_48: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_48, [-1], True);  pow_tensor_scalar_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_170: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_48, 1e-06);  mean_dim_48 = None\\n        rsqrt_default_48: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_170);  add_tensor_170 = None\\n        detach_default_72: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_48)\\n        mul_tensor_216: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_169, rsqrt_default_48);  rsqrt_default_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant217 = self._param_constant217\\n        mul_tensor_217: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant217, mul_tensor_216);  _param_constant217 = mul_tensor_216 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant218 = self._param_constant218\\n        t_default_168: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant218);  _param_constant218 = None\\n        view_default_578: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_217, [136, 4096])\\n        mm_default_168: f32[136, 4096] = torch.ops.aten.mm.default(view_default_578, t_default_168);  view_default_578 = t_default_168 = None\\n        view_default_579: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_168, [1, 136, 4096]);  mm_default_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant219 = self._param_constant219\\n        t_default_169: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant219);  _param_constant219 = None\\n        view_default_580: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_217, [136, 4096])\\n        mm_default_169: f32[136, 4096] = torch.ops.aten.mm.default(view_default_580, t_default_169);  view_default_580 = t_default_169 = None\\n        view_default_581: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_169, [1, 136, 4096]);  mm_default_169 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant220 = self._param_constant220\\n        t_default_170: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant220);  _param_constant220 = None\\n        view_default_582: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_217, [136, 4096]);  mul_tensor_217 = None\\n        mm_default_170: f32[136, 4096] = torch.ops.aten.mm.default(view_default_582, t_default_170);  view_default_582 = t_default_170 = None\\n        view_default_583: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_170, [1, 136, 4096]);  mm_default_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_584: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_579, [1, 136, 32, 128]);  view_default_579 = None\\n        transpose_int_120: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_584, 1, 2);  view_default_584 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_585: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_581, [1, 136, 32, 128]);  view_default_581 = None\\n        transpose_int_121: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_585, 1, 2);  view_default_585 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_586: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_583, [1, 136, 32, 128]);  view_default_583 = None\\n        transpose_int_122: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_586, 1, 2);  view_default_586 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant48 = self._tensor_constant48\\n        slice_tensor_244: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant48, 0, 0, 9223372036854775807);  _tensor_constant48 = None\\n        slice_tensor_245: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_244, 1, 0, 9223372036854775807);  slice_tensor_244 = None\\n        slice_tensor_246: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_245, 2, 0, 136);  slice_tensor_245 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant49 = self._tensor_constant49\\n        slice_tensor_247: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant49, 0, 0, 9223372036854775807);  _tensor_constant49 = None\\n        slice_tensor_248: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_247, 1, 0, 9223372036854775807);  slice_tensor_247 = None\\n        slice_tensor_249: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_248, 2, 0, 136);  slice_tensor_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_96: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_246, 1);  slice_tensor_246 = None\\n        squeeze_dim_97: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_96, 0);  squeeze_dim_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_98: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_249, 1);  slice_tensor_249 = None\\n        squeeze_dim_99: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_98, 0);  squeeze_dim_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_48: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_97, [view_default]);  squeeze_dim_97 = None\\n        unsqueeze_default_53: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_48, 1);  index_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_49: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_99, [view_default]);  squeeze_dim_99 = None\\n        unsqueeze_default_54: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_49, 1);  index_tensor_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_218: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_120, unsqueeze_default_53)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_250: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_251: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 64, 9223372036854775807);  transpose_int_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_48: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_251);  slice_tensor_251 = None\\n        cat_default_48: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_48, slice_tensor_250], -1);  neg_default_48 = slice_tensor_250 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_219: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_48, unsqueeze_default_54);  cat_default_48 = None\\n        add_tensor_171: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_218, mul_tensor_219);  mul_tensor_218 = mul_tensor_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_220: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_121, unsqueeze_default_53);  unsqueeze_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_252: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_253: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 64, 9223372036854775807);  transpose_int_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_49: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_253);  slice_tensor_253 = None\\n        cat_default_49: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_49, slice_tensor_252], -1);  neg_default_49 = slice_tensor_252 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_221: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_49, unsqueeze_default_54);  cat_default_49 = unsqueeze_default_54 = None\\n        add_tensor_172: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_220, mul_tensor_221);  mul_tensor_220 = mul_tensor_221 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_123: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_172, 2, 3)\\n        expand_default_98: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_171, [1, 32, 136, 128]);  add_tensor_171 = None\\n        view_default_587: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_98, [32, 136, 128]);  expand_default_98 = None\\n        expand_default_99: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_123, [1, 32, 128, 136]);  transpose_int_123 = None\\n        view_default_588: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_99, [32, 128, 136]);  expand_default_99 = None\\n        bmm_default_48: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_587, view_default_588);  view_default_587 = view_default_588 = None\\n        view_default_589: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_48, [1, 32, 136, 136]);  bmm_default_48 = None\\n        div_tensor_24: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_589, 11.313708498984761);  view_default_589 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_173: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_24, add_tensor_1);  div_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_24: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_173, -1, False);  add_tensor_173 = None\\n        detach_default_73: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_24)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_100: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_24, [1, 32, 136, 136]);  _softmax_default_24 = None\\n        view_default_590: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_100, [32, 136, 136]);  expand_default_100 = None\\n        expand_default_101: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_122, [1, 32, 136, 128])\\n        view_default_591: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_101, [32, 136, 128]);  expand_default_101 = None\\n        bmm_default_49: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_590, view_default_591);  view_default_590 = view_default_591 = None\\n        view_default_592: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_49, [1, 32, 136, 128]);  bmm_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_124: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_592, 1, 2);  view_default_592 = None\\n        clone_default_24: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_124, memory_format = torch.contiguous_format);  transpose_int_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_593: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_24, [1, 136, 4096]);  clone_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant221 = self._param_constant221\\n        t_default_171: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant221);  _param_constant221 = None\\n        view_default_594: f32[136, 4096] = torch.ops.aten.view.default(view_default_593, [136, 4096]);  view_default_593 = None\\n        mm_default_171: f32[136, 4096] = torch.ops.aten.mm.default(view_default_594, t_default_171);  view_default_594 = t_default_171 = None\\n        view_default_595: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_171, [1, 136, 4096]);  mm_default_171 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_174: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_169, view_default_595);  add_tensor_169 = view_default_595 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_49: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_174, 2)\\n        mean_dim_49: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_49, [-1], True);  pow_tensor_scalar_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_175: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_49, 1e-06);  mean_dim_49 = None\\n        rsqrt_default_49: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_175);  add_tensor_175 = None\\n        detach_default_74: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_49)\\n        mul_tensor_222: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_174, rsqrt_default_49);  rsqrt_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant222 = self._param_constant222\\n        mul_tensor_223: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant222, mul_tensor_222);  _param_constant222 = mul_tensor_222 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant223 = self._param_constant223\\n        t_default_172: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant223);  _param_constant223 = None\\n        view_default_596: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_223, [136, 4096])\\n        mm_default_172: f32[136, 11008] = torch.ops.aten.mm.default(view_default_596, t_default_172);  view_default_596 = t_default_172 = None\\n        view_default_597: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_172, [1, 136, 11008]);  mm_default_172 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_24: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_597);  view_default_597 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant224 = self._param_constant224\\n        t_default_173: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant224);  _param_constant224 = None\\n        view_default_598: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_223, [136, 4096]);  mul_tensor_223 = None\\n        mm_default_173: f32[136, 11008] = torch.ops.aten.mm.default(view_default_598, t_default_173);  view_default_598 = t_default_173 = None\\n        view_default_599: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_173, [1, 136, 11008]);  mm_default_173 = None\\n        mul_tensor_224: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_24, view_default_599);  silu_default_24 = view_default_599 = None\\n        _param_constant225 = self._param_constant225\\n        t_default_174: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant225);  _param_constant225 = None\\n        view_default_600: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_224, [136, 11008]);  mul_tensor_224 = None\\n        mm_default_174: f32[136, 4096] = torch.ops.aten.mm.default(view_default_600, t_default_174);  view_default_600 = t_default_174 = None\\n        view_default_601: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_174, [1, 136, 4096]);  mm_default_174 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_176: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_174, view_default_601);  add_tensor_174 = view_default_601 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_50: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_176, 2)\\n        mean_dim_50: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_50, [-1], True);  pow_tensor_scalar_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_177: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_50, 1e-06);  mean_dim_50 = None\\n        rsqrt_default_50: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_177);  add_tensor_177 = None\\n        detach_default_75: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_50)\\n        mul_tensor_225: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_176, rsqrt_default_50);  rsqrt_default_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant226 = self._param_constant226\\n        mul_tensor_226: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant226, mul_tensor_225);  _param_constant226 = mul_tensor_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant227 = self._param_constant227\\n        t_default_175: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant227);  _param_constant227 = None\\n        view_default_602: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_226, [136, 4096])\\n        mm_default_175: f32[136, 4096] = torch.ops.aten.mm.default(view_default_602, t_default_175);  view_default_602 = t_default_175 = None\\n        view_default_603: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_175, [1, 136, 4096]);  mm_default_175 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant228 = self._param_constant228\\n        t_default_176: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant228);  _param_constant228 = None\\n        view_default_604: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_226, [136, 4096])\\n        mm_default_176: f32[136, 4096] = torch.ops.aten.mm.default(view_default_604, t_default_176);  view_default_604 = t_default_176 = None\\n        view_default_605: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_176, [1, 136, 4096]);  mm_default_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant229 = self._param_constant229\\n        t_default_177: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant229);  _param_constant229 = None\\n        view_default_606: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_226, [136, 4096]);  mul_tensor_226 = None\\n        mm_default_177: f32[136, 4096] = torch.ops.aten.mm.default(view_default_606, t_default_177);  view_default_606 = t_default_177 = None\\n        view_default_607: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_177, [1, 136, 4096]);  mm_default_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_608: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_603, [1, 136, 32, 128]);  view_default_603 = None\\n        transpose_int_125: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_608, 1, 2);  view_default_608 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_609: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_605, [1, 136, 32, 128]);  view_default_605 = None\\n        transpose_int_126: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_609, 1, 2);  view_default_609 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_610: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_607, [1, 136, 32, 128]);  view_default_607 = None\\n        transpose_int_127: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_610, 1, 2);  view_default_610 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant50 = self._tensor_constant50\\n        slice_tensor_254: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant50, 0, 0, 9223372036854775807);  _tensor_constant50 = None\\n        slice_tensor_255: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_254, 1, 0, 9223372036854775807);  slice_tensor_254 = None\\n        slice_tensor_256: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_255, 2, 0, 136);  slice_tensor_255 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant51 = self._tensor_constant51\\n        slice_tensor_257: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant51, 0, 0, 9223372036854775807);  _tensor_constant51 = None\\n        slice_tensor_258: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_257, 1, 0, 9223372036854775807);  slice_tensor_257 = None\\n        slice_tensor_259: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_258, 2, 0, 136);  slice_tensor_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_100: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_256, 1);  slice_tensor_256 = None\\n        squeeze_dim_101: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_100, 0);  squeeze_dim_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_102: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_259, 1);  slice_tensor_259 = None\\n        squeeze_dim_103: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_102, 0);  squeeze_dim_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_50: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_101, [view_default]);  squeeze_dim_101 = None\\n        unsqueeze_default_55: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_50, 1);  index_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_51: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_103, [view_default]);  squeeze_dim_103 = None\\n        unsqueeze_default_56: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_51, 1);  index_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_227: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_125, unsqueeze_default_55)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_260: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_261: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 64, 9223372036854775807);  transpose_int_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_50: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_261);  slice_tensor_261 = None\\n        cat_default_50: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_50, slice_tensor_260], -1);  neg_default_50 = slice_tensor_260 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_228: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_50, unsqueeze_default_56);  cat_default_50 = None\\n        add_tensor_178: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_227, mul_tensor_228);  mul_tensor_227 = mul_tensor_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_229: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_126, unsqueeze_default_55);  unsqueeze_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_262: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_263: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 64, 9223372036854775807);  transpose_int_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_51: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_263);  slice_tensor_263 = None\\n        cat_default_51: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_51, slice_tensor_262], -1);  neg_default_51 = slice_tensor_262 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_230: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_51, unsqueeze_default_56);  cat_default_51 = unsqueeze_default_56 = None\\n        add_tensor_179: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_229, mul_tensor_230);  mul_tensor_229 = mul_tensor_230 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_128: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_179, 2, 3)\\n        expand_default_102: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_178, [1, 32, 136, 128]);  add_tensor_178 = None\\n        view_default_611: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_102, [32, 136, 128]);  expand_default_102 = None\\n        expand_default_103: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_128, [1, 32, 128, 136]);  transpose_int_128 = None\\n        view_default_612: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_103, [32, 128, 136]);  expand_default_103 = None\\n        bmm_default_50: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_611, view_default_612);  view_default_611 = view_default_612 = None\\n        view_default_613: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_50, [1, 32, 136, 136]);  bmm_default_50 = None\\n        div_tensor_25: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_613, 11.313708498984761);  view_default_613 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_180: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_25, add_tensor_1);  div_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_25: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_180, -1, False);  add_tensor_180 = None\\n        detach_default_76: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_25)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_104: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_25, [1, 32, 136, 136]);  _softmax_default_25 = None\\n        view_default_614: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_104, [32, 136, 136]);  expand_default_104 = None\\n        expand_default_105: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_127, [1, 32, 136, 128])\\n        view_default_615: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_105, [32, 136, 128]);  expand_default_105 = None\\n        bmm_default_51: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_614, view_default_615);  view_default_614 = view_default_615 = None\\n        view_default_616: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_51, [1, 32, 136, 128]);  bmm_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_129: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_616, 1, 2);  view_default_616 = None\\n        clone_default_25: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_129, memory_format = torch.contiguous_format);  transpose_int_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_617: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_25, [1, 136, 4096]);  clone_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant230 = self._param_constant230\\n        t_default_178: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant230);  _param_constant230 = None\\n        view_default_618: f32[136, 4096] = torch.ops.aten.view.default(view_default_617, [136, 4096]);  view_default_617 = None\\n        mm_default_178: f32[136, 4096] = torch.ops.aten.mm.default(view_default_618, t_default_178);  view_default_618 = t_default_178 = None\\n        view_default_619: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_178, [1, 136, 4096]);  mm_default_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_181: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_176, view_default_619);  add_tensor_176 = view_default_619 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_51: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_181, 2)\\n        mean_dim_51: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_51, [-1], True);  pow_tensor_scalar_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_182: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_51, 1e-06);  mean_dim_51 = None\\n        rsqrt_default_51: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_182);  add_tensor_182 = None\\n        detach_default_77: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_51)\\n        mul_tensor_231: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_181, rsqrt_default_51);  rsqrt_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant231 = self._param_constant231\\n        mul_tensor_232: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant231, mul_tensor_231);  _param_constant231 = mul_tensor_231 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant232 = self._param_constant232\\n        t_default_179: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant232);  _param_constant232 = None\\n        view_default_620: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_232, [136, 4096])\\n        mm_default_179: f32[136, 11008] = torch.ops.aten.mm.default(view_default_620, t_default_179);  view_default_620 = t_default_179 = None\\n        view_default_621: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_179, [1, 136, 11008]);  mm_default_179 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_25: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_621);  view_default_621 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant233 = self._param_constant233\\n        t_default_180: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant233);  _param_constant233 = None\\n        view_default_622: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_232, [136, 4096]);  mul_tensor_232 = None\\n        mm_default_180: f32[136, 11008] = torch.ops.aten.mm.default(view_default_622, t_default_180);  view_default_622 = t_default_180 = None\\n        view_default_623: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_180, [1, 136, 11008]);  mm_default_180 = None\\n        mul_tensor_233: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_25, view_default_623);  silu_default_25 = view_default_623 = None\\n        _param_constant234 = self._param_constant234\\n        t_default_181: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant234);  _param_constant234 = None\\n        view_default_624: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_233, [136, 11008]);  mul_tensor_233 = None\\n        mm_default_181: f32[136, 4096] = torch.ops.aten.mm.default(view_default_624, t_default_181);  view_default_624 = t_default_181 = None\\n        view_default_625: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_181, [1, 136, 4096]);  mm_default_181 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_183: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_181, view_default_625);  add_tensor_181 = view_default_625 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_52: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_183, 2)\\n        mean_dim_52: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_52, [-1], True);  pow_tensor_scalar_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_184: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_52, 1e-06);  mean_dim_52 = None\\n        rsqrt_default_52: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_184);  add_tensor_184 = None\\n        detach_default_78: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_52)\\n        mul_tensor_234: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_183, rsqrt_default_52);  rsqrt_default_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant235 = self._param_constant235\\n        mul_tensor_235: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant235, mul_tensor_234);  _param_constant235 = mul_tensor_234 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant236 = self._param_constant236\\n        t_default_182: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant236);  _param_constant236 = None\\n        view_default_626: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_235, [136, 4096])\\n        mm_default_182: f32[136, 4096] = torch.ops.aten.mm.default(view_default_626, t_default_182);  view_default_626 = t_default_182 = None\\n        view_default_627: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_182, [1, 136, 4096]);  mm_default_182 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant237 = self._param_constant237\\n        t_default_183: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant237);  _param_constant237 = None\\n        view_default_628: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_235, [136, 4096])\\n        mm_default_183: f32[136, 4096] = torch.ops.aten.mm.default(view_default_628, t_default_183);  view_default_628 = t_default_183 = None\\n        view_default_629: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_183, [1, 136, 4096]);  mm_default_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant238 = self._param_constant238\\n        t_default_184: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant238);  _param_constant238 = None\\n        view_default_630: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_235, [136, 4096]);  mul_tensor_235 = None\\n        mm_default_184: f32[136, 4096] = torch.ops.aten.mm.default(view_default_630, t_default_184);  view_default_630 = t_default_184 = None\\n        view_default_631: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_184, [1, 136, 4096]);  mm_default_184 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_632: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_627, [1, 136, 32, 128]);  view_default_627 = None\\n        transpose_int_130: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_632, 1, 2);  view_default_632 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_633: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_629, [1, 136, 32, 128]);  view_default_629 = None\\n        transpose_int_131: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_633, 1, 2);  view_default_633 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_634: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_631, [1, 136, 32, 128]);  view_default_631 = None\\n        transpose_int_132: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_634, 1, 2);  view_default_634 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant52 = self._tensor_constant52\\n        slice_tensor_264: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant52, 0, 0, 9223372036854775807);  _tensor_constant52 = None\\n        slice_tensor_265: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_264, 1, 0, 9223372036854775807);  slice_tensor_264 = None\\n        slice_tensor_266: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_265, 2, 0, 136);  slice_tensor_265 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant53 = self._tensor_constant53\\n        slice_tensor_267: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant53, 0, 0, 9223372036854775807);  _tensor_constant53 = None\\n        slice_tensor_268: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_267, 1, 0, 9223372036854775807);  slice_tensor_267 = None\\n        slice_tensor_269: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_268, 2, 0, 136);  slice_tensor_268 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_104: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_266, 1);  slice_tensor_266 = None\\n        squeeze_dim_105: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_104, 0);  squeeze_dim_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_106: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_269, 1);  slice_tensor_269 = None\\n        squeeze_dim_107: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_106, 0);  squeeze_dim_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_52: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_105, [view_default]);  squeeze_dim_105 = None\\n        unsqueeze_default_57: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_52, 1);  index_tensor_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_53: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_107, [view_default]);  squeeze_dim_107 = None\\n        unsqueeze_default_58: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_53, 1);  index_tensor_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_236: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_130, unsqueeze_default_57)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_270: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_271: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 64, 9223372036854775807);  transpose_int_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_52: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_271);  slice_tensor_271 = None\\n        cat_default_52: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_52, slice_tensor_270], -1);  neg_default_52 = slice_tensor_270 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_237: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_52, unsqueeze_default_58);  cat_default_52 = None\\n        add_tensor_185: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_236, mul_tensor_237);  mul_tensor_236 = mul_tensor_237 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_238: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_131, unsqueeze_default_57);  unsqueeze_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_272: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_273: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 64, 9223372036854775807);  transpose_int_131 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_53: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_273);  slice_tensor_273 = None\\n        cat_default_53: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_53, slice_tensor_272], -1);  neg_default_53 = slice_tensor_272 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_239: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_53, unsqueeze_default_58);  cat_default_53 = unsqueeze_default_58 = None\\n        add_tensor_186: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_238, mul_tensor_239);  mul_tensor_238 = mul_tensor_239 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_133: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_186, 2, 3)\\n        expand_default_106: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_185, [1, 32, 136, 128]);  add_tensor_185 = None\\n        view_default_635: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_106, [32, 136, 128]);  expand_default_106 = None\\n        expand_default_107: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_133, [1, 32, 128, 136]);  transpose_int_133 = None\\n        view_default_636: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_107, [32, 128, 136]);  expand_default_107 = None\\n        bmm_default_52: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_635, view_default_636);  view_default_635 = view_default_636 = None\\n        view_default_637: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_52, [1, 32, 136, 136]);  bmm_default_52 = None\\n        div_tensor_26: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_637, 11.313708498984761);  view_default_637 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_187: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_26, add_tensor_1);  div_tensor_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_26: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_187, -1, False);  add_tensor_187 = None\\n        detach_default_79: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_26)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_108: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_26, [1, 32, 136, 136]);  _softmax_default_26 = None\\n        view_default_638: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_108, [32, 136, 136]);  expand_default_108 = None\\n        expand_default_109: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_132, [1, 32, 136, 128])\\n        view_default_639: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_109, [32, 136, 128]);  expand_default_109 = None\\n        bmm_default_53: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_638, view_default_639);  view_default_638 = view_default_639 = None\\n        view_default_640: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_53, [1, 32, 136, 128]);  bmm_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_134: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_640, 1, 2);  view_default_640 = None\\n        clone_default_26: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_134, memory_format = torch.contiguous_format);  transpose_int_134 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_641: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_26, [1, 136, 4096]);  clone_default_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant239 = self._param_constant239\\n        t_default_185: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant239);  _param_constant239 = None\\n        view_default_642: f32[136, 4096] = torch.ops.aten.view.default(view_default_641, [136, 4096]);  view_default_641 = None\\n        mm_default_185: f32[136, 4096] = torch.ops.aten.mm.default(view_default_642, t_default_185);  view_default_642 = t_default_185 = None\\n        view_default_643: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_185, [1, 136, 4096]);  mm_default_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_188: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_183, view_default_643);  add_tensor_183 = view_default_643 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_53: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_188, 2)\\n        mean_dim_53: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_53, [-1], True);  pow_tensor_scalar_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_189: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_53, 1e-06);  mean_dim_53 = None\\n        rsqrt_default_53: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_189);  add_tensor_189 = None\\n        detach_default_80: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_53)\\n        mul_tensor_240: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_188, rsqrt_default_53);  rsqrt_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant240 = self._param_constant240\\n        mul_tensor_241: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant240, mul_tensor_240);  _param_constant240 = mul_tensor_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant241 = self._param_constant241\\n        t_default_186: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant241);  _param_constant241 = None\\n        view_default_644: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_241, [136, 4096])\\n        mm_default_186: f32[136, 11008] = torch.ops.aten.mm.default(view_default_644, t_default_186);  view_default_644 = t_default_186 = None\\n        view_default_645: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_186, [1, 136, 11008]);  mm_default_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_26: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_645);  view_default_645 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant242 = self._param_constant242\\n        t_default_187: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant242);  _param_constant242 = None\\n        view_default_646: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_241, [136, 4096]);  mul_tensor_241 = None\\n        mm_default_187: f32[136, 11008] = torch.ops.aten.mm.default(view_default_646, t_default_187);  view_default_646 = t_default_187 = None\\n        view_default_647: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_187, [1, 136, 11008]);  mm_default_187 = None\\n        mul_tensor_242: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_26, view_default_647);  silu_default_26 = view_default_647 = None\\n        _param_constant243 = self._param_constant243\\n        t_default_188: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant243);  _param_constant243 = None\\n        view_default_648: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_242, [136, 11008]);  mul_tensor_242 = None\\n        mm_default_188: f32[136, 4096] = torch.ops.aten.mm.default(view_default_648, t_default_188);  view_default_648 = t_default_188 = None\\n        view_default_649: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_188, [1, 136, 4096]);  mm_default_188 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_190: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_188, view_default_649);  add_tensor_188 = view_default_649 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_54: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_190, 2)\\n        mean_dim_54: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_54, [-1], True);  pow_tensor_scalar_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_191: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_54, 1e-06);  mean_dim_54 = None\\n        rsqrt_default_54: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_191);  add_tensor_191 = None\\n        detach_default_81: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_54)\\n        mul_tensor_243: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_190, rsqrt_default_54);  rsqrt_default_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant244 = self._param_constant244\\n        mul_tensor_244: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant244, mul_tensor_243);  _param_constant244 = mul_tensor_243 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant245 = self._param_constant245\\n        t_default_189: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant245);  _param_constant245 = None\\n        view_default_650: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_244, [136, 4096])\\n        mm_default_189: f32[136, 4096] = torch.ops.aten.mm.default(view_default_650, t_default_189);  view_default_650 = t_default_189 = None\\n        view_default_651: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_189, [1, 136, 4096]);  mm_default_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant246 = self._param_constant246\\n        t_default_190: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant246);  _param_constant246 = None\\n        view_default_652: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_244, [136, 4096])\\n        mm_default_190: f32[136, 4096] = torch.ops.aten.mm.default(view_default_652, t_default_190);  view_default_652 = t_default_190 = None\\n        view_default_653: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_190, [1, 136, 4096]);  mm_default_190 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant247 = self._param_constant247\\n        t_default_191: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant247);  _param_constant247 = None\\n        view_default_654: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_244, [136, 4096]);  mul_tensor_244 = None\\n        mm_default_191: f32[136, 4096] = torch.ops.aten.mm.default(view_default_654, t_default_191);  view_default_654 = t_default_191 = None\\n        view_default_655: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_191, [1, 136, 4096]);  mm_default_191 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_656: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_651, [1, 136, 32, 128]);  view_default_651 = None\\n        transpose_int_135: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_656, 1, 2);  view_default_656 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_657: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_653, [1, 136, 32, 128]);  view_default_653 = None\\n        transpose_int_136: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_657, 1, 2);  view_default_657 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_658: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_655, [1, 136, 32, 128]);  view_default_655 = None\\n        transpose_int_137: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_658, 1, 2);  view_default_658 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant54 = self._tensor_constant54\\n        slice_tensor_274: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant54, 0, 0, 9223372036854775807);  _tensor_constant54 = None\\n        slice_tensor_275: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_274, 1, 0, 9223372036854775807);  slice_tensor_274 = None\\n        slice_tensor_276: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_275, 2, 0, 136);  slice_tensor_275 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant55 = self._tensor_constant55\\n        slice_tensor_277: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant55, 0, 0, 9223372036854775807);  _tensor_constant55 = None\\n        slice_tensor_278: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_277, 1, 0, 9223372036854775807);  slice_tensor_277 = None\\n        slice_tensor_279: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_278, 2, 0, 136);  slice_tensor_278 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_108: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_276, 1);  slice_tensor_276 = None\\n        squeeze_dim_109: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_108, 0);  squeeze_dim_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_110: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_279, 1);  slice_tensor_279 = None\\n        squeeze_dim_111: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_110, 0);  squeeze_dim_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_54: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_109, [view_default]);  squeeze_dim_109 = None\\n        unsqueeze_default_59: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_54, 1);  index_tensor_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_55: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_111, [view_default]);  squeeze_dim_111 = None\\n        unsqueeze_default_60: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_55, 1);  index_tensor_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_245: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_135, unsqueeze_default_59)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_280: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_281: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 64, 9223372036854775807);  transpose_int_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_54: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_281);  slice_tensor_281 = None\\n        cat_default_54: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_54, slice_tensor_280], -1);  neg_default_54 = slice_tensor_280 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_246: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_54, unsqueeze_default_60);  cat_default_54 = None\\n        add_tensor_192: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_245, mul_tensor_246);  mul_tensor_245 = mul_tensor_246 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_247: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_136, unsqueeze_default_59);  unsqueeze_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_282: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_283: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 64, 9223372036854775807);  transpose_int_136 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_55: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_283);  slice_tensor_283 = None\\n        cat_default_55: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_55, slice_tensor_282], -1);  neg_default_55 = slice_tensor_282 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_248: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_55, unsqueeze_default_60);  cat_default_55 = unsqueeze_default_60 = None\\n        add_tensor_193: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_247, mul_tensor_248);  mul_tensor_247 = mul_tensor_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_138: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_193, 2, 3)\\n        expand_default_110: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_192, [1, 32, 136, 128]);  add_tensor_192 = None\\n        view_default_659: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_110, [32, 136, 128]);  expand_default_110 = None\\n        expand_default_111: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_138, [1, 32, 128, 136]);  transpose_int_138 = None\\n        view_default_660: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_111, [32, 128, 136]);  expand_default_111 = None\\n        bmm_default_54: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_659, view_default_660);  view_default_659 = view_default_660 = None\\n        view_default_661: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_54, [1, 32, 136, 136]);  bmm_default_54 = None\\n        div_tensor_27: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_661, 11.313708498984761);  view_default_661 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_194: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_27, add_tensor_1);  div_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_27: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_194, -1, False);  add_tensor_194 = None\\n        detach_default_82: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_27)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_112: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_27, [1, 32, 136, 136]);  _softmax_default_27 = None\\n        view_default_662: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_112, [32, 136, 136]);  expand_default_112 = None\\n        expand_default_113: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_137, [1, 32, 136, 128])\\n        view_default_663: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_113, [32, 136, 128]);  expand_default_113 = None\\n        bmm_default_55: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_662, view_default_663);  view_default_662 = view_default_663 = None\\n        view_default_664: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_55, [1, 32, 136, 128]);  bmm_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_139: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_664, 1, 2);  view_default_664 = None\\n        clone_default_27: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_139, memory_format = torch.contiguous_format);  transpose_int_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_665: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_27, [1, 136, 4096]);  clone_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant248 = self._param_constant248\\n        t_default_192: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant248);  _param_constant248 = None\\n        view_default_666: f32[136, 4096] = torch.ops.aten.view.default(view_default_665, [136, 4096]);  view_default_665 = None\\n        mm_default_192: f32[136, 4096] = torch.ops.aten.mm.default(view_default_666, t_default_192);  view_default_666 = t_default_192 = None\\n        view_default_667: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_192, [1, 136, 4096]);  mm_default_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_195: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_190, view_default_667);  add_tensor_190 = view_default_667 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_55: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_195, 2)\\n        mean_dim_55: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_55, [-1], True);  pow_tensor_scalar_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_196: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_55, 1e-06);  mean_dim_55 = None\\n        rsqrt_default_55: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_196);  add_tensor_196 = None\\n        detach_default_83: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_55)\\n        mul_tensor_249: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_195, rsqrt_default_55);  rsqrt_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant249 = self._param_constant249\\n        mul_tensor_250: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant249, mul_tensor_249);  _param_constant249 = mul_tensor_249 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant250 = self._param_constant250\\n        t_default_193: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant250);  _param_constant250 = None\\n        view_default_668: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_250, [136, 4096])\\n        mm_default_193: f32[136, 11008] = torch.ops.aten.mm.default(view_default_668, t_default_193);  view_default_668 = t_default_193 = None\\n        view_default_669: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_193, [1, 136, 11008]);  mm_default_193 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_27: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_669);  view_default_669 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant251 = self._param_constant251\\n        t_default_194: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant251);  _param_constant251 = None\\n        view_default_670: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_250, [136, 4096]);  mul_tensor_250 = None\\n        mm_default_194: f32[136, 11008] = torch.ops.aten.mm.default(view_default_670, t_default_194);  view_default_670 = t_default_194 = None\\n        view_default_671: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_194, [1, 136, 11008]);  mm_default_194 = None\\n        mul_tensor_251: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_27, view_default_671);  silu_default_27 = view_default_671 = None\\n        _param_constant252 = self._param_constant252\\n        t_default_195: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant252);  _param_constant252 = None\\n        view_default_672: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_251, [136, 11008]);  mul_tensor_251 = None\\n        mm_default_195: f32[136, 4096] = torch.ops.aten.mm.default(view_default_672, t_default_195);  view_default_672 = t_default_195 = None\\n        view_default_673: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_195, [1, 136, 4096]);  mm_default_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_197: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_195, view_default_673);  add_tensor_195 = view_default_673 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_56: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_197, 2)\\n        mean_dim_56: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_56, [-1], True);  pow_tensor_scalar_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_198: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_56, 1e-06);  mean_dim_56 = None\\n        rsqrt_default_56: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_198);  add_tensor_198 = None\\n        detach_default_84: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_56)\\n        mul_tensor_252: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_197, rsqrt_default_56);  rsqrt_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant253 = self._param_constant253\\n        mul_tensor_253: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant253, mul_tensor_252);  _param_constant253 = mul_tensor_252 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant254 = self._param_constant254\\n        t_default_196: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant254);  _param_constant254 = None\\n        view_default_674: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_253, [136, 4096])\\n        mm_default_196: f32[136, 4096] = torch.ops.aten.mm.default(view_default_674, t_default_196);  view_default_674 = t_default_196 = None\\n        view_default_675: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_196, [1, 136, 4096]);  mm_default_196 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant255 = self._param_constant255\\n        t_default_197: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant255);  _param_constant255 = None\\n        view_default_676: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_253, [136, 4096])\\n        mm_default_197: f32[136, 4096] = torch.ops.aten.mm.default(view_default_676, t_default_197);  view_default_676 = t_default_197 = None\\n        view_default_677: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_197, [1, 136, 4096]);  mm_default_197 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant256 = self._param_constant256\\n        t_default_198: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant256);  _param_constant256 = None\\n        view_default_678: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_253, [136, 4096]);  mul_tensor_253 = None\\n        mm_default_198: f32[136, 4096] = torch.ops.aten.mm.default(view_default_678, t_default_198);  view_default_678 = t_default_198 = None\\n        view_default_679: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_198, [1, 136, 4096]);  mm_default_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_680: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_675, [1, 136, 32, 128]);  view_default_675 = None\\n        transpose_int_140: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_680, 1, 2);  view_default_680 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_681: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_677, [1, 136, 32, 128]);  view_default_677 = None\\n        transpose_int_141: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_681, 1, 2);  view_default_681 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_682: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_679, [1, 136, 32, 128]);  view_default_679 = None\\n        transpose_int_142: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_682, 1, 2);  view_default_682 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant56 = self._tensor_constant56\\n        slice_tensor_284: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant56, 0, 0, 9223372036854775807);  _tensor_constant56 = None\\n        slice_tensor_285: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_284, 1, 0, 9223372036854775807);  slice_tensor_284 = None\\n        slice_tensor_286: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_285, 2, 0, 136);  slice_tensor_285 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant57 = self._tensor_constant57\\n        slice_tensor_287: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant57, 0, 0, 9223372036854775807);  _tensor_constant57 = None\\n        slice_tensor_288: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_287, 1, 0, 9223372036854775807);  slice_tensor_287 = None\\n        slice_tensor_289: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_288, 2, 0, 136);  slice_tensor_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_112: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_286, 1);  slice_tensor_286 = None\\n        squeeze_dim_113: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_112, 0);  squeeze_dim_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_114: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_289, 1);  slice_tensor_289 = None\\n        squeeze_dim_115: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_114, 0);  squeeze_dim_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_56: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_113, [view_default]);  squeeze_dim_113 = None\\n        unsqueeze_default_61: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_56, 1);  index_tensor_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_57: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_115, [view_default]);  squeeze_dim_115 = None\\n        unsqueeze_default_62: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_57, 1);  index_tensor_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_254: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_140, unsqueeze_default_61)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_290: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_291: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 64, 9223372036854775807);  transpose_int_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_56: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_291);  slice_tensor_291 = None\\n        cat_default_56: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_56, slice_tensor_290], -1);  neg_default_56 = slice_tensor_290 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_255: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_56, unsqueeze_default_62);  cat_default_56 = None\\n        add_tensor_199: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_254, mul_tensor_255);  mul_tensor_254 = mul_tensor_255 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_256: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_141, unsqueeze_default_61);  unsqueeze_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_292: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_293: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 64, 9223372036854775807);  transpose_int_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_57: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_293);  slice_tensor_293 = None\\n        cat_default_57: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_57, slice_tensor_292], -1);  neg_default_57 = slice_tensor_292 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_257: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_57, unsqueeze_default_62);  cat_default_57 = unsqueeze_default_62 = None\\n        add_tensor_200: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_256, mul_tensor_257);  mul_tensor_256 = mul_tensor_257 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_143: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_200, 2, 3)\\n        expand_default_114: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_199, [1, 32, 136, 128]);  add_tensor_199 = None\\n        view_default_683: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_114, [32, 136, 128]);  expand_default_114 = None\\n        expand_default_115: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_143, [1, 32, 128, 136]);  transpose_int_143 = None\\n        view_default_684: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_115, [32, 128, 136]);  expand_default_115 = None\\n        bmm_default_56: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_683, view_default_684);  view_default_683 = view_default_684 = None\\n        view_default_685: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_56, [1, 32, 136, 136]);  bmm_default_56 = None\\n        div_tensor_28: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_685, 11.313708498984761);  view_default_685 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_201: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_28, add_tensor_1);  div_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_28: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_201, -1, False);  add_tensor_201 = None\\n        detach_default_85: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_28)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_116: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_28, [1, 32, 136, 136]);  _softmax_default_28 = None\\n        view_default_686: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_116, [32, 136, 136]);  expand_default_116 = None\\n        expand_default_117: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_142, [1, 32, 136, 128])\\n        view_default_687: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_117, [32, 136, 128]);  expand_default_117 = None\\n        bmm_default_57: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_686, view_default_687);  view_default_686 = view_default_687 = None\\n        view_default_688: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_57, [1, 32, 136, 128]);  bmm_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_144: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_688, 1, 2);  view_default_688 = None\\n        clone_default_28: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_144, memory_format = torch.contiguous_format);  transpose_int_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_689: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_28, [1, 136, 4096]);  clone_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant257 = self._param_constant257\\n        t_default_199: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant257);  _param_constant257 = None\\n        view_default_690: f32[136, 4096] = torch.ops.aten.view.default(view_default_689, [136, 4096]);  view_default_689 = None\\n        mm_default_199: f32[136, 4096] = torch.ops.aten.mm.default(view_default_690, t_default_199);  view_default_690 = t_default_199 = None\\n        view_default_691: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_199, [1, 136, 4096]);  mm_default_199 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_202: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_197, view_default_691);  add_tensor_197 = view_default_691 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_57: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_202, 2)\\n        mean_dim_57: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_57, [-1], True);  pow_tensor_scalar_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_203: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_57, 1e-06);  mean_dim_57 = None\\n        rsqrt_default_57: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_203);  add_tensor_203 = None\\n        detach_default_86: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_57)\\n        mul_tensor_258: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_202, rsqrt_default_57);  rsqrt_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant258 = self._param_constant258\\n        mul_tensor_259: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant258, mul_tensor_258);  _param_constant258 = mul_tensor_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant259 = self._param_constant259\\n        t_default_200: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant259);  _param_constant259 = None\\n        view_default_692: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_259, [136, 4096])\\n        mm_default_200: f32[136, 11008] = torch.ops.aten.mm.default(view_default_692, t_default_200);  view_default_692 = t_default_200 = None\\n        view_default_693: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_200, [1, 136, 11008]);  mm_default_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_28: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_693);  view_default_693 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant260 = self._param_constant260\\n        t_default_201: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant260);  _param_constant260 = None\\n        view_default_694: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_259, [136, 4096]);  mul_tensor_259 = None\\n        mm_default_201: f32[136, 11008] = torch.ops.aten.mm.default(view_default_694, t_default_201);  view_default_694 = t_default_201 = None\\n        view_default_695: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_201, [1, 136, 11008]);  mm_default_201 = None\\n        mul_tensor_260: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_28, view_default_695);  silu_default_28 = view_default_695 = None\\n        _param_constant261 = self._param_constant261\\n        t_default_202: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant261);  _param_constant261 = None\\n        view_default_696: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_260, [136, 11008]);  mul_tensor_260 = None\\n        mm_default_202: f32[136, 4096] = torch.ops.aten.mm.default(view_default_696, t_default_202);  view_default_696 = t_default_202 = None\\n        view_default_697: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_202, [1, 136, 4096]);  mm_default_202 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_204: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_202, view_default_697);  add_tensor_202 = view_default_697 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_58: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_204, 2)\\n        mean_dim_58: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_58, [-1], True);  pow_tensor_scalar_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_205: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_58, 1e-06);  mean_dim_58 = None\\n        rsqrt_default_58: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_205);  add_tensor_205 = None\\n        detach_default_87: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_58)\\n        mul_tensor_261: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_204, rsqrt_default_58);  rsqrt_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant262 = self._param_constant262\\n        mul_tensor_262: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant262, mul_tensor_261);  _param_constant262 = mul_tensor_261 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant263 = self._param_constant263\\n        t_default_203: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant263);  _param_constant263 = None\\n        view_default_698: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_262, [136, 4096])\\n        mm_default_203: f32[136, 4096] = torch.ops.aten.mm.default(view_default_698, t_default_203);  view_default_698 = t_default_203 = None\\n        view_default_699: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_203, [1, 136, 4096]);  mm_default_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant264 = self._param_constant264\\n        t_default_204: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant264);  _param_constant264 = None\\n        view_default_700: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_262, [136, 4096])\\n        mm_default_204: f32[136, 4096] = torch.ops.aten.mm.default(view_default_700, t_default_204);  view_default_700 = t_default_204 = None\\n        view_default_701: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_204, [1, 136, 4096]);  mm_default_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant265 = self._param_constant265\\n        t_default_205: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant265);  _param_constant265 = None\\n        view_default_702: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_262, [136, 4096]);  mul_tensor_262 = None\\n        mm_default_205: f32[136, 4096] = torch.ops.aten.mm.default(view_default_702, t_default_205);  view_default_702 = t_default_205 = None\\n        view_default_703: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_205, [1, 136, 4096]);  mm_default_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_704: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_699, [1, 136, 32, 128]);  view_default_699 = None\\n        transpose_int_145: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_704, 1, 2);  view_default_704 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_705: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_701, [1, 136, 32, 128]);  view_default_701 = None\\n        transpose_int_146: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_705, 1, 2);  view_default_705 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_706: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_703, [1, 136, 32, 128]);  view_default_703 = None\\n        transpose_int_147: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_706, 1, 2);  view_default_706 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant58 = self._tensor_constant58\\n        slice_tensor_294: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant58, 0, 0, 9223372036854775807);  _tensor_constant58 = None\\n        slice_tensor_295: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_294, 1, 0, 9223372036854775807);  slice_tensor_294 = None\\n        slice_tensor_296: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_295, 2, 0, 136);  slice_tensor_295 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant59 = self._tensor_constant59\\n        slice_tensor_297: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant59, 0, 0, 9223372036854775807);  _tensor_constant59 = None\\n        slice_tensor_298: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_297, 1, 0, 9223372036854775807);  slice_tensor_297 = None\\n        slice_tensor_299: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_298, 2, 0, 136);  slice_tensor_298 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_116: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_296, 1);  slice_tensor_296 = None\\n        squeeze_dim_117: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_116, 0);  squeeze_dim_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_118: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_299, 1);  slice_tensor_299 = None\\n        squeeze_dim_119: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_118, 0);  squeeze_dim_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_58: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_117, [view_default]);  squeeze_dim_117 = None\\n        unsqueeze_default_63: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_58, 1);  index_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_59: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_119, [view_default]);  squeeze_dim_119 = None\\n        unsqueeze_default_64: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_59, 1);  index_tensor_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_263: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_145, unsqueeze_default_63)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_300: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_301: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 64, 9223372036854775807);  transpose_int_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_58: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_301);  slice_tensor_301 = None\\n        cat_default_58: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_58, slice_tensor_300], -1);  neg_default_58 = slice_tensor_300 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_264: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_58, unsqueeze_default_64);  cat_default_58 = None\\n        add_tensor_206: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_263, mul_tensor_264);  mul_tensor_263 = mul_tensor_264 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_265: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_146, unsqueeze_default_63);  unsqueeze_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_302: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_303: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 64, 9223372036854775807);  transpose_int_146 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_59: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_303);  slice_tensor_303 = None\\n        cat_default_59: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_59, slice_tensor_302], -1);  neg_default_59 = slice_tensor_302 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_266: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_59, unsqueeze_default_64);  cat_default_59 = unsqueeze_default_64 = None\\n        add_tensor_207: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_265, mul_tensor_266);  mul_tensor_265 = mul_tensor_266 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_148: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_207, 2, 3)\\n        expand_default_118: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_206, [1, 32, 136, 128]);  add_tensor_206 = None\\n        view_default_707: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_118, [32, 136, 128]);  expand_default_118 = None\\n        expand_default_119: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_148, [1, 32, 128, 136]);  transpose_int_148 = None\\n        view_default_708: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_119, [32, 128, 136]);  expand_default_119 = None\\n        bmm_default_58: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_707, view_default_708);  view_default_707 = view_default_708 = None\\n        view_default_709: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_58, [1, 32, 136, 136]);  bmm_default_58 = None\\n        div_tensor_29: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_709, 11.313708498984761);  view_default_709 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_208: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_29, add_tensor_1);  div_tensor_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_29: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_208, -1, False);  add_tensor_208 = None\\n        detach_default_88: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_29)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_120: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_29, [1, 32, 136, 136]);  _softmax_default_29 = None\\n        view_default_710: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_120, [32, 136, 136]);  expand_default_120 = None\\n        expand_default_121: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_147, [1, 32, 136, 128])\\n        view_default_711: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_121, [32, 136, 128]);  expand_default_121 = None\\n        bmm_default_59: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_710, view_default_711);  view_default_710 = view_default_711 = None\\n        view_default_712: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_59, [1, 32, 136, 128]);  bmm_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_149: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_712, 1, 2);  view_default_712 = None\\n        clone_default_29: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_149, memory_format = torch.contiguous_format);  transpose_int_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_713: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_29, [1, 136, 4096]);  clone_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant266 = self._param_constant266\\n        t_default_206: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant266);  _param_constant266 = None\\n        view_default_714: f32[136, 4096] = torch.ops.aten.view.default(view_default_713, [136, 4096]);  view_default_713 = None\\n        mm_default_206: f32[136, 4096] = torch.ops.aten.mm.default(view_default_714, t_default_206);  view_default_714 = t_default_206 = None\\n        view_default_715: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_206, [1, 136, 4096]);  mm_default_206 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_209: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_204, view_default_715);  add_tensor_204 = view_default_715 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_59: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_209, 2)\\n        mean_dim_59: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_59, [-1], True);  pow_tensor_scalar_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_210: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_59, 1e-06);  mean_dim_59 = None\\n        rsqrt_default_59: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_210);  add_tensor_210 = None\\n        detach_default_89: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_59)\\n        mul_tensor_267: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_209, rsqrt_default_59);  rsqrt_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant267 = self._param_constant267\\n        mul_tensor_268: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant267, mul_tensor_267);  _param_constant267 = mul_tensor_267 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant268 = self._param_constant268\\n        t_default_207: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant268);  _param_constant268 = None\\n        view_default_716: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_268, [136, 4096])\\n        mm_default_207: f32[136, 11008] = torch.ops.aten.mm.default(view_default_716, t_default_207);  view_default_716 = t_default_207 = None\\n        view_default_717: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_207, [1, 136, 11008]);  mm_default_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_29: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_717);  view_default_717 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant269 = self._param_constant269\\n        t_default_208: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant269);  _param_constant269 = None\\n        view_default_718: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_268, [136, 4096]);  mul_tensor_268 = None\\n        mm_default_208: f32[136, 11008] = torch.ops.aten.mm.default(view_default_718, t_default_208);  view_default_718 = t_default_208 = None\\n        view_default_719: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_208, [1, 136, 11008]);  mm_default_208 = None\\n        mul_tensor_269: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_29, view_default_719);  silu_default_29 = view_default_719 = None\\n        _param_constant270 = self._param_constant270\\n        t_default_209: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant270);  _param_constant270 = None\\n        view_default_720: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_269, [136, 11008]);  mul_tensor_269 = None\\n        mm_default_209: f32[136, 4096] = torch.ops.aten.mm.default(view_default_720, t_default_209);  view_default_720 = t_default_209 = None\\n        view_default_721: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_209, [1, 136, 4096]);  mm_default_209 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_211: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_209, view_default_721);  add_tensor_209 = view_default_721 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_60: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_211, 2)\\n        mean_dim_60: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_60, [-1], True);  pow_tensor_scalar_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_212: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_60, 1e-06);  mean_dim_60 = None\\n        rsqrt_default_60: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_212);  add_tensor_212 = None\\n        detach_default_90: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_60)\\n        mul_tensor_270: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_211, rsqrt_default_60);  rsqrt_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant271 = self._param_constant271\\n        mul_tensor_271: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant271, mul_tensor_270);  _param_constant271 = mul_tensor_270 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant272 = self._param_constant272\\n        t_default_210: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant272);  _param_constant272 = None\\n        view_default_722: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_271, [136, 4096])\\n        mm_default_210: f32[136, 4096] = torch.ops.aten.mm.default(view_default_722, t_default_210);  view_default_722 = t_default_210 = None\\n        view_default_723: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_210, [1, 136, 4096]);  mm_default_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant273 = self._param_constant273\\n        t_default_211: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant273);  _param_constant273 = None\\n        view_default_724: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_271, [136, 4096])\\n        mm_default_211: f32[136, 4096] = torch.ops.aten.mm.default(view_default_724, t_default_211);  view_default_724 = t_default_211 = None\\n        view_default_725: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_211, [1, 136, 4096]);  mm_default_211 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant274 = self._param_constant274\\n        t_default_212: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant274);  _param_constant274 = None\\n        view_default_726: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_271, [136, 4096]);  mul_tensor_271 = None\\n        mm_default_212: f32[136, 4096] = torch.ops.aten.mm.default(view_default_726, t_default_212);  view_default_726 = t_default_212 = None\\n        view_default_727: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_212, [1, 136, 4096]);  mm_default_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_728: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_723, [1, 136, 32, 128]);  view_default_723 = None\\n        transpose_int_150: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_728, 1, 2);  view_default_728 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_729: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_725, [1, 136, 32, 128]);  view_default_725 = None\\n        transpose_int_151: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_729, 1, 2);  view_default_729 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_730: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_727, [1, 136, 32, 128]);  view_default_727 = None\\n        transpose_int_152: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_730, 1, 2);  view_default_730 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant60 = self._tensor_constant60\\n        slice_tensor_304: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant60, 0, 0, 9223372036854775807);  _tensor_constant60 = None\\n        slice_tensor_305: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_304, 1, 0, 9223372036854775807);  slice_tensor_304 = None\\n        slice_tensor_306: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_305, 2, 0, 136);  slice_tensor_305 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant61 = self._tensor_constant61\\n        slice_tensor_307: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant61, 0, 0, 9223372036854775807);  _tensor_constant61 = None\\n        slice_tensor_308: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_307, 1, 0, 9223372036854775807);  slice_tensor_307 = None\\n        slice_tensor_309: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_308, 2, 0, 136);  slice_tensor_308 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_120: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_306, 1);  slice_tensor_306 = None\\n        squeeze_dim_121: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_120, 0);  squeeze_dim_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_122: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_309, 1);  slice_tensor_309 = None\\n        squeeze_dim_123: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_122, 0);  squeeze_dim_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_60: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_121, [view_default]);  squeeze_dim_121 = None\\n        unsqueeze_default_65: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_60, 1);  index_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_61: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_123, [view_default]);  squeeze_dim_123 = None\\n        unsqueeze_default_66: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_61, 1);  index_tensor_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_272: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_150, unsqueeze_default_65)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_310: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_311: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 64, 9223372036854775807);  transpose_int_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_60: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_311);  slice_tensor_311 = None\\n        cat_default_60: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_60, slice_tensor_310], -1);  neg_default_60 = slice_tensor_310 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_273: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_60, unsqueeze_default_66);  cat_default_60 = None\\n        add_tensor_213: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_272, mul_tensor_273);  mul_tensor_272 = mul_tensor_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_274: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_151, unsqueeze_default_65);  unsqueeze_default_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_312: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_313: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 64, 9223372036854775807);  transpose_int_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_61: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_313);  slice_tensor_313 = None\\n        cat_default_61: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_61, slice_tensor_312], -1);  neg_default_61 = slice_tensor_312 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_275: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_61, unsqueeze_default_66);  cat_default_61 = unsqueeze_default_66 = None\\n        add_tensor_214: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_274, mul_tensor_275);  mul_tensor_274 = mul_tensor_275 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_153: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_214, 2, 3)\\n        expand_default_122: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_213, [1, 32, 136, 128]);  add_tensor_213 = None\\n        view_default_731: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_122, [32, 136, 128]);  expand_default_122 = None\\n        expand_default_123: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_153, [1, 32, 128, 136]);  transpose_int_153 = None\\n        view_default_732: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_123, [32, 128, 136]);  expand_default_123 = None\\n        bmm_default_60: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_731, view_default_732);  view_default_731 = view_default_732 = None\\n        view_default_733: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_60, [1, 32, 136, 136]);  bmm_default_60 = None\\n        div_tensor_30: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_733, 11.313708498984761);  view_default_733 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_215: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_30, add_tensor_1);  div_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_30: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_215, -1, False);  add_tensor_215 = None\\n        detach_default_91: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_30)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_124: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_30, [1, 32, 136, 136]);  _softmax_default_30 = None\\n        view_default_734: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_124, [32, 136, 136]);  expand_default_124 = None\\n        expand_default_125: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_152, [1, 32, 136, 128])\\n        view_default_735: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_125, [32, 136, 128]);  expand_default_125 = None\\n        bmm_default_61: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_734, view_default_735);  view_default_734 = view_default_735 = None\\n        view_default_736: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_61, [1, 32, 136, 128]);  bmm_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_154: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_736, 1, 2);  view_default_736 = None\\n        clone_default_30: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_154, memory_format = torch.contiguous_format);  transpose_int_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_737: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_30, [1, 136, 4096]);  clone_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant275 = self._param_constant275\\n        t_default_213: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant275);  _param_constant275 = None\\n        view_default_738: f32[136, 4096] = torch.ops.aten.view.default(view_default_737, [136, 4096]);  view_default_737 = None\\n        mm_default_213: f32[136, 4096] = torch.ops.aten.mm.default(view_default_738, t_default_213);  view_default_738 = t_default_213 = None\\n        view_default_739: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_213, [1, 136, 4096]);  mm_default_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_216: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_211, view_default_739);  add_tensor_211 = view_default_739 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_61: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_216, 2)\\n        mean_dim_61: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_61, [-1], True);  pow_tensor_scalar_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_217: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_61, 1e-06);  mean_dim_61 = None\\n        rsqrt_default_61: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_217);  add_tensor_217 = None\\n        detach_default_92: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_61)\\n        mul_tensor_276: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_216, rsqrt_default_61);  rsqrt_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant276 = self._param_constant276\\n        mul_tensor_277: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant276, mul_tensor_276);  _param_constant276 = mul_tensor_276 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant277 = self._param_constant277\\n        t_default_214: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant277);  _param_constant277 = None\\n        view_default_740: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_277, [136, 4096])\\n        mm_default_214: f32[136, 11008] = torch.ops.aten.mm.default(view_default_740, t_default_214);  view_default_740 = t_default_214 = None\\n        view_default_741: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_214, [1, 136, 11008]);  mm_default_214 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_30: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_741);  view_default_741 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant278 = self._param_constant278\\n        t_default_215: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant278);  _param_constant278 = None\\n        view_default_742: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_277, [136, 4096]);  mul_tensor_277 = None\\n        mm_default_215: f32[136, 11008] = torch.ops.aten.mm.default(view_default_742, t_default_215);  view_default_742 = t_default_215 = None\\n        view_default_743: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_215, [1, 136, 11008]);  mm_default_215 = None\\n        mul_tensor_278: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_30, view_default_743);  silu_default_30 = view_default_743 = None\\n        _param_constant279 = self._param_constant279\\n        t_default_216: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant279);  _param_constant279 = None\\n        view_default_744: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_278, [136, 11008]);  mul_tensor_278 = None\\n        mm_default_216: f32[136, 4096] = torch.ops.aten.mm.default(view_default_744, t_default_216);  view_default_744 = t_default_216 = None\\n        view_default_745: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_216, [1, 136, 4096]);  mm_default_216 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_218: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_216, view_default_745);  add_tensor_216 = view_default_745 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_62: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_218, 2)\\n        mean_dim_62: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_62, [-1], True);  pow_tensor_scalar_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_219: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_62, 1e-06);  mean_dim_62 = None\\n        rsqrt_default_62: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_219);  add_tensor_219 = None\\n        detach_default_93: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_62)\\n        mul_tensor_279: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_218, rsqrt_default_62);  rsqrt_default_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant280 = self._param_constant280\\n        mul_tensor_280: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant280, mul_tensor_279);  _param_constant280 = mul_tensor_279 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant281 = self._param_constant281\\n        t_default_217: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant281);  _param_constant281 = None\\n        view_default_746: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_280, [136, 4096])\\n        mm_default_217: f32[136, 4096] = torch.ops.aten.mm.default(view_default_746, t_default_217);  view_default_746 = t_default_217 = None\\n        view_default_747: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_217, [1, 136, 4096]);  mm_default_217 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant282 = self._param_constant282\\n        t_default_218: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant282);  _param_constant282 = None\\n        view_default_748: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_280, [136, 4096])\\n        mm_default_218: f32[136, 4096] = torch.ops.aten.mm.default(view_default_748, t_default_218);  view_default_748 = t_default_218 = None\\n        view_default_749: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_218, [1, 136, 4096]);  mm_default_218 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant283 = self._param_constant283\\n        t_default_219: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant283);  _param_constant283 = None\\n        view_default_750: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_280, [136, 4096]);  mul_tensor_280 = None\\n        mm_default_219: f32[136, 4096] = torch.ops.aten.mm.default(view_default_750, t_default_219);  view_default_750 = t_default_219 = None\\n        view_default_751: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_219, [1, 136, 4096]);  mm_default_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_752: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_747, [1, 136, 32, 128]);  view_default_747 = None\\n        transpose_int_155: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_752, 1, 2);  view_default_752 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_753: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_749, [1, 136, 32, 128]);  view_default_749 = None\\n        transpose_int_156: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_753, 1, 2);  view_default_753 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_754: f32[1, 136, 32, 128] = torch.ops.aten.view.default(view_default_751, [1, 136, 32, 128]);  view_default_751 = None\\n        transpose_int_157: f32[1, 32, 136, 128] = torch.ops.aten.transpose.int(view_default_754, 1, 2);  view_default_754 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant62 = self._tensor_constant62\\n        slice_tensor_314: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant62, 0, 0, 9223372036854775807);  _tensor_constant62 = None\\n        slice_tensor_315: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_314, 1, 0, 9223372036854775807);  slice_tensor_314 = None\\n        slice_tensor_316: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_315, 2, 0, 136);  slice_tensor_315 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant63 = self._tensor_constant63\\n        slice_tensor_317: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant63, 0, 0, 9223372036854775807);  _tensor_constant63 = None\\n        slice_tensor_318: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_317, 1, 0, 9223372036854775807);  slice_tensor_317 = None\\n        slice_tensor_319: f32[1, 1, 136, 128] = torch.ops.aten.slice.Tensor(slice_tensor_318, 2, 0, 136);  slice_tensor_318 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_124: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_316, 1);  slice_tensor_316 = None\\n        squeeze_dim_125: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_124, 0);  squeeze_dim_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_126: f32[1, 136, 128] = torch.ops.aten.squeeze.dim(slice_tensor_319, 1);  slice_tensor_319 = None\\n        squeeze_dim_127: f32[136, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_126, 0);  squeeze_dim_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_62: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_125, [view_default]);  squeeze_dim_125 = None\\n        unsqueeze_default_67: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_62, 1);  index_tensor_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_63: f32[1, 136, 128] = torch.ops.aten.index.Tensor(squeeze_dim_127, [view_default]);  squeeze_dim_127 = view_default = None\\n        unsqueeze_default_68: f32[1, 1, 136, 128] = torch.ops.aten.unsqueeze.default(index_tensor_63, 1);  index_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_281: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_155, unsqueeze_default_67)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_320: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_321: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 64, 9223372036854775807);  transpose_int_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_62: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_321);  slice_tensor_321 = None\\n        cat_default_62: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_62, slice_tensor_320], -1);  neg_default_62 = slice_tensor_320 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_282: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_62, unsqueeze_default_68);  cat_default_62 = None\\n        add_tensor_220: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_281, mul_tensor_282);  mul_tensor_281 = mul_tensor_282 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_283: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(transpose_int_156, unsqueeze_default_67);  unsqueeze_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_322: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_323: f32[1, 32, 136, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 64, 9223372036854775807);  transpose_int_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_63: f32[1, 32, 136, 64] = torch.ops.aten.neg.default(slice_tensor_323);  slice_tensor_323 = None\\n        cat_default_63: f32[1, 32, 136, 128] = torch.ops.aten.cat.default([neg_default_63, slice_tensor_322], -1);  neg_default_63 = slice_tensor_322 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_284: f32[1, 32, 136, 128] = torch.ops.aten.mul.Tensor(cat_default_63, unsqueeze_default_68);  cat_default_63 = unsqueeze_default_68 = None\\n        add_tensor_221: f32[1, 32, 136, 128] = torch.ops.aten.add.Tensor(mul_tensor_283, mul_tensor_284);  mul_tensor_283 = mul_tensor_284 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_158: f32[1, 32, 128, 136] = torch.ops.aten.transpose.int(add_tensor_221, 2, 3)\\n        expand_default_126: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(add_tensor_220, [1, 32, 136, 128]);  add_tensor_220 = None\\n        view_default_755: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_126, [32, 136, 128]);  expand_default_126 = None\\n        expand_default_127: f32[1, 32, 128, 136] = torch.ops.aten.expand.default(transpose_int_158, [1, 32, 128, 136]);  transpose_int_158 = None\\n        view_default_756: f32[32, 128, 136] = torch.ops.aten.view.default(expand_default_127, [32, 128, 136]);  expand_default_127 = None\\n        bmm_default_62: f32[32, 136, 136] = torch.ops.aten.bmm.default(view_default_755, view_default_756);  view_default_755 = view_default_756 = None\\n        view_default_757: f32[1, 32, 136, 136] = torch.ops.aten.view.default(bmm_default_62, [1, 32, 136, 136]);  bmm_default_62 = None\\n        div_tensor_31: f32[1, 32, 136, 136] = torch.ops.aten.div.Tensor(view_default_757, 11.313708498984761);  view_default_757 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_222: f32[1, 32, 136, 136] = torch.ops.aten.add.Tensor(div_tensor_31, add_tensor_1);  div_tensor_31 = add_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_31: f32[1, 32, 136, 136] = torch.ops.aten._softmax.default(add_tensor_222, -1, False);  add_tensor_222 = None\\n        detach_default_94: f32[1, 32, 136, 136] = torch.ops.aten.detach.default(_softmax_default_31)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_128: f32[1, 32, 136, 136] = torch.ops.aten.expand.default(_softmax_default_31, [1, 32, 136, 136]);  _softmax_default_31 = None\\n        view_default_758: f32[32, 136, 136] = torch.ops.aten.view.default(expand_default_128, [32, 136, 136]);  expand_default_128 = None\\n        expand_default_129: f32[1, 32, 136, 128] = torch.ops.aten.expand.default(transpose_int_157, [1, 32, 136, 128])\\n        view_default_759: f32[32, 136, 128] = torch.ops.aten.view.default(expand_default_129, [32, 136, 128]);  expand_default_129 = None\\n        bmm_default_63: f32[32, 136, 128] = torch.ops.aten.bmm.default(view_default_758, view_default_759);  view_default_758 = view_default_759 = None\\n        view_default_760: f32[1, 32, 136, 128] = torch.ops.aten.view.default(bmm_default_63, [1, 32, 136, 128]);  bmm_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_159: f32[1, 136, 32, 128] = torch.ops.aten.transpose.int(view_default_760, 1, 2);  view_default_760 = None\\n        clone_default_31: f32[1, 136, 32, 128] = torch.ops.aten.clone.default(transpose_int_159, memory_format = torch.contiguous_format);  transpose_int_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_761: f32[1, 136, 4096] = torch.ops.aten.view.default(clone_default_31, [1, 136, 4096]);  clone_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant284 = self._param_constant284\\n        t_default_220: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant284);  _param_constant284 = None\\n        view_default_762: f32[136, 4096] = torch.ops.aten.view.default(view_default_761, [136, 4096]);  view_default_761 = None\\n        mm_default_220: f32[136, 4096] = torch.ops.aten.mm.default(view_default_762, t_default_220);  view_default_762 = t_default_220 = None\\n        view_default_763: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_220, [1, 136, 4096]);  mm_default_220 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_223: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_218, view_default_763);  add_tensor_218 = view_default_763 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_63: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_223, 2)\\n        mean_dim_63: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_63, [-1], True);  pow_tensor_scalar_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_224: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_63, 1e-06);  mean_dim_63 = None\\n        rsqrt_default_63: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_224);  add_tensor_224 = None\\n        detach_default_95: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_63)\\n        mul_tensor_285: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_223, rsqrt_default_63);  rsqrt_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant285 = self._param_constant285\\n        mul_tensor_286: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant285, mul_tensor_285);  _param_constant285 = mul_tensor_285 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant286 = self._param_constant286\\n        t_default_221: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant286);  _param_constant286 = None\\n        view_default_764: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_286, [136, 4096])\\n        mm_default_221: f32[136, 11008] = torch.ops.aten.mm.default(view_default_764, t_default_221);  view_default_764 = t_default_221 = None\\n        view_default_765: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_221, [1, 136, 11008]);  mm_default_221 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_31: f32[1, 136, 11008] = torch.ops.aten.silu.default(view_default_765);  view_default_765 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant287 = self._param_constant287\\n        t_default_222: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant287);  _param_constant287 = None\\n        view_default_766: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_286, [136, 4096]);  mul_tensor_286 = None\\n        mm_default_222: f32[136, 11008] = torch.ops.aten.mm.default(view_default_766, t_default_222);  view_default_766 = t_default_222 = None\\n        view_default_767: f32[1, 136, 11008] = torch.ops.aten.view.default(mm_default_222, [1, 136, 11008]);  mm_default_222 = None\\n        mul_tensor_287: f32[1, 136, 11008] = torch.ops.aten.mul.Tensor(silu_default_31, view_default_767);  silu_default_31 = view_default_767 = None\\n        _param_constant288 = self._param_constant288\\n        t_default_223: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant288);  _param_constant288 = None\\n        view_default_768: f32[136, 11008] = torch.ops.aten.view.default(mul_tensor_287, [136, 11008]);  mul_tensor_287 = None\\n        mm_default_223: f32[136, 4096] = torch.ops.aten.mm.default(view_default_768, t_default_223);  view_default_768 = t_default_223 = None\\n        view_default_769: f32[1, 136, 4096] = torch.ops.aten.view.default(mm_default_223, [1, 136, 4096]);  mm_default_223 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_225: f32[1, 136, 4096] = torch.ops.aten.add.Tensor(add_tensor_223, view_default_769);  add_tensor_223 = view_default_769 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_64: f32[1, 136, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_225, 2)\\n        mean_dim_64: f32[1, 136, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_64, [-1], True);  pow_tensor_scalar_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_226: f32[1, 136, 1] = torch.ops.aten.add.Tensor(mean_dim_64, 1e-06);  mean_dim_64 = None\\n        rsqrt_default_64: f32[1, 136, 1] = torch.ops.aten.rsqrt.default(add_tensor_226);  add_tensor_226 = None\\n        detach_default_96: f32[1, 136, 1] = torch.ops.aten.detach.default(rsqrt_default_64)\\n        mul_tensor_288: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(add_tensor_225, rsqrt_default_64);  add_tensor_225 = rsqrt_default_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant289 = self._param_constant289\\n        mul_tensor_289: f32[1, 136, 4096] = torch.ops.aten.mul.Tensor(_param_constant289, mul_tensor_288);  _param_constant289 = mul_tensor_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:838, code: logits = self.lm_head(hidden_states)\\n        _param_constant290 = self._param_constant290\\n        t_default_224: f32[4096, 32000] = torch.ops.aten.t.default(_param_constant290);  _param_constant290 = None\\n        view_default_770: f32[136, 4096] = torch.ops.aten.view.default(mul_tensor_289, [136, 4096]);  mul_tensor_289 = None\\n        mm_default_224: f32[136, 32000] = torch.ops.aten.mm.default(view_default_770, t_default_224);  view_default_770 = t_default_224 = None\\n        view_default_771: f32[1, 136, 32000] = torch.ops.aten.view.default(mm_default_224, [1, 136, 32000]);  mm_default_224 = None\\n        \\n        # File: /tmp/ipykernel_2064425/3922580714.py:36, code: token1 = torch.argmax(result.logits[:, -1, :], dim=1)\\n        slice_tensor_324: f32[1, 136, 32000] = torch.ops.aten.slice.Tensor(view_default_771, 0, 0, 9223372036854775807);  view_default_771 = None\\n        select_int: f32[1, 32000] = torch.ops.aten.select.int(slice_tensor_324, 1, -1);  slice_tensor_324 = None\\n        slice_tensor_325: f32[1, 32000] = torch.ops.aten.slice.Tensor(select_int, 1, 0, 9223372036854775807);  select_int = None\\n        argmax_default: i64[1] = torch.ops.aten.argmax.default(slice_tensor_325, 1);  slice_tensor_325 = None\\n        \\n        # File: /tmp/ipykernel_2064425/3922580714.py:37, code: token1 = token1[None, :]\\n        unsqueeze_default_69: i64[1, 1] = torch.ops.aten.unsqueeze.default(argmax_default, 0);  argmax_default = None\\n        slice_tensor_326: i64[1, 1] = torch.ops.aten.slice.Tensor(unsqueeze_default_69, 1, 0, 9223372036854775807);  unsqueeze_default_69 = None\\n        return pytree.tree_unflatten([slice_tensor_326, add_tensor_4, transpose_int_2, add_tensor_11, transpose_int_7, add_tensor_18, transpose_int_12, add_tensor_25, transpose_int_17, add_tensor_32, transpose_int_22, add_tensor_39, transpose_int_27, add_tensor_46, transpose_int_32, add_tensor_53, transpose_int_37, add_tensor_60, transpose_int_42, add_tensor_67, transpose_int_47, add_tensor_74, transpose_int_52, add_tensor_81, transpose_int_57, add_tensor_88, transpose_int_62, add_tensor_95, transpose_int_67, add_tensor_102, transpose_int_72, add_tensor_109, transpose_int_77, add_tensor_116, transpose_int_82, add_tensor_123, transpose_int_87, add_tensor_130, transpose_int_92, add_tensor_137, transpose_int_97, add_tensor_144, transpose_int_102, add_tensor_151, transpose_int_107, add_tensor_158, transpose_int_112, add_tensor_165, transpose_int_117, add_tensor_172, transpose_int_122, add_tensor_179, transpose_int_127, add_tensor_186, transpose_int_132, add_tensor_193, transpose_int_137, add_tensor_200, transpose_int_142, add_tensor_207, transpose_int_147, add_tensor_214, transpose_int_152, add_tensor_221, transpose_int_157], self._out_spec)\\n        \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export initializer\n",
    "exp_initialize = dynamo.export(\n",
    "    sm.initialize, \n",
    "    aten_graph=True,\n",
    "    assume_static_by_default=True,\n",
    "    constraints=[\n",
    "\n",
    "    ],\n",
    ")\n",
    "g, guards = exp_initialize(input_ids)\n",
    "g.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-07 18:14:02,427] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2023-09-07 18:14:02,450] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 136 for L['state0_flat'][0].size()[2]\n",
      "[2023-09-07 18:14:02,533] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s1 = 136 for L['state0_flat'][1].size()[2]\n",
      "[2023-09-07 18:14:02,545] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s2 = 136 for L['state0_flat'][2].size()[2]\n",
      "[2023-09-07 18:14:02,561] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s3 = 136 for L['state0_flat'][3].size()[2]\n",
      "[2023-09-07 18:14:02,571] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s4 = 136 for L['state0_flat'][4].size()[2]\n",
      "[2023-09-07 18:14:02,594] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s5 = 136 for L['state0_flat'][5].size()[2]\n",
      "[2023-09-07 18:14:02,607] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s6 = 136 for L['state0_flat'][6].size()[2]\n",
      "[2023-09-07 18:14:02,625] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s7 = 136 for L['state0_flat'][7].size()[2]\n",
      "[2023-09-07 18:14:02,635] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s8 = 136 for L['state0_flat'][8].size()[2]\n",
      "[2023-09-07 18:14:02,652] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s9 = 136 for L['state0_flat'][9].size()[2]\n",
      "[2023-09-07 18:14:02,663] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s10 = 136 for L['state0_flat'][10].size()[2]\n",
      "[2023-09-07 18:14:02,680] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s11 = 136 for L['state0_flat'][11].size()[2]\n",
      "[2023-09-07 18:14:02,693] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s12 = 136 for L['state0_flat'][12].size()[2]\n",
      "[2023-09-07 18:14:02,710] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s13 = 136 for L['state0_flat'][13].size()[2]\n",
      "[2023-09-07 18:14:02,721] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s14 = 136 for L['state0_flat'][14].size()[2]\n",
      "[2023-09-07 18:14:02,740] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s15 = 136 for L['state0_flat'][15].size()[2]\n",
      "[2023-09-07 18:14:02,751] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s16 = 136 for L['state0_flat'][16].size()[2]\n",
      "[2023-09-07 18:14:02,769] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s17 = 136 for L['state0_flat'][17].size()[2]\n",
      "[2023-09-07 18:14:02,780] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s18 = 136 for L['state0_flat'][18].size()[2]\n",
      "[2023-09-07 18:14:02,795] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s19 = 136 for L['state0_flat'][19].size()[2]\n",
      "[2023-09-07 18:14:02,805] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s20 = 136 for L['state0_flat'][20].size()[2]\n",
      "[2023-09-07 18:14:02,823] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s21 = 136 for L['state0_flat'][21].size()[2]\n",
      "[2023-09-07 18:14:02,836] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s22 = 136 for L['state0_flat'][22].size()[2]\n",
      "[2023-09-07 18:14:02,851] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s23 = 136 for L['state0_flat'][23].size()[2]\n",
      "[2023-09-07 18:14:02,863] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s24 = 136 for L['state0_flat'][24].size()[2]\n",
      "[2023-09-07 18:14:02,882] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s25 = 136 for L['state0_flat'][25].size()[2]\n",
      "[2023-09-07 18:14:02,895] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s26 = 136 for L['state0_flat'][26].size()[2]\n",
      "[2023-09-07 18:14:02,912] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s27 = 136 for L['state0_flat'][27].size()[2]\n",
      "[2023-09-07 18:14:02,925] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s28 = 136 for L['state0_flat'][28].size()[2]\n",
      "[2023-09-07 18:14:02,943] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s29 = 136 for L['state0_flat'][29].size()[2]\n",
      "[2023-09-07 18:14:02,955] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s30 = 136 for L['state0_flat'][30].size()[2]\n",
      "[2023-09-07 18:14:02,972] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s31 = 136 for L['state0_flat'][31].size()[2]\n",
      "[2023-09-07 18:14:02,986] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s32 = 136 for L['state0_flat'][32].size()[2]\n",
      "[2023-09-07 18:14:03,005] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s33 = 136 for L['state0_flat'][33].size()[2]\n",
      "[2023-09-07 18:14:03,018] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s34 = 136 for L['state0_flat'][34].size()[2]\n",
      "[2023-09-07 18:14:03,036] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s35 = 136 for L['state0_flat'][35].size()[2]\n",
      "[2023-09-07 18:14:03,050] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s36 = 136 for L['state0_flat'][36].size()[2]\n",
      "[2023-09-07 18:14:03,068] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s37 = 136 for L['state0_flat'][37].size()[2]\n",
      "[2023-09-07 18:14:03,080] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s38 = 136 for L['state0_flat'][38].size()[2]\n",
      "[2023-09-07 18:14:03,096] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s39 = 136 for L['state0_flat'][39].size()[2]\n",
      "[2023-09-07 18:14:03,106] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s40 = 136 for L['state0_flat'][40].size()[2]\n",
      "[2023-09-07 18:14:03,124] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s41 = 136 for L['state0_flat'][41].size()[2]\n",
      "[2023-09-07 18:14:03,134] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s42 = 136 for L['state0_flat'][42].size()[2]\n",
      "[2023-09-07 18:14:03,150] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s43 = 136 for L['state0_flat'][43].size()[2]\n",
      "[2023-09-07 18:14:03,163] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s44 = 136 for L['state0_flat'][44].size()[2]\n",
      "[2023-09-07 18:14:03,182] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s45 = 136 for L['state0_flat'][45].size()[2]\n",
      "[2023-09-07 18:14:03,196] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s46 = 136 for L['state0_flat'][46].size()[2]\n",
      "[2023-09-07 18:14:03,213] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s47 = 136 for L['state0_flat'][47].size()[2]\n",
      "[2023-09-07 18:14:03,225] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s48 = 136 for L['state0_flat'][48].size()[2]\n",
      "[2023-09-07 18:14:03,243] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s49 = 136 for L['state0_flat'][49].size()[2]\n",
      "[2023-09-07 18:14:03,256] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s50 = 136 for L['state0_flat'][50].size()[2]\n",
      "[2023-09-07 18:14:03,275] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s51 = 136 for L['state0_flat'][51].size()[2]\n",
      "[2023-09-07 18:14:03,288] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s52 = 136 for L['state0_flat'][52].size()[2]\n",
      "[2023-09-07 18:14:03,303] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s53 = 136 for L['state0_flat'][53].size()[2]\n",
      "[2023-09-07 18:14:03,313] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s54 = 136 for L['state0_flat'][54].size()[2]\n",
      "[2023-09-07 18:14:03,329] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s55 = 136 for L['state0_flat'][55].size()[2]\n",
      "[2023-09-07 18:14:03,341] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s56 = 136 for L['state0_flat'][56].size()[2]\n",
      "[2023-09-07 18:14:03,359] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s57 = 136 for L['state0_flat'][57].size()[2]\n",
      "[2023-09-07 18:14:03,371] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s58 = 136 for L['state0_flat'][58].size()[2]\n",
      "[2023-09-07 18:14:03,388] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s59 = 136 for L['state0_flat'][59].size()[2]\n",
      "[2023-09-07 18:14:03,404] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s60 = 136 for L['state0_flat'][60].size()[2]\n",
      "[2023-09-07 18:14:03,423] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s61 = 136 for L['state0_flat'][61].size()[2]\n",
      "[2023-09-07 18:14:03,433] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s62 = 136 for L['state0_flat'][62].size()[2]\n",
      "[2023-09-07 18:14:03,449] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s63 = 136 for L['state0_flat'][63].size()[2]\n",
      "[2023-09-07 18:14:04,001] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s1 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:04,174] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s2 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:04,185] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s2 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:04,306] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:04,329] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s3 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:04,530] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s4 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:04,541] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s4 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:04,664] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:04,689] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s5 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:04,895] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s6 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:04,908] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s6 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:05,041] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:05,064] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s7 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:05,282] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s8 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:05,294] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s8 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:05,427] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:05,450] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s9 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:05,679] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s10 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:05,694] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s10 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:05,833] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:05,860] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s11 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:06,091] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s12 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:06,102] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s12 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:06,251] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:06,278] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s13 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:06,716] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s14 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:06,727] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s14 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:06,872] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:06,899] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s15 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:07,152] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s16 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:07,166] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s16 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:07,319] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:07,347] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s17 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:07,626] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s18 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:07,640] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s18 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:07,801] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:07,828] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s19 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:08,108] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s20 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:08,123] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s20 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:08,290] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:08,318] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s21 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:08,613] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s22 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:08,625] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s22 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:08,800] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:08,829] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s23 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:09,130] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s24 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:09,142] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s24 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:09,325] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:09,356] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s25 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:09,680] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s26 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:09,695] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s26 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:09,882] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:09,913] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s27 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:10,250] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s28 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:10,265] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s28 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:10,463] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:10,496] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s29 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:11,065] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s30 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:11,077] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s30 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:11,281] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:11,312] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s31 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:11,670] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s32 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:11,685] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s32 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:11,894] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:11,925] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s33 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:12,309] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s34 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:12,326] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s34 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:12,554] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:12,586] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s35 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:12,973] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s36 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:12,986] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s36 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:13,209] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:13,243] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s37 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:13,644] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s38 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:13,659] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s38 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:13,886] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:13,920] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s39 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:14,336] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s40 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:14,349] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s40 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:14,590] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:14,625] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s41 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:15,046] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s42 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:15,061] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s42 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:15,317] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:15,353] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s43 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:15,815] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s44 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:15,832] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s44 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:16,088] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:16,126] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s45 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:16,812] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s46 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:16,827] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s46 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:17,083] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:17,121] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s47 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:17,585] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s48 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:17,601] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s48 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:17,863] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:17,903] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s49 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:18,373] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s50 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:18,389] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s50 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:18,660] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:18,697] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s51 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:19,169] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s52 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:19,184] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s52 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:19,463] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:19,505] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s53 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:19,993] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s54 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:20,008] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s54 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:20,292] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:20,333] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s55 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:20,849] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s56 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:20,863] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s56 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:21,161] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:21,200] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s57 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:21,722] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s58 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:21,737] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s58 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:22,026] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:22,064] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s59 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:22,606] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s60 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:22,620] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s60 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:22,924] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:22,964] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s61 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:23,513] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s62 + 1 <= 4096 [guard added] at transformers/models/llama/modeling_llama.py:119 in forward (_dynamo/variables/tensor.py:704 in evaluate_expr)\n",
      "[2023-09-07 18:14:23,527] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(4096, s62 + 1) [guard added] at transformers/models/llama/modeling_llama.py:123 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:23,838] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval False [guard added] at transformers/models/llama/modeling_llama.py:355 in forward (_dynamo/utils.py:1402 in run_node)\n",
      "[2023-09-07 18:14:23,880] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s63 + 1, s0 + 1) [guard added] at transformers/models/llama/modeling_llama.py:363 in forward (_meta_registrations.py:3477 in common_meta_baddbmm_bmm)\n",
      "[2023-09-07 18:14:27,165] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, token0, state0_flat_0, state0_flat_1, state0_flat_2, state0_flat_3, state0_flat_4, state0_flat_5, state0_flat_6, state0_flat_7, state0_flat_8, state0_flat_9, state0_flat_10, state0_flat_11, state0_flat_12, state0_flat_13, state0_flat_14, state0_flat_15, state0_flat_16, state0_flat_17, state0_flat_18, state0_flat_19, state0_flat_20, state0_flat_21, state0_flat_22, state0_flat_23, state0_flat_24, state0_flat_25, state0_flat_26, state0_flat_27, state0_flat_28, state0_flat_29, state0_flat_30, state0_flat_31, state0_flat_32, state0_flat_33, state0_flat_34, state0_flat_35, state0_flat_36, state0_flat_37, state0_flat_38, state0_flat_39, state0_flat_40, state0_flat_41, state0_flat_42, state0_flat_43, state0_flat_44, state0_flat_45, state0_flat_46, state0_flat_47, state0_flat_48, state0_flat_49, state0_flat_50, state0_flat_51, state0_flat_52, state0_flat_53, state0_flat_54, state0_flat_55, state0_flat_56, state0_flat_57, state0_flat_58, state0_flat_59, state0_flat_60, state0_flat_61, state0_flat_62, state0_flat_63):\n",
      "        arg0: i64[1, 1], arg1: f32[1, 32, s0, 128], arg2: f32[1, 32, s0, 128], arg3: f32[1, 32, s0, 128], arg4: f32[1, 32, s0, 128], arg5: f32[1, 32, s0, 128], arg6: f32[1, 32, s0, 128], arg7: f32[1, 32, s0, 128], arg8: f32[1, 32, s0, 128], arg9: f32[1, 32, s0, 128], arg10: f32[1, 32, s0, 128], arg11: f32[1, 32, s0, 128], arg12: f32[1, 32, s0, 128], arg13: f32[1, 32, s0, 128], arg14: f32[1, 32, s0, 128], arg15: f32[1, 32, s0, 128], arg16: f32[1, 32, s0, 128], arg17: f32[1, 32, s0, 128], arg18: f32[1, 32, s0, 128], arg19: f32[1, 32, s0, 128], arg20: f32[1, 32, s0, 128], arg21: f32[1, 32, s0, 128], arg22: f32[1, 32, s0, 128], arg23: f32[1, 32, s0, 128], arg24: f32[1, 32, s0, 128], arg25: f32[1, 32, s0, 128], arg26: f32[1, 32, s0, 128], arg27: f32[1, 32, s0, 128], arg28: f32[1, 32, s0, 128], arg29: f32[1, 32, s0, 128], arg30: f32[1, 32, s0, 128], arg31: f32[1, 32, s0, 128], arg32: f32[1, 32, s0, 128], arg33: f32[1, 32, s0, 128], arg34: f32[1, 32, s0, 128], arg35: f32[1, 32, s0, 128], arg36: f32[1, 32, s0, 128], arg37: f32[1, 32, s0, 128], arg38: f32[1, 32, s0, 128], arg39: f32[1, 32, s0, 128], arg40: f32[1, 32, s0, 128], arg41: f32[1, 32, s0, 128], arg42: f32[1, 32, s0, 128], arg43: f32[1, 32, s0, 128], arg44: f32[1, 32, s0, 128], arg45: f32[1, 32, s0, 128], arg46: f32[1, 32, s0, 128], arg47: f32[1, 32, s0, 128], arg48: f32[1, 32, s0, 128], arg49: f32[1, 32, s0, 128], arg50: f32[1, 32, s0, 128], arg51: f32[1, 32, s0, 128], arg52: f32[1, 32, s0, 128], arg53: f32[1, 32, s0, 128], arg54: f32[1, 32, s0, 128], arg55: f32[1, 32, s0, 128], arg56: f32[1, 32, s0, 128], arg57: f32[1, 32, s0, 128], arg58: f32[1, 32, s0, 128], arg59: f32[1, 32, s0, 128], arg60: f32[1, 32, s0, 128], arg61: f32[1, 32, s0, 128], arg62: f32[1, 32, s0, 128], arg63: f32[1, 32, s0, 128], arg64: f32[1, 32, s0, 128], = fx_pytree.tree_flatten_spec(([token0, state0_flat_0, state0_flat_1, state0_flat_2, state0_flat_3, state0_flat_4, state0_flat_5, state0_flat_6, state0_flat_7, state0_flat_8, state0_flat_9, state0_flat_10, state0_flat_11, state0_flat_12, state0_flat_13, state0_flat_14, state0_flat_15, state0_flat_16, state0_flat_17, state0_flat_18, state0_flat_19, state0_flat_20, state0_flat_21, state0_flat_22, state0_flat_23, state0_flat_24, state0_flat_25, state0_flat_26, state0_flat_27, state0_flat_28, state0_flat_29, state0_flat_30, state0_flat_31, state0_flat_32, state0_flat_33, state0_flat_34, state0_flat_35, state0_flat_36, state0_flat_37, state0_flat_38, state0_flat_39, state0_flat_40, state0_flat_41, state0_flat_42, state0_flat_43, state0_flat_44, state0_flat_45, state0_flat_46, state0_flat_47, state0_flat_48, state0_flat_49, state0_flat_50, state0_flat_51, state0_flat_52, state0_flat_53, state0_flat_54, state0_flat_55, state0_flat_56, state0_flat_57, state0_flat_58, state0_flat_59, state0_flat_60, state0_flat_61, state0_flat_62, state0_flat_63], {}), self._in_spec)\n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:654, code: position_ids = torch.arange(\n",
      "        sym_size: Sym(s0) = torch.ops.aten.sym_size(arg1, 2)\n",
      "        add: Sym(s0 + 1) = 1 + sym_size\n",
      "        arange_start: i64[1] = torch.ops.aten.arange.start(sym_size, add, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  add = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:657, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        unsqueeze_default: i64[1, 1] = torch.ops.aten.unsqueeze.default(arange_start, 0);  arange_start = None\n",
      "        view_default: i64[1, 1] = torch.ops.aten.view.default(unsqueeze_default, [-1, 1]);  unsqueeze_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:662, code: inputs_embeds = self.embed_tokens(input_ids)\n",
      "        _param_constant0 = self._param_constant0\n",
      "        embedding_default: f32[1, 1, 4096] = torch.ops.aten.embedding.default(_param_constant0, arg0);  _param_constant0 = arg0 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:665, code: attention_mask = torch.ones(\n",
      "        add_1: Sym(s0 + 1) = 1 + sym_size\n",
      "        ones_default: b8[1, s0 + 1] = torch.ops.aten.ones.default([1, add_1], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:68, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "        slice_tensor: b8[1, s0 + 1] = torch.ops.aten.slice.Tensor(ones_default, 0, 0, 9223372036854775807);  ones_default = None\n",
      "        unsqueeze_default_1: b8[1, 1, s0 + 1] = torch.ops.aten.unsqueeze.default(slice_tensor, 1);  slice_tensor = None\n",
      "        unsqueeze_default_2: b8[1, 1, 1, s0 + 1] = torch.ops.aten.unsqueeze.default(unsqueeze_default_1, 2);  unsqueeze_default_1 = None\n",
      "        slice_tensor_1: b8[1, 1, 1, s0 + 1] = torch.ops.aten.slice.Tensor(unsqueeze_default_2, 3, 0, 9223372036854775807);  unsqueeze_default_2 = None\n",
      "        expand_default: b8[1, 1, 1, s0 + 1] = torch.ops.aten.expand.default(slice_tensor_1, [1, 1, 1, add_1]);  slice_tensor_1 = add_1 = None\n",
      "        _to_copy_default: f32[1, 1, 1, s0 + 1] = torch.ops.aten._to_copy.default(expand_default, dtype = torch.float32);  expand_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70, code: inverted_mask = 1.0 - expanded_mask\n",
      "        rsub_scalar: f32[1, 1, 1, s0 + 1] = torch.ops.aten.rsub.Scalar(_to_copy_default, 1.0);  _to_copy_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "        _to_copy_default_1: b8[1, 1, 1, s0 + 1] = torch.ops.aten._to_copy.default(rsub_scalar, dtype = torch.bool)\n",
      "        masked_fill_scalar: f32[1, 1, 1, s0 + 1] = torch.ops.aten.masked_fill.Scalar(rsub_scalar, _to_copy_default_1, -3.4028234663852886e+38);  rsub_scalar = _to_copy_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(embedding_default, 2)\n",
      "        mean_dim: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar, [-1], True);  pow_tensor_scalar = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim, 1e-06);  mean_dim = None\n",
      "        rsqrt_default: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor);  add_tensor = None\n",
      "        detach_default: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default)\n",
      "        mul_tensor: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(embedding_default, rsqrt_default);  rsqrt_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant1 = self._param_constant1\n",
      "        mul_tensor_1: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant1, mul_tensor);  _param_constant1 = mul_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant2 = self._param_constant2\n",
      "        t_default: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant2);  _param_constant2 = None\n",
      "        view_default_1: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_1, [1, 4096])\n",
      "        mm_default: f32[1, 4096] = torch.ops.aten.mm.default(view_default_1, t_default);  view_default_1 = t_default = None\n",
      "        view_default_2: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default, [1, 1, 4096]);  mm_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant3 = self._param_constant3\n",
      "        t_default_1: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant3);  _param_constant3 = None\n",
      "        view_default_3: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_1, [1, 4096])\n",
      "        mm_default_1: f32[1, 4096] = torch.ops.aten.mm.default(view_default_3, t_default_1);  view_default_3 = t_default_1 = None\n",
      "        view_default_4: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_1, [1, 1, 4096]);  mm_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant4 = self._param_constant4\n",
      "        t_default_2: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant4);  _param_constant4 = None\n",
      "        view_default_5: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_1, [1, 4096]);  mul_tensor_1 = None\n",
      "        mm_default_2: f32[1, 4096] = torch.ops.aten.mm.default(view_default_5, t_default_2);  view_default_5 = t_default_2 = None\n",
      "        view_default_6: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_2, [1, 1, 4096]);  mm_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_7: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_2, [1, 1, 32, 128]);  view_default_2 = None\n",
      "        transpose_int: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_7, 1, 2);  view_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_8: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_4, [1, 1, 32, 128]);  view_default_4 = None\n",
      "        transpose_int_1: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_8, 1, 2);  view_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_9: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_6, [1, 1, 32, 128]);  view_default_6 = None\n",
      "        transpose_int_2: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_9, 1, 2);  view_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant0 = self._tensor_constant0\n",
      "        slice_tensor_2: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant0, 0, 0, 9223372036854775807);  _tensor_constant0 = None\n",
      "        slice_tensor_3: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_2, 1, 0, 9223372036854775807);  slice_tensor_2 = None\n",
      "        add_2: Sym(s0 + 1) = 1 + sym_size;  sym_size = None\n",
      "        slice_tensor_4: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_3, 2, 0, add_2);  slice_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant1 = self._tensor_constant1\n",
      "        slice_tensor_5: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant1, 0, 0, 9223372036854775807);  _tensor_constant1 = None\n",
      "        slice_tensor_6: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_5, 1, 0, 9223372036854775807);  slice_tensor_5 = None\n",
      "        slice_tensor_7: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_6, 2, 0, add_2);  slice_tensor_6 = add_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_4, 1);  slice_tensor_4 = None\n",
      "        squeeze_dim_1: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim, 0);  squeeze_dim = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_2: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_7, 1);  slice_tensor_7 = None\n",
      "        squeeze_dim_3: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_2, 0);  squeeze_dim_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_1, [view_default]);  squeeze_dim_1 = None\n",
      "        unsqueeze_default_3: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor, 1);  index_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_1: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_3, [view_default]);  squeeze_dim_3 = None\n",
      "        unsqueeze_default_4: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_1, 1);  index_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_2: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int, unsqueeze_default_3)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_8: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_9: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 64, 9223372036854775807);  transpose_int = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_9);  slice_tensor_9 = None\n",
      "        cat_default: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default, slice_tensor_8], -1);  neg_default = slice_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_3: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default, unsqueeze_default_4);  cat_default = None\n",
      "        add_tensor_1: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_2, mul_tensor_3);  mul_tensor_2 = mul_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_4: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_1, unsqueeze_default_3);  unsqueeze_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_10: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_11: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 64, 9223372036854775807);  transpose_int_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_1: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_11);  slice_tensor_11 = None\n",
      "        cat_default_1: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_1, slice_tensor_10], -1);  neg_default_1 = slice_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_5: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_1, unsqueeze_default_4);  cat_default_1 = unsqueeze_default_4 = None\n",
      "        add_tensor_2: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_4, mul_tensor_5);  mul_tensor_4 = mul_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_2: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg1, add_tensor_2], 2);  arg1 = add_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_3: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg2, transpose_int_2], 2);  arg2 = transpose_int_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_3: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_2, 2, 3)\n",
      "        expand_default_1: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_1, [1, 32, 1, 128]);  add_tensor_1 = None\n",
      "        view_default_10: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_1, [32, 1, 128]);  expand_default_1 = None\n",
      "        sym_size_1: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_2, 2)\n",
      "        expand_default_2: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_3, [1, 32, 128, sym_size_1]);  transpose_int_3 = None\n",
      "        view_default_11: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_2, [32, 128, sym_size_1]);  expand_default_2 = None\n",
      "        bmm_default: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_10, view_default_11);  view_default_10 = view_default_11 = None\n",
      "        view_default_12: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default, [1, 32, 1, sym_size_1]);  bmm_default = None\n",
      "        div_tensor: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_12, 11.313708498984761);  view_default_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor, masked_fill_scalar);  div_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_3, -1, False);  add_tensor_3 = None\n",
      "        detach_default_1: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default, [1, 32, 1, sym_size_1]);  _softmax_default = None\n",
      "        view_default_13: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_3, [32, 1, sym_size_1]);  expand_default_3 = sym_size_1 = None\n",
      "        sym_size_2: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_3, 2)\n",
      "        expand_default_4: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_3, [1, 32, sym_size_2, 128])\n",
      "        view_default_14: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_4, [32, sym_size_2, 128]);  expand_default_4 = sym_size_2 = None\n",
      "        bmm_default_1: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_13, view_default_14);  view_default_13 = view_default_14 = None\n",
      "        view_default_15: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_1, [1, 32, 1, 128]);  bmm_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_4: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_15, 1, 2);  view_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_16: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_4, [1, 1, 4096]);  transpose_int_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant5 = self._param_constant5\n",
      "        t_default_3: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant5);  _param_constant5 = None\n",
      "        view_default_17: f32[1, 4096] = torch.ops.aten.view.default(view_default_16, [1, 4096]);  view_default_16 = None\n",
      "        mm_default_3: f32[1, 4096] = torch.ops.aten.mm.default(view_default_17, t_default_3);  view_default_17 = t_default_3 = None\n",
      "        view_default_18: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_3, [1, 1, 4096]);  mm_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_4: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(embedding_default, view_default_18);  embedding_default = view_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_1: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_4, 2)\n",
      "        mean_dim_1: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_1, [-1], True);  pow_tensor_scalar_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_5: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_1, 1e-06);  mean_dim_1 = None\n",
      "        rsqrt_default_1: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_5);  add_tensor_5 = None\n",
      "        detach_default_2: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_1)\n",
      "        mul_tensor_6: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_4, rsqrt_default_1);  rsqrt_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant6 = self._param_constant6\n",
      "        mul_tensor_7: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant6, mul_tensor_6);  _param_constant6 = mul_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant7 = self._param_constant7\n",
      "        t_default_4: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant7);  _param_constant7 = None\n",
      "        view_default_19: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_7, [1, 4096])\n",
      "        mm_default_4: f32[1, 11008] = torch.ops.aten.mm.default(view_default_19, t_default_4);  view_default_19 = t_default_4 = None\n",
      "        view_default_20: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_4, [1, 1, 11008]);  mm_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_20);  view_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant8 = self._param_constant8\n",
      "        t_default_5: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant8);  _param_constant8 = None\n",
      "        view_default_21: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_7, [1, 4096]);  mul_tensor_7 = None\n",
      "        mm_default_5: f32[1, 11008] = torch.ops.aten.mm.default(view_default_21, t_default_5);  view_default_21 = t_default_5 = None\n",
      "        view_default_22: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_5, [1, 1, 11008]);  mm_default_5 = None\n",
      "        mul_tensor_8: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default, view_default_22);  silu_default = view_default_22 = None\n",
      "        _param_constant9 = self._param_constant9\n",
      "        t_default_6: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant9);  _param_constant9 = None\n",
      "        view_default_23: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_8, [1, 11008]);  mul_tensor_8 = None\n",
      "        mm_default_6: f32[1, 4096] = torch.ops.aten.mm.default(view_default_23, t_default_6);  view_default_23 = t_default_6 = None\n",
      "        view_default_24: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_6, [1, 1, 4096]);  mm_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_6: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_4, view_default_24);  add_tensor_4 = view_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_2: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_6, 2)\n",
      "        mean_dim_2: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_2, [-1], True);  pow_tensor_scalar_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_7: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_2, 1e-06);  mean_dim_2 = None\n",
      "        rsqrt_default_2: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_7);  add_tensor_7 = None\n",
      "        detach_default_3: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_2)\n",
      "        mul_tensor_9: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_6, rsqrt_default_2);  rsqrt_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant10 = self._param_constant10\n",
      "        mul_tensor_10: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant10, mul_tensor_9);  _param_constant10 = mul_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant11 = self._param_constant11\n",
      "        t_default_7: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant11);  _param_constant11 = None\n",
      "        view_default_25: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_10, [1, 4096])\n",
      "        mm_default_7: f32[1, 4096] = torch.ops.aten.mm.default(view_default_25, t_default_7);  view_default_25 = t_default_7 = None\n",
      "        view_default_26: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_7, [1, 1, 4096]);  mm_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant12 = self._param_constant12\n",
      "        t_default_8: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant12);  _param_constant12 = None\n",
      "        view_default_27: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_10, [1, 4096])\n",
      "        mm_default_8: f32[1, 4096] = torch.ops.aten.mm.default(view_default_27, t_default_8);  view_default_27 = t_default_8 = None\n",
      "        view_default_28: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_8, [1, 1, 4096]);  mm_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant13 = self._param_constant13\n",
      "        t_default_9: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant13);  _param_constant13 = None\n",
      "        view_default_29: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_10, [1, 4096]);  mul_tensor_10 = None\n",
      "        mm_default_9: f32[1, 4096] = torch.ops.aten.mm.default(view_default_29, t_default_9);  view_default_29 = t_default_9 = None\n",
      "        view_default_30: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_9, [1, 1, 4096]);  mm_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_31: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_26, [1, 1, 32, 128]);  view_default_26 = None\n",
      "        transpose_int_5: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_31, 1, 2);  view_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_32: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_28, [1, 1, 32, 128]);  view_default_28 = None\n",
      "        transpose_int_6: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_32, 1, 2);  view_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_33: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_30, [1, 1, 32, 128]);  view_default_30 = None\n",
      "        transpose_int_7: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_33, 1, 2);  view_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant2 = self._tensor_constant2\n",
      "        slice_tensor_12: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant2, 0, 0, 9223372036854775807);  _tensor_constant2 = None\n",
      "        slice_tensor_13: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_12, 1, 0, 9223372036854775807);  slice_tensor_12 = None\n",
      "        sym_size_3: Sym(s0) = torch.ops.aten.sym_size(arg3, 2)\n",
      "        add_3: Sym(s0 + 1) = 1 + sym_size_3;  sym_size_3 = None\n",
      "        slice_tensor_14: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_13, 2, 0, add_3);  slice_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant3 = self._tensor_constant3\n",
      "        slice_tensor_15: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant3, 0, 0, 9223372036854775807);  _tensor_constant3 = None\n",
      "        slice_tensor_16: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_15, 1, 0, 9223372036854775807);  slice_tensor_15 = None\n",
      "        slice_tensor_17: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_16, 2, 0, add_3);  slice_tensor_16 = add_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_4: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_14, 1);  slice_tensor_14 = None\n",
      "        squeeze_dim_5: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_4, 0);  squeeze_dim_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_6: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_17, 1);  slice_tensor_17 = None\n",
      "        squeeze_dim_7: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_6, 0);  squeeze_dim_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_2: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_5, [view_default]);  squeeze_dim_5 = None\n",
      "        unsqueeze_default_5: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_2, 1);  index_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_3: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_7, [view_default]);  squeeze_dim_7 = None\n",
      "        unsqueeze_default_6: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_3, 1);  index_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_11: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_5, unsqueeze_default_5)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_18: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_19: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 64, 9223372036854775807);  transpose_int_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_2: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_19);  slice_tensor_19 = None\n",
      "        cat_default_4: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_2, slice_tensor_18], -1);  neg_default_2 = slice_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_12: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_4, unsqueeze_default_6);  cat_default_4 = None\n",
      "        add_tensor_8: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_11, mul_tensor_12);  mul_tensor_11 = mul_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_13: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_6, unsqueeze_default_5);  unsqueeze_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_20: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_21: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 64, 9223372036854775807);  transpose_int_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_3: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_21);  slice_tensor_21 = None\n",
      "        cat_default_5: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_3, slice_tensor_20], -1);  neg_default_3 = slice_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_14: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_5, unsqueeze_default_6);  cat_default_5 = unsqueeze_default_6 = None\n",
      "        add_tensor_9: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_13, mul_tensor_14);  mul_tensor_13 = mul_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_6: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg3, add_tensor_9], 2);  arg3 = add_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_7: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg4, transpose_int_7], 2);  arg4 = transpose_int_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_8: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_6, 2, 3)\n",
      "        expand_default_5: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_8, [1, 32, 1, 128]);  add_tensor_8 = None\n",
      "        view_default_34: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_5, [32, 1, 128]);  expand_default_5 = None\n",
      "        sym_size_4: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_6, 2)\n",
      "        expand_default_6: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_8, [1, 32, 128, sym_size_4]);  transpose_int_8 = None\n",
      "        view_default_35: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_6, [32, 128, sym_size_4]);  expand_default_6 = None\n",
      "        bmm_default_2: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_34, view_default_35);  view_default_34 = view_default_35 = None\n",
      "        view_default_36: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_2, [1, 32, 1, sym_size_4]);  bmm_default_2 = None\n",
      "        div_tensor_1: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_36, 11.313708498984761);  view_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_1, masked_fill_scalar);  div_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_1: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_10, -1, False);  add_tensor_10 = None\n",
      "        detach_default_4: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_1)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_1, [1, 32, 1, sym_size_4]);  _softmax_default_1 = None\n",
      "        view_default_37: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_7, [32, 1, sym_size_4]);  expand_default_7 = sym_size_4 = None\n",
      "        sym_size_5: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_7, 2)\n",
      "        expand_default_8: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_7, [1, 32, sym_size_5, 128])\n",
      "        view_default_38: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_8, [32, sym_size_5, 128]);  expand_default_8 = sym_size_5 = None\n",
      "        bmm_default_3: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_37, view_default_38);  view_default_37 = view_default_38 = None\n",
      "        view_default_39: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_3, [1, 32, 1, 128]);  bmm_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_9: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_39, 1, 2);  view_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_40: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_9, [1, 1, 4096]);  transpose_int_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant14 = self._param_constant14\n",
      "        t_default_10: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant14);  _param_constant14 = None\n",
      "        view_default_41: f32[1, 4096] = torch.ops.aten.view.default(view_default_40, [1, 4096]);  view_default_40 = None\n",
      "        mm_default_10: f32[1, 4096] = torch.ops.aten.mm.default(view_default_41, t_default_10);  view_default_41 = t_default_10 = None\n",
      "        view_default_42: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_10, [1, 1, 4096]);  mm_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_11: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_6, view_default_42);  add_tensor_6 = view_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_3: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_11, 2)\n",
      "        mean_dim_3: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_3, [-1], True);  pow_tensor_scalar_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_12: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_3, 1e-06);  mean_dim_3 = None\n",
      "        rsqrt_default_3: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_12);  add_tensor_12 = None\n",
      "        detach_default_5: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_3)\n",
      "        mul_tensor_15: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_11, rsqrt_default_3);  rsqrt_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant15 = self._param_constant15\n",
      "        mul_tensor_16: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant15, mul_tensor_15);  _param_constant15 = mul_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant16 = self._param_constant16\n",
      "        t_default_11: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant16);  _param_constant16 = None\n",
      "        view_default_43: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_16, [1, 4096])\n",
      "        mm_default_11: f32[1, 11008] = torch.ops.aten.mm.default(view_default_43, t_default_11);  view_default_43 = t_default_11 = None\n",
      "        view_default_44: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_11, [1, 1, 11008]);  mm_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_1: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_44);  view_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant17 = self._param_constant17\n",
      "        t_default_12: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant17);  _param_constant17 = None\n",
      "        view_default_45: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_16, [1, 4096]);  mul_tensor_16 = None\n",
      "        mm_default_12: f32[1, 11008] = torch.ops.aten.mm.default(view_default_45, t_default_12);  view_default_45 = t_default_12 = None\n",
      "        view_default_46: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_12, [1, 1, 11008]);  mm_default_12 = None\n",
      "        mul_tensor_17: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_1, view_default_46);  silu_default_1 = view_default_46 = None\n",
      "        _param_constant18 = self._param_constant18\n",
      "        t_default_13: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant18);  _param_constant18 = None\n",
      "        view_default_47: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_17, [1, 11008]);  mul_tensor_17 = None\n",
      "        mm_default_13: f32[1, 4096] = torch.ops.aten.mm.default(view_default_47, t_default_13);  view_default_47 = t_default_13 = None\n",
      "        view_default_48: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_13, [1, 1, 4096]);  mm_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_13: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_11, view_default_48);  add_tensor_11 = view_default_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_4: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_13, 2)\n",
      "        mean_dim_4: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_4, [-1], True);  pow_tensor_scalar_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_14: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_4, 1e-06);  mean_dim_4 = None\n",
      "        rsqrt_default_4: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_14);  add_tensor_14 = None\n",
      "        detach_default_6: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_4)\n",
      "        mul_tensor_18: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_13, rsqrt_default_4);  rsqrt_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant19 = self._param_constant19\n",
      "        mul_tensor_19: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant19, mul_tensor_18);  _param_constant19 = mul_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant20 = self._param_constant20\n",
      "        t_default_14: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant20);  _param_constant20 = None\n",
      "        view_default_49: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_19, [1, 4096])\n",
      "        mm_default_14: f32[1, 4096] = torch.ops.aten.mm.default(view_default_49, t_default_14);  view_default_49 = t_default_14 = None\n",
      "        view_default_50: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_14, [1, 1, 4096]);  mm_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant21 = self._param_constant21\n",
      "        t_default_15: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant21);  _param_constant21 = None\n",
      "        view_default_51: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_19, [1, 4096])\n",
      "        mm_default_15: f32[1, 4096] = torch.ops.aten.mm.default(view_default_51, t_default_15);  view_default_51 = t_default_15 = None\n",
      "        view_default_52: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_15, [1, 1, 4096]);  mm_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant22 = self._param_constant22\n",
      "        t_default_16: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant22);  _param_constant22 = None\n",
      "        view_default_53: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_19, [1, 4096]);  mul_tensor_19 = None\n",
      "        mm_default_16: f32[1, 4096] = torch.ops.aten.mm.default(view_default_53, t_default_16);  view_default_53 = t_default_16 = None\n",
      "        view_default_54: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_16, [1, 1, 4096]);  mm_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_55: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_50, [1, 1, 32, 128]);  view_default_50 = None\n",
      "        transpose_int_10: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_55, 1, 2);  view_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_56: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_52, [1, 1, 32, 128]);  view_default_52 = None\n",
      "        transpose_int_11: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_56, 1, 2);  view_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_57: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_54, [1, 1, 32, 128]);  view_default_54 = None\n",
      "        transpose_int_12: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_57, 1, 2);  view_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant4 = self._tensor_constant4\n",
      "        slice_tensor_22: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant4, 0, 0, 9223372036854775807);  _tensor_constant4 = None\n",
      "        slice_tensor_23: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_22, 1, 0, 9223372036854775807);  slice_tensor_22 = None\n",
      "        sym_size_6: Sym(s0) = torch.ops.aten.sym_size(arg5, 2)\n",
      "        add_4: Sym(s0 + 1) = 1 + sym_size_6;  sym_size_6 = None\n",
      "        slice_tensor_24: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_23, 2, 0, add_4);  slice_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant5 = self._tensor_constant5\n",
      "        slice_tensor_25: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant5, 0, 0, 9223372036854775807);  _tensor_constant5 = None\n",
      "        slice_tensor_26: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_25, 1, 0, 9223372036854775807);  slice_tensor_25 = None\n",
      "        slice_tensor_27: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_26, 2, 0, add_4);  slice_tensor_26 = add_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_8: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_24, 1);  slice_tensor_24 = None\n",
      "        squeeze_dim_9: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_8, 0);  squeeze_dim_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_10: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_27, 1);  slice_tensor_27 = None\n",
      "        squeeze_dim_11: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_10, 0);  squeeze_dim_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_4: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_9, [view_default]);  squeeze_dim_9 = None\n",
      "        unsqueeze_default_7: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_4, 1);  index_tensor_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_5: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_11, [view_default]);  squeeze_dim_11 = None\n",
      "        unsqueeze_default_8: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_5, 1);  index_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_20: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_10, unsqueeze_default_7)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_28: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_29: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 64, 9223372036854775807);  transpose_int_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_4: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_29);  slice_tensor_29 = None\n",
      "        cat_default_8: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_4, slice_tensor_28], -1);  neg_default_4 = slice_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_21: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_8, unsqueeze_default_8);  cat_default_8 = None\n",
      "        add_tensor_15: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_20, mul_tensor_21);  mul_tensor_20 = mul_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_22: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_11, unsqueeze_default_7);  unsqueeze_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_30: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_31: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 64, 9223372036854775807);  transpose_int_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_5: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_31);  slice_tensor_31 = None\n",
      "        cat_default_9: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_5, slice_tensor_30], -1);  neg_default_5 = slice_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_23: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_9, unsqueeze_default_8);  cat_default_9 = unsqueeze_default_8 = None\n",
      "        add_tensor_16: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_22, mul_tensor_23);  mul_tensor_22 = mul_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_10: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg5, add_tensor_16], 2);  arg5 = add_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_11: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg6, transpose_int_12], 2);  arg6 = transpose_int_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_13: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_10, 2, 3)\n",
      "        expand_default_9: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_15, [1, 32, 1, 128]);  add_tensor_15 = None\n",
      "        view_default_58: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_9, [32, 1, 128]);  expand_default_9 = None\n",
      "        sym_size_7: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_10, 2)\n",
      "        expand_default_10: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_13, [1, 32, 128, sym_size_7]);  transpose_int_13 = None\n",
      "        view_default_59: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_10, [32, 128, sym_size_7]);  expand_default_10 = None\n",
      "        bmm_default_4: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_58, view_default_59);  view_default_58 = view_default_59 = None\n",
      "        view_default_60: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_4, [1, 32, 1, sym_size_7]);  bmm_default_4 = None\n",
      "        div_tensor_2: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_60, 11.313708498984761);  view_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_17: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_2, masked_fill_scalar);  div_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_2: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_17, -1, False);  add_tensor_17 = None\n",
      "        detach_default_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_2)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_11: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_2, [1, 32, 1, sym_size_7]);  _softmax_default_2 = None\n",
      "        view_default_61: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_11, [32, 1, sym_size_7]);  expand_default_11 = sym_size_7 = None\n",
      "        sym_size_8: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_11, 2)\n",
      "        expand_default_12: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_11, [1, 32, sym_size_8, 128])\n",
      "        view_default_62: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_12, [32, sym_size_8, 128]);  expand_default_12 = sym_size_8 = None\n",
      "        bmm_default_5: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_61, view_default_62);  view_default_61 = view_default_62 = None\n",
      "        view_default_63: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_5, [1, 32, 1, 128]);  bmm_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_14: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_63, 1, 2);  view_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_64: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_14, [1, 1, 4096]);  transpose_int_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant23 = self._param_constant23\n",
      "        t_default_17: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant23);  _param_constant23 = None\n",
      "        view_default_65: f32[1, 4096] = torch.ops.aten.view.default(view_default_64, [1, 4096]);  view_default_64 = None\n",
      "        mm_default_17: f32[1, 4096] = torch.ops.aten.mm.default(view_default_65, t_default_17);  view_default_65 = t_default_17 = None\n",
      "        view_default_66: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_17, [1, 1, 4096]);  mm_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_18: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_13, view_default_66);  add_tensor_13 = view_default_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_5: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_18, 2)\n",
      "        mean_dim_5: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_5, [-1], True);  pow_tensor_scalar_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_19: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_5, 1e-06);  mean_dim_5 = None\n",
      "        rsqrt_default_5: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_19);  add_tensor_19 = None\n",
      "        detach_default_8: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_5)\n",
      "        mul_tensor_24: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_18, rsqrt_default_5);  rsqrt_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant24 = self._param_constant24\n",
      "        mul_tensor_25: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant24, mul_tensor_24);  _param_constant24 = mul_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant25 = self._param_constant25\n",
      "        t_default_18: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant25);  _param_constant25 = None\n",
      "        view_default_67: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_25, [1, 4096])\n",
      "        mm_default_18: f32[1, 11008] = torch.ops.aten.mm.default(view_default_67, t_default_18);  view_default_67 = t_default_18 = None\n",
      "        view_default_68: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_18, [1, 1, 11008]);  mm_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_2: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_68);  view_default_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant26 = self._param_constant26\n",
      "        t_default_19: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant26);  _param_constant26 = None\n",
      "        view_default_69: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_25, [1, 4096]);  mul_tensor_25 = None\n",
      "        mm_default_19: f32[1, 11008] = torch.ops.aten.mm.default(view_default_69, t_default_19);  view_default_69 = t_default_19 = None\n",
      "        view_default_70: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_19, [1, 1, 11008]);  mm_default_19 = None\n",
      "        mul_tensor_26: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_2, view_default_70);  silu_default_2 = view_default_70 = None\n",
      "        _param_constant27 = self._param_constant27\n",
      "        t_default_20: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant27);  _param_constant27 = None\n",
      "        view_default_71: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_26, [1, 11008]);  mul_tensor_26 = None\n",
      "        mm_default_20: f32[1, 4096] = torch.ops.aten.mm.default(view_default_71, t_default_20);  view_default_71 = t_default_20 = None\n",
      "        view_default_72: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_20, [1, 1, 4096]);  mm_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_20: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_18, view_default_72);  add_tensor_18 = view_default_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_6: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_20, 2)\n",
      "        mean_dim_6: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_6, [-1], True);  pow_tensor_scalar_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_21: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_6, 1e-06);  mean_dim_6 = None\n",
      "        rsqrt_default_6: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_21);  add_tensor_21 = None\n",
      "        detach_default_9: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_6)\n",
      "        mul_tensor_27: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_20, rsqrt_default_6);  rsqrt_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant28 = self._param_constant28\n",
      "        mul_tensor_28: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant28, mul_tensor_27);  _param_constant28 = mul_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant29 = self._param_constant29\n",
      "        t_default_21: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant29);  _param_constant29 = None\n",
      "        view_default_73: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_28, [1, 4096])\n",
      "        mm_default_21: f32[1, 4096] = torch.ops.aten.mm.default(view_default_73, t_default_21);  view_default_73 = t_default_21 = None\n",
      "        view_default_74: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_21, [1, 1, 4096]);  mm_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant30 = self._param_constant30\n",
      "        t_default_22: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant30);  _param_constant30 = None\n",
      "        view_default_75: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_28, [1, 4096])\n",
      "        mm_default_22: f32[1, 4096] = torch.ops.aten.mm.default(view_default_75, t_default_22);  view_default_75 = t_default_22 = None\n",
      "        view_default_76: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_22, [1, 1, 4096]);  mm_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant31 = self._param_constant31\n",
      "        t_default_23: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant31);  _param_constant31 = None\n",
      "        view_default_77: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_28, [1, 4096]);  mul_tensor_28 = None\n",
      "        mm_default_23: f32[1, 4096] = torch.ops.aten.mm.default(view_default_77, t_default_23);  view_default_77 = t_default_23 = None\n",
      "        view_default_78: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_23, [1, 1, 4096]);  mm_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_79: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_74, [1, 1, 32, 128]);  view_default_74 = None\n",
      "        transpose_int_15: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_79, 1, 2);  view_default_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_80: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_76, [1, 1, 32, 128]);  view_default_76 = None\n",
      "        transpose_int_16: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_80, 1, 2);  view_default_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_81: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_78, [1, 1, 32, 128]);  view_default_78 = None\n",
      "        transpose_int_17: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_81, 1, 2);  view_default_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant6 = self._tensor_constant6\n",
      "        slice_tensor_32: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant6, 0, 0, 9223372036854775807);  _tensor_constant6 = None\n",
      "        slice_tensor_33: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_32, 1, 0, 9223372036854775807);  slice_tensor_32 = None\n",
      "        sym_size_9: Sym(s0) = torch.ops.aten.sym_size(arg7, 2)\n",
      "        add_5: Sym(s0 + 1) = 1 + sym_size_9;  sym_size_9 = None\n",
      "        slice_tensor_34: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_33, 2, 0, add_5);  slice_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant7 = self._tensor_constant7\n",
      "        slice_tensor_35: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant7, 0, 0, 9223372036854775807);  _tensor_constant7 = None\n",
      "        slice_tensor_36: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_35, 1, 0, 9223372036854775807);  slice_tensor_35 = None\n",
      "        slice_tensor_37: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_36, 2, 0, add_5);  slice_tensor_36 = add_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_12: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_34, 1);  slice_tensor_34 = None\n",
      "        squeeze_dim_13: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_12, 0);  squeeze_dim_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_14: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_37, 1);  slice_tensor_37 = None\n",
      "        squeeze_dim_15: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_14, 0);  squeeze_dim_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_6: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_13, [view_default]);  squeeze_dim_13 = None\n",
      "        unsqueeze_default_9: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_6, 1);  index_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_7: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_15, [view_default]);  squeeze_dim_15 = None\n",
      "        unsqueeze_default_10: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_7, 1);  index_tensor_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_29: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_15, unsqueeze_default_9)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_38: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_39: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 64, 9223372036854775807);  transpose_int_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_6: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_39);  slice_tensor_39 = None\n",
      "        cat_default_12: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_6, slice_tensor_38], -1);  neg_default_6 = slice_tensor_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_30: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_12, unsqueeze_default_10);  cat_default_12 = None\n",
      "        add_tensor_22: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_29, mul_tensor_30);  mul_tensor_29 = mul_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_31: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_16, unsqueeze_default_9);  unsqueeze_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_40: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_41: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 64, 9223372036854775807);  transpose_int_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_7: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_41);  slice_tensor_41 = None\n",
      "        cat_default_13: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_7, slice_tensor_40], -1);  neg_default_7 = slice_tensor_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_32: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_13, unsqueeze_default_10);  cat_default_13 = unsqueeze_default_10 = None\n",
      "        add_tensor_23: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_31, mul_tensor_32);  mul_tensor_31 = mul_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_14: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg7, add_tensor_23], 2);  arg7 = add_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_15: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg8, transpose_int_17], 2);  arg8 = transpose_int_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_18: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_14, 2, 3)\n",
      "        expand_default_13: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_22, [1, 32, 1, 128]);  add_tensor_22 = None\n",
      "        view_default_82: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_13, [32, 1, 128]);  expand_default_13 = None\n",
      "        sym_size_10: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_14, 2)\n",
      "        expand_default_14: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_18, [1, 32, 128, sym_size_10]);  transpose_int_18 = None\n",
      "        view_default_83: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_14, [32, 128, sym_size_10]);  expand_default_14 = None\n",
      "        bmm_default_6: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_82, view_default_83);  view_default_82 = view_default_83 = None\n",
      "        view_default_84: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_6, [1, 32, 1, sym_size_10]);  bmm_default_6 = None\n",
      "        div_tensor_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_84, 11.313708498984761);  view_default_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_24: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_3, masked_fill_scalar);  div_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_24, -1, False);  add_tensor_24 = None\n",
      "        detach_default_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_3)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_15: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_3, [1, 32, 1, sym_size_10]);  _softmax_default_3 = None\n",
      "        view_default_85: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_15, [32, 1, sym_size_10]);  expand_default_15 = sym_size_10 = None\n",
      "        sym_size_11: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_15, 2)\n",
      "        expand_default_16: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_15, [1, 32, sym_size_11, 128])\n",
      "        view_default_86: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_16, [32, sym_size_11, 128]);  expand_default_16 = sym_size_11 = None\n",
      "        bmm_default_7: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_85, view_default_86);  view_default_85 = view_default_86 = None\n",
      "        view_default_87: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_7, [1, 32, 1, 128]);  bmm_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_19: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_87, 1, 2);  view_default_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_88: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_19, [1, 1, 4096]);  transpose_int_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant32 = self._param_constant32\n",
      "        t_default_24: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant32);  _param_constant32 = None\n",
      "        view_default_89: f32[1, 4096] = torch.ops.aten.view.default(view_default_88, [1, 4096]);  view_default_88 = None\n",
      "        mm_default_24: f32[1, 4096] = torch.ops.aten.mm.default(view_default_89, t_default_24);  view_default_89 = t_default_24 = None\n",
      "        view_default_90: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_24, [1, 1, 4096]);  mm_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_25: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_20, view_default_90);  add_tensor_20 = view_default_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_7: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_25, 2)\n",
      "        mean_dim_7: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_7, [-1], True);  pow_tensor_scalar_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_26: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_7, 1e-06);  mean_dim_7 = None\n",
      "        rsqrt_default_7: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_26);  add_tensor_26 = None\n",
      "        detach_default_11: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_7)\n",
      "        mul_tensor_33: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_25, rsqrt_default_7);  rsqrt_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant33 = self._param_constant33\n",
      "        mul_tensor_34: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant33, mul_tensor_33);  _param_constant33 = mul_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant34 = self._param_constant34\n",
      "        t_default_25: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant34);  _param_constant34 = None\n",
      "        view_default_91: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_34, [1, 4096])\n",
      "        mm_default_25: f32[1, 11008] = torch.ops.aten.mm.default(view_default_91, t_default_25);  view_default_91 = t_default_25 = None\n",
      "        view_default_92: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_25, [1, 1, 11008]);  mm_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_3: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_92);  view_default_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant35 = self._param_constant35\n",
      "        t_default_26: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant35);  _param_constant35 = None\n",
      "        view_default_93: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_34, [1, 4096]);  mul_tensor_34 = None\n",
      "        mm_default_26: f32[1, 11008] = torch.ops.aten.mm.default(view_default_93, t_default_26);  view_default_93 = t_default_26 = None\n",
      "        view_default_94: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_26, [1, 1, 11008]);  mm_default_26 = None\n",
      "        mul_tensor_35: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_3, view_default_94);  silu_default_3 = view_default_94 = None\n",
      "        _param_constant36 = self._param_constant36\n",
      "        t_default_27: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant36);  _param_constant36 = None\n",
      "        view_default_95: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_35, [1, 11008]);  mul_tensor_35 = None\n",
      "        mm_default_27: f32[1, 4096] = torch.ops.aten.mm.default(view_default_95, t_default_27);  view_default_95 = t_default_27 = None\n",
      "        view_default_96: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_27, [1, 1, 4096]);  mm_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_27: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_25, view_default_96);  add_tensor_25 = view_default_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_8: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_27, 2)\n",
      "        mean_dim_8: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_8, [-1], True);  pow_tensor_scalar_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_28: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_8, 1e-06);  mean_dim_8 = None\n",
      "        rsqrt_default_8: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_28);  add_tensor_28 = None\n",
      "        detach_default_12: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_8)\n",
      "        mul_tensor_36: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_27, rsqrt_default_8);  rsqrt_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant37 = self._param_constant37\n",
      "        mul_tensor_37: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant37, mul_tensor_36);  _param_constant37 = mul_tensor_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant38 = self._param_constant38\n",
      "        t_default_28: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant38);  _param_constant38 = None\n",
      "        view_default_97: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_37, [1, 4096])\n",
      "        mm_default_28: f32[1, 4096] = torch.ops.aten.mm.default(view_default_97, t_default_28);  view_default_97 = t_default_28 = None\n",
      "        view_default_98: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_28, [1, 1, 4096]);  mm_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant39 = self._param_constant39\n",
      "        t_default_29: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant39);  _param_constant39 = None\n",
      "        view_default_99: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_37, [1, 4096])\n",
      "        mm_default_29: f32[1, 4096] = torch.ops.aten.mm.default(view_default_99, t_default_29);  view_default_99 = t_default_29 = None\n",
      "        view_default_100: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_29, [1, 1, 4096]);  mm_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant40 = self._param_constant40\n",
      "        t_default_30: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant40);  _param_constant40 = None\n",
      "        view_default_101: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_37, [1, 4096]);  mul_tensor_37 = None\n",
      "        mm_default_30: f32[1, 4096] = torch.ops.aten.mm.default(view_default_101, t_default_30);  view_default_101 = t_default_30 = None\n",
      "        view_default_102: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_30, [1, 1, 4096]);  mm_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_103: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_98, [1, 1, 32, 128]);  view_default_98 = None\n",
      "        transpose_int_20: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_103, 1, 2);  view_default_103 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_104: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_100, [1, 1, 32, 128]);  view_default_100 = None\n",
      "        transpose_int_21: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_104, 1, 2);  view_default_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_105: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_102, [1, 1, 32, 128]);  view_default_102 = None\n",
      "        transpose_int_22: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_105, 1, 2);  view_default_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant8 = self._tensor_constant8\n",
      "        slice_tensor_42: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant8, 0, 0, 9223372036854775807);  _tensor_constant8 = None\n",
      "        slice_tensor_43: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_42, 1, 0, 9223372036854775807);  slice_tensor_42 = None\n",
      "        sym_size_12: Sym(s0) = torch.ops.aten.sym_size(arg9, 2)\n",
      "        add_6: Sym(s0 + 1) = 1 + sym_size_12;  sym_size_12 = None\n",
      "        slice_tensor_44: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_43, 2, 0, add_6);  slice_tensor_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant9 = self._tensor_constant9\n",
      "        slice_tensor_45: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant9, 0, 0, 9223372036854775807);  _tensor_constant9 = None\n",
      "        slice_tensor_46: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_45, 1, 0, 9223372036854775807);  slice_tensor_45 = None\n",
      "        slice_tensor_47: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_46, 2, 0, add_6);  slice_tensor_46 = add_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_16: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_44, 1);  slice_tensor_44 = None\n",
      "        squeeze_dim_17: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_16, 0);  squeeze_dim_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_18: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_47, 1);  slice_tensor_47 = None\n",
      "        squeeze_dim_19: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_18, 0);  squeeze_dim_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_8: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_17, [view_default]);  squeeze_dim_17 = None\n",
      "        unsqueeze_default_11: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_8, 1);  index_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_9: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_19, [view_default]);  squeeze_dim_19 = None\n",
      "        unsqueeze_default_12: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_9, 1);  index_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_38: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_20, unsqueeze_default_11)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_48: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_49: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 64, 9223372036854775807);  transpose_int_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_8: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_49);  slice_tensor_49 = None\n",
      "        cat_default_16: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_8, slice_tensor_48], -1);  neg_default_8 = slice_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_39: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_16, unsqueeze_default_12);  cat_default_16 = None\n",
      "        add_tensor_29: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_38, mul_tensor_39);  mul_tensor_38 = mul_tensor_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_40: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_21, unsqueeze_default_11);  unsqueeze_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_50: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_51: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 64, 9223372036854775807);  transpose_int_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_9: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_51);  slice_tensor_51 = None\n",
      "        cat_default_17: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_9, slice_tensor_50], -1);  neg_default_9 = slice_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_41: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_17, unsqueeze_default_12);  cat_default_17 = unsqueeze_default_12 = None\n",
      "        add_tensor_30: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_40, mul_tensor_41);  mul_tensor_40 = mul_tensor_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_18: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg9, add_tensor_30], 2);  arg9 = add_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_19: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg10, transpose_int_22], 2);  arg10 = transpose_int_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_23: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_18, 2, 3)\n",
      "        expand_default_17: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_29, [1, 32, 1, 128]);  add_tensor_29 = None\n",
      "        view_default_106: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_17, [32, 1, 128]);  expand_default_17 = None\n",
      "        sym_size_13: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_18, 2)\n",
      "        expand_default_18: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_23, [1, 32, 128, sym_size_13]);  transpose_int_23 = None\n",
      "        view_default_107: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_18, [32, 128, sym_size_13]);  expand_default_18 = None\n",
      "        bmm_default_8: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_106, view_default_107);  view_default_106 = view_default_107 = None\n",
      "        view_default_108: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_8, [1, 32, 1, sym_size_13]);  bmm_default_8 = None\n",
      "        div_tensor_4: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_108, 11.313708498984761);  view_default_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_4, masked_fill_scalar);  div_tensor_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_4: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_31, -1, False);  add_tensor_31 = None\n",
      "        detach_default_13: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_4)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_4, [1, 32, 1, sym_size_13]);  _softmax_default_4 = None\n",
      "        view_default_109: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_19, [32, 1, sym_size_13]);  expand_default_19 = sym_size_13 = None\n",
      "        sym_size_14: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_19, 2)\n",
      "        expand_default_20: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_19, [1, 32, sym_size_14, 128])\n",
      "        view_default_110: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_20, [32, sym_size_14, 128]);  expand_default_20 = sym_size_14 = None\n",
      "        bmm_default_9: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_109, view_default_110);  view_default_109 = view_default_110 = None\n",
      "        view_default_111: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_9, [1, 32, 1, 128]);  bmm_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_24: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_111, 1, 2);  view_default_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_112: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_24, [1, 1, 4096]);  transpose_int_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant41 = self._param_constant41\n",
      "        t_default_31: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant41);  _param_constant41 = None\n",
      "        view_default_113: f32[1, 4096] = torch.ops.aten.view.default(view_default_112, [1, 4096]);  view_default_112 = None\n",
      "        mm_default_31: f32[1, 4096] = torch.ops.aten.mm.default(view_default_113, t_default_31);  view_default_113 = t_default_31 = None\n",
      "        view_default_114: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_31, [1, 1, 4096]);  mm_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_32: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_27, view_default_114);  add_tensor_27 = view_default_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_9: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_32, 2)\n",
      "        mean_dim_9: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_9, [-1], True);  pow_tensor_scalar_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_33: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_9, 1e-06);  mean_dim_9 = None\n",
      "        rsqrt_default_9: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_33);  add_tensor_33 = None\n",
      "        detach_default_14: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_9)\n",
      "        mul_tensor_42: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_32, rsqrt_default_9);  rsqrt_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant42 = self._param_constant42\n",
      "        mul_tensor_43: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant42, mul_tensor_42);  _param_constant42 = mul_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant43 = self._param_constant43\n",
      "        t_default_32: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant43);  _param_constant43 = None\n",
      "        view_default_115: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_43, [1, 4096])\n",
      "        mm_default_32: f32[1, 11008] = torch.ops.aten.mm.default(view_default_115, t_default_32);  view_default_115 = t_default_32 = None\n",
      "        view_default_116: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_32, [1, 1, 11008]);  mm_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_4: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_116);  view_default_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant44 = self._param_constant44\n",
      "        t_default_33: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant44);  _param_constant44 = None\n",
      "        view_default_117: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_43, [1, 4096]);  mul_tensor_43 = None\n",
      "        mm_default_33: f32[1, 11008] = torch.ops.aten.mm.default(view_default_117, t_default_33);  view_default_117 = t_default_33 = None\n",
      "        view_default_118: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_33, [1, 1, 11008]);  mm_default_33 = None\n",
      "        mul_tensor_44: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_4, view_default_118);  silu_default_4 = view_default_118 = None\n",
      "        _param_constant45 = self._param_constant45\n",
      "        t_default_34: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant45);  _param_constant45 = None\n",
      "        view_default_119: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_44, [1, 11008]);  mul_tensor_44 = None\n",
      "        mm_default_34: f32[1, 4096] = torch.ops.aten.mm.default(view_default_119, t_default_34);  view_default_119 = t_default_34 = None\n",
      "        view_default_120: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_34, [1, 1, 4096]);  mm_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_34: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_32, view_default_120);  add_tensor_32 = view_default_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_10: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_34, 2)\n",
      "        mean_dim_10: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_10, [-1], True);  pow_tensor_scalar_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_35: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_10, 1e-06);  mean_dim_10 = None\n",
      "        rsqrt_default_10: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_35);  add_tensor_35 = None\n",
      "        detach_default_15: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_10)\n",
      "        mul_tensor_45: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_34, rsqrt_default_10);  rsqrt_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant46 = self._param_constant46\n",
      "        mul_tensor_46: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant46, mul_tensor_45);  _param_constant46 = mul_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant47 = self._param_constant47\n",
      "        t_default_35: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant47);  _param_constant47 = None\n",
      "        view_default_121: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_46, [1, 4096])\n",
      "        mm_default_35: f32[1, 4096] = torch.ops.aten.mm.default(view_default_121, t_default_35);  view_default_121 = t_default_35 = None\n",
      "        view_default_122: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_35, [1, 1, 4096]);  mm_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant48 = self._param_constant48\n",
      "        t_default_36: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant48);  _param_constant48 = None\n",
      "        view_default_123: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_46, [1, 4096])\n",
      "        mm_default_36: f32[1, 4096] = torch.ops.aten.mm.default(view_default_123, t_default_36);  view_default_123 = t_default_36 = None\n",
      "        view_default_124: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_36, [1, 1, 4096]);  mm_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant49 = self._param_constant49\n",
      "        t_default_37: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant49);  _param_constant49 = None\n",
      "        view_default_125: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_46, [1, 4096]);  mul_tensor_46 = None\n",
      "        mm_default_37: f32[1, 4096] = torch.ops.aten.mm.default(view_default_125, t_default_37);  view_default_125 = t_default_37 = None\n",
      "        view_default_126: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_37, [1, 1, 4096]);  mm_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_127: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_122, [1, 1, 32, 128]);  view_default_122 = None\n",
      "        transpose_int_25: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_127, 1, 2);  view_default_127 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_128: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_124, [1, 1, 32, 128]);  view_default_124 = None\n",
      "        transpose_int_26: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_128, 1, 2);  view_default_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_129: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_126, [1, 1, 32, 128]);  view_default_126 = None\n",
      "        transpose_int_27: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_129, 1, 2);  view_default_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant10 = self._tensor_constant10\n",
      "        slice_tensor_52: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant10, 0, 0, 9223372036854775807);  _tensor_constant10 = None\n",
      "        slice_tensor_53: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_52, 1, 0, 9223372036854775807);  slice_tensor_52 = None\n",
      "        sym_size_15: Sym(s0) = torch.ops.aten.sym_size(arg11, 2)\n",
      "        add_7: Sym(s0 + 1) = 1 + sym_size_15;  sym_size_15 = None\n",
      "        slice_tensor_54: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_53, 2, 0, add_7);  slice_tensor_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant11 = self._tensor_constant11\n",
      "        slice_tensor_55: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant11, 0, 0, 9223372036854775807);  _tensor_constant11 = None\n",
      "        slice_tensor_56: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_55, 1, 0, 9223372036854775807);  slice_tensor_55 = None\n",
      "        slice_tensor_57: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_56, 2, 0, add_7);  slice_tensor_56 = add_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_20: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_54, 1);  slice_tensor_54 = None\n",
      "        squeeze_dim_21: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_20, 0);  squeeze_dim_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_22: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_57, 1);  slice_tensor_57 = None\n",
      "        squeeze_dim_23: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_22, 0);  squeeze_dim_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_10: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_21, [view_default]);  squeeze_dim_21 = None\n",
      "        unsqueeze_default_13: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_10, 1);  index_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_11: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_23, [view_default]);  squeeze_dim_23 = None\n",
      "        unsqueeze_default_14: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_11, 1);  index_tensor_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_47: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_25, unsqueeze_default_13)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_58: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_59: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 64, 9223372036854775807);  transpose_int_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_10: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_59);  slice_tensor_59 = None\n",
      "        cat_default_20: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_10, slice_tensor_58], -1);  neg_default_10 = slice_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_48: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_20, unsqueeze_default_14);  cat_default_20 = None\n",
      "        add_tensor_36: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_47, mul_tensor_48);  mul_tensor_47 = mul_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_49: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_26, unsqueeze_default_13);  unsqueeze_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_60: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_61: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 64, 9223372036854775807);  transpose_int_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_11: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_61);  slice_tensor_61 = None\n",
      "        cat_default_21: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_11, slice_tensor_60], -1);  neg_default_11 = slice_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_50: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_21, unsqueeze_default_14);  cat_default_21 = unsqueeze_default_14 = None\n",
      "        add_tensor_37: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_49, mul_tensor_50);  mul_tensor_49 = mul_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_22: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg11, add_tensor_37], 2);  arg11 = add_tensor_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_23: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg12, transpose_int_27], 2);  arg12 = transpose_int_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_28: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_22, 2, 3)\n",
      "        expand_default_21: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_36, [1, 32, 1, 128]);  add_tensor_36 = None\n",
      "        view_default_130: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_21, [32, 1, 128]);  expand_default_21 = None\n",
      "        sym_size_16: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_22, 2)\n",
      "        expand_default_22: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_28, [1, 32, 128, sym_size_16]);  transpose_int_28 = None\n",
      "        view_default_131: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_22, [32, 128, sym_size_16]);  expand_default_22 = None\n",
      "        bmm_default_10: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_130, view_default_131);  view_default_130 = view_default_131 = None\n",
      "        view_default_132: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_10, [1, 32, 1, sym_size_16]);  bmm_default_10 = None\n",
      "        div_tensor_5: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_132, 11.313708498984761);  view_default_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_38: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_5, masked_fill_scalar);  div_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_5: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_38, -1, False);  add_tensor_38 = None\n",
      "        detach_default_16: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_5)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_23: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_5, [1, 32, 1, sym_size_16]);  _softmax_default_5 = None\n",
      "        view_default_133: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_23, [32, 1, sym_size_16]);  expand_default_23 = sym_size_16 = None\n",
      "        sym_size_17: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_23, 2)\n",
      "        expand_default_24: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_23, [1, 32, sym_size_17, 128])\n",
      "        view_default_134: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_24, [32, sym_size_17, 128]);  expand_default_24 = sym_size_17 = None\n",
      "        bmm_default_11: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_133, view_default_134);  view_default_133 = view_default_134 = None\n",
      "        view_default_135: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_11, [1, 32, 1, 128]);  bmm_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_29: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_135, 1, 2);  view_default_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_136: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_29, [1, 1, 4096]);  transpose_int_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant50 = self._param_constant50\n",
      "        t_default_38: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant50);  _param_constant50 = None\n",
      "        view_default_137: f32[1, 4096] = torch.ops.aten.view.default(view_default_136, [1, 4096]);  view_default_136 = None\n",
      "        mm_default_38: f32[1, 4096] = torch.ops.aten.mm.default(view_default_137, t_default_38);  view_default_137 = t_default_38 = None\n",
      "        view_default_138: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_38, [1, 1, 4096]);  mm_default_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_39: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_34, view_default_138);  add_tensor_34 = view_default_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_11: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_39, 2)\n",
      "        mean_dim_11: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_11, [-1], True);  pow_tensor_scalar_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_40: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_11, 1e-06);  mean_dim_11 = None\n",
      "        rsqrt_default_11: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_40);  add_tensor_40 = None\n",
      "        detach_default_17: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_11)\n",
      "        mul_tensor_51: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_39, rsqrt_default_11);  rsqrt_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant51 = self._param_constant51\n",
      "        mul_tensor_52: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant51, mul_tensor_51);  _param_constant51 = mul_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant52 = self._param_constant52\n",
      "        t_default_39: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant52);  _param_constant52 = None\n",
      "        view_default_139: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_52, [1, 4096])\n",
      "        mm_default_39: f32[1, 11008] = torch.ops.aten.mm.default(view_default_139, t_default_39);  view_default_139 = t_default_39 = None\n",
      "        view_default_140: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_39, [1, 1, 11008]);  mm_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_5: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_140);  view_default_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant53 = self._param_constant53\n",
      "        t_default_40: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant53);  _param_constant53 = None\n",
      "        view_default_141: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_52, [1, 4096]);  mul_tensor_52 = None\n",
      "        mm_default_40: f32[1, 11008] = torch.ops.aten.mm.default(view_default_141, t_default_40);  view_default_141 = t_default_40 = None\n",
      "        view_default_142: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_40, [1, 1, 11008]);  mm_default_40 = None\n",
      "        mul_tensor_53: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_5, view_default_142);  silu_default_5 = view_default_142 = None\n",
      "        _param_constant54 = self._param_constant54\n",
      "        t_default_41: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant54);  _param_constant54 = None\n",
      "        view_default_143: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_53, [1, 11008]);  mul_tensor_53 = None\n",
      "        mm_default_41: f32[1, 4096] = torch.ops.aten.mm.default(view_default_143, t_default_41);  view_default_143 = t_default_41 = None\n",
      "        view_default_144: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_41, [1, 1, 4096]);  mm_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_41: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_39, view_default_144);  add_tensor_39 = view_default_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_12: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_41, 2)\n",
      "        mean_dim_12: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_12, [-1], True);  pow_tensor_scalar_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_42: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_12, 1e-06);  mean_dim_12 = None\n",
      "        rsqrt_default_12: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_42);  add_tensor_42 = None\n",
      "        detach_default_18: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_12)\n",
      "        mul_tensor_54: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_41, rsqrt_default_12);  rsqrt_default_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant55 = self._param_constant55\n",
      "        mul_tensor_55: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant55, mul_tensor_54);  _param_constant55 = mul_tensor_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant56 = self._param_constant56\n",
      "        t_default_42: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant56);  _param_constant56 = None\n",
      "        view_default_145: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_55, [1, 4096])\n",
      "        mm_default_42: f32[1, 4096] = torch.ops.aten.mm.default(view_default_145, t_default_42);  view_default_145 = t_default_42 = None\n",
      "        view_default_146: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_42, [1, 1, 4096]);  mm_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant57 = self._param_constant57\n",
      "        t_default_43: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant57);  _param_constant57 = None\n",
      "        view_default_147: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_55, [1, 4096])\n",
      "        mm_default_43: f32[1, 4096] = torch.ops.aten.mm.default(view_default_147, t_default_43);  view_default_147 = t_default_43 = None\n",
      "        view_default_148: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_43, [1, 1, 4096]);  mm_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant58 = self._param_constant58\n",
      "        t_default_44: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant58);  _param_constant58 = None\n",
      "        view_default_149: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_55, [1, 4096]);  mul_tensor_55 = None\n",
      "        mm_default_44: f32[1, 4096] = torch.ops.aten.mm.default(view_default_149, t_default_44);  view_default_149 = t_default_44 = None\n",
      "        view_default_150: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_44, [1, 1, 4096]);  mm_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_151: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_146, [1, 1, 32, 128]);  view_default_146 = None\n",
      "        transpose_int_30: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_151, 1, 2);  view_default_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_152: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_148, [1, 1, 32, 128]);  view_default_148 = None\n",
      "        transpose_int_31: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_152, 1, 2);  view_default_152 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_153: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_150, [1, 1, 32, 128]);  view_default_150 = None\n",
      "        transpose_int_32: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_153, 1, 2);  view_default_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant12 = self._tensor_constant12\n",
      "        slice_tensor_62: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant12, 0, 0, 9223372036854775807);  _tensor_constant12 = None\n",
      "        slice_tensor_63: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_62, 1, 0, 9223372036854775807);  slice_tensor_62 = None\n",
      "        sym_size_18: Sym(s0) = torch.ops.aten.sym_size(arg13, 2)\n",
      "        add_8: Sym(s0 + 1) = 1 + sym_size_18;  sym_size_18 = None\n",
      "        slice_tensor_64: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_63, 2, 0, add_8);  slice_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant13 = self._tensor_constant13\n",
      "        slice_tensor_65: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant13, 0, 0, 9223372036854775807);  _tensor_constant13 = None\n",
      "        slice_tensor_66: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_65, 1, 0, 9223372036854775807);  slice_tensor_65 = None\n",
      "        slice_tensor_67: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_66, 2, 0, add_8);  slice_tensor_66 = add_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_24: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_64, 1);  slice_tensor_64 = None\n",
      "        squeeze_dim_25: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_24, 0);  squeeze_dim_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_26: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_67, 1);  slice_tensor_67 = None\n",
      "        squeeze_dim_27: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_26, 0);  squeeze_dim_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_12: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_25, [view_default]);  squeeze_dim_25 = None\n",
      "        unsqueeze_default_15: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_12, 1);  index_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_13: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_27, [view_default]);  squeeze_dim_27 = None\n",
      "        unsqueeze_default_16: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_13, 1);  index_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_56: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_30, unsqueeze_default_15)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_68: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_69: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 64, 9223372036854775807);  transpose_int_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_12: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_69);  slice_tensor_69 = None\n",
      "        cat_default_24: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_12, slice_tensor_68], -1);  neg_default_12 = slice_tensor_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_57: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_24, unsqueeze_default_16);  cat_default_24 = None\n",
      "        add_tensor_43: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_56, mul_tensor_57);  mul_tensor_56 = mul_tensor_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_58: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_31, unsqueeze_default_15);  unsqueeze_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_70: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_71: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 64, 9223372036854775807);  transpose_int_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_13: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_71);  slice_tensor_71 = None\n",
      "        cat_default_25: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_13, slice_tensor_70], -1);  neg_default_13 = slice_tensor_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_59: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_25, unsqueeze_default_16);  cat_default_25 = unsqueeze_default_16 = None\n",
      "        add_tensor_44: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_58, mul_tensor_59);  mul_tensor_58 = mul_tensor_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_26: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg13, add_tensor_44], 2);  arg13 = add_tensor_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_27: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg14, transpose_int_32], 2);  arg14 = transpose_int_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_33: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_26, 2, 3)\n",
      "        expand_default_25: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_43, [1, 32, 1, 128]);  add_tensor_43 = None\n",
      "        view_default_154: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_25, [32, 1, 128]);  expand_default_25 = None\n",
      "        sym_size_19: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_26, 2)\n",
      "        expand_default_26: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_33, [1, 32, 128, sym_size_19]);  transpose_int_33 = None\n",
      "        view_default_155: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_26, [32, 128, sym_size_19]);  expand_default_26 = None\n",
      "        bmm_default_12: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_154, view_default_155);  view_default_154 = view_default_155 = None\n",
      "        view_default_156: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_12, [1, 32, 1, sym_size_19]);  bmm_default_12 = None\n",
      "        div_tensor_6: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_156, 11.313708498984761);  view_default_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_45: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_6, masked_fill_scalar);  div_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_6: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_45, -1, False);  add_tensor_45 = None\n",
      "        detach_default_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_6)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_27: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_6, [1, 32, 1, sym_size_19]);  _softmax_default_6 = None\n",
      "        view_default_157: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_27, [32, 1, sym_size_19]);  expand_default_27 = sym_size_19 = None\n",
      "        sym_size_20: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_27, 2)\n",
      "        expand_default_28: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_27, [1, 32, sym_size_20, 128])\n",
      "        view_default_158: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_28, [32, sym_size_20, 128]);  expand_default_28 = sym_size_20 = None\n",
      "        bmm_default_13: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_157, view_default_158);  view_default_157 = view_default_158 = None\n",
      "        view_default_159: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_13, [1, 32, 1, 128]);  bmm_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_34: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_159, 1, 2);  view_default_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_160: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_34, [1, 1, 4096]);  transpose_int_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant59 = self._param_constant59\n",
      "        t_default_45: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant59);  _param_constant59 = None\n",
      "        view_default_161: f32[1, 4096] = torch.ops.aten.view.default(view_default_160, [1, 4096]);  view_default_160 = None\n",
      "        mm_default_45: f32[1, 4096] = torch.ops.aten.mm.default(view_default_161, t_default_45);  view_default_161 = t_default_45 = None\n",
      "        view_default_162: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_45, [1, 1, 4096]);  mm_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_46: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_41, view_default_162);  add_tensor_41 = view_default_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_13: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_46, 2)\n",
      "        mean_dim_13: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_13, [-1], True);  pow_tensor_scalar_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_47: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_13, 1e-06);  mean_dim_13 = None\n",
      "        rsqrt_default_13: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_47);  add_tensor_47 = None\n",
      "        detach_default_20: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_13)\n",
      "        mul_tensor_60: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_46, rsqrt_default_13);  rsqrt_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant60 = self._param_constant60\n",
      "        mul_tensor_61: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant60, mul_tensor_60);  _param_constant60 = mul_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant61 = self._param_constant61\n",
      "        t_default_46: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant61);  _param_constant61 = None\n",
      "        view_default_163: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_61, [1, 4096])\n",
      "        mm_default_46: f32[1, 11008] = torch.ops.aten.mm.default(view_default_163, t_default_46);  view_default_163 = t_default_46 = None\n",
      "        view_default_164: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_46, [1, 1, 11008]);  mm_default_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_6: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_164);  view_default_164 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant62 = self._param_constant62\n",
      "        t_default_47: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant62);  _param_constant62 = None\n",
      "        view_default_165: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_61, [1, 4096]);  mul_tensor_61 = None\n",
      "        mm_default_47: f32[1, 11008] = torch.ops.aten.mm.default(view_default_165, t_default_47);  view_default_165 = t_default_47 = None\n",
      "        view_default_166: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_47, [1, 1, 11008]);  mm_default_47 = None\n",
      "        mul_tensor_62: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_6, view_default_166);  silu_default_6 = view_default_166 = None\n",
      "        _param_constant63 = self._param_constant63\n",
      "        t_default_48: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant63);  _param_constant63 = None\n",
      "        view_default_167: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_62, [1, 11008]);  mul_tensor_62 = None\n",
      "        mm_default_48: f32[1, 4096] = torch.ops.aten.mm.default(view_default_167, t_default_48);  view_default_167 = t_default_48 = None\n",
      "        view_default_168: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_48, [1, 1, 4096]);  mm_default_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_48: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_46, view_default_168);  add_tensor_46 = view_default_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_14: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_48, 2)\n",
      "        mean_dim_14: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_14, [-1], True);  pow_tensor_scalar_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_49: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_14, 1e-06);  mean_dim_14 = None\n",
      "        rsqrt_default_14: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_49);  add_tensor_49 = None\n",
      "        detach_default_21: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_14)\n",
      "        mul_tensor_63: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_48, rsqrt_default_14);  rsqrt_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant64 = self._param_constant64\n",
      "        mul_tensor_64: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant64, mul_tensor_63);  _param_constant64 = mul_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant65 = self._param_constant65\n",
      "        t_default_49: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant65);  _param_constant65 = None\n",
      "        view_default_169: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_64, [1, 4096])\n",
      "        mm_default_49: f32[1, 4096] = torch.ops.aten.mm.default(view_default_169, t_default_49);  view_default_169 = t_default_49 = None\n",
      "        view_default_170: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_49, [1, 1, 4096]);  mm_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant66 = self._param_constant66\n",
      "        t_default_50: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant66);  _param_constant66 = None\n",
      "        view_default_171: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_64, [1, 4096])\n",
      "        mm_default_50: f32[1, 4096] = torch.ops.aten.mm.default(view_default_171, t_default_50);  view_default_171 = t_default_50 = None\n",
      "        view_default_172: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_50, [1, 1, 4096]);  mm_default_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant67 = self._param_constant67\n",
      "        t_default_51: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant67);  _param_constant67 = None\n",
      "        view_default_173: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_64, [1, 4096]);  mul_tensor_64 = None\n",
      "        mm_default_51: f32[1, 4096] = torch.ops.aten.mm.default(view_default_173, t_default_51);  view_default_173 = t_default_51 = None\n",
      "        view_default_174: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_51, [1, 1, 4096]);  mm_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_175: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_170, [1, 1, 32, 128]);  view_default_170 = None\n",
      "        transpose_int_35: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_175, 1, 2);  view_default_175 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_176: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_172, [1, 1, 32, 128]);  view_default_172 = None\n",
      "        transpose_int_36: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_176, 1, 2);  view_default_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_177: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_174, [1, 1, 32, 128]);  view_default_174 = None\n",
      "        transpose_int_37: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_177, 1, 2);  view_default_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant14 = self._tensor_constant14\n",
      "        slice_tensor_72: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant14, 0, 0, 9223372036854775807);  _tensor_constant14 = None\n",
      "        slice_tensor_73: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_72, 1, 0, 9223372036854775807);  slice_tensor_72 = None\n",
      "        sym_size_21: Sym(s0) = torch.ops.aten.sym_size(arg15, 2)\n",
      "        add_9: Sym(s0 + 1) = 1 + sym_size_21;  sym_size_21 = None\n",
      "        slice_tensor_74: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_73, 2, 0, add_9);  slice_tensor_73 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant15 = self._tensor_constant15\n",
      "        slice_tensor_75: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant15, 0, 0, 9223372036854775807);  _tensor_constant15 = None\n",
      "        slice_tensor_76: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_75, 1, 0, 9223372036854775807);  slice_tensor_75 = None\n",
      "        slice_tensor_77: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_76, 2, 0, add_9);  slice_tensor_76 = add_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_28: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_74, 1);  slice_tensor_74 = None\n",
      "        squeeze_dim_29: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_28, 0);  squeeze_dim_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_30: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_77, 1);  slice_tensor_77 = None\n",
      "        squeeze_dim_31: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_30, 0);  squeeze_dim_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_14: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_29, [view_default]);  squeeze_dim_29 = None\n",
      "        unsqueeze_default_17: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_14, 1);  index_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_15: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_31, [view_default]);  squeeze_dim_31 = None\n",
      "        unsqueeze_default_18: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_15, 1);  index_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_65: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_35, unsqueeze_default_17)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_78: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_79: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 64, 9223372036854775807);  transpose_int_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_14: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_79);  slice_tensor_79 = None\n",
      "        cat_default_28: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_14, slice_tensor_78], -1);  neg_default_14 = slice_tensor_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_66: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_28, unsqueeze_default_18);  cat_default_28 = None\n",
      "        add_tensor_50: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_65, mul_tensor_66);  mul_tensor_65 = mul_tensor_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_67: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_36, unsqueeze_default_17);  unsqueeze_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_80: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_81: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 64, 9223372036854775807);  transpose_int_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_15: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_81);  slice_tensor_81 = None\n",
      "        cat_default_29: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_15, slice_tensor_80], -1);  neg_default_15 = slice_tensor_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_68: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_29, unsqueeze_default_18);  cat_default_29 = unsqueeze_default_18 = None\n",
      "        add_tensor_51: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_67, mul_tensor_68);  mul_tensor_67 = mul_tensor_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_30: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg15, add_tensor_51], 2);  arg15 = add_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_31: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg16, transpose_int_37], 2);  arg16 = transpose_int_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_38: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_30, 2, 3)\n",
      "        expand_default_29: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_50, [1, 32, 1, 128]);  add_tensor_50 = None\n",
      "        view_default_178: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_29, [32, 1, 128]);  expand_default_29 = None\n",
      "        sym_size_22: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_30, 2)\n",
      "        expand_default_30: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_38, [1, 32, 128, sym_size_22]);  transpose_int_38 = None\n",
      "        view_default_179: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_30, [32, 128, sym_size_22]);  expand_default_30 = None\n",
      "        bmm_default_14: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_178, view_default_179);  view_default_178 = view_default_179 = None\n",
      "        view_default_180: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_14, [1, 32, 1, sym_size_22]);  bmm_default_14 = None\n",
      "        div_tensor_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_180, 11.313708498984761);  view_default_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_52: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_7, masked_fill_scalar);  div_tensor_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_52, -1, False);  add_tensor_52 = None\n",
      "        detach_default_22: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_7)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_7, [1, 32, 1, sym_size_22]);  _softmax_default_7 = None\n",
      "        view_default_181: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_31, [32, 1, sym_size_22]);  expand_default_31 = sym_size_22 = None\n",
      "        sym_size_23: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_31, 2)\n",
      "        expand_default_32: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_31, [1, 32, sym_size_23, 128])\n",
      "        view_default_182: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_32, [32, sym_size_23, 128]);  expand_default_32 = sym_size_23 = None\n",
      "        bmm_default_15: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_181, view_default_182);  view_default_181 = view_default_182 = None\n",
      "        view_default_183: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_15, [1, 32, 1, 128]);  bmm_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_39: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_183, 1, 2);  view_default_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_184: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_39, [1, 1, 4096]);  transpose_int_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant68 = self._param_constant68\n",
      "        t_default_52: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant68);  _param_constant68 = None\n",
      "        view_default_185: f32[1, 4096] = torch.ops.aten.view.default(view_default_184, [1, 4096]);  view_default_184 = None\n",
      "        mm_default_52: f32[1, 4096] = torch.ops.aten.mm.default(view_default_185, t_default_52);  view_default_185 = t_default_52 = None\n",
      "        view_default_186: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_52, [1, 1, 4096]);  mm_default_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_53: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_48, view_default_186);  add_tensor_48 = view_default_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_15: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_53, 2)\n",
      "        mean_dim_15: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_15, [-1], True);  pow_tensor_scalar_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_54: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_15, 1e-06);  mean_dim_15 = None\n",
      "        rsqrt_default_15: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_54);  add_tensor_54 = None\n",
      "        detach_default_23: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_15)\n",
      "        mul_tensor_69: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_53, rsqrt_default_15);  rsqrt_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant69 = self._param_constant69\n",
      "        mul_tensor_70: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant69, mul_tensor_69);  _param_constant69 = mul_tensor_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant70 = self._param_constant70\n",
      "        t_default_53: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant70);  _param_constant70 = None\n",
      "        view_default_187: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_70, [1, 4096])\n",
      "        mm_default_53: f32[1, 11008] = torch.ops.aten.mm.default(view_default_187, t_default_53);  view_default_187 = t_default_53 = None\n",
      "        view_default_188: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_53, [1, 1, 11008]);  mm_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_7: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_188);  view_default_188 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant71 = self._param_constant71\n",
      "        t_default_54: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant71);  _param_constant71 = None\n",
      "        view_default_189: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_70, [1, 4096]);  mul_tensor_70 = None\n",
      "        mm_default_54: f32[1, 11008] = torch.ops.aten.mm.default(view_default_189, t_default_54);  view_default_189 = t_default_54 = None\n",
      "        view_default_190: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_54, [1, 1, 11008]);  mm_default_54 = None\n",
      "        mul_tensor_71: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_7, view_default_190);  silu_default_7 = view_default_190 = None\n",
      "        _param_constant72 = self._param_constant72\n",
      "        t_default_55: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant72);  _param_constant72 = None\n",
      "        view_default_191: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_71, [1, 11008]);  mul_tensor_71 = None\n",
      "        mm_default_55: f32[1, 4096] = torch.ops.aten.mm.default(view_default_191, t_default_55);  view_default_191 = t_default_55 = None\n",
      "        view_default_192: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_55, [1, 1, 4096]);  mm_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_55: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_53, view_default_192);  add_tensor_53 = view_default_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_16: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_55, 2)\n",
      "        mean_dim_16: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_16, [-1], True);  pow_tensor_scalar_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_56: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_16, 1e-06);  mean_dim_16 = None\n",
      "        rsqrt_default_16: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_56);  add_tensor_56 = None\n",
      "        detach_default_24: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_16)\n",
      "        mul_tensor_72: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_55, rsqrt_default_16);  rsqrt_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant73 = self._param_constant73\n",
      "        mul_tensor_73: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant73, mul_tensor_72);  _param_constant73 = mul_tensor_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant74 = self._param_constant74\n",
      "        t_default_56: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant74);  _param_constant74 = None\n",
      "        view_default_193: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_73, [1, 4096])\n",
      "        mm_default_56: f32[1, 4096] = torch.ops.aten.mm.default(view_default_193, t_default_56);  view_default_193 = t_default_56 = None\n",
      "        view_default_194: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_56, [1, 1, 4096]);  mm_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant75 = self._param_constant75\n",
      "        t_default_57: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant75);  _param_constant75 = None\n",
      "        view_default_195: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_73, [1, 4096])\n",
      "        mm_default_57: f32[1, 4096] = torch.ops.aten.mm.default(view_default_195, t_default_57);  view_default_195 = t_default_57 = None\n",
      "        view_default_196: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_57, [1, 1, 4096]);  mm_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant76 = self._param_constant76\n",
      "        t_default_58: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant76);  _param_constant76 = None\n",
      "        view_default_197: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_73, [1, 4096]);  mul_tensor_73 = None\n",
      "        mm_default_58: f32[1, 4096] = torch.ops.aten.mm.default(view_default_197, t_default_58);  view_default_197 = t_default_58 = None\n",
      "        view_default_198: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_58, [1, 1, 4096]);  mm_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_199: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_194, [1, 1, 32, 128]);  view_default_194 = None\n",
      "        transpose_int_40: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_199, 1, 2);  view_default_199 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_200: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_196, [1, 1, 32, 128]);  view_default_196 = None\n",
      "        transpose_int_41: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_200, 1, 2);  view_default_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_201: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_198, [1, 1, 32, 128]);  view_default_198 = None\n",
      "        transpose_int_42: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_201, 1, 2);  view_default_201 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant16 = self._tensor_constant16\n",
      "        slice_tensor_82: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant16, 0, 0, 9223372036854775807);  _tensor_constant16 = None\n",
      "        slice_tensor_83: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_82, 1, 0, 9223372036854775807);  slice_tensor_82 = None\n",
      "        sym_size_24: Sym(s0) = torch.ops.aten.sym_size(arg17, 2)\n",
      "        add_10: Sym(s0 + 1) = 1 + sym_size_24;  sym_size_24 = None\n",
      "        slice_tensor_84: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_83, 2, 0, add_10);  slice_tensor_83 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant17 = self._tensor_constant17\n",
      "        slice_tensor_85: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant17, 0, 0, 9223372036854775807);  _tensor_constant17 = None\n",
      "        slice_tensor_86: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_85, 1, 0, 9223372036854775807);  slice_tensor_85 = None\n",
      "        slice_tensor_87: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_86, 2, 0, add_10);  slice_tensor_86 = add_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_32: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_84, 1);  slice_tensor_84 = None\n",
      "        squeeze_dim_33: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_32, 0);  squeeze_dim_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_34: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_87, 1);  slice_tensor_87 = None\n",
      "        squeeze_dim_35: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_34, 0);  squeeze_dim_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_16: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_33, [view_default]);  squeeze_dim_33 = None\n",
      "        unsqueeze_default_19: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_16, 1);  index_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_17: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_35, [view_default]);  squeeze_dim_35 = None\n",
      "        unsqueeze_default_20: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_17, 1);  index_tensor_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_74: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_40, unsqueeze_default_19)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_88: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_89: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 64, 9223372036854775807);  transpose_int_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_16: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_89);  slice_tensor_89 = None\n",
      "        cat_default_32: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_16, slice_tensor_88], -1);  neg_default_16 = slice_tensor_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_75: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_32, unsqueeze_default_20);  cat_default_32 = None\n",
      "        add_tensor_57: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_74, mul_tensor_75);  mul_tensor_74 = mul_tensor_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_76: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_41, unsqueeze_default_19);  unsqueeze_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_90: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_91: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 64, 9223372036854775807);  transpose_int_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_17: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_91);  slice_tensor_91 = None\n",
      "        cat_default_33: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_17, slice_tensor_90], -1);  neg_default_17 = slice_tensor_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_77: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_33, unsqueeze_default_20);  cat_default_33 = unsqueeze_default_20 = None\n",
      "        add_tensor_58: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_76, mul_tensor_77);  mul_tensor_76 = mul_tensor_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_34: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg17, add_tensor_58], 2);  arg17 = add_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_35: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg18, transpose_int_42], 2);  arg18 = transpose_int_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_43: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_34, 2, 3)\n",
      "        expand_default_33: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_57, [1, 32, 1, 128]);  add_tensor_57 = None\n",
      "        view_default_202: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_33, [32, 1, 128]);  expand_default_33 = None\n",
      "        sym_size_25: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_34, 2)\n",
      "        expand_default_34: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_43, [1, 32, 128, sym_size_25]);  transpose_int_43 = None\n",
      "        view_default_203: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_34, [32, 128, sym_size_25]);  expand_default_34 = None\n",
      "        bmm_default_16: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_202, view_default_203);  view_default_202 = view_default_203 = None\n",
      "        view_default_204: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_16, [1, 32, 1, sym_size_25]);  bmm_default_16 = None\n",
      "        div_tensor_8: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_204, 11.313708498984761);  view_default_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_59: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_8, masked_fill_scalar);  div_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_8: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_59, -1, False);  add_tensor_59 = None\n",
      "        detach_default_25: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_8)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_35: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_8, [1, 32, 1, sym_size_25]);  _softmax_default_8 = None\n",
      "        view_default_205: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_35, [32, 1, sym_size_25]);  expand_default_35 = sym_size_25 = None\n",
      "        sym_size_26: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_35, 2)\n",
      "        expand_default_36: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_35, [1, 32, sym_size_26, 128])\n",
      "        view_default_206: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_36, [32, sym_size_26, 128]);  expand_default_36 = sym_size_26 = None\n",
      "        bmm_default_17: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_205, view_default_206);  view_default_205 = view_default_206 = None\n",
      "        view_default_207: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_17, [1, 32, 1, 128]);  bmm_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_44: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_207, 1, 2);  view_default_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_208: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_44, [1, 1, 4096]);  transpose_int_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant77 = self._param_constant77\n",
      "        t_default_59: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant77);  _param_constant77 = None\n",
      "        view_default_209: f32[1, 4096] = torch.ops.aten.view.default(view_default_208, [1, 4096]);  view_default_208 = None\n",
      "        mm_default_59: f32[1, 4096] = torch.ops.aten.mm.default(view_default_209, t_default_59);  view_default_209 = t_default_59 = None\n",
      "        view_default_210: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_59, [1, 1, 4096]);  mm_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_60: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_55, view_default_210);  add_tensor_55 = view_default_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_17: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_60, 2)\n",
      "        mean_dim_17: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_17, [-1], True);  pow_tensor_scalar_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_61: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_17, 1e-06);  mean_dim_17 = None\n",
      "        rsqrt_default_17: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_61);  add_tensor_61 = None\n",
      "        detach_default_26: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_17)\n",
      "        mul_tensor_78: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_60, rsqrt_default_17);  rsqrt_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant78 = self._param_constant78\n",
      "        mul_tensor_79: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant78, mul_tensor_78);  _param_constant78 = mul_tensor_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant79 = self._param_constant79\n",
      "        t_default_60: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant79);  _param_constant79 = None\n",
      "        view_default_211: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_79, [1, 4096])\n",
      "        mm_default_60: f32[1, 11008] = torch.ops.aten.mm.default(view_default_211, t_default_60);  view_default_211 = t_default_60 = None\n",
      "        view_default_212: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_60, [1, 1, 11008]);  mm_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_8: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_212);  view_default_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant80 = self._param_constant80\n",
      "        t_default_61: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant80);  _param_constant80 = None\n",
      "        view_default_213: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_79, [1, 4096]);  mul_tensor_79 = None\n",
      "        mm_default_61: f32[1, 11008] = torch.ops.aten.mm.default(view_default_213, t_default_61);  view_default_213 = t_default_61 = None\n",
      "        view_default_214: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_61, [1, 1, 11008]);  mm_default_61 = None\n",
      "        mul_tensor_80: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_8, view_default_214);  silu_default_8 = view_default_214 = None\n",
      "        _param_constant81 = self._param_constant81\n",
      "        t_default_62: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant81);  _param_constant81 = None\n",
      "        view_default_215: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_80, [1, 11008]);  mul_tensor_80 = None\n",
      "        mm_default_62: f32[1, 4096] = torch.ops.aten.mm.default(view_default_215, t_default_62);  view_default_215 = t_default_62 = None\n",
      "        view_default_216: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_62, [1, 1, 4096]);  mm_default_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_62: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_60, view_default_216);  add_tensor_60 = view_default_216 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_18: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_62, 2)\n",
      "        mean_dim_18: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_18, [-1], True);  pow_tensor_scalar_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_63: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_18, 1e-06);  mean_dim_18 = None\n",
      "        rsqrt_default_18: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_63);  add_tensor_63 = None\n",
      "        detach_default_27: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_18)\n",
      "        mul_tensor_81: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_62, rsqrt_default_18);  rsqrt_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant82 = self._param_constant82\n",
      "        mul_tensor_82: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant82, mul_tensor_81);  _param_constant82 = mul_tensor_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant83 = self._param_constant83\n",
      "        t_default_63: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant83);  _param_constant83 = None\n",
      "        view_default_217: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_82, [1, 4096])\n",
      "        mm_default_63: f32[1, 4096] = torch.ops.aten.mm.default(view_default_217, t_default_63);  view_default_217 = t_default_63 = None\n",
      "        view_default_218: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_63, [1, 1, 4096]);  mm_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant84 = self._param_constant84\n",
      "        t_default_64: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant84);  _param_constant84 = None\n",
      "        view_default_219: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_82, [1, 4096])\n",
      "        mm_default_64: f32[1, 4096] = torch.ops.aten.mm.default(view_default_219, t_default_64);  view_default_219 = t_default_64 = None\n",
      "        view_default_220: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_64, [1, 1, 4096]);  mm_default_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant85 = self._param_constant85\n",
      "        t_default_65: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant85);  _param_constant85 = None\n",
      "        view_default_221: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_82, [1, 4096]);  mul_tensor_82 = None\n",
      "        mm_default_65: f32[1, 4096] = torch.ops.aten.mm.default(view_default_221, t_default_65);  view_default_221 = t_default_65 = None\n",
      "        view_default_222: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_65, [1, 1, 4096]);  mm_default_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_223: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_218, [1, 1, 32, 128]);  view_default_218 = None\n",
      "        transpose_int_45: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_223, 1, 2);  view_default_223 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_224: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_220, [1, 1, 32, 128]);  view_default_220 = None\n",
      "        transpose_int_46: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_224, 1, 2);  view_default_224 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_225: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_222, [1, 1, 32, 128]);  view_default_222 = None\n",
      "        transpose_int_47: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_225, 1, 2);  view_default_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant18 = self._tensor_constant18\n",
      "        slice_tensor_92: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant18, 0, 0, 9223372036854775807);  _tensor_constant18 = None\n",
      "        slice_tensor_93: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_92, 1, 0, 9223372036854775807);  slice_tensor_92 = None\n",
      "        sym_size_27: Sym(s0) = torch.ops.aten.sym_size(arg19, 2)\n",
      "        add_11: Sym(s0 + 1) = 1 + sym_size_27;  sym_size_27 = None\n",
      "        slice_tensor_94: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_93, 2, 0, add_11);  slice_tensor_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant19 = self._tensor_constant19\n",
      "        slice_tensor_95: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant19, 0, 0, 9223372036854775807);  _tensor_constant19 = None\n",
      "        slice_tensor_96: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_95, 1, 0, 9223372036854775807);  slice_tensor_95 = None\n",
      "        slice_tensor_97: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_96, 2, 0, add_11);  slice_tensor_96 = add_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_36: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_94, 1);  slice_tensor_94 = None\n",
      "        squeeze_dim_37: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_36, 0);  squeeze_dim_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_38: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_97, 1);  slice_tensor_97 = None\n",
      "        squeeze_dim_39: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_38, 0);  squeeze_dim_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_18: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_37, [view_default]);  squeeze_dim_37 = None\n",
      "        unsqueeze_default_21: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_18, 1);  index_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_19: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_39, [view_default]);  squeeze_dim_39 = None\n",
      "        unsqueeze_default_22: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_19, 1);  index_tensor_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_83: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_45, unsqueeze_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_98: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_99: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 64, 9223372036854775807);  transpose_int_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_18: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_99);  slice_tensor_99 = None\n",
      "        cat_default_36: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_18, slice_tensor_98], -1);  neg_default_18 = slice_tensor_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_84: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_36, unsqueeze_default_22);  cat_default_36 = None\n",
      "        add_tensor_64: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_83, mul_tensor_84);  mul_tensor_83 = mul_tensor_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_85: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_46, unsqueeze_default_21);  unsqueeze_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_100: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_101: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 64, 9223372036854775807);  transpose_int_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_19: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_101);  slice_tensor_101 = None\n",
      "        cat_default_37: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_19, slice_tensor_100], -1);  neg_default_19 = slice_tensor_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_86: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_37, unsqueeze_default_22);  cat_default_37 = unsqueeze_default_22 = None\n",
      "        add_tensor_65: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_85, mul_tensor_86);  mul_tensor_85 = mul_tensor_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_38: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg19, add_tensor_65], 2);  arg19 = add_tensor_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_39: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg20, transpose_int_47], 2);  arg20 = transpose_int_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_48: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_38, 2, 3)\n",
      "        expand_default_37: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_64, [1, 32, 1, 128]);  add_tensor_64 = None\n",
      "        view_default_226: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_37, [32, 1, 128]);  expand_default_37 = None\n",
      "        sym_size_28: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_38, 2)\n",
      "        expand_default_38: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_48, [1, 32, 128, sym_size_28]);  transpose_int_48 = None\n",
      "        view_default_227: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_38, [32, 128, sym_size_28]);  expand_default_38 = None\n",
      "        bmm_default_18: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_226, view_default_227);  view_default_226 = view_default_227 = None\n",
      "        view_default_228: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_18, [1, 32, 1, sym_size_28]);  bmm_default_18 = None\n",
      "        div_tensor_9: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_228, 11.313708498984761);  view_default_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_66: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_9, masked_fill_scalar);  div_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_9: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_66, -1, False);  add_tensor_66 = None\n",
      "        detach_default_28: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_9)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_39: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_9, [1, 32, 1, sym_size_28]);  _softmax_default_9 = None\n",
      "        view_default_229: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_39, [32, 1, sym_size_28]);  expand_default_39 = sym_size_28 = None\n",
      "        sym_size_29: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_39, 2)\n",
      "        expand_default_40: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_39, [1, 32, sym_size_29, 128])\n",
      "        view_default_230: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_40, [32, sym_size_29, 128]);  expand_default_40 = sym_size_29 = None\n",
      "        bmm_default_19: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_229, view_default_230);  view_default_229 = view_default_230 = None\n",
      "        view_default_231: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_19, [1, 32, 1, 128]);  bmm_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_49: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_231, 1, 2);  view_default_231 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_232: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_49, [1, 1, 4096]);  transpose_int_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant86 = self._param_constant86\n",
      "        t_default_66: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant86);  _param_constant86 = None\n",
      "        view_default_233: f32[1, 4096] = torch.ops.aten.view.default(view_default_232, [1, 4096]);  view_default_232 = None\n",
      "        mm_default_66: f32[1, 4096] = torch.ops.aten.mm.default(view_default_233, t_default_66);  view_default_233 = t_default_66 = None\n",
      "        view_default_234: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_66, [1, 1, 4096]);  mm_default_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_67: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_62, view_default_234);  add_tensor_62 = view_default_234 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_19: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_67, 2)\n",
      "        mean_dim_19: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_19, [-1], True);  pow_tensor_scalar_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_68: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_19, 1e-06);  mean_dim_19 = None\n",
      "        rsqrt_default_19: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_68);  add_tensor_68 = None\n",
      "        detach_default_29: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_19)\n",
      "        mul_tensor_87: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_67, rsqrt_default_19);  rsqrt_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant87 = self._param_constant87\n",
      "        mul_tensor_88: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant87, mul_tensor_87);  _param_constant87 = mul_tensor_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant88 = self._param_constant88\n",
      "        t_default_67: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant88);  _param_constant88 = None\n",
      "        view_default_235: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_88, [1, 4096])\n",
      "        mm_default_67: f32[1, 11008] = torch.ops.aten.mm.default(view_default_235, t_default_67);  view_default_235 = t_default_67 = None\n",
      "        view_default_236: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_67, [1, 1, 11008]);  mm_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_9: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_236);  view_default_236 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant89 = self._param_constant89\n",
      "        t_default_68: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant89);  _param_constant89 = None\n",
      "        view_default_237: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_88, [1, 4096]);  mul_tensor_88 = None\n",
      "        mm_default_68: f32[1, 11008] = torch.ops.aten.mm.default(view_default_237, t_default_68);  view_default_237 = t_default_68 = None\n",
      "        view_default_238: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_68, [1, 1, 11008]);  mm_default_68 = None\n",
      "        mul_tensor_89: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_9, view_default_238);  silu_default_9 = view_default_238 = None\n",
      "        _param_constant90 = self._param_constant90\n",
      "        t_default_69: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant90);  _param_constant90 = None\n",
      "        view_default_239: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_89, [1, 11008]);  mul_tensor_89 = None\n",
      "        mm_default_69: f32[1, 4096] = torch.ops.aten.mm.default(view_default_239, t_default_69);  view_default_239 = t_default_69 = None\n",
      "        view_default_240: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_69, [1, 1, 4096]);  mm_default_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_69: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_67, view_default_240);  add_tensor_67 = view_default_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_20: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_69, 2)\n",
      "        mean_dim_20: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_20, [-1], True);  pow_tensor_scalar_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_70: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_20, 1e-06);  mean_dim_20 = None\n",
      "        rsqrt_default_20: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_70);  add_tensor_70 = None\n",
      "        detach_default_30: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_20)\n",
      "        mul_tensor_90: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_69, rsqrt_default_20);  rsqrt_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant91 = self._param_constant91\n",
      "        mul_tensor_91: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant91, mul_tensor_90);  _param_constant91 = mul_tensor_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant92 = self._param_constant92\n",
      "        t_default_70: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant92);  _param_constant92 = None\n",
      "        view_default_241: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_91, [1, 4096])\n",
      "        mm_default_70: f32[1, 4096] = torch.ops.aten.mm.default(view_default_241, t_default_70);  view_default_241 = t_default_70 = None\n",
      "        view_default_242: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_70, [1, 1, 4096]);  mm_default_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant93 = self._param_constant93\n",
      "        t_default_71: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant93);  _param_constant93 = None\n",
      "        view_default_243: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_91, [1, 4096])\n",
      "        mm_default_71: f32[1, 4096] = torch.ops.aten.mm.default(view_default_243, t_default_71);  view_default_243 = t_default_71 = None\n",
      "        view_default_244: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_71, [1, 1, 4096]);  mm_default_71 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant94 = self._param_constant94\n",
      "        t_default_72: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant94);  _param_constant94 = None\n",
      "        view_default_245: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_91, [1, 4096]);  mul_tensor_91 = None\n",
      "        mm_default_72: f32[1, 4096] = torch.ops.aten.mm.default(view_default_245, t_default_72);  view_default_245 = t_default_72 = None\n",
      "        view_default_246: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_72, [1, 1, 4096]);  mm_default_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_247: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_242, [1, 1, 32, 128]);  view_default_242 = None\n",
      "        transpose_int_50: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_247, 1, 2);  view_default_247 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_248: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_244, [1, 1, 32, 128]);  view_default_244 = None\n",
      "        transpose_int_51: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_248, 1, 2);  view_default_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_249: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_246, [1, 1, 32, 128]);  view_default_246 = None\n",
      "        transpose_int_52: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_249, 1, 2);  view_default_249 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant20 = self._tensor_constant20\n",
      "        slice_tensor_102: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant20, 0, 0, 9223372036854775807);  _tensor_constant20 = None\n",
      "        slice_tensor_103: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_102, 1, 0, 9223372036854775807);  slice_tensor_102 = None\n",
      "        sym_size_30: Sym(s0) = torch.ops.aten.sym_size(arg21, 2)\n",
      "        add_12: Sym(s0 + 1) = 1 + sym_size_30;  sym_size_30 = None\n",
      "        slice_tensor_104: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_103, 2, 0, add_12);  slice_tensor_103 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant21 = self._tensor_constant21\n",
      "        slice_tensor_105: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant21, 0, 0, 9223372036854775807);  _tensor_constant21 = None\n",
      "        slice_tensor_106: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_105, 1, 0, 9223372036854775807);  slice_tensor_105 = None\n",
      "        slice_tensor_107: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_106, 2, 0, add_12);  slice_tensor_106 = add_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_40: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_104, 1);  slice_tensor_104 = None\n",
      "        squeeze_dim_41: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_40, 0);  squeeze_dim_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_42: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_107, 1);  slice_tensor_107 = None\n",
      "        squeeze_dim_43: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_42, 0);  squeeze_dim_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_20: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_41, [view_default]);  squeeze_dim_41 = None\n",
      "        unsqueeze_default_23: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_20, 1);  index_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_21: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_43, [view_default]);  squeeze_dim_43 = None\n",
      "        unsqueeze_default_24: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_21, 1);  index_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_92: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_50, unsqueeze_default_23)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_108: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_109: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 64, 9223372036854775807);  transpose_int_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_20: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_109);  slice_tensor_109 = None\n",
      "        cat_default_40: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_20, slice_tensor_108], -1);  neg_default_20 = slice_tensor_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_93: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_40, unsqueeze_default_24);  cat_default_40 = None\n",
      "        add_tensor_71: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_92, mul_tensor_93);  mul_tensor_92 = mul_tensor_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_94: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_51, unsqueeze_default_23);  unsqueeze_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_110: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_111: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 64, 9223372036854775807);  transpose_int_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_21: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_111);  slice_tensor_111 = None\n",
      "        cat_default_41: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_21, slice_tensor_110], -1);  neg_default_21 = slice_tensor_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_95: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_41, unsqueeze_default_24);  cat_default_41 = unsqueeze_default_24 = None\n",
      "        add_tensor_72: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_94, mul_tensor_95);  mul_tensor_94 = mul_tensor_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_42: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg21, add_tensor_72], 2);  arg21 = add_tensor_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_43: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg22, transpose_int_52], 2);  arg22 = transpose_int_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_53: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_42, 2, 3)\n",
      "        expand_default_41: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_71, [1, 32, 1, 128]);  add_tensor_71 = None\n",
      "        view_default_250: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_41, [32, 1, 128]);  expand_default_41 = None\n",
      "        sym_size_31: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_42, 2)\n",
      "        expand_default_42: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_53, [1, 32, 128, sym_size_31]);  transpose_int_53 = None\n",
      "        view_default_251: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_42, [32, 128, sym_size_31]);  expand_default_42 = None\n",
      "        bmm_default_20: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_250, view_default_251);  view_default_250 = view_default_251 = None\n",
      "        view_default_252: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_20, [1, 32, 1, sym_size_31]);  bmm_default_20 = None\n",
      "        div_tensor_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_252, 11.313708498984761);  view_default_252 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_73: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_10, masked_fill_scalar);  div_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_73, -1, False);  add_tensor_73 = None\n",
      "        detach_default_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_10)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_43: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_10, [1, 32, 1, sym_size_31]);  _softmax_default_10 = None\n",
      "        view_default_253: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_43, [32, 1, sym_size_31]);  expand_default_43 = sym_size_31 = None\n",
      "        sym_size_32: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_43, 2)\n",
      "        expand_default_44: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_43, [1, 32, sym_size_32, 128])\n",
      "        view_default_254: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_44, [32, sym_size_32, 128]);  expand_default_44 = sym_size_32 = None\n",
      "        bmm_default_21: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_253, view_default_254);  view_default_253 = view_default_254 = None\n",
      "        view_default_255: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_21, [1, 32, 1, 128]);  bmm_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_54: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_255, 1, 2);  view_default_255 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_256: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_54, [1, 1, 4096]);  transpose_int_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant95 = self._param_constant95\n",
      "        t_default_73: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant95);  _param_constant95 = None\n",
      "        view_default_257: f32[1, 4096] = torch.ops.aten.view.default(view_default_256, [1, 4096]);  view_default_256 = None\n",
      "        mm_default_73: f32[1, 4096] = torch.ops.aten.mm.default(view_default_257, t_default_73);  view_default_257 = t_default_73 = None\n",
      "        view_default_258: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_73, [1, 1, 4096]);  mm_default_73 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_74: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_69, view_default_258);  add_tensor_69 = view_default_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_21: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_74, 2)\n",
      "        mean_dim_21: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_21, [-1], True);  pow_tensor_scalar_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_75: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_21, 1e-06);  mean_dim_21 = None\n",
      "        rsqrt_default_21: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_75);  add_tensor_75 = None\n",
      "        detach_default_32: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_21)\n",
      "        mul_tensor_96: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_74, rsqrt_default_21);  rsqrt_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant96 = self._param_constant96\n",
      "        mul_tensor_97: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant96, mul_tensor_96);  _param_constant96 = mul_tensor_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant97 = self._param_constant97\n",
      "        t_default_74: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant97);  _param_constant97 = None\n",
      "        view_default_259: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_97, [1, 4096])\n",
      "        mm_default_74: f32[1, 11008] = torch.ops.aten.mm.default(view_default_259, t_default_74);  view_default_259 = t_default_74 = None\n",
      "        view_default_260: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_74, [1, 1, 11008]);  mm_default_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_10: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_260);  view_default_260 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant98 = self._param_constant98\n",
      "        t_default_75: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant98);  _param_constant98 = None\n",
      "        view_default_261: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_97, [1, 4096]);  mul_tensor_97 = None\n",
      "        mm_default_75: f32[1, 11008] = torch.ops.aten.mm.default(view_default_261, t_default_75);  view_default_261 = t_default_75 = None\n",
      "        view_default_262: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_75, [1, 1, 11008]);  mm_default_75 = None\n",
      "        mul_tensor_98: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_10, view_default_262);  silu_default_10 = view_default_262 = None\n",
      "        _param_constant99 = self._param_constant99\n",
      "        t_default_76: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant99);  _param_constant99 = None\n",
      "        view_default_263: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_98, [1, 11008]);  mul_tensor_98 = None\n",
      "        mm_default_76: f32[1, 4096] = torch.ops.aten.mm.default(view_default_263, t_default_76);  view_default_263 = t_default_76 = None\n",
      "        view_default_264: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_76, [1, 1, 4096]);  mm_default_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_76: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_74, view_default_264);  add_tensor_74 = view_default_264 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_22: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_76, 2)\n",
      "        mean_dim_22: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_22, [-1], True);  pow_tensor_scalar_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_77: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_22, 1e-06);  mean_dim_22 = None\n",
      "        rsqrt_default_22: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_77);  add_tensor_77 = None\n",
      "        detach_default_33: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_22)\n",
      "        mul_tensor_99: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_76, rsqrt_default_22);  rsqrt_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant100 = self._param_constant100\n",
      "        mul_tensor_100: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant100, mul_tensor_99);  _param_constant100 = mul_tensor_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant101 = self._param_constant101\n",
      "        t_default_77: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant101);  _param_constant101 = None\n",
      "        view_default_265: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_100, [1, 4096])\n",
      "        mm_default_77: f32[1, 4096] = torch.ops.aten.mm.default(view_default_265, t_default_77);  view_default_265 = t_default_77 = None\n",
      "        view_default_266: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_77, [1, 1, 4096]);  mm_default_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant102 = self._param_constant102\n",
      "        t_default_78: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant102);  _param_constant102 = None\n",
      "        view_default_267: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_100, [1, 4096])\n",
      "        mm_default_78: f32[1, 4096] = torch.ops.aten.mm.default(view_default_267, t_default_78);  view_default_267 = t_default_78 = None\n",
      "        view_default_268: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_78, [1, 1, 4096]);  mm_default_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant103 = self._param_constant103\n",
      "        t_default_79: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant103);  _param_constant103 = None\n",
      "        view_default_269: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_100, [1, 4096]);  mul_tensor_100 = None\n",
      "        mm_default_79: f32[1, 4096] = torch.ops.aten.mm.default(view_default_269, t_default_79);  view_default_269 = t_default_79 = None\n",
      "        view_default_270: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_79, [1, 1, 4096]);  mm_default_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_271: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_266, [1, 1, 32, 128]);  view_default_266 = None\n",
      "        transpose_int_55: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_271, 1, 2);  view_default_271 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_272: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_268, [1, 1, 32, 128]);  view_default_268 = None\n",
      "        transpose_int_56: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_272, 1, 2);  view_default_272 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_273: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_270, [1, 1, 32, 128]);  view_default_270 = None\n",
      "        transpose_int_57: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_273, 1, 2);  view_default_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant22 = self._tensor_constant22\n",
      "        slice_tensor_112: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant22, 0, 0, 9223372036854775807);  _tensor_constant22 = None\n",
      "        slice_tensor_113: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_112, 1, 0, 9223372036854775807);  slice_tensor_112 = None\n",
      "        sym_size_33: Sym(s0) = torch.ops.aten.sym_size(arg23, 2)\n",
      "        add_13: Sym(s0 + 1) = 1 + sym_size_33;  sym_size_33 = None\n",
      "        slice_tensor_114: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_113, 2, 0, add_13);  slice_tensor_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant23 = self._tensor_constant23\n",
      "        slice_tensor_115: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant23, 0, 0, 9223372036854775807);  _tensor_constant23 = None\n",
      "        slice_tensor_116: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_115, 1, 0, 9223372036854775807);  slice_tensor_115 = None\n",
      "        slice_tensor_117: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_116, 2, 0, add_13);  slice_tensor_116 = add_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_44: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_114, 1);  slice_tensor_114 = None\n",
      "        squeeze_dim_45: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_44, 0);  squeeze_dim_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_46: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_117, 1);  slice_tensor_117 = None\n",
      "        squeeze_dim_47: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_46, 0);  squeeze_dim_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_22: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_45, [view_default]);  squeeze_dim_45 = None\n",
      "        unsqueeze_default_25: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_22, 1);  index_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_23: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_47, [view_default]);  squeeze_dim_47 = None\n",
      "        unsqueeze_default_26: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_23, 1);  index_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_101: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_55, unsqueeze_default_25)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_118: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_119: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 64, 9223372036854775807);  transpose_int_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_22: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_119);  slice_tensor_119 = None\n",
      "        cat_default_44: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_22, slice_tensor_118], -1);  neg_default_22 = slice_tensor_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_102: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_44, unsqueeze_default_26);  cat_default_44 = None\n",
      "        add_tensor_78: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_101, mul_tensor_102);  mul_tensor_101 = mul_tensor_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_103: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_56, unsqueeze_default_25);  unsqueeze_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_120: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_121: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 64, 9223372036854775807);  transpose_int_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_23: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_121);  slice_tensor_121 = None\n",
      "        cat_default_45: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_23, slice_tensor_120], -1);  neg_default_23 = slice_tensor_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_104: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_45, unsqueeze_default_26);  cat_default_45 = unsqueeze_default_26 = None\n",
      "        add_tensor_79: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_103, mul_tensor_104);  mul_tensor_103 = mul_tensor_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_46: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg23, add_tensor_79], 2);  arg23 = add_tensor_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_47: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg24, transpose_int_57], 2);  arg24 = transpose_int_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_58: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_46, 2, 3)\n",
      "        expand_default_45: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_78, [1, 32, 1, 128]);  add_tensor_78 = None\n",
      "        view_default_274: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_45, [32, 1, 128]);  expand_default_45 = None\n",
      "        sym_size_34: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_46, 2)\n",
      "        expand_default_46: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_58, [1, 32, 128, sym_size_34]);  transpose_int_58 = None\n",
      "        view_default_275: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_46, [32, 128, sym_size_34]);  expand_default_46 = None\n",
      "        bmm_default_22: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_274, view_default_275);  view_default_274 = view_default_275 = None\n",
      "        view_default_276: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_22, [1, 32, 1, sym_size_34]);  bmm_default_22 = None\n",
      "        div_tensor_11: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_276, 11.313708498984761);  view_default_276 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_80: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_11, masked_fill_scalar);  div_tensor_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_11: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_80, -1, False);  add_tensor_80 = None\n",
      "        detach_default_34: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_11)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_47: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_11, [1, 32, 1, sym_size_34]);  _softmax_default_11 = None\n",
      "        view_default_277: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_47, [32, 1, sym_size_34]);  expand_default_47 = sym_size_34 = None\n",
      "        sym_size_35: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_47, 2)\n",
      "        expand_default_48: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_47, [1, 32, sym_size_35, 128])\n",
      "        view_default_278: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_48, [32, sym_size_35, 128]);  expand_default_48 = sym_size_35 = None\n",
      "        bmm_default_23: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_277, view_default_278);  view_default_277 = view_default_278 = None\n",
      "        view_default_279: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_23, [1, 32, 1, 128]);  bmm_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_59: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_279, 1, 2);  view_default_279 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_280: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_59, [1, 1, 4096]);  transpose_int_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant104 = self._param_constant104\n",
      "        t_default_80: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant104);  _param_constant104 = None\n",
      "        view_default_281: f32[1, 4096] = torch.ops.aten.view.default(view_default_280, [1, 4096]);  view_default_280 = None\n",
      "        mm_default_80: f32[1, 4096] = torch.ops.aten.mm.default(view_default_281, t_default_80);  view_default_281 = t_default_80 = None\n",
      "        view_default_282: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_80, [1, 1, 4096]);  mm_default_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_81: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_76, view_default_282);  add_tensor_76 = view_default_282 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_23: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_81, 2)\n",
      "        mean_dim_23: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_23, [-1], True);  pow_tensor_scalar_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_82: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_23, 1e-06);  mean_dim_23 = None\n",
      "        rsqrt_default_23: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_82);  add_tensor_82 = None\n",
      "        detach_default_35: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_23)\n",
      "        mul_tensor_105: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_81, rsqrt_default_23);  rsqrt_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant105 = self._param_constant105\n",
      "        mul_tensor_106: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant105, mul_tensor_105);  _param_constant105 = mul_tensor_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant106 = self._param_constant106\n",
      "        t_default_81: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant106);  _param_constant106 = None\n",
      "        view_default_283: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_106, [1, 4096])\n",
      "        mm_default_81: f32[1, 11008] = torch.ops.aten.mm.default(view_default_283, t_default_81);  view_default_283 = t_default_81 = None\n",
      "        view_default_284: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_81, [1, 1, 11008]);  mm_default_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_11: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_284);  view_default_284 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant107 = self._param_constant107\n",
      "        t_default_82: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant107);  _param_constant107 = None\n",
      "        view_default_285: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_106, [1, 4096]);  mul_tensor_106 = None\n",
      "        mm_default_82: f32[1, 11008] = torch.ops.aten.mm.default(view_default_285, t_default_82);  view_default_285 = t_default_82 = None\n",
      "        view_default_286: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_82, [1, 1, 11008]);  mm_default_82 = None\n",
      "        mul_tensor_107: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_11, view_default_286);  silu_default_11 = view_default_286 = None\n",
      "        _param_constant108 = self._param_constant108\n",
      "        t_default_83: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant108);  _param_constant108 = None\n",
      "        view_default_287: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_107, [1, 11008]);  mul_tensor_107 = None\n",
      "        mm_default_83: f32[1, 4096] = torch.ops.aten.mm.default(view_default_287, t_default_83);  view_default_287 = t_default_83 = None\n",
      "        view_default_288: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_83, [1, 1, 4096]);  mm_default_83 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_83: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_81, view_default_288);  add_tensor_81 = view_default_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_24: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_83, 2)\n",
      "        mean_dim_24: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_24, [-1], True);  pow_tensor_scalar_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_84: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_24, 1e-06);  mean_dim_24 = None\n",
      "        rsqrt_default_24: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_84);  add_tensor_84 = None\n",
      "        detach_default_36: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_24)\n",
      "        mul_tensor_108: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_83, rsqrt_default_24);  rsqrt_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant109 = self._param_constant109\n",
      "        mul_tensor_109: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant109, mul_tensor_108);  _param_constant109 = mul_tensor_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant110 = self._param_constant110\n",
      "        t_default_84: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant110);  _param_constant110 = None\n",
      "        view_default_289: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_109, [1, 4096])\n",
      "        mm_default_84: f32[1, 4096] = torch.ops.aten.mm.default(view_default_289, t_default_84);  view_default_289 = t_default_84 = None\n",
      "        view_default_290: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_84, [1, 1, 4096]);  mm_default_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant111 = self._param_constant111\n",
      "        t_default_85: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant111);  _param_constant111 = None\n",
      "        view_default_291: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_109, [1, 4096])\n",
      "        mm_default_85: f32[1, 4096] = torch.ops.aten.mm.default(view_default_291, t_default_85);  view_default_291 = t_default_85 = None\n",
      "        view_default_292: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_85, [1, 1, 4096]);  mm_default_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant112 = self._param_constant112\n",
      "        t_default_86: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant112);  _param_constant112 = None\n",
      "        view_default_293: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_109, [1, 4096]);  mul_tensor_109 = None\n",
      "        mm_default_86: f32[1, 4096] = torch.ops.aten.mm.default(view_default_293, t_default_86);  view_default_293 = t_default_86 = None\n",
      "        view_default_294: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_86, [1, 1, 4096]);  mm_default_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_295: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_290, [1, 1, 32, 128]);  view_default_290 = None\n",
      "        transpose_int_60: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_295, 1, 2);  view_default_295 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_296: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_292, [1, 1, 32, 128]);  view_default_292 = None\n",
      "        transpose_int_61: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_296, 1, 2);  view_default_296 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_297: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_294, [1, 1, 32, 128]);  view_default_294 = None\n",
      "        transpose_int_62: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_297, 1, 2);  view_default_297 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant24 = self._tensor_constant24\n",
      "        slice_tensor_122: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant24, 0, 0, 9223372036854775807);  _tensor_constant24 = None\n",
      "        slice_tensor_123: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_122, 1, 0, 9223372036854775807);  slice_tensor_122 = None\n",
      "        sym_size_36: Sym(s0) = torch.ops.aten.sym_size(arg25, 2)\n",
      "        add_14: Sym(s0 + 1) = 1 + sym_size_36;  sym_size_36 = None\n",
      "        slice_tensor_124: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_123, 2, 0, add_14);  slice_tensor_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant25 = self._tensor_constant25\n",
      "        slice_tensor_125: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant25, 0, 0, 9223372036854775807);  _tensor_constant25 = None\n",
      "        slice_tensor_126: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_125, 1, 0, 9223372036854775807);  slice_tensor_125 = None\n",
      "        slice_tensor_127: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_126, 2, 0, add_14);  slice_tensor_126 = add_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_48: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_124, 1);  slice_tensor_124 = None\n",
      "        squeeze_dim_49: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_48, 0);  squeeze_dim_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_50: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_127, 1);  slice_tensor_127 = None\n",
      "        squeeze_dim_51: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_50, 0);  squeeze_dim_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_24: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_49, [view_default]);  squeeze_dim_49 = None\n",
      "        unsqueeze_default_27: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_24, 1);  index_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_25: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_51, [view_default]);  squeeze_dim_51 = None\n",
      "        unsqueeze_default_28: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_25, 1);  index_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_110: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_60, unsqueeze_default_27)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_128: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_129: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 64, 9223372036854775807);  transpose_int_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_24: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_129);  slice_tensor_129 = None\n",
      "        cat_default_48: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_24, slice_tensor_128], -1);  neg_default_24 = slice_tensor_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_111: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_48, unsqueeze_default_28);  cat_default_48 = None\n",
      "        add_tensor_85: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_110, mul_tensor_111);  mul_tensor_110 = mul_tensor_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_112: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_61, unsqueeze_default_27);  unsqueeze_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_130: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_131: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 64, 9223372036854775807);  transpose_int_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_25: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_131);  slice_tensor_131 = None\n",
      "        cat_default_49: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_25, slice_tensor_130], -1);  neg_default_25 = slice_tensor_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_113: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_49, unsqueeze_default_28);  cat_default_49 = unsqueeze_default_28 = None\n",
      "        add_tensor_86: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_112, mul_tensor_113);  mul_tensor_112 = mul_tensor_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_50: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg25, add_tensor_86], 2);  arg25 = add_tensor_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_51: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg26, transpose_int_62], 2);  arg26 = transpose_int_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_63: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_50, 2, 3)\n",
      "        expand_default_49: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_85, [1, 32, 1, 128]);  add_tensor_85 = None\n",
      "        view_default_298: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_49, [32, 1, 128]);  expand_default_49 = None\n",
      "        sym_size_37: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_50, 2)\n",
      "        expand_default_50: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_63, [1, 32, 128, sym_size_37]);  transpose_int_63 = None\n",
      "        view_default_299: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_50, [32, 128, sym_size_37]);  expand_default_50 = None\n",
      "        bmm_default_24: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_298, view_default_299);  view_default_298 = view_default_299 = None\n",
      "        view_default_300: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_24, [1, 32, 1, sym_size_37]);  bmm_default_24 = None\n",
      "        div_tensor_12: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_300, 11.313708498984761);  view_default_300 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_87: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_12, masked_fill_scalar);  div_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_12: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_87, -1, False);  add_tensor_87 = None\n",
      "        detach_default_37: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_12)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_51: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_12, [1, 32, 1, sym_size_37]);  _softmax_default_12 = None\n",
      "        view_default_301: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_51, [32, 1, sym_size_37]);  expand_default_51 = sym_size_37 = None\n",
      "        sym_size_38: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_51, 2)\n",
      "        expand_default_52: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_51, [1, 32, sym_size_38, 128])\n",
      "        view_default_302: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_52, [32, sym_size_38, 128]);  expand_default_52 = sym_size_38 = None\n",
      "        bmm_default_25: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_301, view_default_302);  view_default_301 = view_default_302 = None\n",
      "        view_default_303: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_25, [1, 32, 1, 128]);  bmm_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_64: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_303, 1, 2);  view_default_303 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_304: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_64, [1, 1, 4096]);  transpose_int_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant113 = self._param_constant113\n",
      "        t_default_87: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant113);  _param_constant113 = None\n",
      "        view_default_305: f32[1, 4096] = torch.ops.aten.view.default(view_default_304, [1, 4096]);  view_default_304 = None\n",
      "        mm_default_87: f32[1, 4096] = torch.ops.aten.mm.default(view_default_305, t_default_87);  view_default_305 = t_default_87 = None\n",
      "        view_default_306: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_87, [1, 1, 4096]);  mm_default_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_88: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_83, view_default_306);  add_tensor_83 = view_default_306 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_25: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_88, 2)\n",
      "        mean_dim_25: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_25, [-1], True);  pow_tensor_scalar_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_89: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_25, 1e-06);  mean_dim_25 = None\n",
      "        rsqrt_default_25: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_89);  add_tensor_89 = None\n",
      "        detach_default_38: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_25)\n",
      "        mul_tensor_114: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_88, rsqrt_default_25);  rsqrt_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant114 = self._param_constant114\n",
      "        mul_tensor_115: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant114, mul_tensor_114);  _param_constant114 = mul_tensor_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant115 = self._param_constant115\n",
      "        t_default_88: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant115);  _param_constant115 = None\n",
      "        view_default_307: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_115, [1, 4096])\n",
      "        mm_default_88: f32[1, 11008] = torch.ops.aten.mm.default(view_default_307, t_default_88);  view_default_307 = t_default_88 = None\n",
      "        view_default_308: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_88, [1, 1, 11008]);  mm_default_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_12: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_308);  view_default_308 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant116 = self._param_constant116\n",
      "        t_default_89: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant116);  _param_constant116 = None\n",
      "        view_default_309: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_115, [1, 4096]);  mul_tensor_115 = None\n",
      "        mm_default_89: f32[1, 11008] = torch.ops.aten.mm.default(view_default_309, t_default_89);  view_default_309 = t_default_89 = None\n",
      "        view_default_310: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_89, [1, 1, 11008]);  mm_default_89 = None\n",
      "        mul_tensor_116: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_12, view_default_310);  silu_default_12 = view_default_310 = None\n",
      "        _param_constant117 = self._param_constant117\n",
      "        t_default_90: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant117);  _param_constant117 = None\n",
      "        view_default_311: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_116, [1, 11008]);  mul_tensor_116 = None\n",
      "        mm_default_90: f32[1, 4096] = torch.ops.aten.mm.default(view_default_311, t_default_90);  view_default_311 = t_default_90 = None\n",
      "        view_default_312: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_90, [1, 1, 4096]);  mm_default_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_90: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_88, view_default_312);  add_tensor_88 = view_default_312 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_26: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_90, 2)\n",
      "        mean_dim_26: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_26, [-1], True);  pow_tensor_scalar_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_91: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_26, 1e-06);  mean_dim_26 = None\n",
      "        rsqrt_default_26: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_91);  add_tensor_91 = None\n",
      "        detach_default_39: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_26)\n",
      "        mul_tensor_117: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_90, rsqrt_default_26);  rsqrt_default_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant118 = self._param_constant118\n",
      "        mul_tensor_118: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant118, mul_tensor_117);  _param_constant118 = mul_tensor_117 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant119 = self._param_constant119\n",
      "        t_default_91: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant119);  _param_constant119 = None\n",
      "        view_default_313: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_118, [1, 4096])\n",
      "        mm_default_91: f32[1, 4096] = torch.ops.aten.mm.default(view_default_313, t_default_91);  view_default_313 = t_default_91 = None\n",
      "        view_default_314: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_91, [1, 1, 4096]);  mm_default_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant120 = self._param_constant120\n",
      "        t_default_92: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant120);  _param_constant120 = None\n",
      "        view_default_315: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_118, [1, 4096])\n",
      "        mm_default_92: f32[1, 4096] = torch.ops.aten.mm.default(view_default_315, t_default_92);  view_default_315 = t_default_92 = None\n",
      "        view_default_316: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_92, [1, 1, 4096]);  mm_default_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant121 = self._param_constant121\n",
      "        t_default_93: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant121);  _param_constant121 = None\n",
      "        view_default_317: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_118, [1, 4096]);  mul_tensor_118 = None\n",
      "        mm_default_93: f32[1, 4096] = torch.ops.aten.mm.default(view_default_317, t_default_93);  view_default_317 = t_default_93 = None\n",
      "        view_default_318: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_93, [1, 1, 4096]);  mm_default_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_319: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_314, [1, 1, 32, 128]);  view_default_314 = None\n",
      "        transpose_int_65: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_319, 1, 2);  view_default_319 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_320: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_316, [1, 1, 32, 128]);  view_default_316 = None\n",
      "        transpose_int_66: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_320, 1, 2);  view_default_320 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_321: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_318, [1, 1, 32, 128]);  view_default_318 = None\n",
      "        transpose_int_67: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_321, 1, 2);  view_default_321 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant26 = self._tensor_constant26\n",
      "        slice_tensor_132: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant26, 0, 0, 9223372036854775807);  _tensor_constant26 = None\n",
      "        slice_tensor_133: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_132, 1, 0, 9223372036854775807);  slice_tensor_132 = None\n",
      "        sym_size_39: Sym(s0) = torch.ops.aten.sym_size(arg27, 2)\n",
      "        add_15: Sym(s0 + 1) = 1 + sym_size_39;  sym_size_39 = None\n",
      "        slice_tensor_134: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_133, 2, 0, add_15);  slice_tensor_133 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant27 = self._tensor_constant27\n",
      "        slice_tensor_135: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant27, 0, 0, 9223372036854775807);  _tensor_constant27 = None\n",
      "        slice_tensor_136: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_135, 1, 0, 9223372036854775807);  slice_tensor_135 = None\n",
      "        slice_tensor_137: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_136, 2, 0, add_15);  slice_tensor_136 = add_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_52: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_134, 1);  slice_tensor_134 = None\n",
      "        squeeze_dim_53: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_52, 0);  squeeze_dim_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_54: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_137, 1);  slice_tensor_137 = None\n",
      "        squeeze_dim_55: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_54, 0);  squeeze_dim_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_26: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_53, [view_default]);  squeeze_dim_53 = None\n",
      "        unsqueeze_default_29: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_26, 1);  index_tensor_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_27: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_55, [view_default]);  squeeze_dim_55 = None\n",
      "        unsqueeze_default_30: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_27, 1);  index_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_119: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_65, unsqueeze_default_29)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_138: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_139: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 64, 9223372036854775807);  transpose_int_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_26: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_139);  slice_tensor_139 = None\n",
      "        cat_default_52: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_26, slice_tensor_138], -1);  neg_default_26 = slice_tensor_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_120: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_52, unsqueeze_default_30);  cat_default_52 = None\n",
      "        add_tensor_92: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_119, mul_tensor_120);  mul_tensor_119 = mul_tensor_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_121: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_66, unsqueeze_default_29);  unsqueeze_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_140: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_141: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 64, 9223372036854775807);  transpose_int_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_27: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_141);  slice_tensor_141 = None\n",
      "        cat_default_53: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_27, slice_tensor_140], -1);  neg_default_27 = slice_tensor_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_122: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_53, unsqueeze_default_30);  cat_default_53 = unsqueeze_default_30 = None\n",
      "        add_tensor_93: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_121, mul_tensor_122);  mul_tensor_121 = mul_tensor_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_54: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg27, add_tensor_93], 2);  arg27 = add_tensor_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_55: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg28, transpose_int_67], 2);  arg28 = transpose_int_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_68: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_54, 2, 3)\n",
      "        expand_default_53: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_92, [1, 32, 1, 128]);  add_tensor_92 = None\n",
      "        view_default_322: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_53, [32, 1, 128]);  expand_default_53 = None\n",
      "        sym_size_40: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_54, 2)\n",
      "        expand_default_54: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_68, [1, 32, 128, sym_size_40]);  transpose_int_68 = None\n",
      "        view_default_323: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_54, [32, 128, sym_size_40]);  expand_default_54 = None\n",
      "        bmm_default_26: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_322, view_default_323);  view_default_322 = view_default_323 = None\n",
      "        view_default_324: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_26, [1, 32, 1, sym_size_40]);  bmm_default_26 = None\n",
      "        div_tensor_13: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_324, 11.313708498984761);  view_default_324 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_94: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_13, masked_fill_scalar);  div_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_13: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_94, -1, False);  add_tensor_94 = None\n",
      "        detach_default_40: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_13)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_55: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_13, [1, 32, 1, sym_size_40]);  _softmax_default_13 = None\n",
      "        view_default_325: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_55, [32, 1, sym_size_40]);  expand_default_55 = sym_size_40 = None\n",
      "        sym_size_41: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_55, 2)\n",
      "        expand_default_56: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_55, [1, 32, sym_size_41, 128])\n",
      "        view_default_326: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_56, [32, sym_size_41, 128]);  expand_default_56 = sym_size_41 = None\n",
      "        bmm_default_27: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_325, view_default_326);  view_default_325 = view_default_326 = None\n",
      "        view_default_327: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_27, [1, 32, 1, 128]);  bmm_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_69: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_327, 1, 2);  view_default_327 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_328: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_69, [1, 1, 4096]);  transpose_int_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant122 = self._param_constant122\n",
      "        t_default_94: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant122);  _param_constant122 = None\n",
      "        view_default_329: f32[1, 4096] = torch.ops.aten.view.default(view_default_328, [1, 4096]);  view_default_328 = None\n",
      "        mm_default_94: f32[1, 4096] = torch.ops.aten.mm.default(view_default_329, t_default_94);  view_default_329 = t_default_94 = None\n",
      "        view_default_330: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_94, [1, 1, 4096]);  mm_default_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_95: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_90, view_default_330);  add_tensor_90 = view_default_330 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_27: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_95, 2)\n",
      "        mean_dim_27: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_27, [-1], True);  pow_tensor_scalar_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_96: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_27, 1e-06);  mean_dim_27 = None\n",
      "        rsqrt_default_27: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_96);  add_tensor_96 = None\n",
      "        detach_default_41: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_27)\n",
      "        mul_tensor_123: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_95, rsqrt_default_27);  rsqrt_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant123 = self._param_constant123\n",
      "        mul_tensor_124: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant123, mul_tensor_123);  _param_constant123 = mul_tensor_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant124 = self._param_constant124\n",
      "        t_default_95: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant124);  _param_constant124 = None\n",
      "        view_default_331: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_124, [1, 4096])\n",
      "        mm_default_95: f32[1, 11008] = torch.ops.aten.mm.default(view_default_331, t_default_95);  view_default_331 = t_default_95 = None\n",
      "        view_default_332: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_95, [1, 1, 11008]);  mm_default_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_13: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_332);  view_default_332 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant125 = self._param_constant125\n",
      "        t_default_96: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant125);  _param_constant125 = None\n",
      "        view_default_333: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_124, [1, 4096]);  mul_tensor_124 = None\n",
      "        mm_default_96: f32[1, 11008] = torch.ops.aten.mm.default(view_default_333, t_default_96);  view_default_333 = t_default_96 = None\n",
      "        view_default_334: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_96, [1, 1, 11008]);  mm_default_96 = None\n",
      "        mul_tensor_125: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_13, view_default_334);  silu_default_13 = view_default_334 = None\n",
      "        _param_constant126 = self._param_constant126\n",
      "        t_default_97: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant126);  _param_constant126 = None\n",
      "        view_default_335: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_125, [1, 11008]);  mul_tensor_125 = None\n",
      "        mm_default_97: f32[1, 4096] = torch.ops.aten.mm.default(view_default_335, t_default_97);  view_default_335 = t_default_97 = None\n",
      "        view_default_336: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_97, [1, 1, 4096]);  mm_default_97 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_97: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_95, view_default_336);  add_tensor_95 = view_default_336 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_28: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_97, 2)\n",
      "        mean_dim_28: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_28, [-1], True);  pow_tensor_scalar_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_98: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_28, 1e-06);  mean_dim_28 = None\n",
      "        rsqrt_default_28: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_98);  add_tensor_98 = None\n",
      "        detach_default_42: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_28)\n",
      "        mul_tensor_126: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_97, rsqrt_default_28);  rsqrt_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant127 = self._param_constant127\n",
      "        mul_tensor_127: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant127, mul_tensor_126);  _param_constant127 = mul_tensor_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant128 = self._param_constant128\n",
      "        t_default_98: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant128);  _param_constant128 = None\n",
      "        view_default_337: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_127, [1, 4096])\n",
      "        mm_default_98: f32[1, 4096] = torch.ops.aten.mm.default(view_default_337, t_default_98);  view_default_337 = t_default_98 = None\n",
      "        view_default_338: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_98, [1, 1, 4096]);  mm_default_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant129 = self._param_constant129\n",
      "        t_default_99: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant129);  _param_constant129 = None\n",
      "        view_default_339: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_127, [1, 4096])\n",
      "        mm_default_99: f32[1, 4096] = torch.ops.aten.mm.default(view_default_339, t_default_99);  view_default_339 = t_default_99 = None\n",
      "        view_default_340: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_99, [1, 1, 4096]);  mm_default_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant130 = self._param_constant130\n",
      "        t_default_100: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant130);  _param_constant130 = None\n",
      "        view_default_341: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_127, [1, 4096]);  mul_tensor_127 = None\n",
      "        mm_default_100: f32[1, 4096] = torch.ops.aten.mm.default(view_default_341, t_default_100);  view_default_341 = t_default_100 = None\n",
      "        view_default_342: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_100, [1, 1, 4096]);  mm_default_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_343: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_338, [1, 1, 32, 128]);  view_default_338 = None\n",
      "        transpose_int_70: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_343, 1, 2);  view_default_343 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_344: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_340, [1, 1, 32, 128]);  view_default_340 = None\n",
      "        transpose_int_71: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_344, 1, 2);  view_default_344 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_345: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_342, [1, 1, 32, 128]);  view_default_342 = None\n",
      "        transpose_int_72: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_345, 1, 2);  view_default_345 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant28 = self._tensor_constant28\n",
      "        slice_tensor_142: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant28, 0, 0, 9223372036854775807);  _tensor_constant28 = None\n",
      "        slice_tensor_143: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_142, 1, 0, 9223372036854775807);  slice_tensor_142 = None\n",
      "        sym_size_42: Sym(s0) = torch.ops.aten.sym_size(arg29, 2)\n",
      "        add_16: Sym(s0 + 1) = 1 + sym_size_42;  sym_size_42 = None\n",
      "        slice_tensor_144: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_143, 2, 0, add_16);  slice_tensor_143 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant29 = self._tensor_constant29\n",
      "        slice_tensor_145: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant29, 0, 0, 9223372036854775807);  _tensor_constant29 = None\n",
      "        slice_tensor_146: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_145, 1, 0, 9223372036854775807);  slice_tensor_145 = None\n",
      "        slice_tensor_147: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_146, 2, 0, add_16);  slice_tensor_146 = add_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_56: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_144, 1);  slice_tensor_144 = None\n",
      "        squeeze_dim_57: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_56, 0);  squeeze_dim_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_58: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_147, 1);  slice_tensor_147 = None\n",
      "        squeeze_dim_59: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_58, 0);  squeeze_dim_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_28: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_57, [view_default]);  squeeze_dim_57 = None\n",
      "        unsqueeze_default_31: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_28, 1);  index_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_29: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_59, [view_default]);  squeeze_dim_59 = None\n",
      "        unsqueeze_default_32: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_29, 1);  index_tensor_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_128: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_70, unsqueeze_default_31)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_148: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_149: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 64, 9223372036854775807);  transpose_int_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_28: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_149);  slice_tensor_149 = None\n",
      "        cat_default_56: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_28, slice_tensor_148], -1);  neg_default_28 = slice_tensor_148 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_129: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_56, unsqueeze_default_32);  cat_default_56 = None\n",
      "        add_tensor_99: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_128, mul_tensor_129);  mul_tensor_128 = mul_tensor_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_130: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_71, unsqueeze_default_31);  unsqueeze_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_150: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_151: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 64, 9223372036854775807);  transpose_int_71 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_29: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_151);  slice_tensor_151 = None\n",
      "        cat_default_57: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_29, slice_tensor_150], -1);  neg_default_29 = slice_tensor_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_131: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_57, unsqueeze_default_32);  cat_default_57 = unsqueeze_default_32 = None\n",
      "        add_tensor_100: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_130, mul_tensor_131);  mul_tensor_130 = mul_tensor_131 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_58: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg29, add_tensor_100], 2);  arg29 = add_tensor_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_59: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg30, transpose_int_72], 2);  arg30 = transpose_int_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_73: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_58, 2, 3)\n",
      "        expand_default_57: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_99, [1, 32, 1, 128]);  add_tensor_99 = None\n",
      "        view_default_346: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_57, [32, 1, 128]);  expand_default_57 = None\n",
      "        sym_size_43: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_58, 2)\n",
      "        expand_default_58: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_73, [1, 32, 128, sym_size_43]);  transpose_int_73 = None\n",
      "        view_default_347: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_58, [32, 128, sym_size_43]);  expand_default_58 = None\n",
      "        bmm_default_28: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_346, view_default_347);  view_default_346 = view_default_347 = None\n",
      "        view_default_348: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_28, [1, 32, 1, sym_size_43]);  bmm_default_28 = None\n",
      "        div_tensor_14: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_348, 11.313708498984761);  view_default_348 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_101: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_14, masked_fill_scalar);  div_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_14: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_101, -1, False);  add_tensor_101 = None\n",
      "        detach_default_43: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_14)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_59: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_14, [1, 32, 1, sym_size_43]);  _softmax_default_14 = None\n",
      "        view_default_349: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_59, [32, 1, sym_size_43]);  expand_default_59 = sym_size_43 = None\n",
      "        sym_size_44: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_59, 2)\n",
      "        expand_default_60: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_59, [1, 32, sym_size_44, 128])\n",
      "        view_default_350: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_60, [32, sym_size_44, 128]);  expand_default_60 = sym_size_44 = None\n",
      "        bmm_default_29: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_349, view_default_350);  view_default_349 = view_default_350 = None\n",
      "        view_default_351: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_29, [1, 32, 1, 128]);  bmm_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_74: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_351, 1, 2);  view_default_351 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_352: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_74, [1, 1, 4096]);  transpose_int_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant131 = self._param_constant131\n",
      "        t_default_101: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant131);  _param_constant131 = None\n",
      "        view_default_353: f32[1, 4096] = torch.ops.aten.view.default(view_default_352, [1, 4096]);  view_default_352 = None\n",
      "        mm_default_101: f32[1, 4096] = torch.ops.aten.mm.default(view_default_353, t_default_101);  view_default_353 = t_default_101 = None\n",
      "        view_default_354: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_101, [1, 1, 4096]);  mm_default_101 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_102: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_97, view_default_354);  add_tensor_97 = view_default_354 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_29: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_102, 2)\n",
      "        mean_dim_29: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_29, [-1], True);  pow_tensor_scalar_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_103: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_29, 1e-06);  mean_dim_29 = None\n",
      "        rsqrt_default_29: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_103);  add_tensor_103 = None\n",
      "        detach_default_44: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_29)\n",
      "        mul_tensor_132: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_102, rsqrt_default_29);  rsqrt_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant132 = self._param_constant132\n",
      "        mul_tensor_133: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant132, mul_tensor_132);  _param_constant132 = mul_tensor_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant133 = self._param_constant133\n",
      "        t_default_102: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant133);  _param_constant133 = None\n",
      "        view_default_355: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_133, [1, 4096])\n",
      "        mm_default_102: f32[1, 11008] = torch.ops.aten.mm.default(view_default_355, t_default_102);  view_default_355 = t_default_102 = None\n",
      "        view_default_356: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_102, [1, 1, 11008]);  mm_default_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_14: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_356);  view_default_356 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant134 = self._param_constant134\n",
      "        t_default_103: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant134);  _param_constant134 = None\n",
      "        view_default_357: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_133, [1, 4096]);  mul_tensor_133 = None\n",
      "        mm_default_103: f32[1, 11008] = torch.ops.aten.mm.default(view_default_357, t_default_103);  view_default_357 = t_default_103 = None\n",
      "        view_default_358: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_103, [1, 1, 11008]);  mm_default_103 = None\n",
      "        mul_tensor_134: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_14, view_default_358);  silu_default_14 = view_default_358 = None\n",
      "        _param_constant135 = self._param_constant135\n",
      "        t_default_104: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant135);  _param_constant135 = None\n",
      "        view_default_359: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_134, [1, 11008]);  mul_tensor_134 = None\n",
      "        mm_default_104: f32[1, 4096] = torch.ops.aten.mm.default(view_default_359, t_default_104);  view_default_359 = t_default_104 = None\n",
      "        view_default_360: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_104, [1, 1, 4096]);  mm_default_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_104: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_102, view_default_360);  add_tensor_102 = view_default_360 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_30: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_104, 2)\n",
      "        mean_dim_30: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_30, [-1], True);  pow_tensor_scalar_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_105: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_30, 1e-06);  mean_dim_30 = None\n",
      "        rsqrt_default_30: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_105);  add_tensor_105 = None\n",
      "        detach_default_45: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_30)\n",
      "        mul_tensor_135: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_104, rsqrt_default_30);  rsqrt_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant136 = self._param_constant136\n",
      "        mul_tensor_136: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant136, mul_tensor_135);  _param_constant136 = mul_tensor_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant137 = self._param_constant137\n",
      "        t_default_105: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant137);  _param_constant137 = None\n",
      "        view_default_361: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_136, [1, 4096])\n",
      "        mm_default_105: f32[1, 4096] = torch.ops.aten.mm.default(view_default_361, t_default_105);  view_default_361 = t_default_105 = None\n",
      "        view_default_362: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_105, [1, 1, 4096]);  mm_default_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant138 = self._param_constant138\n",
      "        t_default_106: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant138);  _param_constant138 = None\n",
      "        view_default_363: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_136, [1, 4096])\n",
      "        mm_default_106: f32[1, 4096] = torch.ops.aten.mm.default(view_default_363, t_default_106);  view_default_363 = t_default_106 = None\n",
      "        view_default_364: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_106, [1, 1, 4096]);  mm_default_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant139 = self._param_constant139\n",
      "        t_default_107: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant139);  _param_constant139 = None\n",
      "        view_default_365: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_136, [1, 4096]);  mul_tensor_136 = None\n",
      "        mm_default_107: f32[1, 4096] = torch.ops.aten.mm.default(view_default_365, t_default_107);  view_default_365 = t_default_107 = None\n",
      "        view_default_366: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_107, [1, 1, 4096]);  mm_default_107 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_367: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_362, [1, 1, 32, 128]);  view_default_362 = None\n",
      "        transpose_int_75: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_367, 1, 2);  view_default_367 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_368: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_364, [1, 1, 32, 128]);  view_default_364 = None\n",
      "        transpose_int_76: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_368, 1, 2);  view_default_368 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_369: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_366, [1, 1, 32, 128]);  view_default_366 = None\n",
      "        transpose_int_77: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_369, 1, 2);  view_default_369 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant30 = self._tensor_constant30\n",
      "        slice_tensor_152: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant30, 0, 0, 9223372036854775807);  _tensor_constant30 = None\n",
      "        slice_tensor_153: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_152, 1, 0, 9223372036854775807);  slice_tensor_152 = None\n",
      "        sym_size_45: Sym(s0) = torch.ops.aten.sym_size(arg31, 2)\n",
      "        add_17: Sym(s0 + 1) = 1 + sym_size_45;  sym_size_45 = None\n",
      "        slice_tensor_154: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_153, 2, 0, add_17);  slice_tensor_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant31 = self._tensor_constant31\n",
      "        slice_tensor_155: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant31, 0, 0, 9223372036854775807);  _tensor_constant31 = None\n",
      "        slice_tensor_156: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_155, 1, 0, 9223372036854775807);  slice_tensor_155 = None\n",
      "        slice_tensor_157: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_156, 2, 0, add_17);  slice_tensor_156 = add_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_60: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_154, 1);  slice_tensor_154 = None\n",
      "        squeeze_dim_61: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_60, 0);  squeeze_dim_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_62: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_157, 1);  slice_tensor_157 = None\n",
      "        squeeze_dim_63: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_62, 0);  squeeze_dim_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_30: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_61, [view_default]);  squeeze_dim_61 = None\n",
      "        unsqueeze_default_33: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_30, 1);  index_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_31: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_63, [view_default]);  squeeze_dim_63 = None\n",
      "        unsqueeze_default_34: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_31, 1);  index_tensor_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_137: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_75, unsqueeze_default_33)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_158: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_159: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 64, 9223372036854775807);  transpose_int_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_30: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_159);  slice_tensor_159 = None\n",
      "        cat_default_60: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_30, slice_tensor_158], -1);  neg_default_30 = slice_tensor_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_138: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_60, unsqueeze_default_34);  cat_default_60 = None\n",
      "        add_tensor_106: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_137, mul_tensor_138);  mul_tensor_137 = mul_tensor_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_139: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_76, unsqueeze_default_33);  unsqueeze_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_160: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_161: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 64, 9223372036854775807);  transpose_int_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_31: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_161);  slice_tensor_161 = None\n",
      "        cat_default_61: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_31, slice_tensor_160], -1);  neg_default_31 = slice_tensor_160 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_140: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_61, unsqueeze_default_34);  cat_default_61 = unsqueeze_default_34 = None\n",
      "        add_tensor_107: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_139, mul_tensor_140);  mul_tensor_139 = mul_tensor_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_62: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg31, add_tensor_107], 2);  arg31 = add_tensor_107 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_63: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg32, transpose_int_77], 2);  arg32 = transpose_int_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_78: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_62, 2, 3)\n",
      "        expand_default_61: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_106, [1, 32, 1, 128]);  add_tensor_106 = None\n",
      "        view_default_370: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_61, [32, 1, 128]);  expand_default_61 = None\n",
      "        sym_size_46: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_62, 2)\n",
      "        expand_default_62: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_78, [1, 32, 128, sym_size_46]);  transpose_int_78 = None\n",
      "        view_default_371: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_62, [32, 128, sym_size_46]);  expand_default_62 = None\n",
      "        bmm_default_30: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_370, view_default_371);  view_default_370 = view_default_371 = None\n",
      "        view_default_372: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_30, [1, 32, 1, sym_size_46]);  bmm_default_30 = None\n",
      "        div_tensor_15: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_372, 11.313708498984761);  view_default_372 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_108: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_15, masked_fill_scalar);  div_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_15: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_108, -1, False);  add_tensor_108 = None\n",
      "        detach_default_46: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_15)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_63: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_15, [1, 32, 1, sym_size_46]);  _softmax_default_15 = None\n",
      "        view_default_373: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_63, [32, 1, sym_size_46]);  expand_default_63 = sym_size_46 = None\n",
      "        sym_size_47: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_63, 2)\n",
      "        expand_default_64: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_63, [1, 32, sym_size_47, 128])\n",
      "        view_default_374: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_64, [32, sym_size_47, 128]);  expand_default_64 = sym_size_47 = None\n",
      "        bmm_default_31: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_373, view_default_374);  view_default_373 = view_default_374 = None\n",
      "        view_default_375: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_31, [1, 32, 1, 128]);  bmm_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_79: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_375, 1, 2);  view_default_375 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_376: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_79, [1, 1, 4096]);  transpose_int_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant140 = self._param_constant140\n",
      "        t_default_108: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant140);  _param_constant140 = None\n",
      "        view_default_377: f32[1, 4096] = torch.ops.aten.view.default(view_default_376, [1, 4096]);  view_default_376 = None\n",
      "        mm_default_108: f32[1, 4096] = torch.ops.aten.mm.default(view_default_377, t_default_108);  view_default_377 = t_default_108 = None\n",
      "        view_default_378: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_108, [1, 1, 4096]);  mm_default_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_109: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_104, view_default_378);  add_tensor_104 = view_default_378 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_31: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_109, 2)\n",
      "        mean_dim_31: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_31, [-1], True);  pow_tensor_scalar_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_110: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_31, 1e-06);  mean_dim_31 = None\n",
      "        rsqrt_default_31: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_110);  add_tensor_110 = None\n",
      "        detach_default_47: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_31)\n",
      "        mul_tensor_141: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_109, rsqrt_default_31);  rsqrt_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant141 = self._param_constant141\n",
      "        mul_tensor_142: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant141, mul_tensor_141);  _param_constant141 = mul_tensor_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant142 = self._param_constant142\n",
      "        t_default_109: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant142);  _param_constant142 = None\n",
      "        view_default_379: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_142, [1, 4096])\n",
      "        mm_default_109: f32[1, 11008] = torch.ops.aten.mm.default(view_default_379, t_default_109);  view_default_379 = t_default_109 = None\n",
      "        view_default_380: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_109, [1, 1, 11008]);  mm_default_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_15: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_380);  view_default_380 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant143 = self._param_constant143\n",
      "        t_default_110: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant143);  _param_constant143 = None\n",
      "        view_default_381: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_142, [1, 4096]);  mul_tensor_142 = None\n",
      "        mm_default_110: f32[1, 11008] = torch.ops.aten.mm.default(view_default_381, t_default_110);  view_default_381 = t_default_110 = None\n",
      "        view_default_382: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_110, [1, 1, 11008]);  mm_default_110 = None\n",
      "        mul_tensor_143: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_15, view_default_382);  silu_default_15 = view_default_382 = None\n",
      "        _param_constant144 = self._param_constant144\n",
      "        t_default_111: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant144);  _param_constant144 = None\n",
      "        view_default_383: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_143, [1, 11008]);  mul_tensor_143 = None\n",
      "        mm_default_111: f32[1, 4096] = torch.ops.aten.mm.default(view_default_383, t_default_111);  view_default_383 = t_default_111 = None\n",
      "        view_default_384: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_111, [1, 1, 4096]);  mm_default_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_111: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_109, view_default_384);  add_tensor_109 = view_default_384 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_32: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_111, 2)\n",
      "        mean_dim_32: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_32, [-1], True);  pow_tensor_scalar_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_112: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_32, 1e-06);  mean_dim_32 = None\n",
      "        rsqrt_default_32: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_112);  add_tensor_112 = None\n",
      "        detach_default_48: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_32)\n",
      "        mul_tensor_144: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_111, rsqrt_default_32);  rsqrt_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant145 = self._param_constant145\n",
      "        mul_tensor_145: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant145, mul_tensor_144);  _param_constant145 = mul_tensor_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant146 = self._param_constant146\n",
      "        t_default_112: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant146);  _param_constant146 = None\n",
      "        view_default_385: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_145, [1, 4096])\n",
      "        mm_default_112: f32[1, 4096] = torch.ops.aten.mm.default(view_default_385, t_default_112);  view_default_385 = t_default_112 = None\n",
      "        view_default_386: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_112, [1, 1, 4096]);  mm_default_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant147 = self._param_constant147\n",
      "        t_default_113: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant147);  _param_constant147 = None\n",
      "        view_default_387: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_145, [1, 4096])\n",
      "        mm_default_113: f32[1, 4096] = torch.ops.aten.mm.default(view_default_387, t_default_113);  view_default_387 = t_default_113 = None\n",
      "        view_default_388: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_113, [1, 1, 4096]);  mm_default_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant148 = self._param_constant148\n",
      "        t_default_114: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant148);  _param_constant148 = None\n",
      "        view_default_389: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_145, [1, 4096]);  mul_tensor_145 = None\n",
      "        mm_default_114: f32[1, 4096] = torch.ops.aten.mm.default(view_default_389, t_default_114);  view_default_389 = t_default_114 = None\n",
      "        view_default_390: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_114, [1, 1, 4096]);  mm_default_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_391: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_386, [1, 1, 32, 128]);  view_default_386 = None\n",
      "        transpose_int_80: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_391, 1, 2);  view_default_391 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_392: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_388, [1, 1, 32, 128]);  view_default_388 = None\n",
      "        transpose_int_81: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_392, 1, 2);  view_default_392 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_393: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_390, [1, 1, 32, 128]);  view_default_390 = None\n",
      "        transpose_int_82: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_393, 1, 2);  view_default_393 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant32 = self._tensor_constant32\n",
      "        slice_tensor_162: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant32, 0, 0, 9223372036854775807);  _tensor_constant32 = None\n",
      "        slice_tensor_163: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_162, 1, 0, 9223372036854775807);  slice_tensor_162 = None\n",
      "        sym_size_48: Sym(s0) = torch.ops.aten.sym_size(arg33, 2)\n",
      "        add_18: Sym(s0 + 1) = 1 + sym_size_48;  sym_size_48 = None\n",
      "        slice_tensor_164: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_163, 2, 0, add_18);  slice_tensor_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant33 = self._tensor_constant33\n",
      "        slice_tensor_165: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant33, 0, 0, 9223372036854775807);  _tensor_constant33 = None\n",
      "        slice_tensor_166: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_165, 1, 0, 9223372036854775807);  slice_tensor_165 = None\n",
      "        slice_tensor_167: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_166, 2, 0, add_18);  slice_tensor_166 = add_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_64: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_164, 1);  slice_tensor_164 = None\n",
      "        squeeze_dim_65: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_64, 0);  squeeze_dim_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_66: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_167, 1);  slice_tensor_167 = None\n",
      "        squeeze_dim_67: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_66, 0);  squeeze_dim_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_32: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_65, [view_default]);  squeeze_dim_65 = None\n",
      "        unsqueeze_default_35: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_32, 1);  index_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_33: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_67, [view_default]);  squeeze_dim_67 = None\n",
      "        unsqueeze_default_36: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_33, 1);  index_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_146: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_80, unsqueeze_default_35)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_168: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_169: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 64, 9223372036854775807);  transpose_int_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_32: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_169);  slice_tensor_169 = None\n",
      "        cat_default_64: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_32, slice_tensor_168], -1);  neg_default_32 = slice_tensor_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_147: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_64, unsqueeze_default_36);  cat_default_64 = None\n",
      "        add_tensor_113: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_146, mul_tensor_147);  mul_tensor_146 = mul_tensor_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_148: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_81, unsqueeze_default_35);  unsqueeze_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_170: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_171: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 64, 9223372036854775807);  transpose_int_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_33: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_171);  slice_tensor_171 = None\n",
      "        cat_default_65: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_33, slice_tensor_170], -1);  neg_default_33 = slice_tensor_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_149: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_65, unsqueeze_default_36);  cat_default_65 = unsqueeze_default_36 = None\n",
      "        add_tensor_114: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_148, mul_tensor_149);  mul_tensor_148 = mul_tensor_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_66: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg33, add_tensor_114], 2);  arg33 = add_tensor_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_67: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg34, transpose_int_82], 2);  arg34 = transpose_int_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_83: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_66, 2, 3)\n",
      "        expand_default_65: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_113, [1, 32, 1, 128]);  add_tensor_113 = None\n",
      "        view_default_394: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_65, [32, 1, 128]);  expand_default_65 = None\n",
      "        sym_size_49: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_66, 2)\n",
      "        expand_default_66: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_83, [1, 32, 128, sym_size_49]);  transpose_int_83 = None\n",
      "        view_default_395: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_66, [32, 128, sym_size_49]);  expand_default_66 = None\n",
      "        bmm_default_32: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_394, view_default_395);  view_default_394 = view_default_395 = None\n",
      "        view_default_396: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_32, [1, 32, 1, sym_size_49]);  bmm_default_32 = None\n",
      "        div_tensor_16: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_396, 11.313708498984761);  view_default_396 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_115: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_16, masked_fill_scalar);  div_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_16: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_115, -1, False);  add_tensor_115 = None\n",
      "        detach_default_49: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_16)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_67: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_16, [1, 32, 1, sym_size_49]);  _softmax_default_16 = None\n",
      "        view_default_397: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_67, [32, 1, sym_size_49]);  expand_default_67 = sym_size_49 = None\n",
      "        sym_size_50: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_67, 2)\n",
      "        expand_default_68: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_67, [1, 32, sym_size_50, 128])\n",
      "        view_default_398: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_68, [32, sym_size_50, 128]);  expand_default_68 = sym_size_50 = None\n",
      "        bmm_default_33: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_397, view_default_398);  view_default_397 = view_default_398 = None\n",
      "        view_default_399: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_33, [1, 32, 1, 128]);  bmm_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_84: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_399, 1, 2);  view_default_399 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_400: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_84, [1, 1, 4096]);  transpose_int_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant149 = self._param_constant149\n",
      "        t_default_115: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant149);  _param_constant149 = None\n",
      "        view_default_401: f32[1, 4096] = torch.ops.aten.view.default(view_default_400, [1, 4096]);  view_default_400 = None\n",
      "        mm_default_115: f32[1, 4096] = torch.ops.aten.mm.default(view_default_401, t_default_115);  view_default_401 = t_default_115 = None\n",
      "        view_default_402: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_115, [1, 1, 4096]);  mm_default_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_116: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_111, view_default_402);  add_tensor_111 = view_default_402 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_33: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_116, 2)\n",
      "        mean_dim_33: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_33, [-1], True);  pow_tensor_scalar_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_117: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_33, 1e-06);  mean_dim_33 = None\n",
      "        rsqrt_default_33: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_117);  add_tensor_117 = None\n",
      "        detach_default_50: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_33)\n",
      "        mul_tensor_150: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_116, rsqrt_default_33);  rsqrt_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant150 = self._param_constant150\n",
      "        mul_tensor_151: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant150, mul_tensor_150);  _param_constant150 = mul_tensor_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant151 = self._param_constant151\n",
      "        t_default_116: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant151);  _param_constant151 = None\n",
      "        view_default_403: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_151, [1, 4096])\n",
      "        mm_default_116: f32[1, 11008] = torch.ops.aten.mm.default(view_default_403, t_default_116);  view_default_403 = t_default_116 = None\n",
      "        view_default_404: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_116, [1, 1, 11008]);  mm_default_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_16: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_404);  view_default_404 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant152 = self._param_constant152\n",
      "        t_default_117: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant152);  _param_constant152 = None\n",
      "        view_default_405: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_151, [1, 4096]);  mul_tensor_151 = None\n",
      "        mm_default_117: f32[1, 11008] = torch.ops.aten.mm.default(view_default_405, t_default_117);  view_default_405 = t_default_117 = None\n",
      "        view_default_406: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_117, [1, 1, 11008]);  mm_default_117 = None\n",
      "        mul_tensor_152: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_16, view_default_406);  silu_default_16 = view_default_406 = None\n",
      "        _param_constant153 = self._param_constant153\n",
      "        t_default_118: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant153);  _param_constant153 = None\n",
      "        view_default_407: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_152, [1, 11008]);  mul_tensor_152 = None\n",
      "        mm_default_118: f32[1, 4096] = torch.ops.aten.mm.default(view_default_407, t_default_118);  view_default_407 = t_default_118 = None\n",
      "        view_default_408: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_118, [1, 1, 4096]);  mm_default_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_118: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_116, view_default_408);  add_tensor_116 = view_default_408 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_34: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_118, 2)\n",
      "        mean_dim_34: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_34, [-1], True);  pow_tensor_scalar_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_119: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_34, 1e-06);  mean_dim_34 = None\n",
      "        rsqrt_default_34: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_119);  add_tensor_119 = None\n",
      "        detach_default_51: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_34)\n",
      "        mul_tensor_153: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_118, rsqrt_default_34);  rsqrt_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant154 = self._param_constant154\n",
      "        mul_tensor_154: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant154, mul_tensor_153);  _param_constant154 = mul_tensor_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant155 = self._param_constant155\n",
      "        t_default_119: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant155);  _param_constant155 = None\n",
      "        view_default_409: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_154, [1, 4096])\n",
      "        mm_default_119: f32[1, 4096] = torch.ops.aten.mm.default(view_default_409, t_default_119);  view_default_409 = t_default_119 = None\n",
      "        view_default_410: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_119, [1, 1, 4096]);  mm_default_119 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant156 = self._param_constant156\n",
      "        t_default_120: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant156);  _param_constant156 = None\n",
      "        view_default_411: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_154, [1, 4096])\n",
      "        mm_default_120: f32[1, 4096] = torch.ops.aten.mm.default(view_default_411, t_default_120);  view_default_411 = t_default_120 = None\n",
      "        view_default_412: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_120, [1, 1, 4096]);  mm_default_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant157 = self._param_constant157\n",
      "        t_default_121: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant157);  _param_constant157 = None\n",
      "        view_default_413: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_154, [1, 4096]);  mul_tensor_154 = None\n",
      "        mm_default_121: f32[1, 4096] = torch.ops.aten.mm.default(view_default_413, t_default_121);  view_default_413 = t_default_121 = None\n",
      "        view_default_414: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_121, [1, 1, 4096]);  mm_default_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_415: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_410, [1, 1, 32, 128]);  view_default_410 = None\n",
      "        transpose_int_85: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_415, 1, 2);  view_default_415 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_416: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_412, [1, 1, 32, 128]);  view_default_412 = None\n",
      "        transpose_int_86: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_416, 1, 2);  view_default_416 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_417: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_414, [1, 1, 32, 128]);  view_default_414 = None\n",
      "        transpose_int_87: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_417, 1, 2);  view_default_417 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant34 = self._tensor_constant34\n",
      "        slice_tensor_172: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant34, 0, 0, 9223372036854775807);  _tensor_constant34 = None\n",
      "        slice_tensor_173: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_172, 1, 0, 9223372036854775807);  slice_tensor_172 = None\n",
      "        sym_size_51: Sym(s0) = torch.ops.aten.sym_size(arg35, 2)\n",
      "        add_19: Sym(s0 + 1) = 1 + sym_size_51;  sym_size_51 = None\n",
      "        slice_tensor_174: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_173, 2, 0, add_19);  slice_tensor_173 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant35 = self._tensor_constant35\n",
      "        slice_tensor_175: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant35, 0, 0, 9223372036854775807);  _tensor_constant35 = None\n",
      "        slice_tensor_176: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_175, 1, 0, 9223372036854775807);  slice_tensor_175 = None\n",
      "        slice_tensor_177: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_176, 2, 0, add_19);  slice_tensor_176 = add_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_68: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_174, 1);  slice_tensor_174 = None\n",
      "        squeeze_dim_69: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_68, 0);  squeeze_dim_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_70: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_177, 1);  slice_tensor_177 = None\n",
      "        squeeze_dim_71: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_70, 0);  squeeze_dim_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_34: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_69, [view_default]);  squeeze_dim_69 = None\n",
      "        unsqueeze_default_37: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_34, 1);  index_tensor_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_35: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_71, [view_default]);  squeeze_dim_71 = None\n",
      "        unsqueeze_default_38: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_35, 1);  index_tensor_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_155: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_85, unsqueeze_default_37)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_178: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_179: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 64, 9223372036854775807);  transpose_int_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_34: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_179);  slice_tensor_179 = None\n",
      "        cat_default_68: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_34, slice_tensor_178], -1);  neg_default_34 = slice_tensor_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_156: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_68, unsqueeze_default_38);  cat_default_68 = None\n",
      "        add_tensor_120: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_155, mul_tensor_156);  mul_tensor_155 = mul_tensor_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_157: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_86, unsqueeze_default_37);  unsqueeze_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_180: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_181: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 64, 9223372036854775807);  transpose_int_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_35: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_181);  slice_tensor_181 = None\n",
      "        cat_default_69: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_35, slice_tensor_180], -1);  neg_default_35 = slice_tensor_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_158: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_69, unsqueeze_default_38);  cat_default_69 = unsqueeze_default_38 = None\n",
      "        add_tensor_121: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_157, mul_tensor_158);  mul_tensor_157 = mul_tensor_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_70: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg35, add_tensor_121], 2);  arg35 = add_tensor_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_71: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg36, transpose_int_87], 2);  arg36 = transpose_int_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_88: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_70, 2, 3)\n",
      "        expand_default_69: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_120, [1, 32, 1, 128]);  add_tensor_120 = None\n",
      "        view_default_418: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_69, [32, 1, 128]);  expand_default_69 = None\n",
      "        sym_size_52: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_70, 2)\n",
      "        expand_default_70: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_88, [1, 32, 128, sym_size_52]);  transpose_int_88 = None\n",
      "        view_default_419: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_70, [32, 128, sym_size_52]);  expand_default_70 = None\n",
      "        bmm_default_34: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_418, view_default_419);  view_default_418 = view_default_419 = None\n",
      "        view_default_420: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_34, [1, 32, 1, sym_size_52]);  bmm_default_34 = None\n",
      "        div_tensor_17: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_420, 11.313708498984761);  view_default_420 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_122: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_17, masked_fill_scalar);  div_tensor_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_17: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_122, -1, False);  add_tensor_122 = None\n",
      "        detach_default_52: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_17)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_71: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_17, [1, 32, 1, sym_size_52]);  _softmax_default_17 = None\n",
      "        view_default_421: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_71, [32, 1, sym_size_52]);  expand_default_71 = sym_size_52 = None\n",
      "        sym_size_53: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_71, 2)\n",
      "        expand_default_72: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_71, [1, 32, sym_size_53, 128])\n",
      "        view_default_422: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_72, [32, sym_size_53, 128]);  expand_default_72 = sym_size_53 = None\n",
      "        bmm_default_35: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_421, view_default_422);  view_default_421 = view_default_422 = None\n",
      "        view_default_423: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_35, [1, 32, 1, 128]);  bmm_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_89: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_423, 1, 2);  view_default_423 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_424: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_89, [1, 1, 4096]);  transpose_int_89 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant158 = self._param_constant158\n",
      "        t_default_122: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant158);  _param_constant158 = None\n",
      "        view_default_425: f32[1, 4096] = torch.ops.aten.view.default(view_default_424, [1, 4096]);  view_default_424 = None\n",
      "        mm_default_122: f32[1, 4096] = torch.ops.aten.mm.default(view_default_425, t_default_122);  view_default_425 = t_default_122 = None\n",
      "        view_default_426: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_122, [1, 1, 4096]);  mm_default_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_123: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_118, view_default_426);  add_tensor_118 = view_default_426 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_35: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_123, 2)\n",
      "        mean_dim_35: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_35, [-1], True);  pow_tensor_scalar_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_124: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_35, 1e-06);  mean_dim_35 = None\n",
      "        rsqrt_default_35: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_124);  add_tensor_124 = None\n",
      "        detach_default_53: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_35)\n",
      "        mul_tensor_159: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_123, rsqrt_default_35);  rsqrt_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant159 = self._param_constant159\n",
      "        mul_tensor_160: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant159, mul_tensor_159);  _param_constant159 = mul_tensor_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant160 = self._param_constant160\n",
      "        t_default_123: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant160);  _param_constant160 = None\n",
      "        view_default_427: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_160, [1, 4096])\n",
      "        mm_default_123: f32[1, 11008] = torch.ops.aten.mm.default(view_default_427, t_default_123);  view_default_427 = t_default_123 = None\n",
      "        view_default_428: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_123, [1, 1, 11008]);  mm_default_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_17: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_428);  view_default_428 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant161 = self._param_constant161\n",
      "        t_default_124: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant161);  _param_constant161 = None\n",
      "        view_default_429: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_160, [1, 4096]);  mul_tensor_160 = None\n",
      "        mm_default_124: f32[1, 11008] = torch.ops.aten.mm.default(view_default_429, t_default_124);  view_default_429 = t_default_124 = None\n",
      "        view_default_430: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_124, [1, 1, 11008]);  mm_default_124 = None\n",
      "        mul_tensor_161: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_17, view_default_430);  silu_default_17 = view_default_430 = None\n",
      "        _param_constant162 = self._param_constant162\n",
      "        t_default_125: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant162);  _param_constant162 = None\n",
      "        view_default_431: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_161, [1, 11008]);  mul_tensor_161 = None\n",
      "        mm_default_125: f32[1, 4096] = torch.ops.aten.mm.default(view_default_431, t_default_125);  view_default_431 = t_default_125 = None\n",
      "        view_default_432: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_125, [1, 1, 4096]);  mm_default_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_125: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_123, view_default_432);  add_tensor_123 = view_default_432 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_36: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_125, 2)\n",
      "        mean_dim_36: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_36, [-1], True);  pow_tensor_scalar_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_126: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_36, 1e-06);  mean_dim_36 = None\n",
      "        rsqrt_default_36: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_126);  add_tensor_126 = None\n",
      "        detach_default_54: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_36)\n",
      "        mul_tensor_162: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_125, rsqrt_default_36);  rsqrt_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant163 = self._param_constant163\n",
      "        mul_tensor_163: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant163, mul_tensor_162);  _param_constant163 = mul_tensor_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant164 = self._param_constant164\n",
      "        t_default_126: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant164);  _param_constant164 = None\n",
      "        view_default_433: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_163, [1, 4096])\n",
      "        mm_default_126: f32[1, 4096] = torch.ops.aten.mm.default(view_default_433, t_default_126);  view_default_433 = t_default_126 = None\n",
      "        view_default_434: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_126, [1, 1, 4096]);  mm_default_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant165 = self._param_constant165\n",
      "        t_default_127: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant165);  _param_constant165 = None\n",
      "        view_default_435: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_163, [1, 4096])\n",
      "        mm_default_127: f32[1, 4096] = torch.ops.aten.mm.default(view_default_435, t_default_127);  view_default_435 = t_default_127 = None\n",
      "        view_default_436: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_127, [1, 1, 4096]);  mm_default_127 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant166 = self._param_constant166\n",
      "        t_default_128: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant166);  _param_constant166 = None\n",
      "        view_default_437: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_163, [1, 4096]);  mul_tensor_163 = None\n",
      "        mm_default_128: f32[1, 4096] = torch.ops.aten.mm.default(view_default_437, t_default_128);  view_default_437 = t_default_128 = None\n",
      "        view_default_438: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_128, [1, 1, 4096]);  mm_default_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_439: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_434, [1, 1, 32, 128]);  view_default_434 = None\n",
      "        transpose_int_90: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_439, 1, 2);  view_default_439 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_440: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_436, [1, 1, 32, 128]);  view_default_436 = None\n",
      "        transpose_int_91: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_440, 1, 2);  view_default_440 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_441: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_438, [1, 1, 32, 128]);  view_default_438 = None\n",
      "        transpose_int_92: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_441, 1, 2);  view_default_441 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant36 = self._tensor_constant36\n",
      "        slice_tensor_182: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant36, 0, 0, 9223372036854775807);  _tensor_constant36 = None\n",
      "        slice_tensor_183: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_182, 1, 0, 9223372036854775807);  slice_tensor_182 = None\n",
      "        sym_size_54: Sym(s0) = torch.ops.aten.sym_size(arg37, 2)\n",
      "        add_20: Sym(s0 + 1) = 1 + sym_size_54;  sym_size_54 = None\n",
      "        slice_tensor_184: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_183, 2, 0, add_20);  slice_tensor_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant37 = self._tensor_constant37\n",
      "        slice_tensor_185: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant37, 0, 0, 9223372036854775807);  _tensor_constant37 = None\n",
      "        slice_tensor_186: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_185, 1, 0, 9223372036854775807);  slice_tensor_185 = None\n",
      "        slice_tensor_187: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_186, 2, 0, add_20);  slice_tensor_186 = add_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_72: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_184, 1);  slice_tensor_184 = None\n",
      "        squeeze_dim_73: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_72, 0);  squeeze_dim_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_74: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_187, 1);  slice_tensor_187 = None\n",
      "        squeeze_dim_75: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_74, 0);  squeeze_dim_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_36: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_73, [view_default]);  squeeze_dim_73 = None\n",
      "        unsqueeze_default_39: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_36, 1);  index_tensor_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_37: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_75, [view_default]);  squeeze_dim_75 = None\n",
      "        unsqueeze_default_40: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_37, 1);  index_tensor_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_164: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_90, unsqueeze_default_39)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_188: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_189: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 64, 9223372036854775807);  transpose_int_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_36: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_189);  slice_tensor_189 = None\n",
      "        cat_default_72: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_36, slice_tensor_188], -1);  neg_default_36 = slice_tensor_188 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_165: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_72, unsqueeze_default_40);  cat_default_72 = None\n",
      "        add_tensor_127: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_164, mul_tensor_165);  mul_tensor_164 = mul_tensor_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_166: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_91, unsqueeze_default_39);  unsqueeze_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_190: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_191: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 64, 9223372036854775807);  transpose_int_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_37: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_191);  slice_tensor_191 = None\n",
      "        cat_default_73: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_37, slice_tensor_190], -1);  neg_default_37 = slice_tensor_190 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_167: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_73, unsqueeze_default_40);  cat_default_73 = unsqueeze_default_40 = None\n",
      "        add_tensor_128: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_166, mul_tensor_167);  mul_tensor_166 = mul_tensor_167 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_74: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg37, add_tensor_128], 2);  arg37 = add_tensor_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_75: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg38, transpose_int_92], 2);  arg38 = transpose_int_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_93: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_74, 2, 3)\n",
      "        expand_default_73: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_127, [1, 32, 1, 128]);  add_tensor_127 = None\n",
      "        view_default_442: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_73, [32, 1, 128]);  expand_default_73 = None\n",
      "        sym_size_55: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_74, 2)\n",
      "        expand_default_74: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_93, [1, 32, 128, sym_size_55]);  transpose_int_93 = None\n",
      "        view_default_443: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_74, [32, 128, sym_size_55]);  expand_default_74 = None\n",
      "        bmm_default_36: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_442, view_default_443);  view_default_442 = view_default_443 = None\n",
      "        view_default_444: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_36, [1, 32, 1, sym_size_55]);  bmm_default_36 = None\n",
      "        div_tensor_18: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_444, 11.313708498984761);  view_default_444 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_129: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_18, masked_fill_scalar);  div_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_18: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_129, -1, False);  add_tensor_129 = None\n",
      "        detach_default_55: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_18)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_75: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_18, [1, 32, 1, sym_size_55]);  _softmax_default_18 = None\n",
      "        view_default_445: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_75, [32, 1, sym_size_55]);  expand_default_75 = sym_size_55 = None\n",
      "        sym_size_56: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_75, 2)\n",
      "        expand_default_76: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_75, [1, 32, sym_size_56, 128])\n",
      "        view_default_446: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_76, [32, sym_size_56, 128]);  expand_default_76 = sym_size_56 = None\n",
      "        bmm_default_37: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_445, view_default_446);  view_default_445 = view_default_446 = None\n",
      "        view_default_447: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_37, [1, 32, 1, 128]);  bmm_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_94: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_447, 1, 2);  view_default_447 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_448: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_94, [1, 1, 4096]);  transpose_int_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant167 = self._param_constant167\n",
      "        t_default_129: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant167);  _param_constant167 = None\n",
      "        view_default_449: f32[1, 4096] = torch.ops.aten.view.default(view_default_448, [1, 4096]);  view_default_448 = None\n",
      "        mm_default_129: f32[1, 4096] = torch.ops.aten.mm.default(view_default_449, t_default_129);  view_default_449 = t_default_129 = None\n",
      "        view_default_450: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_129, [1, 1, 4096]);  mm_default_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_130: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_125, view_default_450);  add_tensor_125 = view_default_450 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_37: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_130, 2)\n",
      "        mean_dim_37: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_37, [-1], True);  pow_tensor_scalar_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_131: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_37, 1e-06);  mean_dim_37 = None\n",
      "        rsqrt_default_37: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_131);  add_tensor_131 = None\n",
      "        detach_default_56: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_37)\n",
      "        mul_tensor_168: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_130, rsqrt_default_37);  rsqrt_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant168 = self._param_constant168\n",
      "        mul_tensor_169: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant168, mul_tensor_168);  _param_constant168 = mul_tensor_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant169 = self._param_constant169\n",
      "        t_default_130: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant169);  _param_constant169 = None\n",
      "        view_default_451: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_169, [1, 4096])\n",
      "        mm_default_130: f32[1, 11008] = torch.ops.aten.mm.default(view_default_451, t_default_130);  view_default_451 = t_default_130 = None\n",
      "        view_default_452: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_130, [1, 1, 11008]);  mm_default_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_18: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_452);  view_default_452 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant170 = self._param_constant170\n",
      "        t_default_131: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant170);  _param_constant170 = None\n",
      "        view_default_453: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_169, [1, 4096]);  mul_tensor_169 = None\n",
      "        mm_default_131: f32[1, 11008] = torch.ops.aten.mm.default(view_default_453, t_default_131);  view_default_453 = t_default_131 = None\n",
      "        view_default_454: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_131, [1, 1, 11008]);  mm_default_131 = None\n",
      "        mul_tensor_170: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_18, view_default_454);  silu_default_18 = view_default_454 = None\n",
      "        _param_constant171 = self._param_constant171\n",
      "        t_default_132: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant171);  _param_constant171 = None\n",
      "        view_default_455: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_170, [1, 11008]);  mul_tensor_170 = None\n",
      "        mm_default_132: f32[1, 4096] = torch.ops.aten.mm.default(view_default_455, t_default_132);  view_default_455 = t_default_132 = None\n",
      "        view_default_456: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_132, [1, 1, 4096]);  mm_default_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_132: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_130, view_default_456);  add_tensor_130 = view_default_456 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_38: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_132, 2)\n",
      "        mean_dim_38: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_38, [-1], True);  pow_tensor_scalar_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_133: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_38, 1e-06);  mean_dim_38 = None\n",
      "        rsqrt_default_38: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_133);  add_tensor_133 = None\n",
      "        detach_default_57: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_38)\n",
      "        mul_tensor_171: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_132, rsqrt_default_38);  rsqrt_default_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant172 = self._param_constant172\n",
      "        mul_tensor_172: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant172, mul_tensor_171);  _param_constant172 = mul_tensor_171 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant173 = self._param_constant173\n",
      "        t_default_133: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant173);  _param_constant173 = None\n",
      "        view_default_457: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_172, [1, 4096])\n",
      "        mm_default_133: f32[1, 4096] = torch.ops.aten.mm.default(view_default_457, t_default_133);  view_default_457 = t_default_133 = None\n",
      "        view_default_458: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_133, [1, 1, 4096]);  mm_default_133 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant174 = self._param_constant174\n",
      "        t_default_134: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant174);  _param_constant174 = None\n",
      "        view_default_459: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_172, [1, 4096])\n",
      "        mm_default_134: f32[1, 4096] = torch.ops.aten.mm.default(view_default_459, t_default_134);  view_default_459 = t_default_134 = None\n",
      "        view_default_460: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_134, [1, 1, 4096]);  mm_default_134 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant175 = self._param_constant175\n",
      "        t_default_135: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant175);  _param_constant175 = None\n",
      "        view_default_461: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_172, [1, 4096]);  mul_tensor_172 = None\n",
      "        mm_default_135: f32[1, 4096] = torch.ops.aten.mm.default(view_default_461, t_default_135);  view_default_461 = t_default_135 = None\n",
      "        view_default_462: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_135, [1, 1, 4096]);  mm_default_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_463: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_458, [1, 1, 32, 128]);  view_default_458 = None\n",
      "        transpose_int_95: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_463, 1, 2);  view_default_463 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_464: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_460, [1, 1, 32, 128]);  view_default_460 = None\n",
      "        transpose_int_96: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_464, 1, 2);  view_default_464 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_465: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_462, [1, 1, 32, 128]);  view_default_462 = None\n",
      "        transpose_int_97: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_465, 1, 2);  view_default_465 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant38 = self._tensor_constant38\n",
      "        slice_tensor_192: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant38, 0, 0, 9223372036854775807);  _tensor_constant38 = None\n",
      "        slice_tensor_193: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_192, 1, 0, 9223372036854775807);  slice_tensor_192 = None\n",
      "        sym_size_57: Sym(s0) = torch.ops.aten.sym_size(arg39, 2)\n",
      "        add_21: Sym(s0 + 1) = 1 + sym_size_57;  sym_size_57 = None\n",
      "        slice_tensor_194: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_193, 2, 0, add_21);  slice_tensor_193 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant39 = self._tensor_constant39\n",
      "        slice_tensor_195: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant39, 0, 0, 9223372036854775807);  _tensor_constant39 = None\n",
      "        slice_tensor_196: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_195, 1, 0, 9223372036854775807);  slice_tensor_195 = None\n",
      "        slice_tensor_197: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_196, 2, 0, add_21);  slice_tensor_196 = add_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_76: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_194, 1);  slice_tensor_194 = None\n",
      "        squeeze_dim_77: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_76, 0);  squeeze_dim_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_78: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_197, 1);  slice_tensor_197 = None\n",
      "        squeeze_dim_79: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_78, 0);  squeeze_dim_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_38: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_77, [view_default]);  squeeze_dim_77 = None\n",
      "        unsqueeze_default_41: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_38, 1);  index_tensor_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_39: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_79, [view_default]);  squeeze_dim_79 = None\n",
      "        unsqueeze_default_42: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_39, 1);  index_tensor_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_173: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_95, unsqueeze_default_41)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_198: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_199: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 64, 9223372036854775807);  transpose_int_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_38: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_199);  slice_tensor_199 = None\n",
      "        cat_default_76: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_38, slice_tensor_198], -1);  neg_default_38 = slice_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_174: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_76, unsqueeze_default_42);  cat_default_76 = None\n",
      "        add_tensor_134: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_173, mul_tensor_174);  mul_tensor_173 = mul_tensor_174 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_175: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_96, unsqueeze_default_41);  unsqueeze_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_200: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_201: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 64, 9223372036854775807);  transpose_int_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_39: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_201);  slice_tensor_201 = None\n",
      "        cat_default_77: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_39, slice_tensor_200], -1);  neg_default_39 = slice_tensor_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_176: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_77, unsqueeze_default_42);  cat_default_77 = unsqueeze_default_42 = None\n",
      "        add_tensor_135: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_175, mul_tensor_176);  mul_tensor_175 = mul_tensor_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_78: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg39, add_tensor_135], 2);  arg39 = add_tensor_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_79: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg40, transpose_int_97], 2);  arg40 = transpose_int_97 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_98: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_78, 2, 3)\n",
      "        expand_default_77: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_134, [1, 32, 1, 128]);  add_tensor_134 = None\n",
      "        view_default_466: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_77, [32, 1, 128]);  expand_default_77 = None\n",
      "        sym_size_58: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_78, 2)\n",
      "        expand_default_78: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_98, [1, 32, 128, sym_size_58]);  transpose_int_98 = None\n",
      "        view_default_467: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_78, [32, 128, sym_size_58]);  expand_default_78 = None\n",
      "        bmm_default_38: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_466, view_default_467);  view_default_466 = view_default_467 = None\n",
      "        view_default_468: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_38, [1, 32, 1, sym_size_58]);  bmm_default_38 = None\n",
      "        div_tensor_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_468, 11.313708498984761);  view_default_468 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_136: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_19, masked_fill_scalar);  div_tensor_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_136, -1, False);  add_tensor_136 = None\n",
      "        detach_default_58: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_19)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_79: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_19, [1, 32, 1, sym_size_58]);  _softmax_default_19 = None\n",
      "        view_default_469: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_79, [32, 1, sym_size_58]);  expand_default_79 = sym_size_58 = None\n",
      "        sym_size_59: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_79, 2)\n",
      "        expand_default_80: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_79, [1, 32, sym_size_59, 128])\n",
      "        view_default_470: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_80, [32, sym_size_59, 128]);  expand_default_80 = sym_size_59 = None\n",
      "        bmm_default_39: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_469, view_default_470);  view_default_469 = view_default_470 = None\n",
      "        view_default_471: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_39, [1, 32, 1, 128]);  bmm_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_99: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_471, 1, 2);  view_default_471 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_472: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_99, [1, 1, 4096]);  transpose_int_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant176 = self._param_constant176\n",
      "        t_default_136: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant176);  _param_constant176 = None\n",
      "        view_default_473: f32[1, 4096] = torch.ops.aten.view.default(view_default_472, [1, 4096]);  view_default_472 = None\n",
      "        mm_default_136: f32[1, 4096] = torch.ops.aten.mm.default(view_default_473, t_default_136);  view_default_473 = t_default_136 = None\n",
      "        view_default_474: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_136, [1, 1, 4096]);  mm_default_136 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_137: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_132, view_default_474);  add_tensor_132 = view_default_474 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_39: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_137, 2)\n",
      "        mean_dim_39: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_39, [-1], True);  pow_tensor_scalar_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_138: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_39, 1e-06);  mean_dim_39 = None\n",
      "        rsqrt_default_39: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_138);  add_tensor_138 = None\n",
      "        detach_default_59: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_39)\n",
      "        mul_tensor_177: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_137, rsqrt_default_39);  rsqrt_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant177 = self._param_constant177\n",
      "        mul_tensor_178: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant177, mul_tensor_177);  _param_constant177 = mul_tensor_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant178 = self._param_constant178\n",
      "        t_default_137: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant178);  _param_constant178 = None\n",
      "        view_default_475: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_178, [1, 4096])\n",
      "        mm_default_137: f32[1, 11008] = torch.ops.aten.mm.default(view_default_475, t_default_137);  view_default_475 = t_default_137 = None\n",
      "        view_default_476: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_137, [1, 1, 11008]);  mm_default_137 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_19: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_476);  view_default_476 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant179 = self._param_constant179\n",
      "        t_default_138: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant179);  _param_constant179 = None\n",
      "        view_default_477: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_178, [1, 4096]);  mul_tensor_178 = None\n",
      "        mm_default_138: f32[1, 11008] = torch.ops.aten.mm.default(view_default_477, t_default_138);  view_default_477 = t_default_138 = None\n",
      "        view_default_478: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_138, [1, 1, 11008]);  mm_default_138 = None\n",
      "        mul_tensor_179: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_19, view_default_478);  silu_default_19 = view_default_478 = None\n",
      "        _param_constant180 = self._param_constant180\n",
      "        t_default_139: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant180);  _param_constant180 = None\n",
      "        view_default_479: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_179, [1, 11008]);  mul_tensor_179 = None\n",
      "        mm_default_139: f32[1, 4096] = torch.ops.aten.mm.default(view_default_479, t_default_139);  view_default_479 = t_default_139 = None\n",
      "        view_default_480: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_139, [1, 1, 4096]);  mm_default_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_139: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_137, view_default_480);  add_tensor_137 = view_default_480 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_40: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_139, 2)\n",
      "        mean_dim_40: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_40, [-1], True);  pow_tensor_scalar_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_140: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_40, 1e-06);  mean_dim_40 = None\n",
      "        rsqrt_default_40: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_140);  add_tensor_140 = None\n",
      "        detach_default_60: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_40)\n",
      "        mul_tensor_180: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_139, rsqrt_default_40);  rsqrt_default_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant181 = self._param_constant181\n",
      "        mul_tensor_181: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant181, mul_tensor_180);  _param_constant181 = mul_tensor_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant182 = self._param_constant182\n",
      "        t_default_140: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant182);  _param_constant182 = None\n",
      "        view_default_481: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_181, [1, 4096])\n",
      "        mm_default_140: f32[1, 4096] = torch.ops.aten.mm.default(view_default_481, t_default_140);  view_default_481 = t_default_140 = None\n",
      "        view_default_482: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_140, [1, 1, 4096]);  mm_default_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant183 = self._param_constant183\n",
      "        t_default_141: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant183);  _param_constant183 = None\n",
      "        view_default_483: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_181, [1, 4096])\n",
      "        mm_default_141: f32[1, 4096] = torch.ops.aten.mm.default(view_default_483, t_default_141);  view_default_483 = t_default_141 = None\n",
      "        view_default_484: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_141, [1, 1, 4096]);  mm_default_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant184 = self._param_constant184\n",
      "        t_default_142: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant184);  _param_constant184 = None\n",
      "        view_default_485: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_181, [1, 4096]);  mul_tensor_181 = None\n",
      "        mm_default_142: f32[1, 4096] = torch.ops.aten.mm.default(view_default_485, t_default_142);  view_default_485 = t_default_142 = None\n",
      "        view_default_486: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_142, [1, 1, 4096]);  mm_default_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_487: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_482, [1, 1, 32, 128]);  view_default_482 = None\n",
      "        transpose_int_100: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_487, 1, 2);  view_default_487 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_488: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_484, [1, 1, 32, 128]);  view_default_484 = None\n",
      "        transpose_int_101: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_488, 1, 2);  view_default_488 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_489: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_486, [1, 1, 32, 128]);  view_default_486 = None\n",
      "        transpose_int_102: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_489, 1, 2);  view_default_489 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant40 = self._tensor_constant40\n",
      "        slice_tensor_202: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant40, 0, 0, 9223372036854775807);  _tensor_constant40 = None\n",
      "        slice_tensor_203: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_202, 1, 0, 9223372036854775807);  slice_tensor_202 = None\n",
      "        sym_size_60: Sym(s0) = torch.ops.aten.sym_size(arg41, 2)\n",
      "        add_22: Sym(s0 + 1) = 1 + sym_size_60;  sym_size_60 = None\n",
      "        slice_tensor_204: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_203, 2, 0, add_22);  slice_tensor_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant41 = self._tensor_constant41\n",
      "        slice_tensor_205: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant41, 0, 0, 9223372036854775807);  _tensor_constant41 = None\n",
      "        slice_tensor_206: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_205, 1, 0, 9223372036854775807);  slice_tensor_205 = None\n",
      "        slice_tensor_207: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_206, 2, 0, add_22);  slice_tensor_206 = add_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_80: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_204, 1);  slice_tensor_204 = None\n",
      "        squeeze_dim_81: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_80, 0);  squeeze_dim_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_82: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_207, 1);  slice_tensor_207 = None\n",
      "        squeeze_dim_83: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_82, 0);  squeeze_dim_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_40: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_81, [view_default]);  squeeze_dim_81 = None\n",
      "        unsqueeze_default_43: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_40, 1);  index_tensor_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_41: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_83, [view_default]);  squeeze_dim_83 = None\n",
      "        unsqueeze_default_44: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_41, 1);  index_tensor_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_182: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_100, unsqueeze_default_43)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_208: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_209: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 64, 9223372036854775807);  transpose_int_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_40: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_209);  slice_tensor_209 = None\n",
      "        cat_default_80: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_40, slice_tensor_208], -1);  neg_default_40 = slice_tensor_208 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_183: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_80, unsqueeze_default_44);  cat_default_80 = None\n",
      "        add_tensor_141: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_182, mul_tensor_183);  mul_tensor_182 = mul_tensor_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_184: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_101, unsqueeze_default_43);  unsqueeze_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_210: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_211: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 64, 9223372036854775807);  transpose_int_101 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_41: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_211);  slice_tensor_211 = None\n",
      "        cat_default_81: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_41, slice_tensor_210], -1);  neg_default_41 = slice_tensor_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_185: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_81, unsqueeze_default_44);  cat_default_81 = unsqueeze_default_44 = None\n",
      "        add_tensor_142: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_184, mul_tensor_185);  mul_tensor_184 = mul_tensor_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_82: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg41, add_tensor_142], 2);  arg41 = add_tensor_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_83: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg42, transpose_int_102], 2);  arg42 = transpose_int_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_103: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_82, 2, 3)\n",
      "        expand_default_81: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_141, [1, 32, 1, 128]);  add_tensor_141 = None\n",
      "        view_default_490: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_81, [32, 1, 128]);  expand_default_81 = None\n",
      "        sym_size_61: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_82, 2)\n",
      "        expand_default_82: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_103, [1, 32, 128, sym_size_61]);  transpose_int_103 = None\n",
      "        view_default_491: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_82, [32, 128, sym_size_61]);  expand_default_82 = None\n",
      "        bmm_default_40: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_490, view_default_491);  view_default_490 = view_default_491 = None\n",
      "        view_default_492: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_40, [1, 32, 1, sym_size_61]);  bmm_default_40 = None\n",
      "        div_tensor_20: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_492, 11.313708498984761);  view_default_492 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_143: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_20, masked_fill_scalar);  div_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_20: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_143, -1, False);  add_tensor_143 = None\n",
      "        detach_default_61: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_20)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_83: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_20, [1, 32, 1, sym_size_61]);  _softmax_default_20 = None\n",
      "        view_default_493: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_83, [32, 1, sym_size_61]);  expand_default_83 = sym_size_61 = None\n",
      "        sym_size_62: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_83, 2)\n",
      "        expand_default_84: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_83, [1, 32, sym_size_62, 128])\n",
      "        view_default_494: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_84, [32, sym_size_62, 128]);  expand_default_84 = sym_size_62 = None\n",
      "        bmm_default_41: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_493, view_default_494);  view_default_493 = view_default_494 = None\n",
      "        view_default_495: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_41, [1, 32, 1, 128]);  bmm_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_104: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_495, 1, 2);  view_default_495 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_496: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_104, [1, 1, 4096]);  transpose_int_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant185 = self._param_constant185\n",
      "        t_default_143: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant185);  _param_constant185 = None\n",
      "        view_default_497: f32[1, 4096] = torch.ops.aten.view.default(view_default_496, [1, 4096]);  view_default_496 = None\n",
      "        mm_default_143: f32[1, 4096] = torch.ops.aten.mm.default(view_default_497, t_default_143);  view_default_497 = t_default_143 = None\n",
      "        view_default_498: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_143, [1, 1, 4096]);  mm_default_143 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_144: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_139, view_default_498);  add_tensor_139 = view_default_498 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_41: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_144, 2)\n",
      "        mean_dim_41: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_41, [-1], True);  pow_tensor_scalar_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_145: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_41, 1e-06);  mean_dim_41 = None\n",
      "        rsqrt_default_41: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_145);  add_tensor_145 = None\n",
      "        detach_default_62: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_41)\n",
      "        mul_tensor_186: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_144, rsqrt_default_41);  rsqrt_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant186 = self._param_constant186\n",
      "        mul_tensor_187: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant186, mul_tensor_186);  _param_constant186 = mul_tensor_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant187 = self._param_constant187\n",
      "        t_default_144: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant187);  _param_constant187 = None\n",
      "        view_default_499: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_187, [1, 4096])\n",
      "        mm_default_144: f32[1, 11008] = torch.ops.aten.mm.default(view_default_499, t_default_144);  view_default_499 = t_default_144 = None\n",
      "        view_default_500: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_144, [1, 1, 11008]);  mm_default_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_20: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_500);  view_default_500 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant188 = self._param_constant188\n",
      "        t_default_145: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant188);  _param_constant188 = None\n",
      "        view_default_501: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_187, [1, 4096]);  mul_tensor_187 = None\n",
      "        mm_default_145: f32[1, 11008] = torch.ops.aten.mm.default(view_default_501, t_default_145);  view_default_501 = t_default_145 = None\n",
      "        view_default_502: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_145, [1, 1, 11008]);  mm_default_145 = None\n",
      "        mul_tensor_188: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_20, view_default_502);  silu_default_20 = view_default_502 = None\n",
      "        _param_constant189 = self._param_constant189\n",
      "        t_default_146: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant189);  _param_constant189 = None\n",
      "        view_default_503: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_188, [1, 11008]);  mul_tensor_188 = None\n",
      "        mm_default_146: f32[1, 4096] = torch.ops.aten.mm.default(view_default_503, t_default_146);  view_default_503 = t_default_146 = None\n",
      "        view_default_504: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_146, [1, 1, 4096]);  mm_default_146 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_146: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_144, view_default_504);  add_tensor_144 = view_default_504 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_42: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_146, 2)\n",
      "        mean_dim_42: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_42, [-1], True);  pow_tensor_scalar_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_147: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_42, 1e-06);  mean_dim_42 = None\n",
      "        rsqrt_default_42: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_147);  add_tensor_147 = None\n",
      "        detach_default_63: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_42)\n",
      "        mul_tensor_189: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_146, rsqrt_default_42);  rsqrt_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant190 = self._param_constant190\n",
      "        mul_tensor_190: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant190, mul_tensor_189);  _param_constant190 = mul_tensor_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant191 = self._param_constant191\n",
      "        t_default_147: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant191);  _param_constant191 = None\n",
      "        view_default_505: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_190, [1, 4096])\n",
      "        mm_default_147: f32[1, 4096] = torch.ops.aten.mm.default(view_default_505, t_default_147);  view_default_505 = t_default_147 = None\n",
      "        view_default_506: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_147, [1, 1, 4096]);  mm_default_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant192 = self._param_constant192\n",
      "        t_default_148: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant192);  _param_constant192 = None\n",
      "        view_default_507: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_190, [1, 4096])\n",
      "        mm_default_148: f32[1, 4096] = torch.ops.aten.mm.default(view_default_507, t_default_148);  view_default_507 = t_default_148 = None\n",
      "        view_default_508: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_148, [1, 1, 4096]);  mm_default_148 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant193 = self._param_constant193\n",
      "        t_default_149: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant193);  _param_constant193 = None\n",
      "        view_default_509: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_190, [1, 4096]);  mul_tensor_190 = None\n",
      "        mm_default_149: f32[1, 4096] = torch.ops.aten.mm.default(view_default_509, t_default_149);  view_default_509 = t_default_149 = None\n",
      "        view_default_510: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_149, [1, 1, 4096]);  mm_default_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_511: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_506, [1, 1, 32, 128]);  view_default_506 = None\n",
      "        transpose_int_105: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_511, 1, 2);  view_default_511 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_512: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_508, [1, 1, 32, 128]);  view_default_508 = None\n",
      "        transpose_int_106: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_512, 1, 2);  view_default_512 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_513: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_510, [1, 1, 32, 128]);  view_default_510 = None\n",
      "        transpose_int_107: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_513, 1, 2);  view_default_513 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant42 = self._tensor_constant42\n",
      "        slice_tensor_212: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant42, 0, 0, 9223372036854775807);  _tensor_constant42 = None\n",
      "        slice_tensor_213: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_212, 1, 0, 9223372036854775807);  slice_tensor_212 = None\n",
      "        sym_size_63: Sym(s0) = torch.ops.aten.sym_size(arg43, 2)\n",
      "        add_23: Sym(s0 + 1) = 1 + sym_size_63;  sym_size_63 = None\n",
      "        slice_tensor_214: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_213, 2, 0, add_23);  slice_tensor_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant43 = self._tensor_constant43\n",
      "        slice_tensor_215: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant43, 0, 0, 9223372036854775807);  _tensor_constant43 = None\n",
      "        slice_tensor_216: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_215, 1, 0, 9223372036854775807);  slice_tensor_215 = None\n",
      "        slice_tensor_217: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_216, 2, 0, add_23);  slice_tensor_216 = add_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_84: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_214, 1);  slice_tensor_214 = None\n",
      "        squeeze_dim_85: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_84, 0);  squeeze_dim_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_86: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_217, 1);  slice_tensor_217 = None\n",
      "        squeeze_dim_87: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_86, 0);  squeeze_dim_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_42: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_85, [view_default]);  squeeze_dim_85 = None\n",
      "        unsqueeze_default_45: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_42, 1);  index_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_43: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_87, [view_default]);  squeeze_dim_87 = None\n",
      "        unsqueeze_default_46: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_43, 1);  index_tensor_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_191: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_105, unsqueeze_default_45)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_218: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_219: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 64, 9223372036854775807);  transpose_int_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_42: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_219);  slice_tensor_219 = None\n",
      "        cat_default_84: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_42, slice_tensor_218], -1);  neg_default_42 = slice_tensor_218 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_192: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_84, unsqueeze_default_46);  cat_default_84 = None\n",
      "        add_tensor_148: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_191, mul_tensor_192);  mul_tensor_191 = mul_tensor_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_193: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_106, unsqueeze_default_45);  unsqueeze_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_220: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_221: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 64, 9223372036854775807);  transpose_int_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_43: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_221);  slice_tensor_221 = None\n",
      "        cat_default_85: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_43, slice_tensor_220], -1);  neg_default_43 = slice_tensor_220 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_194: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_85, unsqueeze_default_46);  cat_default_85 = unsqueeze_default_46 = None\n",
      "        add_tensor_149: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_193, mul_tensor_194);  mul_tensor_193 = mul_tensor_194 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_86: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg43, add_tensor_149], 2);  arg43 = add_tensor_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_87: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg44, transpose_int_107], 2);  arg44 = transpose_int_107 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_108: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_86, 2, 3)\n",
      "        expand_default_85: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_148, [1, 32, 1, 128]);  add_tensor_148 = None\n",
      "        view_default_514: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_85, [32, 1, 128]);  expand_default_85 = None\n",
      "        sym_size_64: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_86, 2)\n",
      "        expand_default_86: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_108, [1, 32, 128, sym_size_64]);  transpose_int_108 = None\n",
      "        view_default_515: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_86, [32, 128, sym_size_64]);  expand_default_86 = None\n",
      "        bmm_default_42: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_514, view_default_515);  view_default_514 = view_default_515 = None\n",
      "        view_default_516: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_42, [1, 32, 1, sym_size_64]);  bmm_default_42 = None\n",
      "        div_tensor_21: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_516, 11.313708498984761);  view_default_516 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_150: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_21, masked_fill_scalar);  div_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_21: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_150, -1, False);  add_tensor_150 = None\n",
      "        detach_default_64: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_87: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_21, [1, 32, 1, sym_size_64]);  _softmax_default_21 = None\n",
      "        view_default_517: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_87, [32, 1, sym_size_64]);  expand_default_87 = sym_size_64 = None\n",
      "        sym_size_65: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_87, 2)\n",
      "        expand_default_88: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_87, [1, 32, sym_size_65, 128])\n",
      "        view_default_518: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_88, [32, sym_size_65, 128]);  expand_default_88 = sym_size_65 = None\n",
      "        bmm_default_43: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_517, view_default_518);  view_default_517 = view_default_518 = None\n",
      "        view_default_519: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_43, [1, 32, 1, 128]);  bmm_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_109: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_519, 1, 2);  view_default_519 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_520: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_109, [1, 1, 4096]);  transpose_int_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant194 = self._param_constant194\n",
      "        t_default_150: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant194);  _param_constant194 = None\n",
      "        view_default_521: f32[1, 4096] = torch.ops.aten.view.default(view_default_520, [1, 4096]);  view_default_520 = None\n",
      "        mm_default_150: f32[1, 4096] = torch.ops.aten.mm.default(view_default_521, t_default_150);  view_default_521 = t_default_150 = None\n",
      "        view_default_522: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_150, [1, 1, 4096]);  mm_default_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_151: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_146, view_default_522);  add_tensor_146 = view_default_522 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_43: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_151, 2)\n",
      "        mean_dim_43: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_43, [-1], True);  pow_tensor_scalar_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_152: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_43, 1e-06);  mean_dim_43 = None\n",
      "        rsqrt_default_43: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_152);  add_tensor_152 = None\n",
      "        detach_default_65: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_43)\n",
      "        mul_tensor_195: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_151, rsqrt_default_43);  rsqrt_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant195 = self._param_constant195\n",
      "        mul_tensor_196: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant195, mul_tensor_195);  _param_constant195 = mul_tensor_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant196 = self._param_constant196\n",
      "        t_default_151: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant196);  _param_constant196 = None\n",
      "        view_default_523: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_196, [1, 4096])\n",
      "        mm_default_151: f32[1, 11008] = torch.ops.aten.mm.default(view_default_523, t_default_151);  view_default_523 = t_default_151 = None\n",
      "        view_default_524: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_151, [1, 1, 11008]);  mm_default_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_21: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_524);  view_default_524 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant197 = self._param_constant197\n",
      "        t_default_152: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant197);  _param_constant197 = None\n",
      "        view_default_525: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_196, [1, 4096]);  mul_tensor_196 = None\n",
      "        mm_default_152: f32[1, 11008] = torch.ops.aten.mm.default(view_default_525, t_default_152);  view_default_525 = t_default_152 = None\n",
      "        view_default_526: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_152, [1, 1, 11008]);  mm_default_152 = None\n",
      "        mul_tensor_197: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_21, view_default_526);  silu_default_21 = view_default_526 = None\n",
      "        _param_constant198 = self._param_constant198\n",
      "        t_default_153: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant198);  _param_constant198 = None\n",
      "        view_default_527: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_197, [1, 11008]);  mul_tensor_197 = None\n",
      "        mm_default_153: f32[1, 4096] = torch.ops.aten.mm.default(view_default_527, t_default_153);  view_default_527 = t_default_153 = None\n",
      "        view_default_528: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_153, [1, 1, 4096]);  mm_default_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_153: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_151, view_default_528);  add_tensor_151 = view_default_528 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_44: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_153, 2)\n",
      "        mean_dim_44: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_44, [-1], True);  pow_tensor_scalar_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_154: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_44, 1e-06);  mean_dim_44 = None\n",
      "        rsqrt_default_44: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_154);  add_tensor_154 = None\n",
      "        detach_default_66: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_44)\n",
      "        mul_tensor_198: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_153, rsqrt_default_44);  rsqrt_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant199 = self._param_constant199\n",
      "        mul_tensor_199: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant199, mul_tensor_198);  _param_constant199 = mul_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant200 = self._param_constant200\n",
      "        t_default_154: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant200);  _param_constant200 = None\n",
      "        view_default_529: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_199, [1, 4096])\n",
      "        mm_default_154: f32[1, 4096] = torch.ops.aten.mm.default(view_default_529, t_default_154);  view_default_529 = t_default_154 = None\n",
      "        view_default_530: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_154, [1, 1, 4096]);  mm_default_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant201 = self._param_constant201\n",
      "        t_default_155: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant201);  _param_constant201 = None\n",
      "        view_default_531: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_199, [1, 4096])\n",
      "        mm_default_155: f32[1, 4096] = torch.ops.aten.mm.default(view_default_531, t_default_155);  view_default_531 = t_default_155 = None\n",
      "        view_default_532: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_155, [1, 1, 4096]);  mm_default_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant202 = self._param_constant202\n",
      "        t_default_156: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant202);  _param_constant202 = None\n",
      "        view_default_533: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_199, [1, 4096]);  mul_tensor_199 = None\n",
      "        mm_default_156: f32[1, 4096] = torch.ops.aten.mm.default(view_default_533, t_default_156);  view_default_533 = t_default_156 = None\n",
      "        view_default_534: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_156, [1, 1, 4096]);  mm_default_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_535: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_530, [1, 1, 32, 128]);  view_default_530 = None\n",
      "        transpose_int_110: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_535, 1, 2);  view_default_535 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_536: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_532, [1, 1, 32, 128]);  view_default_532 = None\n",
      "        transpose_int_111: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_536, 1, 2);  view_default_536 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_537: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_534, [1, 1, 32, 128]);  view_default_534 = None\n",
      "        transpose_int_112: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_537, 1, 2);  view_default_537 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant44 = self._tensor_constant44\n",
      "        slice_tensor_222: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant44, 0, 0, 9223372036854775807);  _tensor_constant44 = None\n",
      "        slice_tensor_223: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_222, 1, 0, 9223372036854775807);  slice_tensor_222 = None\n",
      "        sym_size_66: Sym(s0) = torch.ops.aten.sym_size(arg45, 2)\n",
      "        add_24: Sym(s0 + 1) = 1 + sym_size_66;  sym_size_66 = None\n",
      "        slice_tensor_224: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_223, 2, 0, add_24);  slice_tensor_223 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant45 = self._tensor_constant45\n",
      "        slice_tensor_225: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant45, 0, 0, 9223372036854775807);  _tensor_constant45 = None\n",
      "        slice_tensor_226: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_225, 1, 0, 9223372036854775807);  slice_tensor_225 = None\n",
      "        slice_tensor_227: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_226, 2, 0, add_24);  slice_tensor_226 = add_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_88: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_224, 1);  slice_tensor_224 = None\n",
      "        squeeze_dim_89: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_88, 0);  squeeze_dim_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_90: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_227, 1);  slice_tensor_227 = None\n",
      "        squeeze_dim_91: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_90, 0);  squeeze_dim_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_44: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_89, [view_default]);  squeeze_dim_89 = None\n",
      "        unsqueeze_default_47: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_44, 1);  index_tensor_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_45: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_91, [view_default]);  squeeze_dim_91 = None\n",
      "        unsqueeze_default_48: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_45, 1);  index_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_200: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_110, unsqueeze_default_47)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_228: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_229: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 64, 9223372036854775807);  transpose_int_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_44: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_229);  slice_tensor_229 = None\n",
      "        cat_default_88: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_44, slice_tensor_228], -1);  neg_default_44 = slice_tensor_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_201: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_88, unsqueeze_default_48);  cat_default_88 = None\n",
      "        add_tensor_155: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_200, mul_tensor_201);  mul_tensor_200 = mul_tensor_201 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_202: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_111, unsqueeze_default_47);  unsqueeze_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_230: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_231: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 64, 9223372036854775807);  transpose_int_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_45: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_231);  slice_tensor_231 = None\n",
      "        cat_default_89: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_45, slice_tensor_230], -1);  neg_default_45 = slice_tensor_230 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_203: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_89, unsqueeze_default_48);  cat_default_89 = unsqueeze_default_48 = None\n",
      "        add_tensor_156: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_202, mul_tensor_203);  mul_tensor_202 = mul_tensor_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_90: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg45, add_tensor_156], 2);  arg45 = add_tensor_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_91: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg46, transpose_int_112], 2);  arg46 = transpose_int_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_113: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_90, 2, 3)\n",
      "        expand_default_89: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_155, [1, 32, 1, 128]);  add_tensor_155 = None\n",
      "        view_default_538: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_89, [32, 1, 128]);  expand_default_89 = None\n",
      "        sym_size_67: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_90, 2)\n",
      "        expand_default_90: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_113, [1, 32, 128, sym_size_67]);  transpose_int_113 = None\n",
      "        view_default_539: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_90, [32, 128, sym_size_67]);  expand_default_90 = None\n",
      "        bmm_default_44: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_538, view_default_539);  view_default_538 = view_default_539 = None\n",
      "        view_default_540: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_44, [1, 32, 1, sym_size_67]);  bmm_default_44 = None\n",
      "        div_tensor_22: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_540, 11.313708498984761);  view_default_540 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_157: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_22, masked_fill_scalar);  div_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_22: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_157, -1, False);  add_tensor_157 = None\n",
      "        detach_default_67: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_22)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_91: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_22, [1, 32, 1, sym_size_67]);  _softmax_default_22 = None\n",
      "        view_default_541: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_91, [32, 1, sym_size_67]);  expand_default_91 = sym_size_67 = None\n",
      "        sym_size_68: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_91, 2)\n",
      "        expand_default_92: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_91, [1, 32, sym_size_68, 128])\n",
      "        view_default_542: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_92, [32, sym_size_68, 128]);  expand_default_92 = sym_size_68 = None\n",
      "        bmm_default_45: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_541, view_default_542);  view_default_541 = view_default_542 = None\n",
      "        view_default_543: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_45, [1, 32, 1, 128]);  bmm_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_114: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_543, 1, 2);  view_default_543 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_544: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_114, [1, 1, 4096]);  transpose_int_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant203 = self._param_constant203\n",
      "        t_default_157: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant203);  _param_constant203 = None\n",
      "        view_default_545: f32[1, 4096] = torch.ops.aten.view.default(view_default_544, [1, 4096]);  view_default_544 = None\n",
      "        mm_default_157: f32[1, 4096] = torch.ops.aten.mm.default(view_default_545, t_default_157);  view_default_545 = t_default_157 = None\n",
      "        view_default_546: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_157, [1, 1, 4096]);  mm_default_157 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_158: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_153, view_default_546);  add_tensor_153 = view_default_546 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_45: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_158, 2)\n",
      "        mean_dim_45: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_45, [-1], True);  pow_tensor_scalar_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_159: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_45, 1e-06);  mean_dim_45 = None\n",
      "        rsqrt_default_45: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_159);  add_tensor_159 = None\n",
      "        detach_default_68: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_45)\n",
      "        mul_tensor_204: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_158, rsqrt_default_45);  rsqrt_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant204 = self._param_constant204\n",
      "        mul_tensor_205: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant204, mul_tensor_204);  _param_constant204 = mul_tensor_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant205 = self._param_constant205\n",
      "        t_default_158: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant205);  _param_constant205 = None\n",
      "        view_default_547: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_205, [1, 4096])\n",
      "        mm_default_158: f32[1, 11008] = torch.ops.aten.mm.default(view_default_547, t_default_158);  view_default_547 = t_default_158 = None\n",
      "        view_default_548: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_158, [1, 1, 11008]);  mm_default_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_22: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_548);  view_default_548 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant206 = self._param_constant206\n",
      "        t_default_159: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant206);  _param_constant206 = None\n",
      "        view_default_549: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_205, [1, 4096]);  mul_tensor_205 = None\n",
      "        mm_default_159: f32[1, 11008] = torch.ops.aten.mm.default(view_default_549, t_default_159);  view_default_549 = t_default_159 = None\n",
      "        view_default_550: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_159, [1, 1, 11008]);  mm_default_159 = None\n",
      "        mul_tensor_206: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_22, view_default_550);  silu_default_22 = view_default_550 = None\n",
      "        _param_constant207 = self._param_constant207\n",
      "        t_default_160: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant207);  _param_constant207 = None\n",
      "        view_default_551: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_206, [1, 11008]);  mul_tensor_206 = None\n",
      "        mm_default_160: f32[1, 4096] = torch.ops.aten.mm.default(view_default_551, t_default_160);  view_default_551 = t_default_160 = None\n",
      "        view_default_552: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_160, [1, 1, 4096]);  mm_default_160 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_160: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_158, view_default_552);  add_tensor_158 = view_default_552 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_46: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_160, 2)\n",
      "        mean_dim_46: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_46, [-1], True);  pow_tensor_scalar_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_161: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_46, 1e-06);  mean_dim_46 = None\n",
      "        rsqrt_default_46: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_161);  add_tensor_161 = None\n",
      "        detach_default_69: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_46)\n",
      "        mul_tensor_207: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_160, rsqrt_default_46);  rsqrt_default_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant208 = self._param_constant208\n",
      "        mul_tensor_208: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant208, mul_tensor_207);  _param_constant208 = mul_tensor_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant209 = self._param_constant209\n",
      "        t_default_161: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant209);  _param_constant209 = None\n",
      "        view_default_553: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_208, [1, 4096])\n",
      "        mm_default_161: f32[1, 4096] = torch.ops.aten.mm.default(view_default_553, t_default_161);  view_default_553 = t_default_161 = None\n",
      "        view_default_554: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_161, [1, 1, 4096]);  mm_default_161 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant210 = self._param_constant210\n",
      "        t_default_162: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant210);  _param_constant210 = None\n",
      "        view_default_555: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_208, [1, 4096])\n",
      "        mm_default_162: f32[1, 4096] = torch.ops.aten.mm.default(view_default_555, t_default_162);  view_default_555 = t_default_162 = None\n",
      "        view_default_556: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_162, [1, 1, 4096]);  mm_default_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant211 = self._param_constant211\n",
      "        t_default_163: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant211);  _param_constant211 = None\n",
      "        view_default_557: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_208, [1, 4096]);  mul_tensor_208 = None\n",
      "        mm_default_163: f32[1, 4096] = torch.ops.aten.mm.default(view_default_557, t_default_163);  view_default_557 = t_default_163 = None\n",
      "        view_default_558: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_163, [1, 1, 4096]);  mm_default_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_559: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_554, [1, 1, 32, 128]);  view_default_554 = None\n",
      "        transpose_int_115: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_559, 1, 2);  view_default_559 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_560: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_556, [1, 1, 32, 128]);  view_default_556 = None\n",
      "        transpose_int_116: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_560, 1, 2);  view_default_560 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_561: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_558, [1, 1, 32, 128]);  view_default_558 = None\n",
      "        transpose_int_117: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_561, 1, 2);  view_default_561 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant46 = self._tensor_constant46\n",
      "        slice_tensor_232: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant46, 0, 0, 9223372036854775807);  _tensor_constant46 = None\n",
      "        slice_tensor_233: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_232, 1, 0, 9223372036854775807);  slice_tensor_232 = None\n",
      "        sym_size_69: Sym(s0) = torch.ops.aten.sym_size(arg47, 2)\n",
      "        add_25: Sym(s0 + 1) = 1 + sym_size_69;  sym_size_69 = None\n",
      "        slice_tensor_234: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_233, 2, 0, add_25);  slice_tensor_233 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant47 = self._tensor_constant47\n",
      "        slice_tensor_235: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant47, 0, 0, 9223372036854775807);  _tensor_constant47 = None\n",
      "        slice_tensor_236: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_235, 1, 0, 9223372036854775807);  slice_tensor_235 = None\n",
      "        slice_tensor_237: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_236, 2, 0, add_25);  slice_tensor_236 = add_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_92: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_234, 1);  slice_tensor_234 = None\n",
      "        squeeze_dim_93: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_92, 0);  squeeze_dim_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_94: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_237, 1);  slice_tensor_237 = None\n",
      "        squeeze_dim_95: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_94, 0);  squeeze_dim_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_46: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_93, [view_default]);  squeeze_dim_93 = None\n",
      "        unsqueeze_default_49: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_46, 1);  index_tensor_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_47: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_95, [view_default]);  squeeze_dim_95 = None\n",
      "        unsqueeze_default_50: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_47, 1);  index_tensor_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_209: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_115, unsqueeze_default_49)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_238: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_239: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 64, 9223372036854775807);  transpose_int_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_46: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_239);  slice_tensor_239 = None\n",
      "        cat_default_92: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_46, slice_tensor_238], -1);  neg_default_46 = slice_tensor_238 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_210: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_92, unsqueeze_default_50);  cat_default_92 = None\n",
      "        add_tensor_162: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_209, mul_tensor_210);  mul_tensor_209 = mul_tensor_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_211: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_116, unsqueeze_default_49);  unsqueeze_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_240: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_241: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 64, 9223372036854775807);  transpose_int_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_47: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_241);  slice_tensor_241 = None\n",
      "        cat_default_93: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_47, slice_tensor_240], -1);  neg_default_47 = slice_tensor_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_212: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_93, unsqueeze_default_50);  cat_default_93 = unsqueeze_default_50 = None\n",
      "        add_tensor_163: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_211, mul_tensor_212);  mul_tensor_211 = mul_tensor_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_94: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg47, add_tensor_163], 2);  arg47 = add_tensor_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_95: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg48, transpose_int_117], 2);  arg48 = transpose_int_117 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_118: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_94, 2, 3)\n",
      "        expand_default_93: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_162, [1, 32, 1, 128]);  add_tensor_162 = None\n",
      "        view_default_562: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_93, [32, 1, 128]);  expand_default_93 = None\n",
      "        sym_size_70: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_94, 2)\n",
      "        expand_default_94: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_118, [1, 32, 128, sym_size_70]);  transpose_int_118 = None\n",
      "        view_default_563: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_94, [32, 128, sym_size_70]);  expand_default_94 = None\n",
      "        bmm_default_46: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_562, view_default_563);  view_default_562 = view_default_563 = None\n",
      "        view_default_564: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_46, [1, 32, 1, sym_size_70]);  bmm_default_46 = None\n",
      "        div_tensor_23: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_564, 11.313708498984761);  view_default_564 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_164: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_23, masked_fill_scalar);  div_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_23: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_164, -1, False);  add_tensor_164 = None\n",
      "        detach_default_70: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_23)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_95: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_23, [1, 32, 1, sym_size_70]);  _softmax_default_23 = None\n",
      "        view_default_565: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_95, [32, 1, sym_size_70]);  expand_default_95 = sym_size_70 = None\n",
      "        sym_size_71: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_95, 2)\n",
      "        expand_default_96: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_95, [1, 32, sym_size_71, 128])\n",
      "        view_default_566: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_96, [32, sym_size_71, 128]);  expand_default_96 = sym_size_71 = None\n",
      "        bmm_default_47: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_565, view_default_566);  view_default_565 = view_default_566 = None\n",
      "        view_default_567: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_47, [1, 32, 1, 128]);  bmm_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_119: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_567, 1, 2);  view_default_567 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_568: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_119, [1, 1, 4096]);  transpose_int_119 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant212 = self._param_constant212\n",
      "        t_default_164: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant212);  _param_constant212 = None\n",
      "        view_default_569: f32[1, 4096] = torch.ops.aten.view.default(view_default_568, [1, 4096]);  view_default_568 = None\n",
      "        mm_default_164: f32[1, 4096] = torch.ops.aten.mm.default(view_default_569, t_default_164);  view_default_569 = t_default_164 = None\n",
      "        view_default_570: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_164, [1, 1, 4096]);  mm_default_164 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_165: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_160, view_default_570);  add_tensor_160 = view_default_570 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_47: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_165, 2)\n",
      "        mean_dim_47: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_47, [-1], True);  pow_tensor_scalar_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_166: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_47, 1e-06);  mean_dim_47 = None\n",
      "        rsqrt_default_47: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_166);  add_tensor_166 = None\n",
      "        detach_default_71: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_47)\n",
      "        mul_tensor_213: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_165, rsqrt_default_47);  rsqrt_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant213 = self._param_constant213\n",
      "        mul_tensor_214: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant213, mul_tensor_213);  _param_constant213 = mul_tensor_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant214 = self._param_constant214\n",
      "        t_default_165: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant214);  _param_constant214 = None\n",
      "        view_default_571: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_214, [1, 4096])\n",
      "        mm_default_165: f32[1, 11008] = torch.ops.aten.mm.default(view_default_571, t_default_165);  view_default_571 = t_default_165 = None\n",
      "        view_default_572: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_165, [1, 1, 11008]);  mm_default_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_23: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_572);  view_default_572 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant215 = self._param_constant215\n",
      "        t_default_166: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant215);  _param_constant215 = None\n",
      "        view_default_573: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_214, [1, 4096]);  mul_tensor_214 = None\n",
      "        mm_default_166: f32[1, 11008] = torch.ops.aten.mm.default(view_default_573, t_default_166);  view_default_573 = t_default_166 = None\n",
      "        view_default_574: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_166, [1, 1, 11008]);  mm_default_166 = None\n",
      "        mul_tensor_215: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_23, view_default_574);  silu_default_23 = view_default_574 = None\n",
      "        _param_constant216 = self._param_constant216\n",
      "        t_default_167: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant216);  _param_constant216 = None\n",
      "        view_default_575: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_215, [1, 11008]);  mul_tensor_215 = None\n",
      "        mm_default_167: f32[1, 4096] = torch.ops.aten.mm.default(view_default_575, t_default_167);  view_default_575 = t_default_167 = None\n",
      "        view_default_576: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_167, [1, 1, 4096]);  mm_default_167 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_167: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_165, view_default_576);  add_tensor_165 = view_default_576 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_48: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_167, 2)\n",
      "        mean_dim_48: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_48, [-1], True);  pow_tensor_scalar_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_168: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_48, 1e-06);  mean_dim_48 = None\n",
      "        rsqrt_default_48: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_168);  add_tensor_168 = None\n",
      "        detach_default_72: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_48)\n",
      "        mul_tensor_216: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_167, rsqrt_default_48);  rsqrt_default_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant217 = self._param_constant217\n",
      "        mul_tensor_217: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant217, mul_tensor_216);  _param_constant217 = mul_tensor_216 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant218 = self._param_constant218\n",
      "        t_default_168: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant218);  _param_constant218 = None\n",
      "        view_default_577: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_217, [1, 4096])\n",
      "        mm_default_168: f32[1, 4096] = torch.ops.aten.mm.default(view_default_577, t_default_168);  view_default_577 = t_default_168 = None\n",
      "        view_default_578: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_168, [1, 1, 4096]);  mm_default_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant219 = self._param_constant219\n",
      "        t_default_169: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant219);  _param_constant219 = None\n",
      "        view_default_579: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_217, [1, 4096])\n",
      "        mm_default_169: f32[1, 4096] = torch.ops.aten.mm.default(view_default_579, t_default_169);  view_default_579 = t_default_169 = None\n",
      "        view_default_580: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_169, [1, 1, 4096]);  mm_default_169 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant220 = self._param_constant220\n",
      "        t_default_170: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant220);  _param_constant220 = None\n",
      "        view_default_581: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_217, [1, 4096]);  mul_tensor_217 = None\n",
      "        mm_default_170: f32[1, 4096] = torch.ops.aten.mm.default(view_default_581, t_default_170);  view_default_581 = t_default_170 = None\n",
      "        view_default_582: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_170, [1, 1, 4096]);  mm_default_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_583: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_578, [1, 1, 32, 128]);  view_default_578 = None\n",
      "        transpose_int_120: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_583, 1, 2);  view_default_583 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_584: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_580, [1, 1, 32, 128]);  view_default_580 = None\n",
      "        transpose_int_121: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_584, 1, 2);  view_default_584 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_585: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_582, [1, 1, 32, 128]);  view_default_582 = None\n",
      "        transpose_int_122: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_585, 1, 2);  view_default_585 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant48 = self._tensor_constant48\n",
      "        slice_tensor_242: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant48, 0, 0, 9223372036854775807);  _tensor_constant48 = None\n",
      "        slice_tensor_243: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_242, 1, 0, 9223372036854775807);  slice_tensor_242 = None\n",
      "        sym_size_72: Sym(s0) = torch.ops.aten.sym_size(arg49, 2)\n",
      "        add_26: Sym(s0 + 1) = 1 + sym_size_72;  sym_size_72 = None\n",
      "        slice_tensor_244: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_243, 2, 0, add_26);  slice_tensor_243 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant49 = self._tensor_constant49\n",
      "        slice_tensor_245: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant49, 0, 0, 9223372036854775807);  _tensor_constant49 = None\n",
      "        slice_tensor_246: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_245, 1, 0, 9223372036854775807);  slice_tensor_245 = None\n",
      "        slice_tensor_247: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_246, 2, 0, add_26);  slice_tensor_246 = add_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_96: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_244, 1);  slice_tensor_244 = None\n",
      "        squeeze_dim_97: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_96, 0);  squeeze_dim_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_98: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_247, 1);  slice_tensor_247 = None\n",
      "        squeeze_dim_99: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_98, 0);  squeeze_dim_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_48: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_97, [view_default]);  squeeze_dim_97 = None\n",
      "        unsqueeze_default_51: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_48, 1);  index_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_49: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_99, [view_default]);  squeeze_dim_99 = None\n",
      "        unsqueeze_default_52: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_49, 1);  index_tensor_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_218: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_120, unsqueeze_default_51)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_248: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_249: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 64, 9223372036854775807);  transpose_int_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_48: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_249);  slice_tensor_249 = None\n",
      "        cat_default_96: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_48, slice_tensor_248], -1);  neg_default_48 = slice_tensor_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_219: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_96, unsqueeze_default_52);  cat_default_96 = None\n",
      "        add_tensor_169: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_218, mul_tensor_219);  mul_tensor_218 = mul_tensor_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_220: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_121, unsqueeze_default_51);  unsqueeze_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_250: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_251: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 64, 9223372036854775807);  transpose_int_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_49: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_251);  slice_tensor_251 = None\n",
      "        cat_default_97: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_49, slice_tensor_250], -1);  neg_default_49 = slice_tensor_250 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_221: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_97, unsqueeze_default_52);  cat_default_97 = unsqueeze_default_52 = None\n",
      "        add_tensor_170: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_220, mul_tensor_221);  mul_tensor_220 = mul_tensor_221 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_98: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg49, add_tensor_170], 2);  arg49 = add_tensor_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_99: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg50, transpose_int_122], 2);  arg50 = transpose_int_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_123: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_98, 2, 3)\n",
      "        expand_default_97: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_169, [1, 32, 1, 128]);  add_tensor_169 = None\n",
      "        view_default_586: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_97, [32, 1, 128]);  expand_default_97 = None\n",
      "        sym_size_73: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_98, 2)\n",
      "        expand_default_98: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_123, [1, 32, 128, sym_size_73]);  transpose_int_123 = None\n",
      "        view_default_587: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_98, [32, 128, sym_size_73]);  expand_default_98 = None\n",
      "        bmm_default_48: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_586, view_default_587);  view_default_586 = view_default_587 = None\n",
      "        view_default_588: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_48, [1, 32, 1, sym_size_73]);  bmm_default_48 = None\n",
      "        div_tensor_24: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_588, 11.313708498984761);  view_default_588 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_171: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_24, masked_fill_scalar);  div_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_24: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_171, -1, False);  add_tensor_171 = None\n",
      "        detach_default_73: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_24)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_99: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_24, [1, 32, 1, sym_size_73]);  _softmax_default_24 = None\n",
      "        view_default_589: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_99, [32, 1, sym_size_73]);  expand_default_99 = sym_size_73 = None\n",
      "        sym_size_74: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_99, 2)\n",
      "        expand_default_100: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_99, [1, 32, sym_size_74, 128])\n",
      "        view_default_590: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_100, [32, sym_size_74, 128]);  expand_default_100 = sym_size_74 = None\n",
      "        bmm_default_49: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_589, view_default_590);  view_default_589 = view_default_590 = None\n",
      "        view_default_591: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_49, [1, 32, 1, 128]);  bmm_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_124: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_591, 1, 2);  view_default_591 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_592: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_124, [1, 1, 4096]);  transpose_int_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant221 = self._param_constant221\n",
      "        t_default_171: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant221);  _param_constant221 = None\n",
      "        view_default_593: f32[1, 4096] = torch.ops.aten.view.default(view_default_592, [1, 4096]);  view_default_592 = None\n",
      "        mm_default_171: f32[1, 4096] = torch.ops.aten.mm.default(view_default_593, t_default_171);  view_default_593 = t_default_171 = None\n",
      "        view_default_594: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_171, [1, 1, 4096]);  mm_default_171 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_172: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_167, view_default_594);  add_tensor_167 = view_default_594 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_49: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_172, 2)\n",
      "        mean_dim_49: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_49, [-1], True);  pow_tensor_scalar_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_173: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_49, 1e-06);  mean_dim_49 = None\n",
      "        rsqrt_default_49: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_173);  add_tensor_173 = None\n",
      "        detach_default_74: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_49)\n",
      "        mul_tensor_222: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_172, rsqrt_default_49);  rsqrt_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant222 = self._param_constant222\n",
      "        mul_tensor_223: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant222, mul_tensor_222);  _param_constant222 = mul_tensor_222 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant223 = self._param_constant223\n",
      "        t_default_172: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant223);  _param_constant223 = None\n",
      "        view_default_595: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_223, [1, 4096])\n",
      "        mm_default_172: f32[1, 11008] = torch.ops.aten.mm.default(view_default_595, t_default_172);  view_default_595 = t_default_172 = None\n",
      "        view_default_596: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_172, [1, 1, 11008]);  mm_default_172 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_24: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_596);  view_default_596 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant224 = self._param_constant224\n",
      "        t_default_173: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant224);  _param_constant224 = None\n",
      "        view_default_597: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_223, [1, 4096]);  mul_tensor_223 = None\n",
      "        mm_default_173: f32[1, 11008] = torch.ops.aten.mm.default(view_default_597, t_default_173);  view_default_597 = t_default_173 = None\n",
      "        view_default_598: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_173, [1, 1, 11008]);  mm_default_173 = None\n",
      "        mul_tensor_224: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_24, view_default_598);  silu_default_24 = view_default_598 = None\n",
      "        _param_constant225 = self._param_constant225\n",
      "        t_default_174: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant225);  _param_constant225 = None\n",
      "        view_default_599: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_224, [1, 11008]);  mul_tensor_224 = None\n",
      "        mm_default_174: f32[1, 4096] = torch.ops.aten.mm.default(view_default_599, t_default_174);  view_default_599 = t_default_174 = None\n",
      "        view_default_600: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_174, [1, 1, 4096]);  mm_default_174 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_174: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_172, view_default_600);  add_tensor_172 = view_default_600 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_50: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_174, 2)\n",
      "        mean_dim_50: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_50, [-1], True);  pow_tensor_scalar_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_175: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_50, 1e-06);  mean_dim_50 = None\n",
      "        rsqrt_default_50: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_175);  add_tensor_175 = None\n",
      "        detach_default_75: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_50)\n",
      "        mul_tensor_225: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_174, rsqrt_default_50);  rsqrt_default_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant226 = self._param_constant226\n",
      "        mul_tensor_226: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant226, mul_tensor_225);  _param_constant226 = mul_tensor_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant227 = self._param_constant227\n",
      "        t_default_175: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant227);  _param_constant227 = None\n",
      "        view_default_601: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_226, [1, 4096])\n",
      "        mm_default_175: f32[1, 4096] = torch.ops.aten.mm.default(view_default_601, t_default_175);  view_default_601 = t_default_175 = None\n",
      "        view_default_602: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_175, [1, 1, 4096]);  mm_default_175 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant228 = self._param_constant228\n",
      "        t_default_176: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant228);  _param_constant228 = None\n",
      "        view_default_603: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_226, [1, 4096])\n",
      "        mm_default_176: f32[1, 4096] = torch.ops.aten.mm.default(view_default_603, t_default_176);  view_default_603 = t_default_176 = None\n",
      "        view_default_604: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_176, [1, 1, 4096]);  mm_default_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant229 = self._param_constant229\n",
      "        t_default_177: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant229);  _param_constant229 = None\n",
      "        view_default_605: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_226, [1, 4096]);  mul_tensor_226 = None\n",
      "        mm_default_177: f32[1, 4096] = torch.ops.aten.mm.default(view_default_605, t_default_177);  view_default_605 = t_default_177 = None\n",
      "        view_default_606: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_177, [1, 1, 4096]);  mm_default_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_607: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_602, [1, 1, 32, 128]);  view_default_602 = None\n",
      "        transpose_int_125: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_607, 1, 2);  view_default_607 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_608: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_604, [1, 1, 32, 128]);  view_default_604 = None\n",
      "        transpose_int_126: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_608, 1, 2);  view_default_608 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_609: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_606, [1, 1, 32, 128]);  view_default_606 = None\n",
      "        transpose_int_127: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_609, 1, 2);  view_default_609 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant50 = self._tensor_constant50\n",
      "        slice_tensor_252: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant50, 0, 0, 9223372036854775807);  _tensor_constant50 = None\n",
      "        slice_tensor_253: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_252, 1, 0, 9223372036854775807);  slice_tensor_252 = None\n",
      "        sym_size_75: Sym(s0) = torch.ops.aten.sym_size(arg51, 2)\n",
      "        add_27: Sym(s0 + 1) = 1 + sym_size_75;  sym_size_75 = None\n",
      "        slice_tensor_254: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_253, 2, 0, add_27);  slice_tensor_253 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant51 = self._tensor_constant51\n",
      "        slice_tensor_255: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant51, 0, 0, 9223372036854775807);  _tensor_constant51 = None\n",
      "        slice_tensor_256: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_255, 1, 0, 9223372036854775807);  slice_tensor_255 = None\n",
      "        slice_tensor_257: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_256, 2, 0, add_27);  slice_tensor_256 = add_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_100: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_254, 1);  slice_tensor_254 = None\n",
      "        squeeze_dim_101: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_100, 0);  squeeze_dim_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_102: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_257, 1);  slice_tensor_257 = None\n",
      "        squeeze_dim_103: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_102, 0);  squeeze_dim_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_50: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_101, [view_default]);  squeeze_dim_101 = None\n",
      "        unsqueeze_default_53: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_50, 1);  index_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_51: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_103, [view_default]);  squeeze_dim_103 = None\n",
      "        unsqueeze_default_54: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_51, 1);  index_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_227: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_125, unsqueeze_default_53)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_258: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_259: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 64, 9223372036854775807);  transpose_int_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_50: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_259);  slice_tensor_259 = None\n",
      "        cat_default_100: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_50, slice_tensor_258], -1);  neg_default_50 = slice_tensor_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_228: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_100, unsqueeze_default_54);  cat_default_100 = None\n",
      "        add_tensor_176: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_227, mul_tensor_228);  mul_tensor_227 = mul_tensor_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_229: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_126, unsqueeze_default_53);  unsqueeze_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_260: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_261: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 64, 9223372036854775807);  transpose_int_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_51: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_261);  slice_tensor_261 = None\n",
      "        cat_default_101: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_51, slice_tensor_260], -1);  neg_default_51 = slice_tensor_260 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_230: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_101, unsqueeze_default_54);  cat_default_101 = unsqueeze_default_54 = None\n",
      "        add_tensor_177: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_229, mul_tensor_230);  mul_tensor_229 = mul_tensor_230 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_102: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg51, add_tensor_177], 2);  arg51 = add_tensor_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_103: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg52, transpose_int_127], 2);  arg52 = transpose_int_127 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_128: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_102, 2, 3)\n",
      "        expand_default_101: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_176, [1, 32, 1, 128]);  add_tensor_176 = None\n",
      "        view_default_610: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_101, [32, 1, 128]);  expand_default_101 = None\n",
      "        sym_size_76: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_102, 2)\n",
      "        expand_default_102: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_128, [1, 32, 128, sym_size_76]);  transpose_int_128 = None\n",
      "        view_default_611: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_102, [32, 128, sym_size_76]);  expand_default_102 = None\n",
      "        bmm_default_50: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_610, view_default_611);  view_default_610 = view_default_611 = None\n",
      "        view_default_612: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_50, [1, 32, 1, sym_size_76]);  bmm_default_50 = None\n",
      "        div_tensor_25: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_612, 11.313708498984761);  view_default_612 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_178: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_25, masked_fill_scalar);  div_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_25: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_178, -1, False);  add_tensor_178 = None\n",
      "        detach_default_76: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_25)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_103: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_25, [1, 32, 1, sym_size_76]);  _softmax_default_25 = None\n",
      "        view_default_613: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_103, [32, 1, sym_size_76]);  expand_default_103 = sym_size_76 = None\n",
      "        sym_size_77: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_103, 2)\n",
      "        expand_default_104: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_103, [1, 32, sym_size_77, 128])\n",
      "        view_default_614: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_104, [32, sym_size_77, 128]);  expand_default_104 = sym_size_77 = None\n",
      "        bmm_default_51: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_613, view_default_614);  view_default_613 = view_default_614 = None\n",
      "        view_default_615: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_51, [1, 32, 1, 128]);  bmm_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_129: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_615, 1, 2);  view_default_615 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_616: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_129, [1, 1, 4096]);  transpose_int_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant230 = self._param_constant230\n",
      "        t_default_178: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant230);  _param_constant230 = None\n",
      "        view_default_617: f32[1, 4096] = torch.ops.aten.view.default(view_default_616, [1, 4096]);  view_default_616 = None\n",
      "        mm_default_178: f32[1, 4096] = torch.ops.aten.mm.default(view_default_617, t_default_178);  view_default_617 = t_default_178 = None\n",
      "        view_default_618: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_178, [1, 1, 4096]);  mm_default_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_179: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_174, view_default_618);  add_tensor_174 = view_default_618 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_51: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_179, 2)\n",
      "        mean_dim_51: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_51, [-1], True);  pow_tensor_scalar_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_180: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_51, 1e-06);  mean_dim_51 = None\n",
      "        rsqrt_default_51: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_180);  add_tensor_180 = None\n",
      "        detach_default_77: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_51)\n",
      "        mul_tensor_231: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_179, rsqrt_default_51);  rsqrt_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant231 = self._param_constant231\n",
      "        mul_tensor_232: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant231, mul_tensor_231);  _param_constant231 = mul_tensor_231 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant232 = self._param_constant232\n",
      "        t_default_179: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant232);  _param_constant232 = None\n",
      "        view_default_619: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_232, [1, 4096])\n",
      "        mm_default_179: f32[1, 11008] = torch.ops.aten.mm.default(view_default_619, t_default_179);  view_default_619 = t_default_179 = None\n",
      "        view_default_620: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_179, [1, 1, 11008]);  mm_default_179 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_25: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_620);  view_default_620 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant233 = self._param_constant233\n",
      "        t_default_180: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant233);  _param_constant233 = None\n",
      "        view_default_621: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_232, [1, 4096]);  mul_tensor_232 = None\n",
      "        mm_default_180: f32[1, 11008] = torch.ops.aten.mm.default(view_default_621, t_default_180);  view_default_621 = t_default_180 = None\n",
      "        view_default_622: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_180, [1, 1, 11008]);  mm_default_180 = None\n",
      "        mul_tensor_233: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_25, view_default_622);  silu_default_25 = view_default_622 = None\n",
      "        _param_constant234 = self._param_constant234\n",
      "        t_default_181: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant234);  _param_constant234 = None\n",
      "        view_default_623: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_233, [1, 11008]);  mul_tensor_233 = None\n",
      "        mm_default_181: f32[1, 4096] = torch.ops.aten.mm.default(view_default_623, t_default_181);  view_default_623 = t_default_181 = None\n",
      "        view_default_624: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_181, [1, 1, 4096]);  mm_default_181 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_181: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_179, view_default_624);  add_tensor_179 = view_default_624 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_52: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_181, 2)\n",
      "        mean_dim_52: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_52, [-1], True);  pow_tensor_scalar_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_182: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_52, 1e-06);  mean_dim_52 = None\n",
      "        rsqrt_default_52: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_182);  add_tensor_182 = None\n",
      "        detach_default_78: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_52)\n",
      "        mul_tensor_234: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_181, rsqrt_default_52);  rsqrt_default_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant235 = self._param_constant235\n",
      "        mul_tensor_235: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant235, mul_tensor_234);  _param_constant235 = mul_tensor_234 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant236 = self._param_constant236\n",
      "        t_default_182: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant236);  _param_constant236 = None\n",
      "        view_default_625: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_235, [1, 4096])\n",
      "        mm_default_182: f32[1, 4096] = torch.ops.aten.mm.default(view_default_625, t_default_182);  view_default_625 = t_default_182 = None\n",
      "        view_default_626: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_182, [1, 1, 4096]);  mm_default_182 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant237 = self._param_constant237\n",
      "        t_default_183: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant237);  _param_constant237 = None\n",
      "        view_default_627: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_235, [1, 4096])\n",
      "        mm_default_183: f32[1, 4096] = torch.ops.aten.mm.default(view_default_627, t_default_183);  view_default_627 = t_default_183 = None\n",
      "        view_default_628: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_183, [1, 1, 4096]);  mm_default_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant238 = self._param_constant238\n",
      "        t_default_184: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant238);  _param_constant238 = None\n",
      "        view_default_629: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_235, [1, 4096]);  mul_tensor_235 = None\n",
      "        mm_default_184: f32[1, 4096] = torch.ops.aten.mm.default(view_default_629, t_default_184);  view_default_629 = t_default_184 = None\n",
      "        view_default_630: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_184, [1, 1, 4096]);  mm_default_184 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_631: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_626, [1, 1, 32, 128]);  view_default_626 = None\n",
      "        transpose_int_130: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_631, 1, 2);  view_default_631 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_632: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_628, [1, 1, 32, 128]);  view_default_628 = None\n",
      "        transpose_int_131: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_632, 1, 2);  view_default_632 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_633: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_630, [1, 1, 32, 128]);  view_default_630 = None\n",
      "        transpose_int_132: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_633, 1, 2);  view_default_633 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant52 = self._tensor_constant52\n",
      "        slice_tensor_262: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant52, 0, 0, 9223372036854775807);  _tensor_constant52 = None\n",
      "        slice_tensor_263: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_262, 1, 0, 9223372036854775807);  slice_tensor_262 = None\n",
      "        sym_size_78: Sym(s0) = torch.ops.aten.sym_size(arg53, 2)\n",
      "        add_28: Sym(s0 + 1) = 1 + sym_size_78;  sym_size_78 = None\n",
      "        slice_tensor_264: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_263, 2, 0, add_28);  slice_tensor_263 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant53 = self._tensor_constant53\n",
      "        slice_tensor_265: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant53, 0, 0, 9223372036854775807);  _tensor_constant53 = None\n",
      "        slice_tensor_266: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_265, 1, 0, 9223372036854775807);  slice_tensor_265 = None\n",
      "        slice_tensor_267: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_266, 2, 0, add_28);  slice_tensor_266 = add_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_104: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_264, 1);  slice_tensor_264 = None\n",
      "        squeeze_dim_105: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_104, 0);  squeeze_dim_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_106: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_267, 1);  slice_tensor_267 = None\n",
      "        squeeze_dim_107: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_106, 0);  squeeze_dim_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_52: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_105, [view_default]);  squeeze_dim_105 = None\n",
      "        unsqueeze_default_55: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_52, 1);  index_tensor_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_53: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_107, [view_default]);  squeeze_dim_107 = None\n",
      "        unsqueeze_default_56: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_53, 1);  index_tensor_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_236: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_130, unsqueeze_default_55)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_268: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_269: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 64, 9223372036854775807);  transpose_int_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_52: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_269);  slice_tensor_269 = None\n",
      "        cat_default_104: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_52, slice_tensor_268], -1);  neg_default_52 = slice_tensor_268 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_237: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_104, unsqueeze_default_56);  cat_default_104 = None\n",
      "        add_tensor_183: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_236, mul_tensor_237);  mul_tensor_236 = mul_tensor_237 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_238: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_131, unsqueeze_default_55);  unsqueeze_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_270: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_271: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 64, 9223372036854775807);  transpose_int_131 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_53: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_271);  slice_tensor_271 = None\n",
      "        cat_default_105: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_53, slice_tensor_270], -1);  neg_default_53 = slice_tensor_270 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_239: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_105, unsqueeze_default_56);  cat_default_105 = unsqueeze_default_56 = None\n",
      "        add_tensor_184: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_238, mul_tensor_239);  mul_tensor_238 = mul_tensor_239 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_106: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg53, add_tensor_184], 2);  arg53 = add_tensor_184 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_107: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg54, transpose_int_132], 2);  arg54 = transpose_int_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_133: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_106, 2, 3)\n",
      "        expand_default_105: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_183, [1, 32, 1, 128]);  add_tensor_183 = None\n",
      "        view_default_634: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_105, [32, 1, 128]);  expand_default_105 = None\n",
      "        sym_size_79: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_106, 2)\n",
      "        expand_default_106: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_133, [1, 32, 128, sym_size_79]);  transpose_int_133 = None\n",
      "        view_default_635: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_106, [32, 128, sym_size_79]);  expand_default_106 = None\n",
      "        bmm_default_52: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_634, view_default_635);  view_default_634 = view_default_635 = None\n",
      "        view_default_636: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_52, [1, 32, 1, sym_size_79]);  bmm_default_52 = None\n",
      "        div_tensor_26: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_636, 11.313708498984761);  view_default_636 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_185: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_26, masked_fill_scalar);  div_tensor_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_26: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_185, -1, False);  add_tensor_185 = None\n",
      "        detach_default_79: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_26)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_107: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_26, [1, 32, 1, sym_size_79]);  _softmax_default_26 = None\n",
      "        view_default_637: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_107, [32, 1, sym_size_79]);  expand_default_107 = sym_size_79 = None\n",
      "        sym_size_80: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_107, 2)\n",
      "        expand_default_108: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_107, [1, 32, sym_size_80, 128])\n",
      "        view_default_638: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_108, [32, sym_size_80, 128]);  expand_default_108 = sym_size_80 = None\n",
      "        bmm_default_53: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_637, view_default_638);  view_default_637 = view_default_638 = None\n",
      "        view_default_639: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_53, [1, 32, 1, 128]);  bmm_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_134: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_639, 1, 2);  view_default_639 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_640: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_134, [1, 1, 4096]);  transpose_int_134 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant239 = self._param_constant239\n",
      "        t_default_185: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant239);  _param_constant239 = None\n",
      "        view_default_641: f32[1, 4096] = torch.ops.aten.view.default(view_default_640, [1, 4096]);  view_default_640 = None\n",
      "        mm_default_185: f32[1, 4096] = torch.ops.aten.mm.default(view_default_641, t_default_185);  view_default_641 = t_default_185 = None\n",
      "        view_default_642: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_185, [1, 1, 4096]);  mm_default_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_186: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_181, view_default_642);  add_tensor_181 = view_default_642 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_53: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_186, 2)\n",
      "        mean_dim_53: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_53, [-1], True);  pow_tensor_scalar_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_187: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_53, 1e-06);  mean_dim_53 = None\n",
      "        rsqrt_default_53: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_187);  add_tensor_187 = None\n",
      "        detach_default_80: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_53)\n",
      "        mul_tensor_240: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_186, rsqrt_default_53);  rsqrt_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant240 = self._param_constant240\n",
      "        mul_tensor_241: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant240, mul_tensor_240);  _param_constant240 = mul_tensor_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant241 = self._param_constant241\n",
      "        t_default_186: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant241);  _param_constant241 = None\n",
      "        view_default_643: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_241, [1, 4096])\n",
      "        mm_default_186: f32[1, 11008] = torch.ops.aten.mm.default(view_default_643, t_default_186);  view_default_643 = t_default_186 = None\n",
      "        view_default_644: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_186, [1, 1, 11008]);  mm_default_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_26: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_644);  view_default_644 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant242 = self._param_constant242\n",
      "        t_default_187: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant242);  _param_constant242 = None\n",
      "        view_default_645: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_241, [1, 4096]);  mul_tensor_241 = None\n",
      "        mm_default_187: f32[1, 11008] = torch.ops.aten.mm.default(view_default_645, t_default_187);  view_default_645 = t_default_187 = None\n",
      "        view_default_646: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_187, [1, 1, 11008]);  mm_default_187 = None\n",
      "        mul_tensor_242: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_26, view_default_646);  silu_default_26 = view_default_646 = None\n",
      "        _param_constant243 = self._param_constant243\n",
      "        t_default_188: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant243);  _param_constant243 = None\n",
      "        view_default_647: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_242, [1, 11008]);  mul_tensor_242 = None\n",
      "        mm_default_188: f32[1, 4096] = torch.ops.aten.mm.default(view_default_647, t_default_188);  view_default_647 = t_default_188 = None\n",
      "        view_default_648: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_188, [1, 1, 4096]);  mm_default_188 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_188: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_186, view_default_648);  add_tensor_186 = view_default_648 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_54: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_188, 2)\n",
      "        mean_dim_54: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_54, [-1], True);  pow_tensor_scalar_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_189: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_54, 1e-06);  mean_dim_54 = None\n",
      "        rsqrt_default_54: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_189);  add_tensor_189 = None\n",
      "        detach_default_81: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_54)\n",
      "        mul_tensor_243: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_188, rsqrt_default_54);  rsqrt_default_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant244 = self._param_constant244\n",
      "        mul_tensor_244: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant244, mul_tensor_243);  _param_constant244 = mul_tensor_243 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant245 = self._param_constant245\n",
      "        t_default_189: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant245);  _param_constant245 = None\n",
      "        view_default_649: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_244, [1, 4096])\n",
      "        mm_default_189: f32[1, 4096] = torch.ops.aten.mm.default(view_default_649, t_default_189);  view_default_649 = t_default_189 = None\n",
      "        view_default_650: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_189, [1, 1, 4096]);  mm_default_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant246 = self._param_constant246\n",
      "        t_default_190: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant246);  _param_constant246 = None\n",
      "        view_default_651: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_244, [1, 4096])\n",
      "        mm_default_190: f32[1, 4096] = torch.ops.aten.mm.default(view_default_651, t_default_190);  view_default_651 = t_default_190 = None\n",
      "        view_default_652: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_190, [1, 1, 4096]);  mm_default_190 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant247 = self._param_constant247\n",
      "        t_default_191: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant247);  _param_constant247 = None\n",
      "        view_default_653: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_244, [1, 4096]);  mul_tensor_244 = None\n",
      "        mm_default_191: f32[1, 4096] = torch.ops.aten.mm.default(view_default_653, t_default_191);  view_default_653 = t_default_191 = None\n",
      "        view_default_654: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_191, [1, 1, 4096]);  mm_default_191 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_655: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_650, [1, 1, 32, 128]);  view_default_650 = None\n",
      "        transpose_int_135: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_655, 1, 2);  view_default_655 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_656: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_652, [1, 1, 32, 128]);  view_default_652 = None\n",
      "        transpose_int_136: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_656, 1, 2);  view_default_656 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_657: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_654, [1, 1, 32, 128]);  view_default_654 = None\n",
      "        transpose_int_137: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_657, 1, 2);  view_default_657 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant54 = self._tensor_constant54\n",
      "        slice_tensor_272: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant54, 0, 0, 9223372036854775807);  _tensor_constant54 = None\n",
      "        slice_tensor_273: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_272, 1, 0, 9223372036854775807);  slice_tensor_272 = None\n",
      "        sym_size_81: Sym(s0) = torch.ops.aten.sym_size(arg55, 2)\n",
      "        add_29: Sym(s0 + 1) = 1 + sym_size_81;  sym_size_81 = None\n",
      "        slice_tensor_274: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_273, 2, 0, add_29);  slice_tensor_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant55 = self._tensor_constant55\n",
      "        slice_tensor_275: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant55, 0, 0, 9223372036854775807);  _tensor_constant55 = None\n",
      "        slice_tensor_276: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_275, 1, 0, 9223372036854775807);  slice_tensor_275 = None\n",
      "        slice_tensor_277: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_276, 2, 0, add_29);  slice_tensor_276 = add_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_108: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_274, 1);  slice_tensor_274 = None\n",
      "        squeeze_dim_109: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_108, 0);  squeeze_dim_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_110: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_277, 1);  slice_tensor_277 = None\n",
      "        squeeze_dim_111: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_110, 0);  squeeze_dim_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_54: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_109, [view_default]);  squeeze_dim_109 = None\n",
      "        unsqueeze_default_57: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_54, 1);  index_tensor_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_55: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_111, [view_default]);  squeeze_dim_111 = None\n",
      "        unsqueeze_default_58: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_55, 1);  index_tensor_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_245: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_135, unsqueeze_default_57)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_278: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_279: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 64, 9223372036854775807);  transpose_int_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_54: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_279);  slice_tensor_279 = None\n",
      "        cat_default_108: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_54, slice_tensor_278], -1);  neg_default_54 = slice_tensor_278 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_246: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_108, unsqueeze_default_58);  cat_default_108 = None\n",
      "        add_tensor_190: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_245, mul_tensor_246);  mul_tensor_245 = mul_tensor_246 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_247: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_136, unsqueeze_default_57);  unsqueeze_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_280: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_281: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 64, 9223372036854775807);  transpose_int_136 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_55: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_281);  slice_tensor_281 = None\n",
      "        cat_default_109: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_55, slice_tensor_280], -1);  neg_default_55 = slice_tensor_280 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_248: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_109, unsqueeze_default_58);  cat_default_109 = unsqueeze_default_58 = None\n",
      "        add_tensor_191: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_247, mul_tensor_248);  mul_tensor_247 = mul_tensor_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_110: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg55, add_tensor_191], 2);  arg55 = add_tensor_191 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_111: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg56, transpose_int_137], 2);  arg56 = transpose_int_137 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_138: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_110, 2, 3)\n",
      "        expand_default_109: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_190, [1, 32, 1, 128]);  add_tensor_190 = None\n",
      "        view_default_658: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_109, [32, 1, 128]);  expand_default_109 = None\n",
      "        sym_size_82: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_110, 2)\n",
      "        expand_default_110: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_138, [1, 32, 128, sym_size_82]);  transpose_int_138 = None\n",
      "        view_default_659: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_110, [32, 128, sym_size_82]);  expand_default_110 = None\n",
      "        bmm_default_54: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_658, view_default_659);  view_default_658 = view_default_659 = None\n",
      "        view_default_660: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_54, [1, 32, 1, sym_size_82]);  bmm_default_54 = None\n",
      "        div_tensor_27: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_660, 11.313708498984761);  view_default_660 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_192: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_27, masked_fill_scalar);  div_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_27: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_192, -1, False);  add_tensor_192 = None\n",
      "        detach_default_82: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_27)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_111: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_27, [1, 32, 1, sym_size_82]);  _softmax_default_27 = None\n",
      "        view_default_661: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_111, [32, 1, sym_size_82]);  expand_default_111 = sym_size_82 = None\n",
      "        sym_size_83: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_111, 2)\n",
      "        expand_default_112: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_111, [1, 32, sym_size_83, 128])\n",
      "        view_default_662: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_112, [32, sym_size_83, 128]);  expand_default_112 = sym_size_83 = None\n",
      "        bmm_default_55: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_661, view_default_662);  view_default_661 = view_default_662 = None\n",
      "        view_default_663: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_55, [1, 32, 1, 128]);  bmm_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_139: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_663, 1, 2);  view_default_663 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_664: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_139, [1, 1, 4096]);  transpose_int_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant248 = self._param_constant248\n",
      "        t_default_192: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant248);  _param_constant248 = None\n",
      "        view_default_665: f32[1, 4096] = torch.ops.aten.view.default(view_default_664, [1, 4096]);  view_default_664 = None\n",
      "        mm_default_192: f32[1, 4096] = torch.ops.aten.mm.default(view_default_665, t_default_192);  view_default_665 = t_default_192 = None\n",
      "        view_default_666: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_192, [1, 1, 4096]);  mm_default_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_193: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_188, view_default_666);  add_tensor_188 = view_default_666 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_55: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_193, 2)\n",
      "        mean_dim_55: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_55, [-1], True);  pow_tensor_scalar_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_194: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_55, 1e-06);  mean_dim_55 = None\n",
      "        rsqrt_default_55: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_194);  add_tensor_194 = None\n",
      "        detach_default_83: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_55)\n",
      "        mul_tensor_249: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_193, rsqrt_default_55);  rsqrt_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant249 = self._param_constant249\n",
      "        mul_tensor_250: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant249, mul_tensor_249);  _param_constant249 = mul_tensor_249 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant250 = self._param_constant250\n",
      "        t_default_193: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant250);  _param_constant250 = None\n",
      "        view_default_667: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_250, [1, 4096])\n",
      "        mm_default_193: f32[1, 11008] = torch.ops.aten.mm.default(view_default_667, t_default_193);  view_default_667 = t_default_193 = None\n",
      "        view_default_668: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_193, [1, 1, 11008]);  mm_default_193 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_27: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_668);  view_default_668 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant251 = self._param_constant251\n",
      "        t_default_194: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant251);  _param_constant251 = None\n",
      "        view_default_669: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_250, [1, 4096]);  mul_tensor_250 = None\n",
      "        mm_default_194: f32[1, 11008] = torch.ops.aten.mm.default(view_default_669, t_default_194);  view_default_669 = t_default_194 = None\n",
      "        view_default_670: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_194, [1, 1, 11008]);  mm_default_194 = None\n",
      "        mul_tensor_251: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_27, view_default_670);  silu_default_27 = view_default_670 = None\n",
      "        _param_constant252 = self._param_constant252\n",
      "        t_default_195: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant252);  _param_constant252 = None\n",
      "        view_default_671: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_251, [1, 11008]);  mul_tensor_251 = None\n",
      "        mm_default_195: f32[1, 4096] = torch.ops.aten.mm.default(view_default_671, t_default_195);  view_default_671 = t_default_195 = None\n",
      "        view_default_672: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_195, [1, 1, 4096]);  mm_default_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_195: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_193, view_default_672);  add_tensor_193 = view_default_672 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_56: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_195, 2)\n",
      "        mean_dim_56: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_56, [-1], True);  pow_tensor_scalar_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_196: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_56, 1e-06);  mean_dim_56 = None\n",
      "        rsqrt_default_56: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_196);  add_tensor_196 = None\n",
      "        detach_default_84: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_56)\n",
      "        mul_tensor_252: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_195, rsqrt_default_56);  rsqrt_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant253 = self._param_constant253\n",
      "        mul_tensor_253: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant253, mul_tensor_252);  _param_constant253 = mul_tensor_252 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant254 = self._param_constant254\n",
      "        t_default_196: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant254);  _param_constant254 = None\n",
      "        view_default_673: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_253, [1, 4096])\n",
      "        mm_default_196: f32[1, 4096] = torch.ops.aten.mm.default(view_default_673, t_default_196);  view_default_673 = t_default_196 = None\n",
      "        view_default_674: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_196, [1, 1, 4096]);  mm_default_196 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant255 = self._param_constant255\n",
      "        t_default_197: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant255);  _param_constant255 = None\n",
      "        view_default_675: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_253, [1, 4096])\n",
      "        mm_default_197: f32[1, 4096] = torch.ops.aten.mm.default(view_default_675, t_default_197);  view_default_675 = t_default_197 = None\n",
      "        view_default_676: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_197, [1, 1, 4096]);  mm_default_197 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant256 = self._param_constant256\n",
      "        t_default_198: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant256);  _param_constant256 = None\n",
      "        view_default_677: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_253, [1, 4096]);  mul_tensor_253 = None\n",
      "        mm_default_198: f32[1, 4096] = torch.ops.aten.mm.default(view_default_677, t_default_198);  view_default_677 = t_default_198 = None\n",
      "        view_default_678: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_198, [1, 1, 4096]);  mm_default_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_679: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_674, [1, 1, 32, 128]);  view_default_674 = None\n",
      "        transpose_int_140: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_679, 1, 2);  view_default_679 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_680: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_676, [1, 1, 32, 128]);  view_default_676 = None\n",
      "        transpose_int_141: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_680, 1, 2);  view_default_680 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_681: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_678, [1, 1, 32, 128]);  view_default_678 = None\n",
      "        transpose_int_142: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_681, 1, 2);  view_default_681 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant56 = self._tensor_constant56\n",
      "        slice_tensor_282: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant56, 0, 0, 9223372036854775807);  _tensor_constant56 = None\n",
      "        slice_tensor_283: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_282, 1, 0, 9223372036854775807);  slice_tensor_282 = None\n",
      "        sym_size_84: Sym(s0) = torch.ops.aten.sym_size(arg57, 2)\n",
      "        add_30: Sym(s0 + 1) = 1 + sym_size_84;  sym_size_84 = None\n",
      "        slice_tensor_284: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_283, 2, 0, add_30);  slice_tensor_283 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant57 = self._tensor_constant57\n",
      "        slice_tensor_285: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant57, 0, 0, 9223372036854775807);  _tensor_constant57 = None\n",
      "        slice_tensor_286: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_285, 1, 0, 9223372036854775807);  slice_tensor_285 = None\n",
      "        slice_tensor_287: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_286, 2, 0, add_30);  slice_tensor_286 = add_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_112: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_284, 1);  slice_tensor_284 = None\n",
      "        squeeze_dim_113: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_112, 0);  squeeze_dim_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_114: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_287, 1);  slice_tensor_287 = None\n",
      "        squeeze_dim_115: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_114, 0);  squeeze_dim_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_56: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_113, [view_default]);  squeeze_dim_113 = None\n",
      "        unsqueeze_default_59: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_56, 1);  index_tensor_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_57: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_115, [view_default]);  squeeze_dim_115 = None\n",
      "        unsqueeze_default_60: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_57, 1);  index_tensor_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_254: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_140, unsqueeze_default_59)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_288: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_289: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 64, 9223372036854775807);  transpose_int_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_56: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_289);  slice_tensor_289 = None\n",
      "        cat_default_112: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_56, slice_tensor_288], -1);  neg_default_56 = slice_tensor_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_255: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_112, unsqueeze_default_60);  cat_default_112 = None\n",
      "        add_tensor_197: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_254, mul_tensor_255);  mul_tensor_254 = mul_tensor_255 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_256: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_141, unsqueeze_default_59);  unsqueeze_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_290: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_291: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 64, 9223372036854775807);  transpose_int_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_57: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_291);  slice_tensor_291 = None\n",
      "        cat_default_113: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_57, slice_tensor_290], -1);  neg_default_57 = slice_tensor_290 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_257: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_113, unsqueeze_default_60);  cat_default_113 = unsqueeze_default_60 = None\n",
      "        add_tensor_198: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_256, mul_tensor_257);  mul_tensor_256 = mul_tensor_257 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_114: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg57, add_tensor_198], 2);  arg57 = add_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_115: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg58, transpose_int_142], 2);  arg58 = transpose_int_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_143: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_114, 2, 3)\n",
      "        expand_default_113: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_197, [1, 32, 1, 128]);  add_tensor_197 = None\n",
      "        view_default_682: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_113, [32, 1, 128]);  expand_default_113 = None\n",
      "        sym_size_85: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_114, 2)\n",
      "        expand_default_114: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_143, [1, 32, 128, sym_size_85]);  transpose_int_143 = None\n",
      "        view_default_683: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_114, [32, 128, sym_size_85]);  expand_default_114 = None\n",
      "        bmm_default_56: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_682, view_default_683);  view_default_682 = view_default_683 = None\n",
      "        view_default_684: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_56, [1, 32, 1, sym_size_85]);  bmm_default_56 = None\n",
      "        div_tensor_28: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_684, 11.313708498984761);  view_default_684 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_199: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_28, masked_fill_scalar);  div_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_28: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_199, -1, False);  add_tensor_199 = None\n",
      "        detach_default_85: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_28)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_115: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_28, [1, 32, 1, sym_size_85]);  _softmax_default_28 = None\n",
      "        view_default_685: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_115, [32, 1, sym_size_85]);  expand_default_115 = sym_size_85 = None\n",
      "        sym_size_86: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_115, 2)\n",
      "        expand_default_116: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_115, [1, 32, sym_size_86, 128])\n",
      "        view_default_686: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_116, [32, sym_size_86, 128]);  expand_default_116 = sym_size_86 = None\n",
      "        bmm_default_57: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_685, view_default_686);  view_default_685 = view_default_686 = None\n",
      "        view_default_687: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_57, [1, 32, 1, 128]);  bmm_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_144: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_687, 1, 2);  view_default_687 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_688: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_144, [1, 1, 4096]);  transpose_int_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant257 = self._param_constant257\n",
      "        t_default_199: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant257);  _param_constant257 = None\n",
      "        view_default_689: f32[1, 4096] = torch.ops.aten.view.default(view_default_688, [1, 4096]);  view_default_688 = None\n",
      "        mm_default_199: f32[1, 4096] = torch.ops.aten.mm.default(view_default_689, t_default_199);  view_default_689 = t_default_199 = None\n",
      "        view_default_690: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_199, [1, 1, 4096]);  mm_default_199 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_200: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_195, view_default_690);  add_tensor_195 = view_default_690 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_57: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_200, 2)\n",
      "        mean_dim_57: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_57, [-1], True);  pow_tensor_scalar_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_201: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_57, 1e-06);  mean_dim_57 = None\n",
      "        rsqrt_default_57: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_201);  add_tensor_201 = None\n",
      "        detach_default_86: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_57)\n",
      "        mul_tensor_258: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_200, rsqrt_default_57);  rsqrt_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant258 = self._param_constant258\n",
      "        mul_tensor_259: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant258, mul_tensor_258);  _param_constant258 = mul_tensor_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant259 = self._param_constant259\n",
      "        t_default_200: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant259);  _param_constant259 = None\n",
      "        view_default_691: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_259, [1, 4096])\n",
      "        mm_default_200: f32[1, 11008] = torch.ops.aten.mm.default(view_default_691, t_default_200);  view_default_691 = t_default_200 = None\n",
      "        view_default_692: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_200, [1, 1, 11008]);  mm_default_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_28: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_692);  view_default_692 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant260 = self._param_constant260\n",
      "        t_default_201: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant260);  _param_constant260 = None\n",
      "        view_default_693: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_259, [1, 4096]);  mul_tensor_259 = None\n",
      "        mm_default_201: f32[1, 11008] = torch.ops.aten.mm.default(view_default_693, t_default_201);  view_default_693 = t_default_201 = None\n",
      "        view_default_694: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_201, [1, 1, 11008]);  mm_default_201 = None\n",
      "        mul_tensor_260: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_28, view_default_694);  silu_default_28 = view_default_694 = None\n",
      "        _param_constant261 = self._param_constant261\n",
      "        t_default_202: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant261);  _param_constant261 = None\n",
      "        view_default_695: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_260, [1, 11008]);  mul_tensor_260 = None\n",
      "        mm_default_202: f32[1, 4096] = torch.ops.aten.mm.default(view_default_695, t_default_202);  view_default_695 = t_default_202 = None\n",
      "        view_default_696: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_202, [1, 1, 4096]);  mm_default_202 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_202: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_200, view_default_696);  add_tensor_200 = view_default_696 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_58: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_202, 2)\n",
      "        mean_dim_58: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_58, [-1], True);  pow_tensor_scalar_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_203: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_58, 1e-06);  mean_dim_58 = None\n",
      "        rsqrt_default_58: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_203);  add_tensor_203 = None\n",
      "        detach_default_87: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_58)\n",
      "        mul_tensor_261: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_202, rsqrt_default_58);  rsqrt_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant262 = self._param_constant262\n",
      "        mul_tensor_262: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant262, mul_tensor_261);  _param_constant262 = mul_tensor_261 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant263 = self._param_constant263\n",
      "        t_default_203: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant263);  _param_constant263 = None\n",
      "        view_default_697: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_262, [1, 4096])\n",
      "        mm_default_203: f32[1, 4096] = torch.ops.aten.mm.default(view_default_697, t_default_203);  view_default_697 = t_default_203 = None\n",
      "        view_default_698: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_203, [1, 1, 4096]);  mm_default_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant264 = self._param_constant264\n",
      "        t_default_204: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant264);  _param_constant264 = None\n",
      "        view_default_699: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_262, [1, 4096])\n",
      "        mm_default_204: f32[1, 4096] = torch.ops.aten.mm.default(view_default_699, t_default_204);  view_default_699 = t_default_204 = None\n",
      "        view_default_700: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_204, [1, 1, 4096]);  mm_default_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant265 = self._param_constant265\n",
      "        t_default_205: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant265);  _param_constant265 = None\n",
      "        view_default_701: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_262, [1, 4096]);  mul_tensor_262 = None\n",
      "        mm_default_205: f32[1, 4096] = torch.ops.aten.mm.default(view_default_701, t_default_205);  view_default_701 = t_default_205 = None\n",
      "        view_default_702: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_205, [1, 1, 4096]);  mm_default_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_703: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_698, [1, 1, 32, 128]);  view_default_698 = None\n",
      "        transpose_int_145: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_703, 1, 2);  view_default_703 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_704: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_700, [1, 1, 32, 128]);  view_default_700 = None\n",
      "        transpose_int_146: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_704, 1, 2);  view_default_704 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_705: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_702, [1, 1, 32, 128]);  view_default_702 = None\n",
      "        transpose_int_147: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_705, 1, 2);  view_default_705 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant58 = self._tensor_constant58\n",
      "        slice_tensor_292: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant58, 0, 0, 9223372036854775807);  _tensor_constant58 = None\n",
      "        slice_tensor_293: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_292, 1, 0, 9223372036854775807);  slice_tensor_292 = None\n",
      "        sym_size_87: Sym(s0) = torch.ops.aten.sym_size(arg59, 2)\n",
      "        add_31: Sym(s0 + 1) = 1 + sym_size_87;  sym_size_87 = None\n",
      "        slice_tensor_294: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_293, 2, 0, add_31);  slice_tensor_293 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant59 = self._tensor_constant59\n",
      "        slice_tensor_295: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant59, 0, 0, 9223372036854775807);  _tensor_constant59 = None\n",
      "        slice_tensor_296: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_295, 1, 0, 9223372036854775807);  slice_tensor_295 = None\n",
      "        slice_tensor_297: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_296, 2, 0, add_31);  slice_tensor_296 = add_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_116: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_294, 1);  slice_tensor_294 = None\n",
      "        squeeze_dim_117: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_116, 0);  squeeze_dim_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_118: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_297, 1);  slice_tensor_297 = None\n",
      "        squeeze_dim_119: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_118, 0);  squeeze_dim_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_58: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_117, [view_default]);  squeeze_dim_117 = None\n",
      "        unsqueeze_default_61: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_58, 1);  index_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_59: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_119, [view_default]);  squeeze_dim_119 = None\n",
      "        unsqueeze_default_62: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_59, 1);  index_tensor_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_263: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_145, unsqueeze_default_61)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_298: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_299: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 64, 9223372036854775807);  transpose_int_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_58: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_299);  slice_tensor_299 = None\n",
      "        cat_default_116: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_58, slice_tensor_298], -1);  neg_default_58 = slice_tensor_298 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_264: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_116, unsqueeze_default_62);  cat_default_116 = None\n",
      "        add_tensor_204: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_263, mul_tensor_264);  mul_tensor_263 = mul_tensor_264 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_265: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_146, unsqueeze_default_61);  unsqueeze_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_300: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_301: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 64, 9223372036854775807);  transpose_int_146 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_59: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_301);  slice_tensor_301 = None\n",
      "        cat_default_117: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_59, slice_tensor_300], -1);  neg_default_59 = slice_tensor_300 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_266: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_117, unsqueeze_default_62);  cat_default_117 = unsqueeze_default_62 = None\n",
      "        add_tensor_205: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_265, mul_tensor_266);  mul_tensor_265 = mul_tensor_266 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_118: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg59, add_tensor_205], 2);  arg59 = add_tensor_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_119: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg60, transpose_int_147], 2);  arg60 = transpose_int_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_148: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_118, 2, 3)\n",
      "        expand_default_117: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_204, [1, 32, 1, 128]);  add_tensor_204 = None\n",
      "        view_default_706: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_117, [32, 1, 128]);  expand_default_117 = None\n",
      "        sym_size_88: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_118, 2)\n",
      "        expand_default_118: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_148, [1, 32, 128, sym_size_88]);  transpose_int_148 = None\n",
      "        view_default_707: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_118, [32, 128, sym_size_88]);  expand_default_118 = None\n",
      "        bmm_default_58: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_706, view_default_707);  view_default_706 = view_default_707 = None\n",
      "        view_default_708: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_58, [1, 32, 1, sym_size_88]);  bmm_default_58 = None\n",
      "        div_tensor_29: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_708, 11.313708498984761);  view_default_708 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_206: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_29, masked_fill_scalar);  div_tensor_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_29: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_206, -1, False);  add_tensor_206 = None\n",
      "        detach_default_88: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_29)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_119: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_29, [1, 32, 1, sym_size_88]);  _softmax_default_29 = None\n",
      "        view_default_709: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_119, [32, 1, sym_size_88]);  expand_default_119 = sym_size_88 = None\n",
      "        sym_size_89: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_119, 2)\n",
      "        expand_default_120: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_119, [1, 32, sym_size_89, 128])\n",
      "        view_default_710: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_120, [32, sym_size_89, 128]);  expand_default_120 = sym_size_89 = None\n",
      "        bmm_default_59: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_709, view_default_710);  view_default_709 = view_default_710 = None\n",
      "        view_default_711: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_59, [1, 32, 1, 128]);  bmm_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_149: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_711, 1, 2);  view_default_711 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_712: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_149, [1, 1, 4096]);  transpose_int_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant266 = self._param_constant266\n",
      "        t_default_206: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant266);  _param_constant266 = None\n",
      "        view_default_713: f32[1, 4096] = torch.ops.aten.view.default(view_default_712, [1, 4096]);  view_default_712 = None\n",
      "        mm_default_206: f32[1, 4096] = torch.ops.aten.mm.default(view_default_713, t_default_206);  view_default_713 = t_default_206 = None\n",
      "        view_default_714: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_206, [1, 1, 4096]);  mm_default_206 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_207: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_202, view_default_714);  add_tensor_202 = view_default_714 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_59: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_207, 2)\n",
      "        mean_dim_59: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_59, [-1], True);  pow_tensor_scalar_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_208: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_59, 1e-06);  mean_dim_59 = None\n",
      "        rsqrt_default_59: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_208);  add_tensor_208 = None\n",
      "        detach_default_89: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_59)\n",
      "        mul_tensor_267: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_207, rsqrt_default_59);  rsqrt_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant267 = self._param_constant267\n",
      "        mul_tensor_268: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant267, mul_tensor_267);  _param_constant267 = mul_tensor_267 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant268 = self._param_constant268\n",
      "        t_default_207: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant268);  _param_constant268 = None\n",
      "        view_default_715: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_268, [1, 4096])\n",
      "        mm_default_207: f32[1, 11008] = torch.ops.aten.mm.default(view_default_715, t_default_207);  view_default_715 = t_default_207 = None\n",
      "        view_default_716: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_207, [1, 1, 11008]);  mm_default_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_29: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_716);  view_default_716 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant269 = self._param_constant269\n",
      "        t_default_208: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant269);  _param_constant269 = None\n",
      "        view_default_717: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_268, [1, 4096]);  mul_tensor_268 = None\n",
      "        mm_default_208: f32[1, 11008] = torch.ops.aten.mm.default(view_default_717, t_default_208);  view_default_717 = t_default_208 = None\n",
      "        view_default_718: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_208, [1, 1, 11008]);  mm_default_208 = None\n",
      "        mul_tensor_269: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_29, view_default_718);  silu_default_29 = view_default_718 = None\n",
      "        _param_constant270 = self._param_constant270\n",
      "        t_default_209: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant270);  _param_constant270 = None\n",
      "        view_default_719: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_269, [1, 11008]);  mul_tensor_269 = None\n",
      "        mm_default_209: f32[1, 4096] = torch.ops.aten.mm.default(view_default_719, t_default_209);  view_default_719 = t_default_209 = None\n",
      "        view_default_720: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_209, [1, 1, 4096]);  mm_default_209 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_209: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_207, view_default_720);  add_tensor_207 = view_default_720 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_60: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_209, 2)\n",
      "        mean_dim_60: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_60, [-1], True);  pow_tensor_scalar_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_210: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_60, 1e-06);  mean_dim_60 = None\n",
      "        rsqrt_default_60: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_210);  add_tensor_210 = None\n",
      "        detach_default_90: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_60)\n",
      "        mul_tensor_270: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_209, rsqrt_default_60);  rsqrt_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant271 = self._param_constant271\n",
      "        mul_tensor_271: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant271, mul_tensor_270);  _param_constant271 = mul_tensor_270 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant272 = self._param_constant272\n",
      "        t_default_210: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant272);  _param_constant272 = None\n",
      "        view_default_721: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_271, [1, 4096])\n",
      "        mm_default_210: f32[1, 4096] = torch.ops.aten.mm.default(view_default_721, t_default_210);  view_default_721 = t_default_210 = None\n",
      "        view_default_722: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_210, [1, 1, 4096]);  mm_default_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant273 = self._param_constant273\n",
      "        t_default_211: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant273);  _param_constant273 = None\n",
      "        view_default_723: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_271, [1, 4096])\n",
      "        mm_default_211: f32[1, 4096] = torch.ops.aten.mm.default(view_default_723, t_default_211);  view_default_723 = t_default_211 = None\n",
      "        view_default_724: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_211, [1, 1, 4096]);  mm_default_211 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant274 = self._param_constant274\n",
      "        t_default_212: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant274);  _param_constant274 = None\n",
      "        view_default_725: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_271, [1, 4096]);  mul_tensor_271 = None\n",
      "        mm_default_212: f32[1, 4096] = torch.ops.aten.mm.default(view_default_725, t_default_212);  view_default_725 = t_default_212 = None\n",
      "        view_default_726: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_212, [1, 1, 4096]);  mm_default_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_727: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_722, [1, 1, 32, 128]);  view_default_722 = None\n",
      "        transpose_int_150: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_727, 1, 2);  view_default_727 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_728: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_724, [1, 1, 32, 128]);  view_default_724 = None\n",
      "        transpose_int_151: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_728, 1, 2);  view_default_728 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_729: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_726, [1, 1, 32, 128]);  view_default_726 = None\n",
      "        transpose_int_152: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_729, 1, 2);  view_default_729 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant60 = self._tensor_constant60\n",
      "        slice_tensor_302: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant60, 0, 0, 9223372036854775807);  _tensor_constant60 = None\n",
      "        slice_tensor_303: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_302, 1, 0, 9223372036854775807);  slice_tensor_302 = None\n",
      "        sym_size_90: Sym(s0) = torch.ops.aten.sym_size(arg61, 2)\n",
      "        add_32: Sym(s0 + 1) = 1 + sym_size_90;  sym_size_90 = None\n",
      "        slice_tensor_304: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_303, 2, 0, add_32);  slice_tensor_303 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant61 = self._tensor_constant61\n",
      "        slice_tensor_305: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant61, 0, 0, 9223372036854775807);  _tensor_constant61 = None\n",
      "        slice_tensor_306: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_305, 1, 0, 9223372036854775807);  slice_tensor_305 = None\n",
      "        slice_tensor_307: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_306, 2, 0, add_32);  slice_tensor_306 = add_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_120: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_304, 1);  slice_tensor_304 = None\n",
      "        squeeze_dim_121: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_120, 0);  squeeze_dim_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_122: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_307, 1);  slice_tensor_307 = None\n",
      "        squeeze_dim_123: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_122, 0);  squeeze_dim_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_60: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_121, [view_default]);  squeeze_dim_121 = None\n",
      "        unsqueeze_default_63: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_60, 1);  index_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_61: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_123, [view_default]);  squeeze_dim_123 = None\n",
      "        unsqueeze_default_64: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_61, 1);  index_tensor_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_272: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_150, unsqueeze_default_63)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_308: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_309: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 64, 9223372036854775807);  transpose_int_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_60: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_309);  slice_tensor_309 = None\n",
      "        cat_default_120: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_60, slice_tensor_308], -1);  neg_default_60 = slice_tensor_308 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_273: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_120, unsqueeze_default_64);  cat_default_120 = None\n",
      "        add_tensor_211: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_272, mul_tensor_273);  mul_tensor_272 = mul_tensor_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_274: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_151, unsqueeze_default_63);  unsqueeze_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_310: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_311: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 64, 9223372036854775807);  transpose_int_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_61: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_311);  slice_tensor_311 = None\n",
      "        cat_default_121: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_61, slice_tensor_310], -1);  neg_default_61 = slice_tensor_310 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_275: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_121, unsqueeze_default_64);  cat_default_121 = unsqueeze_default_64 = None\n",
      "        add_tensor_212: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_274, mul_tensor_275);  mul_tensor_274 = mul_tensor_275 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_122: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg61, add_tensor_212], 2);  arg61 = add_tensor_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_123: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg62, transpose_int_152], 2);  arg62 = transpose_int_152 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_153: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_122, 2, 3)\n",
      "        expand_default_121: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_211, [1, 32, 1, 128]);  add_tensor_211 = None\n",
      "        view_default_730: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_121, [32, 1, 128]);  expand_default_121 = None\n",
      "        sym_size_91: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_122, 2)\n",
      "        expand_default_122: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_153, [1, 32, 128, sym_size_91]);  transpose_int_153 = None\n",
      "        view_default_731: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_122, [32, 128, sym_size_91]);  expand_default_122 = None\n",
      "        bmm_default_60: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_730, view_default_731);  view_default_730 = view_default_731 = None\n",
      "        view_default_732: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_60, [1, 32, 1, sym_size_91]);  bmm_default_60 = None\n",
      "        div_tensor_30: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_732, 11.313708498984761);  view_default_732 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_213: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_30, masked_fill_scalar);  div_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_30: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_213, -1, False);  add_tensor_213 = None\n",
      "        detach_default_91: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_30)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_123: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_30, [1, 32, 1, sym_size_91]);  _softmax_default_30 = None\n",
      "        view_default_733: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_123, [32, 1, sym_size_91]);  expand_default_123 = sym_size_91 = None\n",
      "        sym_size_92: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_123, 2)\n",
      "        expand_default_124: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_123, [1, 32, sym_size_92, 128])\n",
      "        view_default_734: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_124, [32, sym_size_92, 128]);  expand_default_124 = sym_size_92 = None\n",
      "        bmm_default_61: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_733, view_default_734);  view_default_733 = view_default_734 = None\n",
      "        view_default_735: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_61, [1, 32, 1, 128]);  bmm_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_154: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_735, 1, 2);  view_default_735 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_736: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_154, [1, 1, 4096]);  transpose_int_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant275 = self._param_constant275\n",
      "        t_default_213: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant275);  _param_constant275 = None\n",
      "        view_default_737: f32[1, 4096] = torch.ops.aten.view.default(view_default_736, [1, 4096]);  view_default_736 = None\n",
      "        mm_default_213: f32[1, 4096] = torch.ops.aten.mm.default(view_default_737, t_default_213);  view_default_737 = t_default_213 = None\n",
      "        view_default_738: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_213, [1, 1, 4096]);  mm_default_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_214: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_209, view_default_738);  add_tensor_209 = view_default_738 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_61: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_214, 2)\n",
      "        mean_dim_61: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_61, [-1], True);  pow_tensor_scalar_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_215: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_61, 1e-06);  mean_dim_61 = None\n",
      "        rsqrt_default_61: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_215);  add_tensor_215 = None\n",
      "        detach_default_92: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_61)\n",
      "        mul_tensor_276: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_214, rsqrt_default_61);  rsqrt_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant276 = self._param_constant276\n",
      "        mul_tensor_277: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant276, mul_tensor_276);  _param_constant276 = mul_tensor_276 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant277 = self._param_constant277\n",
      "        t_default_214: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant277);  _param_constant277 = None\n",
      "        view_default_739: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_277, [1, 4096])\n",
      "        mm_default_214: f32[1, 11008] = torch.ops.aten.mm.default(view_default_739, t_default_214);  view_default_739 = t_default_214 = None\n",
      "        view_default_740: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_214, [1, 1, 11008]);  mm_default_214 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_30: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_740);  view_default_740 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant278 = self._param_constant278\n",
      "        t_default_215: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant278);  _param_constant278 = None\n",
      "        view_default_741: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_277, [1, 4096]);  mul_tensor_277 = None\n",
      "        mm_default_215: f32[1, 11008] = torch.ops.aten.mm.default(view_default_741, t_default_215);  view_default_741 = t_default_215 = None\n",
      "        view_default_742: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_215, [1, 1, 11008]);  mm_default_215 = None\n",
      "        mul_tensor_278: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_30, view_default_742);  silu_default_30 = view_default_742 = None\n",
      "        _param_constant279 = self._param_constant279\n",
      "        t_default_216: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant279);  _param_constant279 = None\n",
      "        view_default_743: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_278, [1, 11008]);  mul_tensor_278 = None\n",
      "        mm_default_216: f32[1, 4096] = torch.ops.aten.mm.default(view_default_743, t_default_216);  view_default_743 = t_default_216 = None\n",
      "        view_default_744: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_216, [1, 1, 4096]);  mm_default_216 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_216: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_214, view_default_744);  add_tensor_214 = view_default_744 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_62: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_216, 2)\n",
      "        mean_dim_62: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_62, [-1], True);  pow_tensor_scalar_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_217: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_62, 1e-06);  mean_dim_62 = None\n",
      "        rsqrt_default_62: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_217);  add_tensor_217 = None\n",
      "        detach_default_93: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_62)\n",
      "        mul_tensor_279: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_216, rsqrt_default_62);  rsqrt_default_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant280 = self._param_constant280\n",
      "        mul_tensor_280: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant280, mul_tensor_279);  _param_constant280 = mul_tensor_279 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant281 = self._param_constant281\n",
      "        t_default_217: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant281);  _param_constant281 = None\n",
      "        view_default_745: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_280, [1, 4096])\n",
      "        mm_default_217: f32[1, 4096] = torch.ops.aten.mm.default(view_default_745, t_default_217);  view_default_745 = t_default_217 = None\n",
      "        view_default_746: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_217, [1, 1, 4096]);  mm_default_217 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant282 = self._param_constant282\n",
      "        t_default_218: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant282);  _param_constant282 = None\n",
      "        view_default_747: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_280, [1, 4096])\n",
      "        mm_default_218: f32[1, 4096] = torch.ops.aten.mm.default(view_default_747, t_default_218);  view_default_747 = t_default_218 = None\n",
      "        view_default_748: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_218, [1, 1, 4096]);  mm_default_218 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant283 = self._param_constant283\n",
      "        t_default_219: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant283);  _param_constant283 = None\n",
      "        view_default_749: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_280, [1, 4096]);  mul_tensor_280 = None\n",
      "        mm_default_219: f32[1, 4096] = torch.ops.aten.mm.default(view_default_749, t_default_219);  view_default_749 = t_default_219 = None\n",
      "        view_default_750: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_219, [1, 1, 4096]);  mm_default_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_751: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_746, [1, 1, 32, 128]);  view_default_746 = None\n",
      "        transpose_int_155: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_751, 1, 2);  view_default_751 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_752: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_748, [1, 1, 32, 128]);  view_default_748 = None\n",
      "        transpose_int_156: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_752, 1, 2);  view_default_752 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_753: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_750, [1, 1, 32, 128]);  view_default_750 = None\n",
      "        transpose_int_157: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_753, 1, 2);  view_default_753 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant62 = self._tensor_constant62\n",
      "        slice_tensor_312: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant62, 0, 0, 9223372036854775807);  _tensor_constant62 = None\n",
      "        slice_tensor_313: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_312, 1, 0, 9223372036854775807);  slice_tensor_312 = None\n",
      "        sym_size_93: Sym(s0) = torch.ops.aten.sym_size(arg63, 2)\n",
      "        add_33: Sym(s0 + 1) = 1 + sym_size_93;  sym_size_93 = None\n",
      "        slice_tensor_314: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_313, 2, 0, add_33);  slice_tensor_313 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant63 = self._tensor_constant63\n",
      "        slice_tensor_315: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant63, 0, 0, 9223372036854775807);  _tensor_constant63 = None\n",
      "        slice_tensor_316: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_315, 1, 0, 9223372036854775807);  slice_tensor_315 = None\n",
      "        slice_tensor_317: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_316, 2, 0, add_33);  slice_tensor_316 = add_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_124: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_314, 1);  slice_tensor_314 = None\n",
      "        squeeze_dim_125: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_124, 0);  squeeze_dim_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_126: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_317, 1);  slice_tensor_317 = None\n",
      "        squeeze_dim_127: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_126, 0);  squeeze_dim_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_62: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_125, [view_default]);  squeeze_dim_125 = None\n",
      "        unsqueeze_default_65: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_62, 1);  index_tensor_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_63: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_127, [view_default]);  squeeze_dim_127 = view_default = None\n",
      "        unsqueeze_default_66: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_63, 1);  index_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_281: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_155, unsqueeze_default_65)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_318: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_319: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 64, 9223372036854775807);  transpose_int_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_62: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_319);  slice_tensor_319 = None\n",
      "        cat_default_124: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_62, slice_tensor_318], -1);  neg_default_62 = slice_tensor_318 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_282: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_124, unsqueeze_default_66);  cat_default_124 = None\n",
      "        add_tensor_218: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_281, mul_tensor_282);  mul_tensor_281 = mul_tensor_282 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_283: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_156, unsqueeze_default_65);  unsqueeze_default_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_320: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_321: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 64, 9223372036854775807);  transpose_int_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_63: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_321);  slice_tensor_321 = None\n",
      "        cat_default_125: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_63, slice_tensor_320], -1);  neg_default_63 = slice_tensor_320 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_284: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_125, unsqueeze_default_66);  cat_default_125 = unsqueeze_default_66 = None\n",
      "        add_tensor_219: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_283, mul_tensor_284);  mul_tensor_283 = mul_tensor_284 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "        cat_default_126: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg63, add_tensor_219], 2);  arg63 = add_tensor_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "        cat_default_127: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg64, transpose_int_157], 2);  arg64 = transpose_int_157 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_158: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_126, 2, 3)\n",
      "        expand_default_125: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_218, [1, 32, 1, 128]);  add_tensor_218 = None\n",
      "        view_default_754: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_125, [32, 1, 128]);  expand_default_125 = None\n",
      "        sym_size_94: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_126, 2)\n",
      "        expand_default_126: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_158, [1, 32, 128, sym_size_94]);  transpose_int_158 = None\n",
      "        view_default_755: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_126, [32, 128, sym_size_94]);  expand_default_126 = None\n",
      "        bmm_default_62: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_754, view_default_755);  view_default_754 = view_default_755 = None\n",
      "        view_default_756: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_62, [1, 32, 1, sym_size_94]);  bmm_default_62 = None\n",
      "        div_tensor_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_756, 11.313708498984761);  view_default_756 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_220: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_31, masked_fill_scalar);  div_tensor_31 = masked_fill_scalar = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_220, -1, False);  add_tensor_220 = None\n",
      "        detach_default_94: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_31)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_127: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_31, [1, 32, 1, sym_size_94]);  _softmax_default_31 = None\n",
      "        view_default_757: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_127, [32, 1, sym_size_94]);  expand_default_127 = sym_size_94 = None\n",
      "        sym_size_95: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_127, 2)\n",
      "        expand_default_128: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_127, [1, 32, sym_size_95, 128])\n",
      "        view_default_758: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_128, [32, sym_size_95, 128]);  expand_default_128 = sym_size_95 = None\n",
      "        bmm_default_63: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_757, view_default_758);  view_default_757 = view_default_758 = None\n",
      "        view_default_759: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_63, [1, 32, 1, 128]);  bmm_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_159: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_759, 1, 2);  view_default_759 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_760: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_159, [1, 1, 4096]);  transpose_int_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant284 = self._param_constant284\n",
      "        t_default_220: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant284);  _param_constant284 = None\n",
      "        view_default_761: f32[1, 4096] = torch.ops.aten.view.default(view_default_760, [1, 4096]);  view_default_760 = None\n",
      "        mm_default_220: f32[1, 4096] = torch.ops.aten.mm.default(view_default_761, t_default_220);  view_default_761 = t_default_220 = None\n",
      "        view_default_762: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_220, [1, 1, 4096]);  mm_default_220 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_221: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_216, view_default_762);  add_tensor_216 = view_default_762 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_63: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_221, 2)\n",
      "        mean_dim_63: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_63, [-1], True);  pow_tensor_scalar_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_222: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_63, 1e-06);  mean_dim_63 = None\n",
      "        rsqrt_default_63: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_222);  add_tensor_222 = None\n",
      "        detach_default_95: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_63)\n",
      "        mul_tensor_285: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_221, rsqrt_default_63);  rsqrt_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant285 = self._param_constant285\n",
      "        mul_tensor_286: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant285, mul_tensor_285);  _param_constant285 = mul_tensor_285 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant286 = self._param_constant286\n",
      "        t_default_221: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant286);  _param_constant286 = None\n",
      "        view_default_763: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_286, [1, 4096])\n",
      "        mm_default_221: f32[1, 11008] = torch.ops.aten.mm.default(view_default_763, t_default_221);  view_default_763 = t_default_221 = None\n",
      "        view_default_764: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_221, [1, 1, 11008]);  mm_default_221 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_31: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_764);  view_default_764 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant287 = self._param_constant287\n",
      "        t_default_222: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant287);  _param_constant287 = None\n",
      "        view_default_765: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_286, [1, 4096]);  mul_tensor_286 = None\n",
      "        mm_default_222: f32[1, 11008] = torch.ops.aten.mm.default(view_default_765, t_default_222);  view_default_765 = t_default_222 = None\n",
      "        view_default_766: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_222, [1, 1, 11008]);  mm_default_222 = None\n",
      "        mul_tensor_287: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_31, view_default_766);  silu_default_31 = view_default_766 = None\n",
      "        _param_constant288 = self._param_constant288\n",
      "        t_default_223: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant288);  _param_constant288 = None\n",
      "        view_default_767: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_287, [1, 11008]);  mul_tensor_287 = None\n",
      "        mm_default_223: f32[1, 4096] = torch.ops.aten.mm.default(view_default_767, t_default_223);  view_default_767 = t_default_223 = None\n",
      "        view_default_768: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_223, [1, 1, 4096]);  mm_default_223 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_223: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_221, view_default_768);  add_tensor_221 = view_default_768 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_64: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_223, 2)\n",
      "        mean_dim_64: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_64, [-1], True);  pow_tensor_scalar_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_224: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_64, 1e-06);  mean_dim_64 = None\n",
      "        rsqrt_default_64: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_224);  add_tensor_224 = None\n",
      "        detach_default_96: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_64)\n",
      "        mul_tensor_288: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_223, rsqrt_default_64);  add_tensor_223 = rsqrt_default_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant289 = self._param_constant289\n",
      "        mul_tensor_289: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant289, mul_tensor_288);  _param_constant289 = mul_tensor_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:838, code: logits = self.lm_head(hidden_states)\n",
      "        _param_constant290 = self._param_constant290\n",
      "        t_default_224: f32[4096, 32000] = torch.ops.aten.t.default(_param_constant290);  _param_constant290 = None\n",
      "        view_default_769: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_289, [1, 4096]);  mul_tensor_289 = None\n",
      "        mm_default_224: f32[1, 32000] = torch.ops.aten.mm.default(view_default_769, t_default_224);  view_default_769 = t_default_224 = None\n",
      "        view_default_770: f32[1, 1, 32000] = torch.ops.aten.view.default(mm_default_224, [1, 1, 32000]);  mm_default_224 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_2064425/3922580714.py:45, code: state1_flat = [x[:, :, -2:-1, :] for x in state1_flat]\n",
      "        slice_tensor_322: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_2, 0, 0, 9223372036854775807);  cat_default_2 = None\n",
      "        slice_tensor_323: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_322, 1, 0, 9223372036854775807);  slice_tensor_322 = None\n",
      "        slice_tensor_324: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_323, 2, -2, -1);  slice_tensor_323 = None\n",
      "        slice_tensor_325: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_324, 3, 0, 9223372036854775807);  slice_tensor_324 = None\n",
      "        slice_tensor_326: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_3, 0, 0, 9223372036854775807);  cat_default_3 = None\n",
      "        slice_tensor_327: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_326, 1, 0, 9223372036854775807);  slice_tensor_326 = None\n",
      "        slice_tensor_328: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_327, 2, -2, -1);  slice_tensor_327 = None\n",
      "        slice_tensor_329: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_328, 3, 0, 9223372036854775807);  slice_tensor_328 = None\n",
      "        slice_tensor_330: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_6, 0, 0, 9223372036854775807);  cat_default_6 = None\n",
      "        slice_tensor_331: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_330, 1, 0, 9223372036854775807);  slice_tensor_330 = None\n",
      "        slice_tensor_332: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_331, 2, -2, -1);  slice_tensor_331 = None\n",
      "        slice_tensor_333: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_332, 3, 0, 9223372036854775807);  slice_tensor_332 = None\n",
      "        slice_tensor_334: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_7, 0, 0, 9223372036854775807);  cat_default_7 = None\n",
      "        slice_tensor_335: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_334, 1, 0, 9223372036854775807);  slice_tensor_334 = None\n",
      "        slice_tensor_336: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_335, 2, -2, -1);  slice_tensor_335 = None\n",
      "        slice_tensor_337: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_336, 3, 0, 9223372036854775807);  slice_tensor_336 = None\n",
      "        slice_tensor_338: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_10, 0, 0, 9223372036854775807);  cat_default_10 = None\n",
      "        slice_tensor_339: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_338, 1, 0, 9223372036854775807);  slice_tensor_338 = None\n",
      "        slice_tensor_340: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_339, 2, -2, -1);  slice_tensor_339 = None\n",
      "        slice_tensor_341: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_340, 3, 0, 9223372036854775807);  slice_tensor_340 = None\n",
      "        slice_tensor_342: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_11, 0, 0, 9223372036854775807);  cat_default_11 = None\n",
      "        slice_tensor_343: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_342, 1, 0, 9223372036854775807);  slice_tensor_342 = None\n",
      "        slice_tensor_344: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_343, 2, -2, -1);  slice_tensor_343 = None\n",
      "        slice_tensor_345: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_344, 3, 0, 9223372036854775807);  slice_tensor_344 = None\n",
      "        slice_tensor_346: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_14, 0, 0, 9223372036854775807);  cat_default_14 = None\n",
      "        slice_tensor_347: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_346, 1, 0, 9223372036854775807);  slice_tensor_346 = None\n",
      "        slice_tensor_348: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_347, 2, -2, -1);  slice_tensor_347 = None\n",
      "        slice_tensor_349: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_348, 3, 0, 9223372036854775807);  slice_tensor_348 = None\n",
      "        slice_tensor_350: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_15, 0, 0, 9223372036854775807);  cat_default_15 = None\n",
      "        slice_tensor_351: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_350, 1, 0, 9223372036854775807);  slice_tensor_350 = None\n",
      "        slice_tensor_352: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_351, 2, -2, -1);  slice_tensor_351 = None\n",
      "        slice_tensor_353: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_352, 3, 0, 9223372036854775807);  slice_tensor_352 = None\n",
      "        slice_tensor_354: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_18, 0, 0, 9223372036854775807);  cat_default_18 = None\n",
      "        slice_tensor_355: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_354, 1, 0, 9223372036854775807);  slice_tensor_354 = None\n",
      "        slice_tensor_356: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_355, 2, -2, -1);  slice_tensor_355 = None\n",
      "        slice_tensor_357: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_356, 3, 0, 9223372036854775807);  slice_tensor_356 = None\n",
      "        slice_tensor_358: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_19, 0, 0, 9223372036854775807);  cat_default_19 = None\n",
      "        slice_tensor_359: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_358, 1, 0, 9223372036854775807);  slice_tensor_358 = None\n",
      "        slice_tensor_360: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_359, 2, -2, -1);  slice_tensor_359 = None\n",
      "        slice_tensor_361: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_360, 3, 0, 9223372036854775807);  slice_tensor_360 = None\n",
      "        slice_tensor_362: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_22, 0, 0, 9223372036854775807);  cat_default_22 = None\n",
      "        slice_tensor_363: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_362, 1, 0, 9223372036854775807);  slice_tensor_362 = None\n",
      "        slice_tensor_364: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_363, 2, -2, -1);  slice_tensor_363 = None\n",
      "        slice_tensor_365: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_364, 3, 0, 9223372036854775807);  slice_tensor_364 = None\n",
      "        slice_tensor_366: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_23, 0, 0, 9223372036854775807);  cat_default_23 = None\n",
      "        slice_tensor_367: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_366, 1, 0, 9223372036854775807);  slice_tensor_366 = None\n",
      "        slice_tensor_368: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_367, 2, -2, -1);  slice_tensor_367 = None\n",
      "        slice_tensor_369: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_368, 3, 0, 9223372036854775807);  slice_tensor_368 = None\n",
      "        slice_tensor_370: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_26, 0, 0, 9223372036854775807);  cat_default_26 = None\n",
      "        slice_tensor_371: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_370, 1, 0, 9223372036854775807);  slice_tensor_370 = None\n",
      "        slice_tensor_372: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_371, 2, -2, -1);  slice_tensor_371 = None\n",
      "        slice_tensor_373: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_372, 3, 0, 9223372036854775807);  slice_tensor_372 = None\n",
      "        slice_tensor_374: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_27, 0, 0, 9223372036854775807);  cat_default_27 = None\n",
      "        slice_tensor_375: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_374, 1, 0, 9223372036854775807);  slice_tensor_374 = None\n",
      "        slice_tensor_376: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_375, 2, -2, -1);  slice_tensor_375 = None\n",
      "        slice_tensor_377: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_376, 3, 0, 9223372036854775807);  slice_tensor_376 = None\n",
      "        slice_tensor_378: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_30, 0, 0, 9223372036854775807);  cat_default_30 = None\n",
      "        slice_tensor_379: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_378, 1, 0, 9223372036854775807);  slice_tensor_378 = None\n",
      "        slice_tensor_380: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_379, 2, -2, -1);  slice_tensor_379 = None\n",
      "        slice_tensor_381: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_380, 3, 0, 9223372036854775807);  slice_tensor_380 = None\n",
      "        slice_tensor_382: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_31, 0, 0, 9223372036854775807);  cat_default_31 = None\n",
      "        slice_tensor_383: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_382, 1, 0, 9223372036854775807);  slice_tensor_382 = None\n",
      "        slice_tensor_384: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_383, 2, -2, -1);  slice_tensor_383 = None\n",
      "        slice_tensor_385: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_384, 3, 0, 9223372036854775807);  slice_tensor_384 = None\n",
      "        slice_tensor_386: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_34, 0, 0, 9223372036854775807);  cat_default_34 = None\n",
      "        slice_tensor_387: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_386, 1, 0, 9223372036854775807);  slice_tensor_386 = None\n",
      "        slice_tensor_388: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_387, 2, -2, -1);  slice_tensor_387 = None\n",
      "        slice_tensor_389: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_388, 3, 0, 9223372036854775807);  slice_tensor_388 = None\n",
      "        slice_tensor_390: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_35, 0, 0, 9223372036854775807);  cat_default_35 = None\n",
      "        slice_tensor_391: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_390, 1, 0, 9223372036854775807);  slice_tensor_390 = None\n",
      "        slice_tensor_392: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_391, 2, -2, -1);  slice_tensor_391 = None\n",
      "        slice_tensor_393: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_392, 3, 0, 9223372036854775807);  slice_tensor_392 = None\n",
      "        slice_tensor_394: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_38, 0, 0, 9223372036854775807);  cat_default_38 = None\n",
      "        slice_tensor_395: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_394, 1, 0, 9223372036854775807);  slice_tensor_394 = None\n",
      "        slice_tensor_396: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_395, 2, -2, -1);  slice_tensor_395 = None\n",
      "        slice_tensor_397: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_396, 3, 0, 9223372036854775807);  slice_tensor_396 = None\n",
      "        slice_tensor_398: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_39, 0, 0, 9223372036854775807);  cat_default_39 = None\n",
      "        slice_tensor_399: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_398, 1, 0, 9223372036854775807);  slice_tensor_398 = None\n",
      "        slice_tensor_400: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_399, 2, -2, -1);  slice_tensor_399 = None\n",
      "        slice_tensor_401: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_400, 3, 0, 9223372036854775807);  slice_tensor_400 = None\n",
      "        slice_tensor_402: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_42, 0, 0, 9223372036854775807);  cat_default_42 = None\n",
      "        slice_tensor_403: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_402, 1, 0, 9223372036854775807);  slice_tensor_402 = None\n",
      "        slice_tensor_404: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_403, 2, -2, -1);  slice_tensor_403 = None\n",
      "        slice_tensor_405: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_404, 3, 0, 9223372036854775807);  slice_tensor_404 = None\n",
      "        slice_tensor_406: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_43, 0, 0, 9223372036854775807);  cat_default_43 = None\n",
      "        slice_tensor_407: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_406, 1, 0, 9223372036854775807);  slice_tensor_406 = None\n",
      "        slice_tensor_408: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_407, 2, -2, -1);  slice_tensor_407 = None\n",
      "        slice_tensor_409: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_408, 3, 0, 9223372036854775807);  slice_tensor_408 = None\n",
      "        slice_tensor_410: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_46, 0, 0, 9223372036854775807);  cat_default_46 = None\n",
      "        slice_tensor_411: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_410, 1, 0, 9223372036854775807);  slice_tensor_410 = None\n",
      "        slice_tensor_412: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_411, 2, -2, -1);  slice_tensor_411 = None\n",
      "        slice_tensor_413: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_412, 3, 0, 9223372036854775807);  slice_tensor_412 = None\n",
      "        slice_tensor_414: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_47, 0, 0, 9223372036854775807);  cat_default_47 = None\n",
      "        slice_tensor_415: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_414, 1, 0, 9223372036854775807);  slice_tensor_414 = None\n",
      "        slice_tensor_416: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_415, 2, -2, -1);  slice_tensor_415 = None\n",
      "        slice_tensor_417: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_416, 3, 0, 9223372036854775807);  slice_tensor_416 = None\n",
      "        slice_tensor_418: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_50, 0, 0, 9223372036854775807);  cat_default_50 = None\n",
      "        slice_tensor_419: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_418, 1, 0, 9223372036854775807);  slice_tensor_418 = None\n",
      "        slice_tensor_420: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_419, 2, -2, -1);  slice_tensor_419 = None\n",
      "        slice_tensor_421: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_420, 3, 0, 9223372036854775807);  slice_tensor_420 = None\n",
      "        slice_tensor_422: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_51, 0, 0, 9223372036854775807);  cat_default_51 = None\n",
      "        slice_tensor_423: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_422, 1, 0, 9223372036854775807);  slice_tensor_422 = None\n",
      "        slice_tensor_424: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_423, 2, -2, -1);  slice_tensor_423 = None\n",
      "        slice_tensor_425: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_424, 3, 0, 9223372036854775807);  slice_tensor_424 = None\n",
      "        slice_tensor_426: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_54, 0, 0, 9223372036854775807);  cat_default_54 = None\n",
      "        slice_tensor_427: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_426, 1, 0, 9223372036854775807);  slice_tensor_426 = None\n",
      "        slice_tensor_428: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_427, 2, -2, -1);  slice_tensor_427 = None\n",
      "        slice_tensor_429: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_428, 3, 0, 9223372036854775807);  slice_tensor_428 = None\n",
      "        slice_tensor_430: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_55, 0, 0, 9223372036854775807);  cat_default_55 = None\n",
      "        slice_tensor_431: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_430, 1, 0, 9223372036854775807);  slice_tensor_430 = None\n",
      "        slice_tensor_432: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_431, 2, -2, -1);  slice_tensor_431 = None\n",
      "        slice_tensor_433: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_432, 3, 0, 9223372036854775807);  slice_tensor_432 = None\n",
      "        slice_tensor_434: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_58, 0, 0, 9223372036854775807);  cat_default_58 = None\n",
      "        slice_tensor_435: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_434, 1, 0, 9223372036854775807);  slice_tensor_434 = None\n",
      "        slice_tensor_436: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_435, 2, -2, -1);  slice_tensor_435 = None\n",
      "        slice_tensor_437: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_436, 3, 0, 9223372036854775807);  slice_tensor_436 = None\n",
      "        slice_tensor_438: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_59, 0, 0, 9223372036854775807);  cat_default_59 = None\n",
      "        slice_tensor_439: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_438, 1, 0, 9223372036854775807);  slice_tensor_438 = None\n",
      "        slice_tensor_440: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_439, 2, -2, -1);  slice_tensor_439 = None\n",
      "        slice_tensor_441: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_440, 3, 0, 9223372036854775807);  slice_tensor_440 = None\n",
      "        slice_tensor_442: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_62, 0, 0, 9223372036854775807);  cat_default_62 = None\n",
      "        slice_tensor_443: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_442, 1, 0, 9223372036854775807);  slice_tensor_442 = None\n",
      "        slice_tensor_444: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_443, 2, -2, -1);  slice_tensor_443 = None\n",
      "        slice_tensor_445: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_444, 3, 0, 9223372036854775807);  slice_tensor_444 = None\n",
      "        slice_tensor_446: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_63, 0, 0, 9223372036854775807);  cat_default_63 = None\n",
      "        slice_tensor_447: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_446, 1, 0, 9223372036854775807);  slice_tensor_446 = None\n",
      "        slice_tensor_448: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_447, 2, -2, -1);  slice_tensor_447 = None\n",
      "        slice_tensor_449: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_448, 3, 0, 9223372036854775807);  slice_tensor_448 = None\n",
      "        slice_tensor_450: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_66, 0, 0, 9223372036854775807);  cat_default_66 = None\n",
      "        slice_tensor_451: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_450, 1, 0, 9223372036854775807);  slice_tensor_450 = None\n",
      "        slice_tensor_452: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_451, 2, -2, -1);  slice_tensor_451 = None\n",
      "        slice_tensor_453: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_452, 3, 0, 9223372036854775807);  slice_tensor_452 = None\n",
      "        slice_tensor_454: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_67, 0, 0, 9223372036854775807);  cat_default_67 = None\n",
      "        slice_tensor_455: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_454, 1, 0, 9223372036854775807);  slice_tensor_454 = None\n",
      "        slice_tensor_456: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_455, 2, -2, -1);  slice_tensor_455 = None\n",
      "        slice_tensor_457: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_456, 3, 0, 9223372036854775807);  slice_tensor_456 = None\n",
      "        slice_tensor_458: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_70, 0, 0, 9223372036854775807);  cat_default_70 = None\n",
      "        slice_tensor_459: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_458, 1, 0, 9223372036854775807);  slice_tensor_458 = None\n",
      "        slice_tensor_460: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_459, 2, -2, -1);  slice_tensor_459 = None\n",
      "        slice_tensor_461: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_460, 3, 0, 9223372036854775807);  slice_tensor_460 = None\n",
      "        slice_tensor_462: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_71, 0, 0, 9223372036854775807);  cat_default_71 = None\n",
      "        slice_tensor_463: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_462, 1, 0, 9223372036854775807);  slice_tensor_462 = None\n",
      "        slice_tensor_464: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_463, 2, -2, -1);  slice_tensor_463 = None\n",
      "        slice_tensor_465: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_464, 3, 0, 9223372036854775807);  slice_tensor_464 = None\n",
      "        slice_tensor_466: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_74, 0, 0, 9223372036854775807);  cat_default_74 = None\n",
      "        slice_tensor_467: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_466, 1, 0, 9223372036854775807);  slice_tensor_466 = None\n",
      "        slice_tensor_468: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_467, 2, -2, -1);  slice_tensor_467 = None\n",
      "        slice_tensor_469: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_468, 3, 0, 9223372036854775807);  slice_tensor_468 = None\n",
      "        slice_tensor_470: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_75, 0, 0, 9223372036854775807);  cat_default_75 = None\n",
      "        slice_tensor_471: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_470, 1, 0, 9223372036854775807);  slice_tensor_470 = None\n",
      "        slice_tensor_472: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_471, 2, -2, -1);  slice_tensor_471 = None\n",
      "        slice_tensor_473: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_472, 3, 0, 9223372036854775807);  slice_tensor_472 = None\n",
      "        slice_tensor_474: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_78, 0, 0, 9223372036854775807);  cat_default_78 = None\n",
      "        slice_tensor_475: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_474, 1, 0, 9223372036854775807);  slice_tensor_474 = None\n",
      "        slice_tensor_476: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_475, 2, -2, -1);  slice_tensor_475 = None\n",
      "        slice_tensor_477: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_476, 3, 0, 9223372036854775807);  slice_tensor_476 = None\n",
      "        slice_tensor_478: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_79, 0, 0, 9223372036854775807);  cat_default_79 = None\n",
      "        slice_tensor_479: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_478, 1, 0, 9223372036854775807);  slice_tensor_478 = None\n",
      "        slice_tensor_480: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_479, 2, -2, -1);  slice_tensor_479 = None\n",
      "        slice_tensor_481: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_480, 3, 0, 9223372036854775807);  slice_tensor_480 = None\n",
      "        slice_tensor_482: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_82, 0, 0, 9223372036854775807);  cat_default_82 = None\n",
      "        slice_tensor_483: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_482, 1, 0, 9223372036854775807);  slice_tensor_482 = None\n",
      "        slice_tensor_484: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_483, 2, -2, -1);  slice_tensor_483 = None\n",
      "        slice_tensor_485: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_484, 3, 0, 9223372036854775807);  slice_tensor_484 = None\n",
      "        slice_tensor_486: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_83, 0, 0, 9223372036854775807);  cat_default_83 = None\n",
      "        slice_tensor_487: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_486, 1, 0, 9223372036854775807);  slice_tensor_486 = None\n",
      "        slice_tensor_488: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_487, 2, -2, -1);  slice_tensor_487 = None\n",
      "        slice_tensor_489: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_488, 3, 0, 9223372036854775807);  slice_tensor_488 = None\n",
      "        slice_tensor_490: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_86, 0, 0, 9223372036854775807);  cat_default_86 = None\n",
      "        slice_tensor_491: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_490, 1, 0, 9223372036854775807);  slice_tensor_490 = None\n",
      "        slice_tensor_492: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_491, 2, -2, -1);  slice_tensor_491 = None\n",
      "        slice_tensor_493: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_492, 3, 0, 9223372036854775807);  slice_tensor_492 = None\n",
      "        slice_tensor_494: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_87, 0, 0, 9223372036854775807);  cat_default_87 = None\n",
      "        slice_tensor_495: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_494, 1, 0, 9223372036854775807);  slice_tensor_494 = None\n",
      "        slice_tensor_496: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_495, 2, -2, -1);  slice_tensor_495 = None\n",
      "        slice_tensor_497: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_496, 3, 0, 9223372036854775807);  slice_tensor_496 = None\n",
      "        slice_tensor_498: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_90, 0, 0, 9223372036854775807);  cat_default_90 = None\n",
      "        slice_tensor_499: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_498, 1, 0, 9223372036854775807);  slice_tensor_498 = None\n",
      "        slice_tensor_500: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_499, 2, -2, -1);  slice_tensor_499 = None\n",
      "        slice_tensor_501: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_500, 3, 0, 9223372036854775807);  slice_tensor_500 = None\n",
      "        slice_tensor_502: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_91, 0, 0, 9223372036854775807);  cat_default_91 = None\n",
      "        slice_tensor_503: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_502, 1, 0, 9223372036854775807);  slice_tensor_502 = None\n",
      "        slice_tensor_504: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_503, 2, -2, -1);  slice_tensor_503 = None\n",
      "        slice_tensor_505: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_504, 3, 0, 9223372036854775807);  slice_tensor_504 = None\n",
      "        slice_tensor_506: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_94, 0, 0, 9223372036854775807);  cat_default_94 = None\n",
      "        slice_tensor_507: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_506, 1, 0, 9223372036854775807);  slice_tensor_506 = None\n",
      "        slice_tensor_508: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_507, 2, -2, -1);  slice_tensor_507 = None\n",
      "        slice_tensor_509: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_508, 3, 0, 9223372036854775807);  slice_tensor_508 = None\n",
      "        slice_tensor_510: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_95, 0, 0, 9223372036854775807);  cat_default_95 = None\n",
      "        slice_tensor_511: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_510, 1, 0, 9223372036854775807);  slice_tensor_510 = None\n",
      "        slice_tensor_512: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_511, 2, -2, -1);  slice_tensor_511 = None\n",
      "        slice_tensor_513: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_512, 3, 0, 9223372036854775807);  slice_tensor_512 = None\n",
      "        slice_tensor_514: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_98, 0, 0, 9223372036854775807);  cat_default_98 = None\n",
      "        slice_tensor_515: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_514, 1, 0, 9223372036854775807);  slice_tensor_514 = None\n",
      "        slice_tensor_516: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_515, 2, -2, -1);  slice_tensor_515 = None\n",
      "        slice_tensor_517: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_516, 3, 0, 9223372036854775807);  slice_tensor_516 = None\n",
      "        slice_tensor_518: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_99, 0, 0, 9223372036854775807);  cat_default_99 = None\n",
      "        slice_tensor_519: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_518, 1, 0, 9223372036854775807);  slice_tensor_518 = None\n",
      "        slice_tensor_520: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_519, 2, -2, -1);  slice_tensor_519 = None\n",
      "        slice_tensor_521: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_520, 3, 0, 9223372036854775807);  slice_tensor_520 = None\n",
      "        slice_tensor_522: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_102, 0, 0, 9223372036854775807);  cat_default_102 = None\n",
      "        slice_tensor_523: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_522, 1, 0, 9223372036854775807);  slice_tensor_522 = None\n",
      "        slice_tensor_524: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_523, 2, -2, -1);  slice_tensor_523 = None\n",
      "        slice_tensor_525: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_524, 3, 0, 9223372036854775807);  slice_tensor_524 = None\n",
      "        slice_tensor_526: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_103, 0, 0, 9223372036854775807);  cat_default_103 = None\n",
      "        slice_tensor_527: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_526, 1, 0, 9223372036854775807);  slice_tensor_526 = None\n",
      "        slice_tensor_528: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_527, 2, -2, -1);  slice_tensor_527 = None\n",
      "        slice_tensor_529: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_528, 3, 0, 9223372036854775807);  slice_tensor_528 = None\n",
      "        slice_tensor_530: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_106, 0, 0, 9223372036854775807);  cat_default_106 = None\n",
      "        slice_tensor_531: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_530, 1, 0, 9223372036854775807);  slice_tensor_530 = None\n",
      "        slice_tensor_532: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_531, 2, -2, -1);  slice_tensor_531 = None\n",
      "        slice_tensor_533: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_532, 3, 0, 9223372036854775807);  slice_tensor_532 = None\n",
      "        slice_tensor_534: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_107, 0, 0, 9223372036854775807);  cat_default_107 = None\n",
      "        slice_tensor_535: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_534, 1, 0, 9223372036854775807);  slice_tensor_534 = None\n",
      "        slice_tensor_536: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_535, 2, -2, -1);  slice_tensor_535 = None\n",
      "        slice_tensor_537: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_536, 3, 0, 9223372036854775807);  slice_tensor_536 = None\n",
      "        slice_tensor_538: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_110, 0, 0, 9223372036854775807);  cat_default_110 = None\n",
      "        slice_tensor_539: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_538, 1, 0, 9223372036854775807);  slice_tensor_538 = None\n",
      "        slice_tensor_540: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_539, 2, -2, -1);  slice_tensor_539 = None\n",
      "        slice_tensor_541: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_540, 3, 0, 9223372036854775807);  slice_tensor_540 = None\n",
      "        slice_tensor_542: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_111, 0, 0, 9223372036854775807);  cat_default_111 = None\n",
      "        slice_tensor_543: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_542, 1, 0, 9223372036854775807);  slice_tensor_542 = None\n",
      "        slice_tensor_544: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_543, 2, -2, -1);  slice_tensor_543 = None\n",
      "        slice_tensor_545: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_544, 3, 0, 9223372036854775807);  slice_tensor_544 = None\n",
      "        slice_tensor_546: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_114, 0, 0, 9223372036854775807);  cat_default_114 = None\n",
      "        slice_tensor_547: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_546, 1, 0, 9223372036854775807);  slice_tensor_546 = None\n",
      "        slice_tensor_548: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_547, 2, -2, -1);  slice_tensor_547 = None\n",
      "        slice_tensor_549: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_548, 3, 0, 9223372036854775807);  slice_tensor_548 = None\n",
      "        slice_tensor_550: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_115, 0, 0, 9223372036854775807);  cat_default_115 = None\n",
      "        slice_tensor_551: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_550, 1, 0, 9223372036854775807);  slice_tensor_550 = None\n",
      "        slice_tensor_552: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_551, 2, -2, -1);  slice_tensor_551 = None\n",
      "        slice_tensor_553: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_552, 3, 0, 9223372036854775807);  slice_tensor_552 = None\n",
      "        slice_tensor_554: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_118, 0, 0, 9223372036854775807);  cat_default_118 = None\n",
      "        slice_tensor_555: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_554, 1, 0, 9223372036854775807);  slice_tensor_554 = None\n",
      "        slice_tensor_556: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_555, 2, -2, -1);  slice_tensor_555 = None\n",
      "        slice_tensor_557: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_556, 3, 0, 9223372036854775807);  slice_tensor_556 = None\n",
      "        slice_tensor_558: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_119, 0, 0, 9223372036854775807);  cat_default_119 = None\n",
      "        slice_tensor_559: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_558, 1, 0, 9223372036854775807);  slice_tensor_558 = None\n",
      "        slice_tensor_560: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_559, 2, -2, -1);  slice_tensor_559 = None\n",
      "        slice_tensor_561: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_560, 3, 0, 9223372036854775807);  slice_tensor_560 = None\n",
      "        slice_tensor_562: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_122, 0, 0, 9223372036854775807);  cat_default_122 = None\n",
      "        slice_tensor_563: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_562, 1, 0, 9223372036854775807);  slice_tensor_562 = None\n",
      "        slice_tensor_564: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_563, 2, -2, -1);  slice_tensor_563 = None\n",
      "        slice_tensor_565: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_564, 3, 0, 9223372036854775807);  slice_tensor_564 = None\n",
      "        slice_tensor_566: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_123, 0, 0, 9223372036854775807);  cat_default_123 = None\n",
      "        slice_tensor_567: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_566, 1, 0, 9223372036854775807);  slice_tensor_566 = None\n",
      "        slice_tensor_568: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_567, 2, -2, -1);  slice_tensor_567 = None\n",
      "        slice_tensor_569: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_568, 3, 0, 9223372036854775807);  slice_tensor_568 = None\n",
      "        slice_tensor_570: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_126, 0, 0, 9223372036854775807);  cat_default_126 = None\n",
      "        slice_tensor_571: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_570, 1, 0, 9223372036854775807);  slice_tensor_570 = None\n",
      "        slice_tensor_572: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_571, 2, -2, -1);  slice_tensor_571 = None\n",
      "        slice_tensor_573: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_572, 3, 0, 9223372036854775807);  slice_tensor_572 = None\n",
      "        slice_tensor_574: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_127, 0, 0, 9223372036854775807);  cat_default_127 = None\n",
      "        slice_tensor_575: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_574, 1, 0, 9223372036854775807);  slice_tensor_574 = None\n",
      "        slice_tensor_576: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_575, 2, -2, -1);  slice_tensor_575 = None\n",
      "        slice_tensor_577: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_576, 3, 0, 9223372036854775807);  slice_tensor_576 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_2064425/3922580714.py:46, code: token1 = torch.argmax(result.logits[:, -1, :], dim=1)\n",
      "        slice_tensor_578: f32[1, 1, 32000] = torch.ops.aten.slice.Tensor(view_default_770, 0, 0, 9223372036854775807);  view_default_770 = None\n",
      "        select_int: f32[1, 32000] = torch.ops.aten.select.int(slice_tensor_578, 1, -1);  slice_tensor_578 = None\n",
      "        slice_tensor_579: f32[1, 32000] = torch.ops.aten.slice.Tensor(select_int, 1, 0, 9223372036854775807);  select_int = None\n",
      "        argmax_default: i64[1] = torch.ops.aten.argmax.default(slice_tensor_579, 1);  slice_tensor_579 = None\n",
      "        return pytree.tree_unflatten([argmax_default, slice_tensor_325, slice_tensor_329, slice_tensor_333, slice_tensor_337, slice_tensor_341, slice_tensor_345, slice_tensor_349, slice_tensor_353, slice_tensor_357, slice_tensor_361, slice_tensor_365, slice_tensor_369, slice_tensor_373, slice_tensor_377, slice_tensor_381, slice_tensor_385, slice_tensor_389, slice_tensor_393, slice_tensor_397, slice_tensor_401, slice_tensor_405, slice_tensor_409, slice_tensor_413, slice_tensor_417, slice_tensor_421, slice_tensor_425, slice_tensor_429, slice_tensor_433, slice_tensor_437, slice_tensor_441, slice_tensor_445, slice_tensor_449, slice_tensor_453, slice_tensor_457, slice_tensor_461, slice_tensor_465, slice_tensor_469, slice_tensor_473, slice_tensor_477, slice_tensor_481, slice_tensor_485, slice_tensor_489, slice_tensor_493, slice_tensor_497, slice_tensor_501, slice_tensor_505, slice_tensor_509, slice_tensor_513, slice_tensor_517, slice_tensor_521, slice_tensor_525, slice_tensor_529, slice_tensor_533, slice_tensor_537, slice_tensor_541, slice_tensor_545, slice_tensor_549, slice_tensor_553, slice_tensor_557, slice_tensor_561, slice_tensor_565, slice_tensor_569, slice_tensor_573, slice_tensor_577], self._out_spec)\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"class GraphModule(torch.nn.Module):\\n    def forward(self, token0, state0_flat_0, state0_flat_1, state0_flat_2, state0_flat_3, state0_flat_4, state0_flat_5, state0_flat_6, state0_flat_7, state0_flat_8, state0_flat_9, state0_flat_10, state0_flat_11, state0_flat_12, state0_flat_13, state0_flat_14, state0_flat_15, state0_flat_16, state0_flat_17, state0_flat_18, state0_flat_19, state0_flat_20, state0_flat_21, state0_flat_22, state0_flat_23, state0_flat_24, state0_flat_25, state0_flat_26, state0_flat_27, state0_flat_28, state0_flat_29, state0_flat_30, state0_flat_31, state0_flat_32, state0_flat_33, state0_flat_34, state0_flat_35, state0_flat_36, state0_flat_37, state0_flat_38, state0_flat_39, state0_flat_40, state0_flat_41, state0_flat_42, state0_flat_43, state0_flat_44, state0_flat_45, state0_flat_46, state0_flat_47, state0_flat_48, state0_flat_49, state0_flat_50, state0_flat_51, state0_flat_52, state0_flat_53, state0_flat_54, state0_flat_55, state0_flat_56, state0_flat_57, state0_flat_58, state0_flat_59, state0_flat_60, state0_flat_61, state0_flat_62, state0_flat_63):\\n        arg0: i64[1, 1], arg1: f32[1, 32, s0, 128], arg2: f32[1, 32, s0, 128], arg3: f32[1, 32, s0, 128], arg4: f32[1, 32, s0, 128], arg5: f32[1, 32, s0, 128], arg6: f32[1, 32, s0, 128], arg7: f32[1, 32, s0, 128], arg8: f32[1, 32, s0, 128], arg9: f32[1, 32, s0, 128], arg10: f32[1, 32, s0, 128], arg11: f32[1, 32, s0, 128], arg12: f32[1, 32, s0, 128], arg13: f32[1, 32, s0, 128], arg14: f32[1, 32, s0, 128], arg15: f32[1, 32, s0, 128], arg16: f32[1, 32, s0, 128], arg17: f32[1, 32, s0, 128], arg18: f32[1, 32, s0, 128], arg19: f32[1, 32, s0, 128], arg20: f32[1, 32, s0, 128], arg21: f32[1, 32, s0, 128], arg22: f32[1, 32, s0, 128], arg23: f32[1, 32, s0, 128], arg24: f32[1, 32, s0, 128], arg25: f32[1, 32, s0, 128], arg26: f32[1, 32, s0, 128], arg27: f32[1, 32, s0, 128], arg28: f32[1, 32, s0, 128], arg29: f32[1, 32, s0, 128], arg30: f32[1, 32, s0, 128], arg31: f32[1, 32, s0, 128], arg32: f32[1, 32, s0, 128], arg33: f32[1, 32, s0, 128], arg34: f32[1, 32, s0, 128], arg35: f32[1, 32, s0, 128], arg36: f32[1, 32, s0, 128], arg37: f32[1, 32, s0, 128], arg38: f32[1, 32, s0, 128], arg39: f32[1, 32, s0, 128], arg40: f32[1, 32, s0, 128], arg41: f32[1, 32, s0, 128], arg42: f32[1, 32, s0, 128], arg43: f32[1, 32, s0, 128], arg44: f32[1, 32, s0, 128], arg45: f32[1, 32, s0, 128], arg46: f32[1, 32, s0, 128], arg47: f32[1, 32, s0, 128], arg48: f32[1, 32, s0, 128], arg49: f32[1, 32, s0, 128], arg50: f32[1, 32, s0, 128], arg51: f32[1, 32, s0, 128], arg52: f32[1, 32, s0, 128], arg53: f32[1, 32, s0, 128], arg54: f32[1, 32, s0, 128], arg55: f32[1, 32, s0, 128], arg56: f32[1, 32, s0, 128], arg57: f32[1, 32, s0, 128], arg58: f32[1, 32, s0, 128], arg59: f32[1, 32, s0, 128], arg60: f32[1, 32, s0, 128], arg61: f32[1, 32, s0, 128], arg62: f32[1, 32, s0, 128], arg63: f32[1, 32, s0, 128], arg64: f32[1, 32, s0, 128], = fx_pytree.tree_flatten_spec(([token0, state0_flat_0, state0_flat_1, state0_flat_2, state0_flat_3, state0_flat_4, state0_flat_5, state0_flat_6, state0_flat_7, state0_flat_8, state0_flat_9, state0_flat_10, state0_flat_11, state0_flat_12, state0_flat_13, state0_flat_14, state0_flat_15, state0_flat_16, state0_flat_17, state0_flat_18, state0_flat_19, state0_flat_20, state0_flat_21, state0_flat_22, state0_flat_23, state0_flat_24, state0_flat_25, state0_flat_26, state0_flat_27, state0_flat_28, state0_flat_29, state0_flat_30, state0_flat_31, state0_flat_32, state0_flat_33, state0_flat_34, state0_flat_35, state0_flat_36, state0_flat_37, state0_flat_38, state0_flat_39, state0_flat_40, state0_flat_41, state0_flat_42, state0_flat_43, state0_flat_44, state0_flat_45, state0_flat_46, state0_flat_47, state0_flat_48, state0_flat_49, state0_flat_50, state0_flat_51, state0_flat_52, state0_flat_53, state0_flat_54, state0_flat_55, state0_flat_56, state0_flat_57, state0_flat_58, state0_flat_59, state0_flat_60, state0_flat_61, state0_flat_62, state0_flat_63], {}), self._in_spec)\\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:654, code: position_ids = torch.arange(\\n        sym_size: Sym(s0) = torch.ops.aten.sym_size(arg1, 2)\\n        add: Sym(s0 + 1) = 1 + sym_size\\n        arange_start: i64[1] = torch.ops.aten.arange.start(sym_size, add, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  add = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:657, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\\n        unsqueeze_default: i64[1, 1] = torch.ops.aten.unsqueeze.default(arange_start, 0);  arange_start = None\\n        view_default: i64[1, 1] = torch.ops.aten.view.default(unsqueeze_default, [-1, 1]);  unsqueeze_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:662, code: inputs_embeds = self.embed_tokens(input_ids)\\n        _param_constant0 = self._param_constant0\\n        embedding_default: f32[1, 1, 4096] = torch.ops.aten.embedding.default(_param_constant0, arg0);  _param_constant0 = arg0 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:665, code: attention_mask = torch.ones(\\n        add_1: Sym(s0 + 1) = 1 + sym_size\\n        ones_default: b8[1, s0 + 1] = torch.ops.aten.ones.default([1, add_1], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:68, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\\n        slice_tensor: b8[1, s0 + 1] = torch.ops.aten.slice.Tensor(ones_default, 0, 0, 9223372036854775807);  ones_default = None\\n        unsqueeze_default_1: b8[1, 1, s0 + 1] = torch.ops.aten.unsqueeze.default(slice_tensor, 1);  slice_tensor = None\\n        unsqueeze_default_2: b8[1, 1, 1, s0 + 1] = torch.ops.aten.unsqueeze.default(unsqueeze_default_1, 2);  unsqueeze_default_1 = None\\n        slice_tensor_1: b8[1, 1, 1, s0 + 1] = torch.ops.aten.slice.Tensor(unsqueeze_default_2, 3, 0, 9223372036854775807);  unsqueeze_default_2 = None\\n        expand_default: b8[1, 1, 1, s0 + 1] = torch.ops.aten.expand.default(slice_tensor_1, [1, 1, 1, add_1]);  slice_tensor_1 = add_1 = None\\n        _to_copy_default: f32[1, 1, 1, s0 + 1] = torch.ops.aten._to_copy.default(expand_default, dtype = torch.float32);  expand_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70, code: inverted_mask = 1.0 - expanded_mask\\n        rsub_scalar: f32[1, 1, 1, s0 + 1] = torch.ops.aten.rsub.Scalar(_to_copy_default, 1.0);  _to_copy_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\\n        _to_copy_default_1: b8[1, 1, 1, s0 + 1] = torch.ops.aten._to_copy.default(rsub_scalar, dtype = torch.bool)\\n        masked_fill_scalar: f32[1, 1, 1, s0 + 1] = torch.ops.aten.masked_fill.Scalar(rsub_scalar, _to_copy_default_1, -3.4028234663852886e+38);  rsub_scalar = _to_copy_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(embedding_default, 2)\\n        mean_dim: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar, [-1], True);  pow_tensor_scalar = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim, 1e-06);  mean_dim = None\\n        rsqrt_default: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor);  add_tensor = None\\n        detach_default: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default)\\n        mul_tensor: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(embedding_default, rsqrt_default);  rsqrt_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant1 = self._param_constant1\\n        mul_tensor_1: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant1, mul_tensor);  _param_constant1 = mul_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant2 = self._param_constant2\\n        t_default: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant2);  _param_constant2 = None\\n        view_default_1: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_1, [1, 4096])\\n        mm_default: f32[1, 4096] = torch.ops.aten.mm.default(view_default_1, t_default);  view_default_1 = t_default = None\\n        view_default_2: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default, [1, 1, 4096]);  mm_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant3 = self._param_constant3\\n        t_default_1: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant3);  _param_constant3 = None\\n        view_default_3: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_1, [1, 4096])\\n        mm_default_1: f32[1, 4096] = torch.ops.aten.mm.default(view_default_3, t_default_1);  view_default_3 = t_default_1 = None\\n        view_default_4: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_1, [1, 1, 4096]);  mm_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant4 = self._param_constant4\\n        t_default_2: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant4);  _param_constant4 = None\\n        view_default_5: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_1, [1, 4096]);  mul_tensor_1 = None\\n        mm_default_2: f32[1, 4096] = torch.ops.aten.mm.default(view_default_5, t_default_2);  view_default_5 = t_default_2 = None\\n        view_default_6: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_2, [1, 1, 4096]);  mm_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_7: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_2, [1, 1, 32, 128]);  view_default_2 = None\\n        transpose_int: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_7, 1, 2);  view_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_8: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_4, [1, 1, 32, 128]);  view_default_4 = None\\n        transpose_int_1: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_8, 1, 2);  view_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_9: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_6, [1, 1, 32, 128]);  view_default_6 = None\\n        transpose_int_2: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_9, 1, 2);  view_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant0 = self._tensor_constant0\\n        slice_tensor_2: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant0, 0, 0, 9223372036854775807);  _tensor_constant0 = None\\n        slice_tensor_3: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_2, 1, 0, 9223372036854775807);  slice_tensor_2 = None\\n        add_2: Sym(s0 + 1) = 1 + sym_size;  sym_size = None\\n        slice_tensor_4: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_3, 2, 0, add_2);  slice_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant1 = self._tensor_constant1\\n        slice_tensor_5: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant1, 0, 0, 9223372036854775807);  _tensor_constant1 = None\\n        slice_tensor_6: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_5, 1, 0, 9223372036854775807);  slice_tensor_5 = None\\n        slice_tensor_7: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_6, 2, 0, add_2);  slice_tensor_6 = add_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_4, 1);  slice_tensor_4 = None\\n        squeeze_dim_1: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim, 0);  squeeze_dim = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_2: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_7, 1);  slice_tensor_7 = None\\n        squeeze_dim_3: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_2, 0);  squeeze_dim_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_1, [view_default]);  squeeze_dim_1 = None\\n        unsqueeze_default_3: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor, 1);  index_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_1: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_3, [view_default]);  squeeze_dim_3 = None\\n        unsqueeze_default_4: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_1, 1);  index_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_2: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int, unsqueeze_default_3)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_8: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_9: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 64, 9223372036854775807);  transpose_int = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_9);  slice_tensor_9 = None\\n        cat_default: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default, slice_tensor_8], -1);  neg_default = slice_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_3: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default, unsqueeze_default_4);  cat_default = None\\n        add_tensor_1: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_2, mul_tensor_3);  mul_tensor_2 = mul_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_4: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_1, unsqueeze_default_3);  unsqueeze_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_10: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_11: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 64, 9223372036854775807);  transpose_int_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_1: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_11);  slice_tensor_11 = None\\n        cat_default_1: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_1, slice_tensor_10], -1);  neg_default_1 = slice_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_5: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_1, unsqueeze_default_4);  cat_default_1 = unsqueeze_default_4 = None\\n        add_tensor_2: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_4, mul_tensor_5);  mul_tensor_4 = mul_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_2: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg1, add_tensor_2], 2);  arg1 = add_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_3: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg2, transpose_int_2], 2);  arg2 = transpose_int_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_3: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_2, 2, 3)\\n        expand_default_1: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_1, [1, 32, 1, 128]);  add_tensor_1 = None\\n        view_default_10: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_1, [32, 1, 128]);  expand_default_1 = None\\n        sym_size_1: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_2, 2)\\n        expand_default_2: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_3, [1, 32, 128, sym_size_1]);  transpose_int_3 = None\\n        view_default_11: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_2, [32, 128, sym_size_1]);  expand_default_2 = None\\n        bmm_default: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_10, view_default_11);  view_default_10 = view_default_11 = None\\n        view_default_12: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default, [1, 32, 1, sym_size_1]);  bmm_default = None\\n        div_tensor: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_12, 11.313708498984761);  view_default_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor, masked_fill_scalar);  div_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_3, -1, False);  add_tensor_3 = None\\n        detach_default_1: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default, [1, 32, 1, sym_size_1]);  _softmax_default = None\\n        view_default_13: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_3, [32, 1, sym_size_1]);  expand_default_3 = sym_size_1 = None\\n        sym_size_2: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_3, 2)\\n        expand_default_4: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_3, [1, 32, sym_size_2, 128])\\n        view_default_14: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_4, [32, sym_size_2, 128]);  expand_default_4 = sym_size_2 = None\\n        bmm_default_1: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_13, view_default_14);  view_default_13 = view_default_14 = None\\n        view_default_15: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_1, [1, 32, 1, 128]);  bmm_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_4: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_15, 1, 2);  view_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_16: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_4, [1, 1, 4096]);  transpose_int_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant5 = self._param_constant5\\n        t_default_3: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant5);  _param_constant5 = None\\n        view_default_17: f32[1, 4096] = torch.ops.aten.view.default(view_default_16, [1, 4096]);  view_default_16 = None\\n        mm_default_3: f32[1, 4096] = torch.ops.aten.mm.default(view_default_17, t_default_3);  view_default_17 = t_default_3 = None\\n        view_default_18: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_3, [1, 1, 4096]);  mm_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_4: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(embedding_default, view_default_18);  embedding_default = view_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_1: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_4, 2)\\n        mean_dim_1: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_1, [-1], True);  pow_tensor_scalar_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_5: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_1, 1e-06);  mean_dim_1 = None\\n        rsqrt_default_1: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_5);  add_tensor_5 = None\\n        detach_default_2: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_1)\\n        mul_tensor_6: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_4, rsqrt_default_1);  rsqrt_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant6 = self._param_constant6\\n        mul_tensor_7: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant6, mul_tensor_6);  _param_constant6 = mul_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant7 = self._param_constant7\\n        t_default_4: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant7);  _param_constant7 = None\\n        view_default_19: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_7, [1, 4096])\\n        mm_default_4: f32[1, 11008] = torch.ops.aten.mm.default(view_default_19, t_default_4);  view_default_19 = t_default_4 = None\\n        view_default_20: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_4, [1, 1, 11008]);  mm_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_20);  view_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant8 = self._param_constant8\\n        t_default_5: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant8);  _param_constant8 = None\\n        view_default_21: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_7, [1, 4096]);  mul_tensor_7 = None\\n        mm_default_5: f32[1, 11008] = torch.ops.aten.mm.default(view_default_21, t_default_5);  view_default_21 = t_default_5 = None\\n        view_default_22: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_5, [1, 1, 11008]);  mm_default_5 = None\\n        mul_tensor_8: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default, view_default_22);  silu_default = view_default_22 = None\\n        _param_constant9 = self._param_constant9\\n        t_default_6: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant9);  _param_constant9 = None\\n        view_default_23: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_8, [1, 11008]);  mul_tensor_8 = None\\n        mm_default_6: f32[1, 4096] = torch.ops.aten.mm.default(view_default_23, t_default_6);  view_default_23 = t_default_6 = None\\n        view_default_24: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_6, [1, 1, 4096]);  mm_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_6: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_4, view_default_24);  add_tensor_4 = view_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_2: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_6, 2)\\n        mean_dim_2: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_2, [-1], True);  pow_tensor_scalar_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_7: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_2, 1e-06);  mean_dim_2 = None\\n        rsqrt_default_2: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_7);  add_tensor_7 = None\\n        detach_default_3: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_2)\\n        mul_tensor_9: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_6, rsqrt_default_2);  rsqrt_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant10 = self._param_constant10\\n        mul_tensor_10: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant10, mul_tensor_9);  _param_constant10 = mul_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant11 = self._param_constant11\\n        t_default_7: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant11);  _param_constant11 = None\\n        view_default_25: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_10, [1, 4096])\\n        mm_default_7: f32[1, 4096] = torch.ops.aten.mm.default(view_default_25, t_default_7);  view_default_25 = t_default_7 = None\\n        view_default_26: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_7, [1, 1, 4096]);  mm_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant12 = self._param_constant12\\n        t_default_8: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant12);  _param_constant12 = None\\n        view_default_27: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_10, [1, 4096])\\n        mm_default_8: f32[1, 4096] = torch.ops.aten.mm.default(view_default_27, t_default_8);  view_default_27 = t_default_8 = None\\n        view_default_28: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_8, [1, 1, 4096]);  mm_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant13 = self._param_constant13\\n        t_default_9: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant13);  _param_constant13 = None\\n        view_default_29: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_10, [1, 4096]);  mul_tensor_10 = None\\n        mm_default_9: f32[1, 4096] = torch.ops.aten.mm.default(view_default_29, t_default_9);  view_default_29 = t_default_9 = None\\n        view_default_30: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_9, [1, 1, 4096]);  mm_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_31: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_26, [1, 1, 32, 128]);  view_default_26 = None\\n        transpose_int_5: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_31, 1, 2);  view_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_32: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_28, [1, 1, 32, 128]);  view_default_28 = None\\n        transpose_int_6: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_32, 1, 2);  view_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_33: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_30, [1, 1, 32, 128]);  view_default_30 = None\\n        transpose_int_7: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_33, 1, 2);  view_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant2 = self._tensor_constant2\\n        slice_tensor_12: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant2, 0, 0, 9223372036854775807);  _tensor_constant2 = None\\n        slice_tensor_13: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_12, 1, 0, 9223372036854775807);  slice_tensor_12 = None\\n        sym_size_3: Sym(s0) = torch.ops.aten.sym_size(arg3, 2)\\n        add_3: Sym(s0 + 1) = 1 + sym_size_3;  sym_size_3 = None\\n        slice_tensor_14: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_13, 2, 0, add_3);  slice_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant3 = self._tensor_constant3\\n        slice_tensor_15: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant3, 0, 0, 9223372036854775807);  _tensor_constant3 = None\\n        slice_tensor_16: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_15, 1, 0, 9223372036854775807);  slice_tensor_15 = None\\n        slice_tensor_17: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_16, 2, 0, add_3);  slice_tensor_16 = add_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_4: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_14, 1);  slice_tensor_14 = None\\n        squeeze_dim_5: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_4, 0);  squeeze_dim_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_6: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_17, 1);  slice_tensor_17 = None\\n        squeeze_dim_7: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_6, 0);  squeeze_dim_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_2: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_5, [view_default]);  squeeze_dim_5 = None\\n        unsqueeze_default_5: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_2, 1);  index_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_3: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_7, [view_default]);  squeeze_dim_7 = None\\n        unsqueeze_default_6: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_3, 1);  index_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_11: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_5, unsqueeze_default_5)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_18: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_19: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 64, 9223372036854775807);  transpose_int_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_2: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_19);  slice_tensor_19 = None\\n        cat_default_4: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_2, slice_tensor_18], -1);  neg_default_2 = slice_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_12: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_4, unsqueeze_default_6);  cat_default_4 = None\\n        add_tensor_8: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_11, mul_tensor_12);  mul_tensor_11 = mul_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_13: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_6, unsqueeze_default_5);  unsqueeze_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_20: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_21: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 64, 9223372036854775807);  transpose_int_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_3: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_21);  slice_tensor_21 = None\\n        cat_default_5: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_3, slice_tensor_20], -1);  neg_default_3 = slice_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_14: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_5, unsqueeze_default_6);  cat_default_5 = unsqueeze_default_6 = None\\n        add_tensor_9: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_13, mul_tensor_14);  mul_tensor_13 = mul_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_6: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg3, add_tensor_9], 2);  arg3 = add_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_7: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg4, transpose_int_7], 2);  arg4 = transpose_int_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_8: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_6, 2, 3)\\n        expand_default_5: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_8, [1, 32, 1, 128]);  add_tensor_8 = None\\n        view_default_34: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_5, [32, 1, 128]);  expand_default_5 = None\\n        sym_size_4: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_6, 2)\\n        expand_default_6: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_8, [1, 32, 128, sym_size_4]);  transpose_int_8 = None\\n        view_default_35: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_6, [32, 128, sym_size_4]);  expand_default_6 = None\\n        bmm_default_2: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_34, view_default_35);  view_default_34 = view_default_35 = None\\n        view_default_36: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_2, [1, 32, 1, sym_size_4]);  bmm_default_2 = None\\n        div_tensor_1: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_36, 11.313708498984761);  view_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_1, masked_fill_scalar);  div_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_1: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_10, -1, False);  add_tensor_10 = None\\n        detach_default_4: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_1)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_1, [1, 32, 1, sym_size_4]);  _softmax_default_1 = None\\n        view_default_37: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_7, [32, 1, sym_size_4]);  expand_default_7 = sym_size_4 = None\\n        sym_size_5: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_7, 2)\\n        expand_default_8: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_7, [1, 32, sym_size_5, 128])\\n        view_default_38: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_8, [32, sym_size_5, 128]);  expand_default_8 = sym_size_5 = None\\n        bmm_default_3: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_37, view_default_38);  view_default_37 = view_default_38 = None\\n        view_default_39: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_3, [1, 32, 1, 128]);  bmm_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_9: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_39, 1, 2);  view_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_40: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_9, [1, 1, 4096]);  transpose_int_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant14 = self._param_constant14\\n        t_default_10: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant14);  _param_constant14 = None\\n        view_default_41: f32[1, 4096] = torch.ops.aten.view.default(view_default_40, [1, 4096]);  view_default_40 = None\\n        mm_default_10: f32[1, 4096] = torch.ops.aten.mm.default(view_default_41, t_default_10);  view_default_41 = t_default_10 = None\\n        view_default_42: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_10, [1, 1, 4096]);  mm_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_11: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_6, view_default_42);  add_tensor_6 = view_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_3: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_11, 2)\\n        mean_dim_3: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_3, [-1], True);  pow_tensor_scalar_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_12: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_3, 1e-06);  mean_dim_3 = None\\n        rsqrt_default_3: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_12);  add_tensor_12 = None\\n        detach_default_5: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_3)\\n        mul_tensor_15: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_11, rsqrt_default_3);  rsqrt_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant15 = self._param_constant15\\n        mul_tensor_16: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant15, mul_tensor_15);  _param_constant15 = mul_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant16 = self._param_constant16\\n        t_default_11: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant16);  _param_constant16 = None\\n        view_default_43: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_16, [1, 4096])\\n        mm_default_11: f32[1, 11008] = torch.ops.aten.mm.default(view_default_43, t_default_11);  view_default_43 = t_default_11 = None\\n        view_default_44: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_11, [1, 1, 11008]);  mm_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_1: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_44);  view_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant17 = self._param_constant17\\n        t_default_12: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant17);  _param_constant17 = None\\n        view_default_45: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_16, [1, 4096]);  mul_tensor_16 = None\\n        mm_default_12: f32[1, 11008] = torch.ops.aten.mm.default(view_default_45, t_default_12);  view_default_45 = t_default_12 = None\\n        view_default_46: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_12, [1, 1, 11008]);  mm_default_12 = None\\n        mul_tensor_17: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_1, view_default_46);  silu_default_1 = view_default_46 = None\\n        _param_constant18 = self._param_constant18\\n        t_default_13: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant18);  _param_constant18 = None\\n        view_default_47: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_17, [1, 11008]);  mul_tensor_17 = None\\n        mm_default_13: f32[1, 4096] = torch.ops.aten.mm.default(view_default_47, t_default_13);  view_default_47 = t_default_13 = None\\n        view_default_48: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_13, [1, 1, 4096]);  mm_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_13: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_11, view_default_48);  add_tensor_11 = view_default_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_4: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_13, 2)\\n        mean_dim_4: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_4, [-1], True);  pow_tensor_scalar_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_14: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_4, 1e-06);  mean_dim_4 = None\\n        rsqrt_default_4: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_14);  add_tensor_14 = None\\n        detach_default_6: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_4)\\n        mul_tensor_18: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_13, rsqrt_default_4);  rsqrt_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant19 = self._param_constant19\\n        mul_tensor_19: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant19, mul_tensor_18);  _param_constant19 = mul_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant20 = self._param_constant20\\n        t_default_14: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant20);  _param_constant20 = None\\n        view_default_49: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_19, [1, 4096])\\n        mm_default_14: f32[1, 4096] = torch.ops.aten.mm.default(view_default_49, t_default_14);  view_default_49 = t_default_14 = None\\n        view_default_50: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_14, [1, 1, 4096]);  mm_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant21 = self._param_constant21\\n        t_default_15: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant21);  _param_constant21 = None\\n        view_default_51: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_19, [1, 4096])\\n        mm_default_15: f32[1, 4096] = torch.ops.aten.mm.default(view_default_51, t_default_15);  view_default_51 = t_default_15 = None\\n        view_default_52: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_15, [1, 1, 4096]);  mm_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant22 = self._param_constant22\\n        t_default_16: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant22);  _param_constant22 = None\\n        view_default_53: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_19, [1, 4096]);  mul_tensor_19 = None\\n        mm_default_16: f32[1, 4096] = torch.ops.aten.mm.default(view_default_53, t_default_16);  view_default_53 = t_default_16 = None\\n        view_default_54: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_16, [1, 1, 4096]);  mm_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_55: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_50, [1, 1, 32, 128]);  view_default_50 = None\\n        transpose_int_10: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_55, 1, 2);  view_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_56: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_52, [1, 1, 32, 128]);  view_default_52 = None\\n        transpose_int_11: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_56, 1, 2);  view_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_57: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_54, [1, 1, 32, 128]);  view_default_54 = None\\n        transpose_int_12: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_57, 1, 2);  view_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant4 = self._tensor_constant4\\n        slice_tensor_22: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant4, 0, 0, 9223372036854775807);  _tensor_constant4 = None\\n        slice_tensor_23: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_22, 1, 0, 9223372036854775807);  slice_tensor_22 = None\\n        sym_size_6: Sym(s0) = torch.ops.aten.sym_size(arg5, 2)\\n        add_4: Sym(s0 + 1) = 1 + sym_size_6;  sym_size_6 = None\\n        slice_tensor_24: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_23, 2, 0, add_4);  slice_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant5 = self._tensor_constant5\\n        slice_tensor_25: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant5, 0, 0, 9223372036854775807);  _tensor_constant5 = None\\n        slice_tensor_26: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_25, 1, 0, 9223372036854775807);  slice_tensor_25 = None\\n        slice_tensor_27: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_26, 2, 0, add_4);  slice_tensor_26 = add_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_8: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_24, 1);  slice_tensor_24 = None\\n        squeeze_dim_9: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_8, 0);  squeeze_dim_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_10: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_27, 1);  slice_tensor_27 = None\\n        squeeze_dim_11: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_10, 0);  squeeze_dim_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_4: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_9, [view_default]);  squeeze_dim_9 = None\\n        unsqueeze_default_7: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_4, 1);  index_tensor_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_5: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_11, [view_default]);  squeeze_dim_11 = None\\n        unsqueeze_default_8: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_5, 1);  index_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_20: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_10, unsqueeze_default_7)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_28: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_29: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 64, 9223372036854775807);  transpose_int_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_4: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_29);  slice_tensor_29 = None\\n        cat_default_8: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_4, slice_tensor_28], -1);  neg_default_4 = slice_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_21: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_8, unsqueeze_default_8);  cat_default_8 = None\\n        add_tensor_15: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_20, mul_tensor_21);  mul_tensor_20 = mul_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_22: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_11, unsqueeze_default_7);  unsqueeze_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_30: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_31: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 64, 9223372036854775807);  transpose_int_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_5: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_31);  slice_tensor_31 = None\\n        cat_default_9: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_5, slice_tensor_30], -1);  neg_default_5 = slice_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_23: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_9, unsqueeze_default_8);  cat_default_9 = unsqueeze_default_8 = None\\n        add_tensor_16: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_22, mul_tensor_23);  mul_tensor_22 = mul_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_10: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg5, add_tensor_16], 2);  arg5 = add_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_11: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg6, transpose_int_12], 2);  arg6 = transpose_int_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_13: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_10, 2, 3)\\n        expand_default_9: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_15, [1, 32, 1, 128]);  add_tensor_15 = None\\n        view_default_58: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_9, [32, 1, 128]);  expand_default_9 = None\\n        sym_size_7: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_10, 2)\\n        expand_default_10: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_13, [1, 32, 128, sym_size_7]);  transpose_int_13 = None\\n        view_default_59: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_10, [32, 128, sym_size_7]);  expand_default_10 = None\\n        bmm_default_4: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_58, view_default_59);  view_default_58 = view_default_59 = None\\n        view_default_60: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_4, [1, 32, 1, sym_size_7]);  bmm_default_4 = None\\n        div_tensor_2: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_60, 11.313708498984761);  view_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_17: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_2, masked_fill_scalar);  div_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_2: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_17, -1, False);  add_tensor_17 = None\\n        detach_default_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_2)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_11: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_2, [1, 32, 1, sym_size_7]);  _softmax_default_2 = None\\n        view_default_61: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_11, [32, 1, sym_size_7]);  expand_default_11 = sym_size_7 = None\\n        sym_size_8: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_11, 2)\\n        expand_default_12: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_11, [1, 32, sym_size_8, 128])\\n        view_default_62: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_12, [32, sym_size_8, 128]);  expand_default_12 = sym_size_8 = None\\n        bmm_default_5: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_61, view_default_62);  view_default_61 = view_default_62 = None\\n        view_default_63: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_5, [1, 32, 1, 128]);  bmm_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_14: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_63, 1, 2);  view_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_64: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_14, [1, 1, 4096]);  transpose_int_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant23 = self._param_constant23\\n        t_default_17: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant23);  _param_constant23 = None\\n        view_default_65: f32[1, 4096] = torch.ops.aten.view.default(view_default_64, [1, 4096]);  view_default_64 = None\\n        mm_default_17: f32[1, 4096] = torch.ops.aten.mm.default(view_default_65, t_default_17);  view_default_65 = t_default_17 = None\\n        view_default_66: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_17, [1, 1, 4096]);  mm_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_18: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_13, view_default_66);  add_tensor_13 = view_default_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_5: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_18, 2)\\n        mean_dim_5: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_5, [-1], True);  pow_tensor_scalar_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_19: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_5, 1e-06);  mean_dim_5 = None\\n        rsqrt_default_5: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_19);  add_tensor_19 = None\\n        detach_default_8: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_5)\\n        mul_tensor_24: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_18, rsqrt_default_5);  rsqrt_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant24 = self._param_constant24\\n        mul_tensor_25: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant24, mul_tensor_24);  _param_constant24 = mul_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant25 = self._param_constant25\\n        t_default_18: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant25);  _param_constant25 = None\\n        view_default_67: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_25, [1, 4096])\\n        mm_default_18: f32[1, 11008] = torch.ops.aten.mm.default(view_default_67, t_default_18);  view_default_67 = t_default_18 = None\\n        view_default_68: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_18, [1, 1, 11008]);  mm_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_2: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_68);  view_default_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant26 = self._param_constant26\\n        t_default_19: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant26);  _param_constant26 = None\\n        view_default_69: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_25, [1, 4096]);  mul_tensor_25 = None\\n        mm_default_19: f32[1, 11008] = torch.ops.aten.mm.default(view_default_69, t_default_19);  view_default_69 = t_default_19 = None\\n        view_default_70: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_19, [1, 1, 11008]);  mm_default_19 = None\\n        mul_tensor_26: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_2, view_default_70);  silu_default_2 = view_default_70 = None\\n        _param_constant27 = self._param_constant27\\n        t_default_20: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant27);  _param_constant27 = None\\n        view_default_71: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_26, [1, 11008]);  mul_tensor_26 = None\\n        mm_default_20: f32[1, 4096] = torch.ops.aten.mm.default(view_default_71, t_default_20);  view_default_71 = t_default_20 = None\\n        view_default_72: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_20, [1, 1, 4096]);  mm_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_20: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_18, view_default_72);  add_tensor_18 = view_default_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_6: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_20, 2)\\n        mean_dim_6: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_6, [-1], True);  pow_tensor_scalar_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_21: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_6, 1e-06);  mean_dim_6 = None\\n        rsqrt_default_6: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_21);  add_tensor_21 = None\\n        detach_default_9: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_6)\\n        mul_tensor_27: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_20, rsqrt_default_6);  rsqrt_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant28 = self._param_constant28\\n        mul_tensor_28: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant28, mul_tensor_27);  _param_constant28 = mul_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant29 = self._param_constant29\\n        t_default_21: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant29);  _param_constant29 = None\\n        view_default_73: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_28, [1, 4096])\\n        mm_default_21: f32[1, 4096] = torch.ops.aten.mm.default(view_default_73, t_default_21);  view_default_73 = t_default_21 = None\\n        view_default_74: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_21, [1, 1, 4096]);  mm_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant30 = self._param_constant30\\n        t_default_22: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant30);  _param_constant30 = None\\n        view_default_75: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_28, [1, 4096])\\n        mm_default_22: f32[1, 4096] = torch.ops.aten.mm.default(view_default_75, t_default_22);  view_default_75 = t_default_22 = None\\n        view_default_76: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_22, [1, 1, 4096]);  mm_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant31 = self._param_constant31\\n        t_default_23: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant31);  _param_constant31 = None\\n        view_default_77: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_28, [1, 4096]);  mul_tensor_28 = None\\n        mm_default_23: f32[1, 4096] = torch.ops.aten.mm.default(view_default_77, t_default_23);  view_default_77 = t_default_23 = None\\n        view_default_78: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_23, [1, 1, 4096]);  mm_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_79: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_74, [1, 1, 32, 128]);  view_default_74 = None\\n        transpose_int_15: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_79, 1, 2);  view_default_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_80: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_76, [1, 1, 32, 128]);  view_default_76 = None\\n        transpose_int_16: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_80, 1, 2);  view_default_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_81: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_78, [1, 1, 32, 128]);  view_default_78 = None\\n        transpose_int_17: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_81, 1, 2);  view_default_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant6 = self._tensor_constant6\\n        slice_tensor_32: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant6, 0, 0, 9223372036854775807);  _tensor_constant6 = None\\n        slice_tensor_33: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_32, 1, 0, 9223372036854775807);  slice_tensor_32 = None\\n        sym_size_9: Sym(s0) = torch.ops.aten.sym_size(arg7, 2)\\n        add_5: Sym(s0 + 1) = 1 + sym_size_9;  sym_size_9 = None\\n        slice_tensor_34: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_33, 2, 0, add_5);  slice_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant7 = self._tensor_constant7\\n        slice_tensor_35: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant7, 0, 0, 9223372036854775807);  _tensor_constant7 = None\\n        slice_tensor_36: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_35, 1, 0, 9223372036854775807);  slice_tensor_35 = None\\n        slice_tensor_37: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_36, 2, 0, add_5);  slice_tensor_36 = add_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_12: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_34, 1);  slice_tensor_34 = None\\n        squeeze_dim_13: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_12, 0);  squeeze_dim_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_14: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_37, 1);  slice_tensor_37 = None\\n        squeeze_dim_15: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_14, 0);  squeeze_dim_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_6: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_13, [view_default]);  squeeze_dim_13 = None\\n        unsqueeze_default_9: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_6, 1);  index_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_7: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_15, [view_default]);  squeeze_dim_15 = None\\n        unsqueeze_default_10: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_7, 1);  index_tensor_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_29: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_15, unsqueeze_default_9)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_38: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_39: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 64, 9223372036854775807);  transpose_int_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_6: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_39);  slice_tensor_39 = None\\n        cat_default_12: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_6, slice_tensor_38], -1);  neg_default_6 = slice_tensor_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_30: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_12, unsqueeze_default_10);  cat_default_12 = None\\n        add_tensor_22: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_29, mul_tensor_30);  mul_tensor_29 = mul_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_31: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_16, unsqueeze_default_9);  unsqueeze_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_40: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_41: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 64, 9223372036854775807);  transpose_int_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_7: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_41);  slice_tensor_41 = None\\n        cat_default_13: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_7, slice_tensor_40], -1);  neg_default_7 = slice_tensor_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_32: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_13, unsqueeze_default_10);  cat_default_13 = unsqueeze_default_10 = None\\n        add_tensor_23: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_31, mul_tensor_32);  mul_tensor_31 = mul_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_14: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg7, add_tensor_23], 2);  arg7 = add_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_15: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg8, transpose_int_17], 2);  arg8 = transpose_int_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_18: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_14, 2, 3)\\n        expand_default_13: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_22, [1, 32, 1, 128]);  add_tensor_22 = None\\n        view_default_82: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_13, [32, 1, 128]);  expand_default_13 = None\\n        sym_size_10: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_14, 2)\\n        expand_default_14: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_18, [1, 32, 128, sym_size_10]);  transpose_int_18 = None\\n        view_default_83: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_14, [32, 128, sym_size_10]);  expand_default_14 = None\\n        bmm_default_6: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_82, view_default_83);  view_default_82 = view_default_83 = None\\n        view_default_84: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_6, [1, 32, 1, sym_size_10]);  bmm_default_6 = None\\n        div_tensor_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_84, 11.313708498984761);  view_default_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_24: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_3, masked_fill_scalar);  div_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_3: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_24, -1, False);  add_tensor_24 = None\\n        detach_default_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_3)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_15: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_3, [1, 32, 1, sym_size_10]);  _softmax_default_3 = None\\n        view_default_85: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_15, [32, 1, sym_size_10]);  expand_default_15 = sym_size_10 = None\\n        sym_size_11: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_15, 2)\\n        expand_default_16: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_15, [1, 32, sym_size_11, 128])\\n        view_default_86: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_16, [32, sym_size_11, 128]);  expand_default_16 = sym_size_11 = None\\n        bmm_default_7: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_85, view_default_86);  view_default_85 = view_default_86 = None\\n        view_default_87: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_7, [1, 32, 1, 128]);  bmm_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_19: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_87, 1, 2);  view_default_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_88: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_19, [1, 1, 4096]);  transpose_int_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant32 = self._param_constant32\\n        t_default_24: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant32);  _param_constant32 = None\\n        view_default_89: f32[1, 4096] = torch.ops.aten.view.default(view_default_88, [1, 4096]);  view_default_88 = None\\n        mm_default_24: f32[1, 4096] = torch.ops.aten.mm.default(view_default_89, t_default_24);  view_default_89 = t_default_24 = None\\n        view_default_90: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_24, [1, 1, 4096]);  mm_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_25: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_20, view_default_90);  add_tensor_20 = view_default_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_7: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_25, 2)\\n        mean_dim_7: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_7, [-1], True);  pow_tensor_scalar_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_26: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_7, 1e-06);  mean_dim_7 = None\\n        rsqrt_default_7: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_26);  add_tensor_26 = None\\n        detach_default_11: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_7)\\n        mul_tensor_33: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_25, rsqrt_default_7);  rsqrt_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant33 = self._param_constant33\\n        mul_tensor_34: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant33, mul_tensor_33);  _param_constant33 = mul_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant34 = self._param_constant34\\n        t_default_25: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant34);  _param_constant34 = None\\n        view_default_91: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_34, [1, 4096])\\n        mm_default_25: f32[1, 11008] = torch.ops.aten.mm.default(view_default_91, t_default_25);  view_default_91 = t_default_25 = None\\n        view_default_92: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_25, [1, 1, 11008]);  mm_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_3: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_92);  view_default_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant35 = self._param_constant35\\n        t_default_26: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant35);  _param_constant35 = None\\n        view_default_93: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_34, [1, 4096]);  mul_tensor_34 = None\\n        mm_default_26: f32[1, 11008] = torch.ops.aten.mm.default(view_default_93, t_default_26);  view_default_93 = t_default_26 = None\\n        view_default_94: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_26, [1, 1, 11008]);  mm_default_26 = None\\n        mul_tensor_35: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_3, view_default_94);  silu_default_3 = view_default_94 = None\\n        _param_constant36 = self._param_constant36\\n        t_default_27: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant36);  _param_constant36 = None\\n        view_default_95: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_35, [1, 11008]);  mul_tensor_35 = None\\n        mm_default_27: f32[1, 4096] = torch.ops.aten.mm.default(view_default_95, t_default_27);  view_default_95 = t_default_27 = None\\n        view_default_96: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_27, [1, 1, 4096]);  mm_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_27: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_25, view_default_96);  add_tensor_25 = view_default_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_8: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_27, 2)\\n        mean_dim_8: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_8, [-1], True);  pow_tensor_scalar_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_28: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_8, 1e-06);  mean_dim_8 = None\\n        rsqrt_default_8: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_28);  add_tensor_28 = None\\n        detach_default_12: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_8)\\n        mul_tensor_36: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_27, rsqrt_default_8);  rsqrt_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant37 = self._param_constant37\\n        mul_tensor_37: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant37, mul_tensor_36);  _param_constant37 = mul_tensor_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant38 = self._param_constant38\\n        t_default_28: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant38);  _param_constant38 = None\\n        view_default_97: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_37, [1, 4096])\\n        mm_default_28: f32[1, 4096] = torch.ops.aten.mm.default(view_default_97, t_default_28);  view_default_97 = t_default_28 = None\\n        view_default_98: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_28, [1, 1, 4096]);  mm_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant39 = self._param_constant39\\n        t_default_29: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant39);  _param_constant39 = None\\n        view_default_99: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_37, [1, 4096])\\n        mm_default_29: f32[1, 4096] = torch.ops.aten.mm.default(view_default_99, t_default_29);  view_default_99 = t_default_29 = None\\n        view_default_100: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_29, [1, 1, 4096]);  mm_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant40 = self._param_constant40\\n        t_default_30: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant40);  _param_constant40 = None\\n        view_default_101: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_37, [1, 4096]);  mul_tensor_37 = None\\n        mm_default_30: f32[1, 4096] = torch.ops.aten.mm.default(view_default_101, t_default_30);  view_default_101 = t_default_30 = None\\n        view_default_102: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_30, [1, 1, 4096]);  mm_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_103: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_98, [1, 1, 32, 128]);  view_default_98 = None\\n        transpose_int_20: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_103, 1, 2);  view_default_103 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_104: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_100, [1, 1, 32, 128]);  view_default_100 = None\\n        transpose_int_21: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_104, 1, 2);  view_default_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_105: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_102, [1, 1, 32, 128]);  view_default_102 = None\\n        transpose_int_22: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_105, 1, 2);  view_default_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant8 = self._tensor_constant8\\n        slice_tensor_42: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant8, 0, 0, 9223372036854775807);  _tensor_constant8 = None\\n        slice_tensor_43: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_42, 1, 0, 9223372036854775807);  slice_tensor_42 = None\\n        sym_size_12: Sym(s0) = torch.ops.aten.sym_size(arg9, 2)\\n        add_6: Sym(s0 + 1) = 1 + sym_size_12;  sym_size_12 = None\\n        slice_tensor_44: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_43, 2, 0, add_6);  slice_tensor_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant9 = self._tensor_constant9\\n        slice_tensor_45: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant9, 0, 0, 9223372036854775807);  _tensor_constant9 = None\\n        slice_tensor_46: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_45, 1, 0, 9223372036854775807);  slice_tensor_45 = None\\n        slice_tensor_47: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_46, 2, 0, add_6);  slice_tensor_46 = add_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_16: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_44, 1);  slice_tensor_44 = None\\n        squeeze_dim_17: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_16, 0);  squeeze_dim_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_18: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_47, 1);  slice_tensor_47 = None\\n        squeeze_dim_19: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_18, 0);  squeeze_dim_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_8: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_17, [view_default]);  squeeze_dim_17 = None\\n        unsqueeze_default_11: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_8, 1);  index_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_9: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_19, [view_default]);  squeeze_dim_19 = None\\n        unsqueeze_default_12: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_9, 1);  index_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_38: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_20, unsqueeze_default_11)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_48: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_49: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 64, 9223372036854775807);  transpose_int_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_8: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_49);  slice_tensor_49 = None\\n        cat_default_16: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_8, slice_tensor_48], -1);  neg_default_8 = slice_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_39: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_16, unsqueeze_default_12);  cat_default_16 = None\\n        add_tensor_29: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_38, mul_tensor_39);  mul_tensor_38 = mul_tensor_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_40: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_21, unsqueeze_default_11);  unsqueeze_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_50: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_51: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 64, 9223372036854775807);  transpose_int_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_9: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_51);  slice_tensor_51 = None\\n        cat_default_17: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_9, slice_tensor_50], -1);  neg_default_9 = slice_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_41: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_17, unsqueeze_default_12);  cat_default_17 = unsqueeze_default_12 = None\\n        add_tensor_30: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_40, mul_tensor_41);  mul_tensor_40 = mul_tensor_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_18: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg9, add_tensor_30], 2);  arg9 = add_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_19: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg10, transpose_int_22], 2);  arg10 = transpose_int_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_23: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_18, 2, 3)\\n        expand_default_17: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_29, [1, 32, 1, 128]);  add_tensor_29 = None\\n        view_default_106: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_17, [32, 1, 128]);  expand_default_17 = None\\n        sym_size_13: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_18, 2)\\n        expand_default_18: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_23, [1, 32, 128, sym_size_13]);  transpose_int_23 = None\\n        view_default_107: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_18, [32, 128, sym_size_13]);  expand_default_18 = None\\n        bmm_default_8: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_106, view_default_107);  view_default_106 = view_default_107 = None\\n        view_default_108: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_8, [1, 32, 1, sym_size_13]);  bmm_default_8 = None\\n        div_tensor_4: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_108, 11.313708498984761);  view_default_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_4, masked_fill_scalar);  div_tensor_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_4: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_31, -1, False);  add_tensor_31 = None\\n        detach_default_13: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_4)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_4, [1, 32, 1, sym_size_13]);  _softmax_default_4 = None\\n        view_default_109: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_19, [32, 1, sym_size_13]);  expand_default_19 = sym_size_13 = None\\n        sym_size_14: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_19, 2)\\n        expand_default_20: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_19, [1, 32, sym_size_14, 128])\\n        view_default_110: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_20, [32, sym_size_14, 128]);  expand_default_20 = sym_size_14 = None\\n        bmm_default_9: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_109, view_default_110);  view_default_109 = view_default_110 = None\\n        view_default_111: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_9, [1, 32, 1, 128]);  bmm_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_24: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_111, 1, 2);  view_default_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_112: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_24, [1, 1, 4096]);  transpose_int_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant41 = self._param_constant41\\n        t_default_31: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant41);  _param_constant41 = None\\n        view_default_113: f32[1, 4096] = torch.ops.aten.view.default(view_default_112, [1, 4096]);  view_default_112 = None\\n        mm_default_31: f32[1, 4096] = torch.ops.aten.mm.default(view_default_113, t_default_31);  view_default_113 = t_default_31 = None\\n        view_default_114: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_31, [1, 1, 4096]);  mm_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_32: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_27, view_default_114);  add_tensor_27 = view_default_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_9: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_32, 2)\\n        mean_dim_9: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_9, [-1], True);  pow_tensor_scalar_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_33: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_9, 1e-06);  mean_dim_9 = None\\n        rsqrt_default_9: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_33);  add_tensor_33 = None\\n        detach_default_14: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_9)\\n        mul_tensor_42: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_32, rsqrt_default_9);  rsqrt_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant42 = self._param_constant42\\n        mul_tensor_43: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant42, mul_tensor_42);  _param_constant42 = mul_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant43 = self._param_constant43\\n        t_default_32: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant43);  _param_constant43 = None\\n        view_default_115: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_43, [1, 4096])\\n        mm_default_32: f32[1, 11008] = torch.ops.aten.mm.default(view_default_115, t_default_32);  view_default_115 = t_default_32 = None\\n        view_default_116: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_32, [1, 1, 11008]);  mm_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_4: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_116);  view_default_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant44 = self._param_constant44\\n        t_default_33: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant44);  _param_constant44 = None\\n        view_default_117: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_43, [1, 4096]);  mul_tensor_43 = None\\n        mm_default_33: f32[1, 11008] = torch.ops.aten.mm.default(view_default_117, t_default_33);  view_default_117 = t_default_33 = None\\n        view_default_118: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_33, [1, 1, 11008]);  mm_default_33 = None\\n        mul_tensor_44: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_4, view_default_118);  silu_default_4 = view_default_118 = None\\n        _param_constant45 = self._param_constant45\\n        t_default_34: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant45);  _param_constant45 = None\\n        view_default_119: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_44, [1, 11008]);  mul_tensor_44 = None\\n        mm_default_34: f32[1, 4096] = torch.ops.aten.mm.default(view_default_119, t_default_34);  view_default_119 = t_default_34 = None\\n        view_default_120: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_34, [1, 1, 4096]);  mm_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_34: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_32, view_default_120);  add_tensor_32 = view_default_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_10: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_34, 2)\\n        mean_dim_10: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_10, [-1], True);  pow_tensor_scalar_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_35: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_10, 1e-06);  mean_dim_10 = None\\n        rsqrt_default_10: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_35);  add_tensor_35 = None\\n        detach_default_15: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_10)\\n        mul_tensor_45: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_34, rsqrt_default_10);  rsqrt_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant46 = self._param_constant46\\n        mul_tensor_46: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant46, mul_tensor_45);  _param_constant46 = mul_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant47 = self._param_constant47\\n        t_default_35: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant47);  _param_constant47 = None\\n        view_default_121: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_46, [1, 4096])\\n        mm_default_35: f32[1, 4096] = torch.ops.aten.mm.default(view_default_121, t_default_35);  view_default_121 = t_default_35 = None\\n        view_default_122: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_35, [1, 1, 4096]);  mm_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant48 = self._param_constant48\\n        t_default_36: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant48);  _param_constant48 = None\\n        view_default_123: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_46, [1, 4096])\\n        mm_default_36: f32[1, 4096] = torch.ops.aten.mm.default(view_default_123, t_default_36);  view_default_123 = t_default_36 = None\\n        view_default_124: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_36, [1, 1, 4096]);  mm_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant49 = self._param_constant49\\n        t_default_37: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant49);  _param_constant49 = None\\n        view_default_125: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_46, [1, 4096]);  mul_tensor_46 = None\\n        mm_default_37: f32[1, 4096] = torch.ops.aten.mm.default(view_default_125, t_default_37);  view_default_125 = t_default_37 = None\\n        view_default_126: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_37, [1, 1, 4096]);  mm_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_127: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_122, [1, 1, 32, 128]);  view_default_122 = None\\n        transpose_int_25: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_127, 1, 2);  view_default_127 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_128: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_124, [1, 1, 32, 128]);  view_default_124 = None\\n        transpose_int_26: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_128, 1, 2);  view_default_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_129: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_126, [1, 1, 32, 128]);  view_default_126 = None\\n        transpose_int_27: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_129, 1, 2);  view_default_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant10 = self._tensor_constant10\\n        slice_tensor_52: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant10, 0, 0, 9223372036854775807);  _tensor_constant10 = None\\n        slice_tensor_53: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_52, 1, 0, 9223372036854775807);  slice_tensor_52 = None\\n        sym_size_15: Sym(s0) = torch.ops.aten.sym_size(arg11, 2)\\n        add_7: Sym(s0 + 1) = 1 + sym_size_15;  sym_size_15 = None\\n        slice_tensor_54: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_53, 2, 0, add_7);  slice_tensor_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant11 = self._tensor_constant11\\n        slice_tensor_55: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant11, 0, 0, 9223372036854775807);  _tensor_constant11 = None\\n        slice_tensor_56: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_55, 1, 0, 9223372036854775807);  slice_tensor_55 = None\\n        slice_tensor_57: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_56, 2, 0, add_7);  slice_tensor_56 = add_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_20: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_54, 1);  slice_tensor_54 = None\\n        squeeze_dim_21: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_20, 0);  squeeze_dim_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_22: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_57, 1);  slice_tensor_57 = None\\n        squeeze_dim_23: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_22, 0);  squeeze_dim_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_10: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_21, [view_default]);  squeeze_dim_21 = None\\n        unsqueeze_default_13: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_10, 1);  index_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_11: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_23, [view_default]);  squeeze_dim_23 = None\\n        unsqueeze_default_14: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_11, 1);  index_tensor_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_47: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_25, unsqueeze_default_13)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_58: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_59: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 64, 9223372036854775807);  transpose_int_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_10: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_59);  slice_tensor_59 = None\\n        cat_default_20: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_10, slice_tensor_58], -1);  neg_default_10 = slice_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_48: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_20, unsqueeze_default_14);  cat_default_20 = None\\n        add_tensor_36: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_47, mul_tensor_48);  mul_tensor_47 = mul_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_49: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_26, unsqueeze_default_13);  unsqueeze_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_60: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_61: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 64, 9223372036854775807);  transpose_int_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_11: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_61);  slice_tensor_61 = None\\n        cat_default_21: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_11, slice_tensor_60], -1);  neg_default_11 = slice_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_50: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_21, unsqueeze_default_14);  cat_default_21 = unsqueeze_default_14 = None\\n        add_tensor_37: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_49, mul_tensor_50);  mul_tensor_49 = mul_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_22: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg11, add_tensor_37], 2);  arg11 = add_tensor_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_23: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg12, transpose_int_27], 2);  arg12 = transpose_int_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_28: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_22, 2, 3)\\n        expand_default_21: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_36, [1, 32, 1, 128]);  add_tensor_36 = None\\n        view_default_130: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_21, [32, 1, 128]);  expand_default_21 = None\\n        sym_size_16: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_22, 2)\\n        expand_default_22: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_28, [1, 32, 128, sym_size_16]);  transpose_int_28 = None\\n        view_default_131: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_22, [32, 128, sym_size_16]);  expand_default_22 = None\\n        bmm_default_10: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_130, view_default_131);  view_default_130 = view_default_131 = None\\n        view_default_132: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_10, [1, 32, 1, sym_size_16]);  bmm_default_10 = None\\n        div_tensor_5: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_132, 11.313708498984761);  view_default_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_38: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_5, masked_fill_scalar);  div_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_5: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_38, -1, False);  add_tensor_38 = None\\n        detach_default_16: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_5)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_23: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_5, [1, 32, 1, sym_size_16]);  _softmax_default_5 = None\\n        view_default_133: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_23, [32, 1, sym_size_16]);  expand_default_23 = sym_size_16 = None\\n        sym_size_17: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_23, 2)\\n        expand_default_24: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_23, [1, 32, sym_size_17, 128])\\n        view_default_134: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_24, [32, sym_size_17, 128]);  expand_default_24 = sym_size_17 = None\\n        bmm_default_11: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_133, view_default_134);  view_default_133 = view_default_134 = None\\n        view_default_135: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_11, [1, 32, 1, 128]);  bmm_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_29: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_135, 1, 2);  view_default_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_136: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_29, [1, 1, 4096]);  transpose_int_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant50 = self._param_constant50\\n        t_default_38: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant50);  _param_constant50 = None\\n        view_default_137: f32[1, 4096] = torch.ops.aten.view.default(view_default_136, [1, 4096]);  view_default_136 = None\\n        mm_default_38: f32[1, 4096] = torch.ops.aten.mm.default(view_default_137, t_default_38);  view_default_137 = t_default_38 = None\\n        view_default_138: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_38, [1, 1, 4096]);  mm_default_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_39: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_34, view_default_138);  add_tensor_34 = view_default_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_11: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_39, 2)\\n        mean_dim_11: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_11, [-1], True);  pow_tensor_scalar_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_40: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_11, 1e-06);  mean_dim_11 = None\\n        rsqrt_default_11: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_40);  add_tensor_40 = None\\n        detach_default_17: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_11)\\n        mul_tensor_51: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_39, rsqrt_default_11);  rsqrt_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant51 = self._param_constant51\\n        mul_tensor_52: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant51, mul_tensor_51);  _param_constant51 = mul_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant52 = self._param_constant52\\n        t_default_39: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant52);  _param_constant52 = None\\n        view_default_139: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_52, [1, 4096])\\n        mm_default_39: f32[1, 11008] = torch.ops.aten.mm.default(view_default_139, t_default_39);  view_default_139 = t_default_39 = None\\n        view_default_140: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_39, [1, 1, 11008]);  mm_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_5: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_140);  view_default_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant53 = self._param_constant53\\n        t_default_40: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant53);  _param_constant53 = None\\n        view_default_141: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_52, [1, 4096]);  mul_tensor_52 = None\\n        mm_default_40: f32[1, 11008] = torch.ops.aten.mm.default(view_default_141, t_default_40);  view_default_141 = t_default_40 = None\\n        view_default_142: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_40, [1, 1, 11008]);  mm_default_40 = None\\n        mul_tensor_53: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_5, view_default_142);  silu_default_5 = view_default_142 = None\\n        _param_constant54 = self._param_constant54\\n        t_default_41: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant54);  _param_constant54 = None\\n        view_default_143: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_53, [1, 11008]);  mul_tensor_53 = None\\n        mm_default_41: f32[1, 4096] = torch.ops.aten.mm.default(view_default_143, t_default_41);  view_default_143 = t_default_41 = None\\n        view_default_144: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_41, [1, 1, 4096]);  mm_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_41: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_39, view_default_144);  add_tensor_39 = view_default_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_12: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_41, 2)\\n        mean_dim_12: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_12, [-1], True);  pow_tensor_scalar_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_42: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_12, 1e-06);  mean_dim_12 = None\\n        rsqrt_default_12: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_42);  add_tensor_42 = None\\n        detach_default_18: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_12)\\n        mul_tensor_54: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_41, rsqrt_default_12);  rsqrt_default_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant55 = self._param_constant55\\n        mul_tensor_55: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant55, mul_tensor_54);  _param_constant55 = mul_tensor_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant56 = self._param_constant56\\n        t_default_42: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant56);  _param_constant56 = None\\n        view_default_145: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_55, [1, 4096])\\n        mm_default_42: f32[1, 4096] = torch.ops.aten.mm.default(view_default_145, t_default_42);  view_default_145 = t_default_42 = None\\n        view_default_146: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_42, [1, 1, 4096]);  mm_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant57 = self._param_constant57\\n        t_default_43: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant57);  _param_constant57 = None\\n        view_default_147: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_55, [1, 4096])\\n        mm_default_43: f32[1, 4096] = torch.ops.aten.mm.default(view_default_147, t_default_43);  view_default_147 = t_default_43 = None\\n        view_default_148: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_43, [1, 1, 4096]);  mm_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant58 = self._param_constant58\\n        t_default_44: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant58);  _param_constant58 = None\\n        view_default_149: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_55, [1, 4096]);  mul_tensor_55 = None\\n        mm_default_44: f32[1, 4096] = torch.ops.aten.mm.default(view_default_149, t_default_44);  view_default_149 = t_default_44 = None\\n        view_default_150: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_44, [1, 1, 4096]);  mm_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_151: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_146, [1, 1, 32, 128]);  view_default_146 = None\\n        transpose_int_30: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_151, 1, 2);  view_default_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_152: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_148, [1, 1, 32, 128]);  view_default_148 = None\\n        transpose_int_31: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_152, 1, 2);  view_default_152 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_153: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_150, [1, 1, 32, 128]);  view_default_150 = None\\n        transpose_int_32: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_153, 1, 2);  view_default_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant12 = self._tensor_constant12\\n        slice_tensor_62: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant12, 0, 0, 9223372036854775807);  _tensor_constant12 = None\\n        slice_tensor_63: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_62, 1, 0, 9223372036854775807);  slice_tensor_62 = None\\n        sym_size_18: Sym(s0) = torch.ops.aten.sym_size(arg13, 2)\\n        add_8: Sym(s0 + 1) = 1 + sym_size_18;  sym_size_18 = None\\n        slice_tensor_64: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_63, 2, 0, add_8);  slice_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant13 = self._tensor_constant13\\n        slice_tensor_65: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant13, 0, 0, 9223372036854775807);  _tensor_constant13 = None\\n        slice_tensor_66: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_65, 1, 0, 9223372036854775807);  slice_tensor_65 = None\\n        slice_tensor_67: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_66, 2, 0, add_8);  slice_tensor_66 = add_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_24: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_64, 1);  slice_tensor_64 = None\\n        squeeze_dim_25: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_24, 0);  squeeze_dim_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_26: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_67, 1);  slice_tensor_67 = None\\n        squeeze_dim_27: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_26, 0);  squeeze_dim_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_12: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_25, [view_default]);  squeeze_dim_25 = None\\n        unsqueeze_default_15: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_12, 1);  index_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_13: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_27, [view_default]);  squeeze_dim_27 = None\\n        unsqueeze_default_16: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_13, 1);  index_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_56: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_30, unsqueeze_default_15)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_68: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_69: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 64, 9223372036854775807);  transpose_int_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_12: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_69);  slice_tensor_69 = None\\n        cat_default_24: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_12, slice_tensor_68], -1);  neg_default_12 = slice_tensor_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_57: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_24, unsqueeze_default_16);  cat_default_24 = None\\n        add_tensor_43: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_56, mul_tensor_57);  mul_tensor_56 = mul_tensor_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_58: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_31, unsqueeze_default_15);  unsqueeze_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_70: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_71: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 64, 9223372036854775807);  transpose_int_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_13: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_71);  slice_tensor_71 = None\\n        cat_default_25: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_13, slice_tensor_70], -1);  neg_default_13 = slice_tensor_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_59: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_25, unsqueeze_default_16);  cat_default_25 = unsqueeze_default_16 = None\\n        add_tensor_44: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_58, mul_tensor_59);  mul_tensor_58 = mul_tensor_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_26: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg13, add_tensor_44], 2);  arg13 = add_tensor_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_27: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg14, transpose_int_32], 2);  arg14 = transpose_int_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_33: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_26, 2, 3)\\n        expand_default_25: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_43, [1, 32, 1, 128]);  add_tensor_43 = None\\n        view_default_154: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_25, [32, 1, 128]);  expand_default_25 = None\\n        sym_size_19: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_26, 2)\\n        expand_default_26: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_33, [1, 32, 128, sym_size_19]);  transpose_int_33 = None\\n        view_default_155: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_26, [32, 128, sym_size_19]);  expand_default_26 = None\\n        bmm_default_12: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_154, view_default_155);  view_default_154 = view_default_155 = None\\n        view_default_156: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_12, [1, 32, 1, sym_size_19]);  bmm_default_12 = None\\n        div_tensor_6: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_156, 11.313708498984761);  view_default_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_45: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_6, masked_fill_scalar);  div_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_6: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_45, -1, False);  add_tensor_45 = None\\n        detach_default_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_6)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_27: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_6, [1, 32, 1, sym_size_19]);  _softmax_default_6 = None\\n        view_default_157: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_27, [32, 1, sym_size_19]);  expand_default_27 = sym_size_19 = None\\n        sym_size_20: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_27, 2)\\n        expand_default_28: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_27, [1, 32, sym_size_20, 128])\\n        view_default_158: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_28, [32, sym_size_20, 128]);  expand_default_28 = sym_size_20 = None\\n        bmm_default_13: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_157, view_default_158);  view_default_157 = view_default_158 = None\\n        view_default_159: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_13, [1, 32, 1, 128]);  bmm_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_34: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_159, 1, 2);  view_default_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_160: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_34, [1, 1, 4096]);  transpose_int_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant59 = self._param_constant59\\n        t_default_45: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant59);  _param_constant59 = None\\n        view_default_161: f32[1, 4096] = torch.ops.aten.view.default(view_default_160, [1, 4096]);  view_default_160 = None\\n        mm_default_45: f32[1, 4096] = torch.ops.aten.mm.default(view_default_161, t_default_45);  view_default_161 = t_default_45 = None\\n        view_default_162: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_45, [1, 1, 4096]);  mm_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_46: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_41, view_default_162);  add_tensor_41 = view_default_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_13: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_46, 2)\\n        mean_dim_13: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_13, [-1], True);  pow_tensor_scalar_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_47: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_13, 1e-06);  mean_dim_13 = None\\n        rsqrt_default_13: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_47);  add_tensor_47 = None\\n        detach_default_20: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_13)\\n        mul_tensor_60: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_46, rsqrt_default_13);  rsqrt_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant60 = self._param_constant60\\n        mul_tensor_61: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant60, mul_tensor_60);  _param_constant60 = mul_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant61 = self._param_constant61\\n        t_default_46: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant61);  _param_constant61 = None\\n        view_default_163: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_61, [1, 4096])\\n        mm_default_46: f32[1, 11008] = torch.ops.aten.mm.default(view_default_163, t_default_46);  view_default_163 = t_default_46 = None\\n        view_default_164: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_46, [1, 1, 11008]);  mm_default_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_6: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_164);  view_default_164 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant62 = self._param_constant62\\n        t_default_47: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant62);  _param_constant62 = None\\n        view_default_165: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_61, [1, 4096]);  mul_tensor_61 = None\\n        mm_default_47: f32[1, 11008] = torch.ops.aten.mm.default(view_default_165, t_default_47);  view_default_165 = t_default_47 = None\\n        view_default_166: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_47, [1, 1, 11008]);  mm_default_47 = None\\n        mul_tensor_62: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_6, view_default_166);  silu_default_6 = view_default_166 = None\\n        _param_constant63 = self._param_constant63\\n        t_default_48: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant63);  _param_constant63 = None\\n        view_default_167: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_62, [1, 11008]);  mul_tensor_62 = None\\n        mm_default_48: f32[1, 4096] = torch.ops.aten.mm.default(view_default_167, t_default_48);  view_default_167 = t_default_48 = None\\n        view_default_168: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_48, [1, 1, 4096]);  mm_default_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_48: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_46, view_default_168);  add_tensor_46 = view_default_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_14: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_48, 2)\\n        mean_dim_14: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_14, [-1], True);  pow_tensor_scalar_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_49: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_14, 1e-06);  mean_dim_14 = None\\n        rsqrt_default_14: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_49);  add_tensor_49 = None\\n        detach_default_21: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_14)\\n        mul_tensor_63: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_48, rsqrt_default_14);  rsqrt_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant64 = self._param_constant64\\n        mul_tensor_64: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant64, mul_tensor_63);  _param_constant64 = mul_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant65 = self._param_constant65\\n        t_default_49: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant65);  _param_constant65 = None\\n        view_default_169: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_64, [1, 4096])\\n        mm_default_49: f32[1, 4096] = torch.ops.aten.mm.default(view_default_169, t_default_49);  view_default_169 = t_default_49 = None\\n        view_default_170: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_49, [1, 1, 4096]);  mm_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant66 = self._param_constant66\\n        t_default_50: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant66);  _param_constant66 = None\\n        view_default_171: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_64, [1, 4096])\\n        mm_default_50: f32[1, 4096] = torch.ops.aten.mm.default(view_default_171, t_default_50);  view_default_171 = t_default_50 = None\\n        view_default_172: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_50, [1, 1, 4096]);  mm_default_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant67 = self._param_constant67\\n        t_default_51: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant67);  _param_constant67 = None\\n        view_default_173: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_64, [1, 4096]);  mul_tensor_64 = None\\n        mm_default_51: f32[1, 4096] = torch.ops.aten.mm.default(view_default_173, t_default_51);  view_default_173 = t_default_51 = None\\n        view_default_174: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_51, [1, 1, 4096]);  mm_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_175: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_170, [1, 1, 32, 128]);  view_default_170 = None\\n        transpose_int_35: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_175, 1, 2);  view_default_175 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_176: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_172, [1, 1, 32, 128]);  view_default_172 = None\\n        transpose_int_36: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_176, 1, 2);  view_default_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_177: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_174, [1, 1, 32, 128]);  view_default_174 = None\\n        transpose_int_37: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_177, 1, 2);  view_default_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant14 = self._tensor_constant14\\n        slice_tensor_72: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant14, 0, 0, 9223372036854775807);  _tensor_constant14 = None\\n        slice_tensor_73: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_72, 1, 0, 9223372036854775807);  slice_tensor_72 = None\\n        sym_size_21: Sym(s0) = torch.ops.aten.sym_size(arg15, 2)\\n        add_9: Sym(s0 + 1) = 1 + sym_size_21;  sym_size_21 = None\\n        slice_tensor_74: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_73, 2, 0, add_9);  slice_tensor_73 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant15 = self._tensor_constant15\\n        slice_tensor_75: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant15, 0, 0, 9223372036854775807);  _tensor_constant15 = None\\n        slice_tensor_76: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_75, 1, 0, 9223372036854775807);  slice_tensor_75 = None\\n        slice_tensor_77: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_76, 2, 0, add_9);  slice_tensor_76 = add_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_28: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_74, 1);  slice_tensor_74 = None\\n        squeeze_dim_29: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_28, 0);  squeeze_dim_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_30: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_77, 1);  slice_tensor_77 = None\\n        squeeze_dim_31: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_30, 0);  squeeze_dim_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_14: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_29, [view_default]);  squeeze_dim_29 = None\\n        unsqueeze_default_17: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_14, 1);  index_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_15: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_31, [view_default]);  squeeze_dim_31 = None\\n        unsqueeze_default_18: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_15, 1);  index_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_65: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_35, unsqueeze_default_17)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_78: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_79: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 64, 9223372036854775807);  transpose_int_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_14: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_79);  slice_tensor_79 = None\\n        cat_default_28: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_14, slice_tensor_78], -1);  neg_default_14 = slice_tensor_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_66: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_28, unsqueeze_default_18);  cat_default_28 = None\\n        add_tensor_50: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_65, mul_tensor_66);  mul_tensor_65 = mul_tensor_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_67: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_36, unsqueeze_default_17);  unsqueeze_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_80: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_81: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 64, 9223372036854775807);  transpose_int_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_15: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_81);  slice_tensor_81 = None\\n        cat_default_29: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_15, slice_tensor_80], -1);  neg_default_15 = slice_tensor_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_68: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_29, unsqueeze_default_18);  cat_default_29 = unsqueeze_default_18 = None\\n        add_tensor_51: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_67, mul_tensor_68);  mul_tensor_67 = mul_tensor_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_30: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg15, add_tensor_51], 2);  arg15 = add_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_31: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg16, transpose_int_37], 2);  arg16 = transpose_int_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_38: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_30, 2, 3)\\n        expand_default_29: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_50, [1, 32, 1, 128]);  add_tensor_50 = None\\n        view_default_178: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_29, [32, 1, 128]);  expand_default_29 = None\\n        sym_size_22: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_30, 2)\\n        expand_default_30: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_38, [1, 32, 128, sym_size_22]);  transpose_int_38 = None\\n        view_default_179: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_30, [32, 128, sym_size_22]);  expand_default_30 = None\\n        bmm_default_14: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_178, view_default_179);  view_default_178 = view_default_179 = None\\n        view_default_180: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_14, [1, 32, 1, sym_size_22]);  bmm_default_14 = None\\n        div_tensor_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_180, 11.313708498984761);  view_default_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_52: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_7, masked_fill_scalar);  div_tensor_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_7: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_52, -1, False);  add_tensor_52 = None\\n        detach_default_22: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_7)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_7, [1, 32, 1, sym_size_22]);  _softmax_default_7 = None\\n        view_default_181: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_31, [32, 1, sym_size_22]);  expand_default_31 = sym_size_22 = None\\n        sym_size_23: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_31, 2)\\n        expand_default_32: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_31, [1, 32, sym_size_23, 128])\\n        view_default_182: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_32, [32, sym_size_23, 128]);  expand_default_32 = sym_size_23 = None\\n        bmm_default_15: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_181, view_default_182);  view_default_181 = view_default_182 = None\\n        view_default_183: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_15, [1, 32, 1, 128]);  bmm_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_39: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_183, 1, 2);  view_default_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_184: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_39, [1, 1, 4096]);  transpose_int_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant68 = self._param_constant68\\n        t_default_52: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant68);  _param_constant68 = None\\n        view_default_185: f32[1, 4096] = torch.ops.aten.view.default(view_default_184, [1, 4096]);  view_default_184 = None\\n        mm_default_52: f32[1, 4096] = torch.ops.aten.mm.default(view_default_185, t_default_52);  view_default_185 = t_default_52 = None\\n        view_default_186: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_52, [1, 1, 4096]);  mm_default_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_53: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_48, view_default_186);  add_tensor_48 = view_default_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_15: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_53, 2)\\n        mean_dim_15: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_15, [-1], True);  pow_tensor_scalar_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_54: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_15, 1e-06);  mean_dim_15 = None\\n        rsqrt_default_15: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_54);  add_tensor_54 = None\\n        detach_default_23: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_15)\\n        mul_tensor_69: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_53, rsqrt_default_15);  rsqrt_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant69 = self._param_constant69\\n        mul_tensor_70: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant69, mul_tensor_69);  _param_constant69 = mul_tensor_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant70 = self._param_constant70\\n        t_default_53: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant70);  _param_constant70 = None\\n        view_default_187: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_70, [1, 4096])\\n        mm_default_53: f32[1, 11008] = torch.ops.aten.mm.default(view_default_187, t_default_53);  view_default_187 = t_default_53 = None\\n        view_default_188: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_53, [1, 1, 11008]);  mm_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_7: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_188);  view_default_188 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant71 = self._param_constant71\\n        t_default_54: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant71);  _param_constant71 = None\\n        view_default_189: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_70, [1, 4096]);  mul_tensor_70 = None\\n        mm_default_54: f32[1, 11008] = torch.ops.aten.mm.default(view_default_189, t_default_54);  view_default_189 = t_default_54 = None\\n        view_default_190: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_54, [1, 1, 11008]);  mm_default_54 = None\\n        mul_tensor_71: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_7, view_default_190);  silu_default_7 = view_default_190 = None\\n        _param_constant72 = self._param_constant72\\n        t_default_55: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant72);  _param_constant72 = None\\n        view_default_191: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_71, [1, 11008]);  mul_tensor_71 = None\\n        mm_default_55: f32[1, 4096] = torch.ops.aten.mm.default(view_default_191, t_default_55);  view_default_191 = t_default_55 = None\\n        view_default_192: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_55, [1, 1, 4096]);  mm_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_55: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_53, view_default_192);  add_tensor_53 = view_default_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_16: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_55, 2)\\n        mean_dim_16: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_16, [-1], True);  pow_tensor_scalar_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_56: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_16, 1e-06);  mean_dim_16 = None\\n        rsqrt_default_16: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_56);  add_tensor_56 = None\\n        detach_default_24: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_16)\\n        mul_tensor_72: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_55, rsqrt_default_16);  rsqrt_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant73 = self._param_constant73\\n        mul_tensor_73: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant73, mul_tensor_72);  _param_constant73 = mul_tensor_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant74 = self._param_constant74\\n        t_default_56: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant74);  _param_constant74 = None\\n        view_default_193: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_73, [1, 4096])\\n        mm_default_56: f32[1, 4096] = torch.ops.aten.mm.default(view_default_193, t_default_56);  view_default_193 = t_default_56 = None\\n        view_default_194: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_56, [1, 1, 4096]);  mm_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant75 = self._param_constant75\\n        t_default_57: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant75);  _param_constant75 = None\\n        view_default_195: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_73, [1, 4096])\\n        mm_default_57: f32[1, 4096] = torch.ops.aten.mm.default(view_default_195, t_default_57);  view_default_195 = t_default_57 = None\\n        view_default_196: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_57, [1, 1, 4096]);  mm_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant76 = self._param_constant76\\n        t_default_58: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant76);  _param_constant76 = None\\n        view_default_197: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_73, [1, 4096]);  mul_tensor_73 = None\\n        mm_default_58: f32[1, 4096] = torch.ops.aten.mm.default(view_default_197, t_default_58);  view_default_197 = t_default_58 = None\\n        view_default_198: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_58, [1, 1, 4096]);  mm_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_199: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_194, [1, 1, 32, 128]);  view_default_194 = None\\n        transpose_int_40: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_199, 1, 2);  view_default_199 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_200: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_196, [1, 1, 32, 128]);  view_default_196 = None\\n        transpose_int_41: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_200, 1, 2);  view_default_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_201: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_198, [1, 1, 32, 128]);  view_default_198 = None\\n        transpose_int_42: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_201, 1, 2);  view_default_201 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant16 = self._tensor_constant16\\n        slice_tensor_82: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant16, 0, 0, 9223372036854775807);  _tensor_constant16 = None\\n        slice_tensor_83: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_82, 1, 0, 9223372036854775807);  slice_tensor_82 = None\\n        sym_size_24: Sym(s0) = torch.ops.aten.sym_size(arg17, 2)\\n        add_10: Sym(s0 + 1) = 1 + sym_size_24;  sym_size_24 = None\\n        slice_tensor_84: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_83, 2, 0, add_10);  slice_tensor_83 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant17 = self._tensor_constant17\\n        slice_tensor_85: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant17, 0, 0, 9223372036854775807);  _tensor_constant17 = None\\n        slice_tensor_86: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_85, 1, 0, 9223372036854775807);  slice_tensor_85 = None\\n        slice_tensor_87: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_86, 2, 0, add_10);  slice_tensor_86 = add_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_32: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_84, 1);  slice_tensor_84 = None\\n        squeeze_dim_33: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_32, 0);  squeeze_dim_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_34: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_87, 1);  slice_tensor_87 = None\\n        squeeze_dim_35: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_34, 0);  squeeze_dim_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_16: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_33, [view_default]);  squeeze_dim_33 = None\\n        unsqueeze_default_19: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_16, 1);  index_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_17: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_35, [view_default]);  squeeze_dim_35 = None\\n        unsqueeze_default_20: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_17, 1);  index_tensor_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_74: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_40, unsqueeze_default_19)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_88: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_89: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 64, 9223372036854775807);  transpose_int_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_16: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_89);  slice_tensor_89 = None\\n        cat_default_32: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_16, slice_tensor_88], -1);  neg_default_16 = slice_tensor_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_75: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_32, unsqueeze_default_20);  cat_default_32 = None\\n        add_tensor_57: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_74, mul_tensor_75);  mul_tensor_74 = mul_tensor_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_76: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_41, unsqueeze_default_19);  unsqueeze_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_90: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_91: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 64, 9223372036854775807);  transpose_int_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_17: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_91);  slice_tensor_91 = None\\n        cat_default_33: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_17, slice_tensor_90], -1);  neg_default_17 = slice_tensor_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_77: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_33, unsqueeze_default_20);  cat_default_33 = unsqueeze_default_20 = None\\n        add_tensor_58: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_76, mul_tensor_77);  mul_tensor_76 = mul_tensor_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_34: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg17, add_tensor_58], 2);  arg17 = add_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_35: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg18, transpose_int_42], 2);  arg18 = transpose_int_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_43: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_34, 2, 3)\\n        expand_default_33: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_57, [1, 32, 1, 128]);  add_tensor_57 = None\\n        view_default_202: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_33, [32, 1, 128]);  expand_default_33 = None\\n        sym_size_25: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_34, 2)\\n        expand_default_34: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_43, [1, 32, 128, sym_size_25]);  transpose_int_43 = None\\n        view_default_203: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_34, [32, 128, sym_size_25]);  expand_default_34 = None\\n        bmm_default_16: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_202, view_default_203);  view_default_202 = view_default_203 = None\\n        view_default_204: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_16, [1, 32, 1, sym_size_25]);  bmm_default_16 = None\\n        div_tensor_8: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_204, 11.313708498984761);  view_default_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_59: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_8, masked_fill_scalar);  div_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_8: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_59, -1, False);  add_tensor_59 = None\\n        detach_default_25: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_8)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_35: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_8, [1, 32, 1, sym_size_25]);  _softmax_default_8 = None\\n        view_default_205: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_35, [32, 1, sym_size_25]);  expand_default_35 = sym_size_25 = None\\n        sym_size_26: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_35, 2)\\n        expand_default_36: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_35, [1, 32, sym_size_26, 128])\\n        view_default_206: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_36, [32, sym_size_26, 128]);  expand_default_36 = sym_size_26 = None\\n        bmm_default_17: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_205, view_default_206);  view_default_205 = view_default_206 = None\\n        view_default_207: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_17, [1, 32, 1, 128]);  bmm_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_44: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_207, 1, 2);  view_default_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_208: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_44, [1, 1, 4096]);  transpose_int_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant77 = self._param_constant77\\n        t_default_59: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant77);  _param_constant77 = None\\n        view_default_209: f32[1, 4096] = torch.ops.aten.view.default(view_default_208, [1, 4096]);  view_default_208 = None\\n        mm_default_59: f32[1, 4096] = torch.ops.aten.mm.default(view_default_209, t_default_59);  view_default_209 = t_default_59 = None\\n        view_default_210: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_59, [1, 1, 4096]);  mm_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_60: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_55, view_default_210);  add_tensor_55 = view_default_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_17: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_60, 2)\\n        mean_dim_17: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_17, [-1], True);  pow_tensor_scalar_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_61: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_17, 1e-06);  mean_dim_17 = None\\n        rsqrt_default_17: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_61);  add_tensor_61 = None\\n        detach_default_26: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_17)\\n        mul_tensor_78: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_60, rsqrt_default_17);  rsqrt_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant78 = self._param_constant78\\n        mul_tensor_79: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant78, mul_tensor_78);  _param_constant78 = mul_tensor_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant79 = self._param_constant79\\n        t_default_60: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant79);  _param_constant79 = None\\n        view_default_211: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_79, [1, 4096])\\n        mm_default_60: f32[1, 11008] = torch.ops.aten.mm.default(view_default_211, t_default_60);  view_default_211 = t_default_60 = None\\n        view_default_212: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_60, [1, 1, 11008]);  mm_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_8: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_212);  view_default_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant80 = self._param_constant80\\n        t_default_61: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant80);  _param_constant80 = None\\n        view_default_213: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_79, [1, 4096]);  mul_tensor_79 = None\\n        mm_default_61: f32[1, 11008] = torch.ops.aten.mm.default(view_default_213, t_default_61);  view_default_213 = t_default_61 = None\\n        view_default_214: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_61, [1, 1, 11008]);  mm_default_61 = None\\n        mul_tensor_80: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_8, view_default_214);  silu_default_8 = view_default_214 = None\\n        _param_constant81 = self._param_constant81\\n        t_default_62: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant81);  _param_constant81 = None\\n        view_default_215: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_80, [1, 11008]);  mul_tensor_80 = None\\n        mm_default_62: f32[1, 4096] = torch.ops.aten.mm.default(view_default_215, t_default_62);  view_default_215 = t_default_62 = None\\n        view_default_216: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_62, [1, 1, 4096]);  mm_default_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_62: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_60, view_default_216);  add_tensor_60 = view_default_216 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_18: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_62, 2)\\n        mean_dim_18: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_18, [-1], True);  pow_tensor_scalar_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_63: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_18, 1e-06);  mean_dim_18 = None\\n        rsqrt_default_18: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_63);  add_tensor_63 = None\\n        detach_default_27: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_18)\\n        mul_tensor_81: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_62, rsqrt_default_18);  rsqrt_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant82 = self._param_constant82\\n        mul_tensor_82: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant82, mul_tensor_81);  _param_constant82 = mul_tensor_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant83 = self._param_constant83\\n        t_default_63: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant83);  _param_constant83 = None\\n        view_default_217: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_82, [1, 4096])\\n        mm_default_63: f32[1, 4096] = torch.ops.aten.mm.default(view_default_217, t_default_63);  view_default_217 = t_default_63 = None\\n        view_default_218: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_63, [1, 1, 4096]);  mm_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant84 = self._param_constant84\\n        t_default_64: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant84);  _param_constant84 = None\\n        view_default_219: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_82, [1, 4096])\\n        mm_default_64: f32[1, 4096] = torch.ops.aten.mm.default(view_default_219, t_default_64);  view_default_219 = t_default_64 = None\\n        view_default_220: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_64, [1, 1, 4096]);  mm_default_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant85 = self._param_constant85\\n        t_default_65: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant85);  _param_constant85 = None\\n        view_default_221: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_82, [1, 4096]);  mul_tensor_82 = None\\n        mm_default_65: f32[1, 4096] = torch.ops.aten.mm.default(view_default_221, t_default_65);  view_default_221 = t_default_65 = None\\n        view_default_222: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_65, [1, 1, 4096]);  mm_default_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_223: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_218, [1, 1, 32, 128]);  view_default_218 = None\\n        transpose_int_45: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_223, 1, 2);  view_default_223 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_224: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_220, [1, 1, 32, 128]);  view_default_220 = None\\n        transpose_int_46: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_224, 1, 2);  view_default_224 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_225: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_222, [1, 1, 32, 128]);  view_default_222 = None\\n        transpose_int_47: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_225, 1, 2);  view_default_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant18 = self._tensor_constant18\\n        slice_tensor_92: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant18, 0, 0, 9223372036854775807);  _tensor_constant18 = None\\n        slice_tensor_93: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_92, 1, 0, 9223372036854775807);  slice_tensor_92 = None\\n        sym_size_27: Sym(s0) = torch.ops.aten.sym_size(arg19, 2)\\n        add_11: Sym(s0 + 1) = 1 + sym_size_27;  sym_size_27 = None\\n        slice_tensor_94: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_93, 2, 0, add_11);  slice_tensor_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant19 = self._tensor_constant19\\n        slice_tensor_95: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant19, 0, 0, 9223372036854775807);  _tensor_constant19 = None\\n        slice_tensor_96: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_95, 1, 0, 9223372036854775807);  slice_tensor_95 = None\\n        slice_tensor_97: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_96, 2, 0, add_11);  slice_tensor_96 = add_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_36: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_94, 1);  slice_tensor_94 = None\\n        squeeze_dim_37: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_36, 0);  squeeze_dim_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_38: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_97, 1);  slice_tensor_97 = None\\n        squeeze_dim_39: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_38, 0);  squeeze_dim_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_18: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_37, [view_default]);  squeeze_dim_37 = None\\n        unsqueeze_default_21: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_18, 1);  index_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_19: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_39, [view_default]);  squeeze_dim_39 = None\\n        unsqueeze_default_22: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_19, 1);  index_tensor_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_83: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_45, unsqueeze_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_98: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_99: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 64, 9223372036854775807);  transpose_int_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_18: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_99);  slice_tensor_99 = None\\n        cat_default_36: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_18, slice_tensor_98], -1);  neg_default_18 = slice_tensor_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_84: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_36, unsqueeze_default_22);  cat_default_36 = None\\n        add_tensor_64: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_83, mul_tensor_84);  mul_tensor_83 = mul_tensor_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_85: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_46, unsqueeze_default_21);  unsqueeze_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_100: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_101: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 64, 9223372036854775807);  transpose_int_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_19: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_101);  slice_tensor_101 = None\\n        cat_default_37: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_19, slice_tensor_100], -1);  neg_default_19 = slice_tensor_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_86: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_37, unsqueeze_default_22);  cat_default_37 = unsqueeze_default_22 = None\\n        add_tensor_65: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_85, mul_tensor_86);  mul_tensor_85 = mul_tensor_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_38: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg19, add_tensor_65], 2);  arg19 = add_tensor_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_39: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg20, transpose_int_47], 2);  arg20 = transpose_int_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_48: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_38, 2, 3)\\n        expand_default_37: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_64, [1, 32, 1, 128]);  add_tensor_64 = None\\n        view_default_226: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_37, [32, 1, 128]);  expand_default_37 = None\\n        sym_size_28: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_38, 2)\\n        expand_default_38: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_48, [1, 32, 128, sym_size_28]);  transpose_int_48 = None\\n        view_default_227: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_38, [32, 128, sym_size_28]);  expand_default_38 = None\\n        bmm_default_18: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_226, view_default_227);  view_default_226 = view_default_227 = None\\n        view_default_228: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_18, [1, 32, 1, sym_size_28]);  bmm_default_18 = None\\n        div_tensor_9: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_228, 11.313708498984761);  view_default_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_66: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_9, masked_fill_scalar);  div_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_9: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_66, -1, False);  add_tensor_66 = None\\n        detach_default_28: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_9)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_39: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_9, [1, 32, 1, sym_size_28]);  _softmax_default_9 = None\\n        view_default_229: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_39, [32, 1, sym_size_28]);  expand_default_39 = sym_size_28 = None\\n        sym_size_29: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_39, 2)\\n        expand_default_40: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_39, [1, 32, sym_size_29, 128])\\n        view_default_230: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_40, [32, sym_size_29, 128]);  expand_default_40 = sym_size_29 = None\\n        bmm_default_19: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_229, view_default_230);  view_default_229 = view_default_230 = None\\n        view_default_231: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_19, [1, 32, 1, 128]);  bmm_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_49: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_231, 1, 2);  view_default_231 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_232: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_49, [1, 1, 4096]);  transpose_int_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant86 = self._param_constant86\\n        t_default_66: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant86);  _param_constant86 = None\\n        view_default_233: f32[1, 4096] = torch.ops.aten.view.default(view_default_232, [1, 4096]);  view_default_232 = None\\n        mm_default_66: f32[1, 4096] = torch.ops.aten.mm.default(view_default_233, t_default_66);  view_default_233 = t_default_66 = None\\n        view_default_234: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_66, [1, 1, 4096]);  mm_default_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_67: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_62, view_default_234);  add_tensor_62 = view_default_234 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_19: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_67, 2)\\n        mean_dim_19: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_19, [-1], True);  pow_tensor_scalar_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_68: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_19, 1e-06);  mean_dim_19 = None\\n        rsqrt_default_19: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_68);  add_tensor_68 = None\\n        detach_default_29: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_19)\\n        mul_tensor_87: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_67, rsqrt_default_19);  rsqrt_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant87 = self._param_constant87\\n        mul_tensor_88: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant87, mul_tensor_87);  _param_constant87 = mul_tensor_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant88 = self._param_constant88\\n        t_default_67: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant88);  _param_constant88 = None\\n        view_default_235: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_88, [1, 4096])\\n        mm_default_67: f32[1, 11008] = torch.ops.aten.mm.default(view_default_235, t_default_67);  view_default_235 = t_default_67 = None\\n        view_default_236: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_67, [1, 1, 11008]);  mm_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_9: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_236);  view_default_236 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant89 = self._param_constant89\\n        t_default_68: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant89);  _param_constant89 = None\\n        view_default_237: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_88, [1, 4096]);  mul_tensor_88 = None\\n        mm_default_68: f32[1, 11008] = torch.ops.aten.mm.default(view_default_237, t_default_68);  view_default_237 = t_default_68 = None\\n        view_default_238: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_68, [1, 1, 11008]);  mm_default_68 = None\\n        mul_tensor_89: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_9, view_default_238);  silu_default_9 = view_default_238 = None\\n        _param_constant90 = self._param_constant90\\n        t_default_69: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant90);  _param_constant90 = None\\n        view_default_239: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_89, [1, 11008]);  mul_tensor_89 = None\\n        mm_default_69: f32[1, 4096] = torch.ops.aten.mm.default(view_default_239, t_default_69);  view_default_239 = t_default_69 = None\\n        view_default_240: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_69, [1, 1, 4096]);  mm_default_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_69: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_67, view_default_240);  add_tensor_67 = view_default_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_20: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_69, 2)\\n        mean_dim_20: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_20, [-1], True);  pow_tensor_scalar_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_70: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_20, 1e-06);  mean_dim_20 = None\\n        rsqrt_default_20: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_70);  add_tensor_70 = None\\n        detach_default_30: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_20)\\n        mul_tensor_90: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_69, rsqrt_default_20);  rsqrt_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant91 = self._param_constant91\\n        mul_tensor_91: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant91, mul_tensor_90);  _param_constant91 = mul_tensor_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant92 = self._param_constant92\\n        t_default_70: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant92);  _param_constant92 = None\\n        view_default_241: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_91, [1, 4096])\\n        mm_default_70: f32[1, 4096] = torch.ops.aten.mm.default(view_default_241, t_default_70);  view_default_241 = t_default_70 = None\\n        view_default_242: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_70, [1, 1, 4096]);  mm_default_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant93 = self._param_constant93\\n        t_default_71: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant93);  _param_constant93 = None\\n        view_default_243: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_91, [1, 4096])\\n        mm_default_71: f32[1, 4096] = torch.ops.aten.mm.default(view_default_243, t_default_71);  view_default_243 = t_default_71 = None\\n        view_default_244: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_71, [1, 1, 4096]);  mm_default_71 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant94 = self._param_constant94\\n        t_default_72: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant94);  _param_constant94 = None\\n        view_default_245: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_91, [1, 4096]);  mul_tensor_91 = None\\n        mm_default_72: f32[1, 4096] = torch.ops.aten.mm.default(view_default_245, t_default_72);  view_default_245 = t_default_72 = None\\n        view_default_246: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_72, [1, 1, 4096]);  mm_default_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_247: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_242, [1, 1, 32, 128]);  view_default_242 = None\\n        transpose_int_50: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_247, 1, 2);  view_default_247 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_248: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_244, [1, 1, 32, 128]);  view_default_244 = None\\n        transpose_int_51: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_248, 1, 2);  view_default_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_249: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_246, [1, 1, 32, 128]);  view_default_246 = None\\n        transpose_int_52: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_249, 1, 2);  view_default_249 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant20 = self._tensor_constant20\\n        slice_tensor_102: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant20, 0, 0, 9223372036854775807);  _tensor_constant20 = None\\n        slice_tensor_103: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_102, 1, 0, 9223372036854775807);  slice_tensor_102 = None\\n        sym_size_30: Sym(s0) = torch.ops.aten.sym_size(arg21, 2)\\n        add_12: Sym(s0 + 1) = 1 + sym_size_30;  sym_size_30 = None\\n        slice_tensor_104: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_103, 2, 0, add_12);  slice_tensor_103 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant21 = self._tensor_constant21\\n        slice_tensor_105: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant21, 0, 0, 9223372036854775807);  _tensor_constant21 = None\\n        slice_tensor_106: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_105, 1, 0, 9223372036854775807);  slice_tensor_105 = None\\n        slice_tensor_107: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_106, 2, 0, add_12);  slice_tensor_106 = add_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_40: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_104, 1);  slice_tensor_104 = None\\n        squeeze_dim_41: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_40, 0);  squeeze_dim_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_42: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_107, 1);  slice_tensor_107 = None\\n        squeeze_dim_43: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_42, 0);  squeeze_dim_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_20: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_41, [view_default]);  squeeze_dim_41 = None\\n        unsqueeze_default_23: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_20, 1);  index_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_21: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_43, [view_default]);  squeeze_dim_43 = None\\n        unsqueeze_default_24: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_21, 1);  index_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_92: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_50, unsqueeze_default_23)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_108: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_109: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 64, 9223372036854775807);  transpose_int_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_20: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_109);  slice_tensor_109 = None\\n        cat_default_40: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_20, slice_tensor_108], -1);  neg_default_20 = slice_tensor_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_93: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_40, unsqueeze_default_24);  cat_default_40 = None\\n        add_tensor_71: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_92, mul_tensor_93);  mul_tensor_92 = mul_tensor_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_94: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_51, unsqueeze_default_23);  unsqueeze_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_110: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_111: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 64, 9223372036854775807);  transpose_int_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_21: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_111);  slice_tensor_111 = None\\n        cat_default_41: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_21, slice_tensor_110], -1);  neg_default_21 = slice_tensor_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_95: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_41, unsqueeze_default_24);  cat_default_41 = unsqueeze_default_24 = None\\n        add_tensor_72: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_94, mul_tensor_95);  mul_tensor_94 = mul_tensor_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_42: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg21, add_tensor_72], 2);  arg21 = add_tensor_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_43: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg22, transpose_int_52], 2);  arg22 = transpose_int_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_53: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_42, 2, 3)\\n        expand_default_41: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_71, [1, 32, 1, 128]);  add_tensor_71 = None\\n        view_default_250: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_41, [32, 1, 128]);  expand_default_41 = None\\n        sym_size_31: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_42, 2)\\n        expand_default_42: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_53, [1, 32, 128, sym_size_31]);  transpose_int_53 = None\\n        view_default_251: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_42, [32, 128, sym_size_31]);  expand_default_42 = None\\n        bmm_default_20: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_250, view_default_251);  view_default_250 = view_default_251 = None\\n        view_default_252: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_20, [1, 32, 1, sym_size_31]);  bmm_default_20 = None\\n        div_tensor_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_252, 11.313708498984761);  view_default_252 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_73: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_10, masked_fill_scalar);  div_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_10: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_73, -1, False);  add_tensor_73 = None\\n        detach_default_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_10)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_43: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_10, [1, 32, 1, sym_size_31]);  _softmax_default_10 = None\\n        view_default_253: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_43, [32, 1, sym_size_31]);  expand_default_43 = sym_size_31 = None\\n        sym_size_32: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_43, 2)\\n        expand_default_44: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_43, [1, 32, sym_size_32, 128])\\n        view_default_254: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_44, [32, sym_size_32, 128]);  expand_default_44 = sym_size_32 = None\\n        bmm_default_21: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_253, view_default_254);  view_default_253 = view_default_254 = None\\n        view_default_255: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_21, [1, 32, 1, 128]);  bmm_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_54: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_255, 1, 2);  view_default_255 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_256: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_54, [1, 1, 4096]);  transpose_int_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant95 = self._param_constant95\\n        t_default_73: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant95);  _param_constant95 = None\\n        view_default_257: f32[1, 4096] = torch.ops.aten.view.default(view_default_256, [1, 4096]);  view_default_256 = None\\n        mm_default_73: f32[1, 4096] = torch.ops.aten.mm.default(view_default_257, t_default_73);  view_default_257 = t_default_73 = None\\n        view_default_258: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_73, [1, 1, 4096]);  mm_default_73 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_74: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_69, view_default_258);  add_tensor_69 = view_default_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_21: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_74, 2)\\n        mean_dim_21: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_21, [-1], True);  pow_tensor_scalar_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_75: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_21, 1e-06);  mean_dim_21 = None\\n        rsqrt_default_21: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_75);  add_tensor_75 = None\\n        detach_default_32: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_21)\\n        mul_tensor_96: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_74, rsqrt_default_21);  rsqrt_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant96 = self._param_constant96\\n        mul_tensor_97: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant96, mul_tensor_96);  _param_constant96 = mul_tensor_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant97 = self._param_constant97\\n        t_default_74: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant97);  _param_constant97 = None\\n        view_default_259: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_97, [1, 4096])\\n        mm_default_74: f32[1, 11008] = torch.ops.aten.mm.default(view_default_259, t_default_74);  view_default_259 = t_default_74 = None\\n        view_default_260: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_74, [1, 1, 11008]);  mm_default_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_10: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_260);  view_default_260 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant98 = self._param_constant98\\n        t_default_75: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant98);  _param_constant98 = None\\n        view_default_261: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_97, [1, 4096]);  mul_tensor_97 = None\\n        mm_default_75: f32[1, 11008] = torch.ops.aten.mm.default(view_default_261, t_default_75);  view_default_261 = t_default_75 = None\\n        view_default_262: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_75, [1, 1, 11008]);  mm_default_75 = None\\n        mul_tensor_98: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_10, view_default_262);  silu_default_10 = view_default_262 = None\\n        _param_constant99 = self._param_constant99\\n        t_default_76: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant99);  _param_constant99 = None\\n        view_default_263: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_98, [1, 11008]);  mul_tensor_98 = None\\n        mm_default_76: f32[1, 4096] = torch.ops.aten.mm.default(view_default_263, t_default_76);  view_default_263 = t_default_76 = None\\n        view_default_264: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_76, [1, 1, 4096]);  mm_default_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_76: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_74, view_default_264);  add_tensor_74 = view_default_264 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_22: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_76, 2)\\n        mean_dim_22: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_22, [-1], True);  pow_tensor_scalar_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_77: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_22, 1e-06);  mean_dim_22 = None\\n        rsqrt_default_22: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_77);  add_tensor_77 = None\\n        detach_default_33: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_22)\\n        mul_tensor_99: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_76, rsqrt_default_22);  rsqrt_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant100 = self._param_constant100\\n        mul_tensor_100: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant100, mul_tensor_99);  _param_constant100 = mul_tensor_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant101 = self._param_constant101\\n        t_default_77: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant101);  _param_constant101 = None\\n        view_default_265: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_100, [1, 4096])\\n        mm_default_77: f32[1, 4096] = torch.ops.aten.mm.default(view_default_265, t_default_77);  view_default_265 = t_default_77 = None\\n        view_default_266: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_77, [1, 1, 4096]);  mm_default_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant102 = self._param_constant102\\n        t_default_78: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant102);  _param_constant102 = None\\n        view_default_267: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_100, [1, 4096])\\n        mm_default_78: f32[1, 4096] = torch.ops.aten.mm.default(view_default_267, t_default_78);  view_default_267 = t_default_78 = None\\n        view_default_268: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_78, [1, 1, 4096]);  mm_default_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant103 = self._param_constant103\\n        t_default_79: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant103);  _param_constant103 = None\\n        view_default_269: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_100, [1, 4096]);  mul_tensor_100 = None\\n        mm_default_79: f32[1, 4096] = torch.ops.aten.mm.default(view_default_269, t_default_79);  view_default_269 = t_default_79 = None\\n        view_default_270: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_79, [1, 1, 4096]);  mm_default_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_271: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_266, [1, 1, 32, 128]);  view_default_266 = None\\n        transpose_int_55: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_271, 1, 2);  view_default_271 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_272: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_268, [1, 1, 32, 128]);  view_default_268 = None\\n        transpose_int_56: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_272, 1, 2);  view_default_272 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_273: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_270, [1, 1, 32, 128]);  view_default_270 = None\\n        transpose_int_57: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_273, 1, 2);  view_default_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant22 = self._tensor_constant22\\n        slice_tensor_112: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant22, 0, 0, 9223372036854775807);  _tensor_constant22 = None\\n        slice_tensor_113: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_112, 1, 0, 9223372036854775807);  slice_tensor_112 = None\\n        sym_size_33: Sym(s0) = torch.ops.aten.sym_size(arg23, 2)\\n        add_13: Sym(s0 + 1) = 1 + sym_size_33;  sym_size_33 = None\\n        slice_tensor_114: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_113, 2, 0, add_13);  slice_tensor_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant23 = self._tensor_constant23\\n        slice_tensor_115: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant23, 0, 0, 9223372036854775807);  _tensor_constant23 = None\\n        slice_tensor_116: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_115, 1, 0, 9223372036854775807);  slice_tensor_115 = None\\n        slice_tensor_117: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_116, 2, 0, add_13);  slice_tensor_116 = add_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_44: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_114, 1);  slice_tensor_114 = None\\n        squeeze_dim_45: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_44, 0);  squeeze_dim_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_46: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_117, 1);  slice_tensor_117 = None\\n        squeeze_dim_47: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_46, 0);  squeeze_dim_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_22: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_45, [view_default]);  squeeze_dim_45 = None\\n        unsqueeze_default_25: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_22, 1);  index_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_23: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_47, [view_default]);  squeeze_dim_47 = None\\n        unsqueeze_default_26: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_23, 1);  index_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_101: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_55, unsqueeze_default_25)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_118: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_119: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 64, 9223372036854775807);  transpose_int_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_22: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_119);  slice_tensor_119 = None\\n        cat_default_44: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_22, slice_tensor_118], -1);  neg_default_22 = slice_tensor_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_102: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_44, unsqueeze_default_26);  cat_default_44 = None\\n        add_tensor_78: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_101, mul_tensor_102);  mul_tensor_101 = mul_tensor_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_103: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_56, unsqueeze_default_25);  unsqueeze_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_120: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_121: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 64, 9223372036854775807);  transpose_int_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_23: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_121);  slice_tensor_121 = None\\n        cat_default_45: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_23, slice_tensor_120], -1);  neg_default_23 = slice_tensor_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_104: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_45, unsqueeze_default_26);  cat_default_45 = unsqueeze_default_26 = None\\n        add_tensor_79: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_103, mul_tensor_104);  mul_tensor_103 = mul_tensor_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_46: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg23, add_tensor_79], 2);  arg23 = add_tensor_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_47: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg24, transpose_int_57], 2);  arg24 = transpose_int_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_58: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_46, 2, 3)\\n        expand_default_45: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_78, [1, 32, 1, 128]);  add_tensor_78 = None\\n        view_default_274: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_45, [32, 1, 128]);  expand_default_45 = None\\n        sym_size_34: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_46, 2)\\n        expand_default_46: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_58, [1, 32, 128, sym_size_34]);  transpose_int_58 = None\\n        view_default_275: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_46, [32, 128, sym_size_34]);  expand_default_46 = None\\n        bmm_default_22: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_274, view_default_275);  view_default_274 = view_default_275 = None\\n        view_default_276: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_22, [1, 32, 1, sym_size_34]);  bmm_default_22 = None\\n        div_tensor_11: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_276, 11.313708498984761);  view_default_276 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_80: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_11, masked_fill_scalar);  div_tensor_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_11: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_80, -1, False);  add_tensor_80 = None\\n        detach_default_34: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_11)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_47: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_11, [1, 32, 1, sym_size_34]);  _softmax_default_11 = None\\n        view_default_277: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_47, [32, 1, sym_size_34]);  expand_default_47 = sym_size_34 = None\\n        sym_size_35: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_47, 2)\\n        expand_default_48: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_47, [1, 32, sym_size_35, 128])\\n        view_default_278: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_48, [32, sym_size_35, 128]);  expand_default_48 = sym_size_35 = None\\n        bmm_default_23: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_277, view_default_278);  view_default_277 = view_default_278 = None\\n        view_default_279: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_23, [1, 32, 1, 128]);  bmm_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_59: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_279, 1, 2);  view_default_279 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_280: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_59, [1, 1, 4096]);  transpose_int_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant104 = self._param_constant104\\n        t_default_80: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant104);  _param_constant104 = None\\n        view_default_281: f32[1, 4096] = torch.ops.aten.view.default(view_default_280, [1, 4096]);  view_default_280 = None\\n        mm_default_80: f32[1, 4096] = torch.ops.aten.mm.default(view_default_281, t_default_80);  view_default_281 = t_default_80 = None\\n        view_default_282: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_80, [1, 1, 4096]);  mm_default_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_81: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_76, view_default_282);  add_tensor_76 = view_default_282 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_23: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_81, 2)\\n        mean_dim_23: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_23, [-1], True);  pow_tensor_scalar_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_82: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_23, 1e-06);  mean_dim_23 = None\\n        rsqrt_default_23: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_82);  add_tensor_82 = None\\n        detach_default_35: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_23)\\n        mul_tensor_105: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_81, rsqrt_default_23);  rsqrt_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant105 = self._param_constant105\\n        mul_tensor_106: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant105, mul_tensor_105);  _param_constant105 = mul_tensor_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant106 = self._param_constant106\\n        t_default_81: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant106);  _param_constant106 = None\\n        view_default_283: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_106, [1, 4096])\\n        mm_default_81: f32[1, 11008] = torch.ops.aten.mm.default(view_default_283, t_default_81);  view_default_283 = t_default_81 = None\\n        view_default_284: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_81, [1, 1, 11008]);  mm_default_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_11: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_284);  view_default_284 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant107 = self._param_constant107\\n        t_default_82: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant107);  _param_constant107 = None\\n        view_default_285: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_106, [1, 4096]);  mul_tensor_106 = None\\n        mm_default_82: f32[1, 11008] = torch.ops.aten.mm.default(view_default_285, t_default_82);  view_default_285 = t_default_82 = None\\n        view_default_286: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_82, [1, 1, 11008]);  mm_default_82 = None\\n        mul_tensor_107: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_11, view_default_286);  silu_default_11 = view_default_286 = None\\n        _param_constant108 = self._param_constant108\\n        t_default_83: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant108);  _param_constant108 = None\\n        view_default_287: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_107, [1, 11008]);  mul_tensor_107 = None\\n        mm_default_83: f32[1, 4096] = torch.ops.aten.mm.default(view_default_287, t_default_83);  view_default_287 = t_default_83 = None\\n        view_default_288: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_83, [1, 1, 4096]);  mm_default_83 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_83: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_81, view_default_288);  add_tensor_81 = view_default_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_24: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_83, 2)\\n        mean_dim_24: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_24, [-1], True);  pow_tensor_scalar_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_84: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_24, 1e-06);  mean_dim_24 = None\\n        rsqrt_default_24: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_84);  add_tensor_84 = None\\n        detach_default_36: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_24)\\n        mul_tensor_108: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_83, rsqrt_default_24);  rsqrt_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant109 = self._param_constant109\\n        mul_tensor_109: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant109, mul_tensor_108);  _param_constant109 = mul_tensor_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant110 = self._param_constant110\\n        t_default_84: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant110);  _param_constant110 = None\\n        view_default_289: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_109, [1, 4096])\\n        mm_default_84: f32[1, 4096] = torch.ops.aten.mm.default(view_default_289, t_default_84);  view_default_289 = t_default_84 = None\\n        view_default_290: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_84, [1, 1, 4096]);  mm_default_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant111 = self._param_constant111\\n        t_default_85: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant111);  _param_constant111 = None\\n        view_default_291: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_109, [1, 4096])\\n        mm_default_85: f32[1, 4096] = torch.ops.aten.mm.default(view_default_291, t_default_85);  view_default_291 = t_default_85 = None\\n        view_default_292: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_85, [1, 1, 4096]);  mm_default_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant112 = self._param_constant112\\n        t_default_86: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant112);  _param_constant112 = None\\n        view_default_293: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_109, [1, 4096]);  mul_tensor_109 = None\\n        mm_default_86: f32[1, 4096] = torch.ops.aten.mm.default(view_default_293, t_default_86);  view_default_293 = t_default_86 = None\\n        view_default_294: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_86, [1, 1, 4096]);  mm_default_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_295: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_290, [1, 1, 32, 128]);  view_default_290 = None\\n        transpose_int_60: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_295, 1, 2);  view_default_295 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_296: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_292, [1, 1, 32, 128]);  view_default_292 = None\\n        transpose_int_61: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_296, 1, 2);  view_default_296 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_297: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_294, [1, 1, 32, 128]);  view_default_294 = None\\n        transpose_int_62: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_297, 1, 2);  view_default_297 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant24 = self._tensor_constant24\\n        slice_tensor_122: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant24, 0, 0, 9223372036854775807);  _tensor_constant24 = None\\n        slice_tensor_123: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_122, 1, 0, 9223372036854775807);  slice_tensor_122 = None\\n        sym_size_36: Sym(s0) = torch.ops.aten.sym_size(arg25, 2)\\n        add_14: Sym(s0 + 1) = 1 + sym_size_36;  sym_size_36 = None\\n        slice_tensor_124: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_123, 2, 0, add_14);  slice_tensor_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant25 = self._tensor_constant25\\n        slice_tensor_125: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant25, 0, 0, 9223372036854775807);  _tensor_constant25 = None\\n        slice_tensor_126: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_125, 1, 0, 9223372036854775807);  slice_tensor_125 = None\\n        slice_tensor_127: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_126, 2, 0, add_14);  slice_tensor_126 = add_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_48: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_124, 1);  slice_tensor_124 = None\\n        squeeze_dim_49: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_48, 0);  squeeze_dim_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_50: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_127, 1);  slice_tensor_127 = None\\n        squeeze_dim_51: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_50, 0);  squeeze_dim_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_24: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_49, [view_default]);  squeeze_dim_49 = None\\n        unsqueeze_default_27: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_24, 1);  index_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_25: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_51, [view_default]);  squeeze_dim_51 = None\\n        unsqueeze_default_28: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_25, 1);  index_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_110: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_60, unsqueeze_default_27)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_128: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_129: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 64, 9223372036854775807);  transpose_int_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_24: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_129);  slice_tensor_129 = None\\n        cat_default_48: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_24, slice_tensor_128], -1);  neg_default_24 = slice_tensor_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_111: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_48, unsqueeze_default_28);  cat_default_48 = None\\n        add_tensor_85: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_110, mul_tensor_111);  mul_tensor_110 = mul_tensor_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_112: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_61, unsqueeze_default_27);  unsqueeze_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_130: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_131: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 64, 9223372036854775807);  transpose_int_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_25: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_131);  slice_tensor_131 = None\\n        cat_default_49: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_25, slice_tensor_130], -1);  neg_default_25 = slice_tensor_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_113: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_49, unsqueeze_default_28);  cat_default_49 = unsqueeze_default_28 = None\\n        add_tensor_86: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_112, mul_tensor_113);  mul_tensor_112 = mul_tensor_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_50: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg25, add_tensor_86], 2);  arg25 = add_tensor_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_51: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg26, transpose_int_62], 2);  arg26 = transpose_int_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_63: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_50, 2, 3)\\n        expand_default_49: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_85, [1, 32, 1, 128]);  add_tensor_85 = None\\n        view_default_298: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_49, [32, 1, 128]);  expand_default_49 = None\\n        sym_size_37: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_50, 2)\\n        expand_default_50: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_63, [1, 32, 128, sym_size_37]);  transpose_int_63 = None\\n        view_default_299: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_50, [32, 128, sym_size_37]);  expand_default_50 = None\\n        bmm_default_24: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_298, view_default_299);  view_default_298 = view_default_299 = None\\n        view_default_300: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_24, [1, 32, 1, sym_size_37]);  bmm_default_24 = None\\n        div_tensor_12: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_300, 11.313708498984761);  view_default_300 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_87: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_12, masked_fill_scalar);  div_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_12: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_87, -1, False);  add_tensor_87 = None\\n        detach_default_37: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_12)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_51: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_12, [1, 32, 1, sym_size_37]);  _softmax_default_12 = None\\n        view_default_301: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_51, [32, 1, sym_size_37]);  expand_default_51 = sym_size_37 = None\\n        sym_size_38: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_51, 2)\\n        expand_default_52: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_51, [1, 32, sym_size_38, 128])\\n        view_default_302: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_52, [32, sym_size_38, 128]);  expand_default_52 = sym_size_38 = None\\n        bmm_default_25: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_301, view_default_302);  view_default_301 = view_default_302 = None\\n        view_default_303: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_25, [1, 32, 1, 128]);  bmm_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_64: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_303, 1, 2);  view_default_303 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_304: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_64, [1, 1, 4096]);  transpose_int_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant113 = self._param_constant113\\n        t_default_87: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant113);  _param_constant113 = None\\n        view_default_305: f32[1, 4096] = torch.ops.aten.view.default(view_default_304, [1, 4096]);  view_default_304 = None\\n        mm_default_87: f32[1, 4096] = torch.ops.aten.mm.default(view_default_305, t_default_87);  view_default_305 = t_default_87 = None\\n        view_default_306: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_87, [1, 1, 4096]);  mm_default_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_88: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_83, view_default_306);  add_tensor_83 = view_default_306 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_25: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_88, 2)\\n        mean_dim_25: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_25, [-1], True);  pow_tensor_scalar_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_89: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_25, 1e-06);  mean_dim_25 = None\\n        rsqrt_default_25: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_89);  add_tensor_89 = None\\n        detach_default_38: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_25)\\n        mul_tensor_114: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_88, rsqrt_default_25);  rsqrt_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant114 = self._param_constant114\\n        mul_tensor_115: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant114, mul_tensor_114);  _param_constant114 = mul_tensor_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant115 = self._param_constant115\\n        t_default_88: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant115);  _param_constant115 = None\\n        view_default_307: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_115, [1, 4096])\\n        mm_default_88: f32[1, 11008] = torch.ops.aten.mm.default(view_default_307, t_default_88);  view_default_307 = t_default_88 = None\\n        view_default_308: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_88, [1, 1, 11008]);  mm_default_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_12: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_308);  view_default_308 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant116 = self._param_constant116\\n        t_default_89: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant116);  _param_constant116 = None\\n        view_default_309: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_115, [1, 4096]);  mul_tensor_115 = None\\n        mm_default_89: f32[1, 11008] = torch.ops.aten.mm.default(view_default_309, t_default_89);  view_default_309 = t_default_89 = None\\n        view_default_310: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_89, [1, 1, 11008]);  mm_default_89 = None\\n        mul_tensor_116: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_12, view_default_310);  silu_default_12 = view_default_310 = None\\n        _param_constant117 = self._param_constant117\\n        t_default_90: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant117);  _param_constant117 = None\\n        view_default_311: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_116, [1, 11008]);  mul_tensor_116 = None\\n        mm_default_90: f32[1, 4096] = torch.ops.aten.mm.default(view_default_311, t_default_90);  view_default_311 = t_default_90 = None\\n        view_default_312: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_90, [1, 1, 4096]);  mm_default_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_90: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_88, view_default_312);  add_tensor_88 = view_default_312 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_26: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_90, 2)\\n        mean_dim_26: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_26, [-1], True);  pow_tensor_scalar_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_91: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_26, 1e-06);  mean_dim_26 = None\\n        rsqrt_default_26: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_91);  add_tensor_91 = None\\n        detach_default_39: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_26)\\n        mul_tensor_117: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_90, rsqrt_default_26);  rsqrt_default_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant118 = self._param_constant118\\n        mul_tensor_118: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant118, mul_tensor_117);  _param_constant118 = mul_tensor_117 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant119 = self._param_constant119\\n        t_default_91: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant119);  _param_constant119 = None\\n        view_default_313: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_118, [1, 4096])\\n        mm_default_91: f32[1, 4096] = torch.ops.aten.mm.default(view_default_313, t_default_91);  view_default_313 = t_default_91 = None\\n        view_default_314: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_91, [1, 1, 4096]);  mm_default_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant120 = self._param_constant120\\n        t_default_92: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant120);  _param_constant120 = None\\n        view_default_315: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_118, [1, 4096])\\n        mm_default_92: f32[1, 4096] = torch.ops.aten.mm.default(view_default_315, t_default_92);  view_default_315 = t_default_92 = None\\n        view_default_316: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_92, [1, 1, 4096]);  mm_default_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant121 = self._param_constant121\\n        t_default_93: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant121);  _param_constant121 = None\\n        view_default_317: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_118, [1, 4096]);  mul_tensor_118 = None\\n        mm_default_93: f32[1, 4096] = torch.ops.aten.mm.default(view_default_317, t_default_93);  view_default_317 = t_default_93 = None\\n        view_default_318: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_93, [1, 1, 4096]);  mm_default_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_319: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_314, [1, 1, 32, 128]);  view_default_314 = None\\n        transpose_int_65: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_319, 1, 2);  view_default_319 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_320: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_316, [1, 1, 32, 128]);  view_default_316 = None\\n        transpose_int_66: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_320, 1, 2);  view_default_320 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_321: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_318, [1, 1, 32, 128]);  view_default_318 = None\\n        transpose_int_67: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_321, 1, 2);  view_default_321 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant26 = self._tensor_constant26\\n        slice_tensor_132: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant26, 0, 0, 9223372036854775807);  _tensor_constant26 = None\\n        slice_tensor_133: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_132, 1, 0, 9223372036854775807);  slice_tensor_132 = None\\n        sym_size_39: Sym(s0) = torch.ops.aten.sym_size(arg27, 2)\\n        add_15: Sym(s0 + 1) = 1 + sym_size_39;  sym_size_39 = None\\n        slice_tensor_134: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_133, 2, 0, add_15);  slice_tensor_133 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant27 = self._tensor_constant27\\n        slice_tensor_135: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant27, 0, 0, 9223372036854775807);  _tensor_constant27 = None\\n        slice_tensor_136: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_135, 1, 0, 9223372036854775807);  slice_tensor_135 = None\\n        slice_tensor_137: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_136, 2, 0, add_15);  slice_tensor_136 = add_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_52: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_134, 1);  slice_tensor_134 = None\\n        squeeze_dim_53: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_52, 0);  squeeze_dim_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_54: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_137, 1);  slice_tensor_137 = None\\n        squeeze_dim_55: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_54, 0);  squeeze_dim_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_26: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_53, [view_default]);  squeeze_dim_53 = None\\n        unsqueeze_default_29: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_26, 1);  index_tensor_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_27: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_55, [view_default]);  squeeze_dim_55 = None\\n        unsqueeze_default_30: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_27, 1);  index_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_119: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_65, unsqueeze_default_29)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_138: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_139: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 64, 9223372036854775807);  transpose_int_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_26: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_139);  slice_tensor_139 = None\\n        cat_default_52: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_26, slice_tensor_138], -1);  neg_default_26 = slice_tensor_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_120: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_52, unsqueeze_default_30);  cat_default_52 = None\\n        add_tensor_92: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_119, mul_tensor_120);  mul_tensor_119 = mul_tensor_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_121: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_66, unsqueeze_default_29);  unsqueeze_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_140: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_141: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 64, 9223372036854775807);  transpose_int_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_27: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_141);  slice_tensor_141 = None\\n        cat_default_53: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_27, slice_tensor_140], -1);  neg_default_27 = slice_tensor_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_122: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_53, unsqueeze_default_30);  cat_default_53 = unsqueeze_default_30 = None\\n        add_tensor_93: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_121, mul_tensor_122);  mul_tensor_121 = mul_tensor_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_54: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg27, add_tensor_93], 2);  arg27 = add_tensor_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_55: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg28, transpose_int_67], 2);  arg28 = transpose_int_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_68: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_54, 2, 3)\\n        expand_default_53: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_92, [1, 32, 1, 128]);  add_tensor_92 = None\\n        view_default_322: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_53, [32, 1, 128]);  expand_default_53 = None\\n        sym_size_40: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_54, 2)\\n        expand_default_54: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_68, [1, 32, 128, sym_size_40]);  transpose_int_68 = None\\n        view_default_323: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_54, [32, 128, sym_size_40]);  expand_default_54 = None\\n        bmm_default_26: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_322, view_default_323);  view_default_322 = view_default_323 = None\\n        view_default_324: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_26, [1, 32, 1, sym_size_40]);  bmm_default_26 = None\\n        div_tensor_13: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_324, 11.313708498984761);  view_default_324 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_94: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_13, masked_fill_scalar);  div_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_13: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_94, -1, False);  add_tensor_94 = None\\n        detach_default_40: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_13)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_55: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_13, [1, 32, 1, sym_size_40]);  _softmax_default_13 = None\\n        view_default_325: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_55, [32, 1, sym_size_40]);  expand_default_55 = sym_size_40 = None\\n        sym_size_41: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_55, 2)\\n        expand_default_56: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_55, [1, 32, sym_size_41, 128])\\n        view_default_326: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_56, [32, sym_size_41, 128]);  expand_default_56 = sym_size_41 = None\\n        bmm_default_27: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_325, view_default_326);  view_default_325 = view_default_326 = None\\n        view_default_327: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_27, [1, 32, 1, 128]);  bmm_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_69: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_327, 1, 2);  view_default_327 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_328: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_69, [1, 1, 4096]);  transpose_int_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant122 = self._param_constant122\\n        t_default_94: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant122);  _param_constant122 = None\\n        view_default_329: f32[1, 4096] = torch.ops.aten.view.default(view_default_328, [1, 4096]);  view_default_328 = None\\n        mm_default_94: f32[1, 4096] = torch.ops.aten.mm.default(view_default_329, t_default_94);  view_default_329 = t_default_94 = None\\n        view_default_330: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_94, [1, 1, 4096]);  mm_default_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_95: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_90, view_default_330);  add_tensor_90 = view_default_330 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_27: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_95, 2)\\n        mean_dim_27: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_27, [-1], True);  pow_tensor_scalar_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_96: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_27, 1e-06);  mean_dim_27 = None\\n        rsqrt_default_27: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_96);  add_tensor_96 = None\\n        detach_default_41: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_27)\\n        mul_tensor_123: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_95, rsqrt_default_27);  rsqrt_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant123 = self._param_constant123\\n        mul_tensor_124: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant123, mul_tensor_123);  _param_constant123 = mul_tensor_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant124 = self._param_constant124\\n        t_default_95: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant124);  _param_constant124 = None\\n        view_default_331: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_124, [1, 4096])\\n        mm_default_95: f32[1, 11008] = torch.ops.aten.mm.default(view_default_331, t_default_95);  view_default_331 = t_default_95 = None\\n        view_default_332: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_95, [1, 1, 11008]);  mm_default_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_13: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_332);  view_default_332 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant125 = self._param_constant125\\n        t_default_96: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant125);  _param_constant125 = None\\n        view_default_333: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_124, [1, 4096]);  mul_tensor_124 = None\\n        mm_default_96: f32[1, 11008] = torch.ops.aten.mm.default(view_default_333, t_default_96);  view_default_333 = t_default_96 = None\\n        view_default_334: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_96, [1, 1, 11008]);  mm_default_96 = None\\n        mul_tensor_125: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_13, view_default_334);  silu_default_13 = view_default_334 = None\\n        _param_constant126 = self._param_constant126\\n        t_default_97: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant126);  _param_constant126 = None\\n        view_default_335: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_125, [1, 11008]);  mul_tensor_125 = None\\n        mm_default_97: f32[1, 4096] = torch.ops.aten.mm.default(view_default_335, t_default_97);  view_default_335 = t_default_97 = None\\n        view_default_336: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_97, [1, 1, 4096]);  mm_default_97 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_97: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_95, view_default_336);  add_tensor_95 = view_default_336 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_28: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_97, 2)\\n        mean_dim_28: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_28, [-1], True);  pow_tensor_scalar_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_98: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_28, 1e-06);  mean_dim_28 = None\\n        rsqrt_default_28: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_98);  add_tensor_98 = None\\n        detach_default_42: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_28)\\n        mul_tensor_126: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_97, rsqrt_default_28);  rsqrt_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant127 = self._param_constant127\\n        mul_tensor_127: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant127, mul_tensor_126);  _param_constant127 = mul_tensor_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant128 = self._param_constant128\\n        t_default_98: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant128);  _param_constant128 = None\\n        view_default_337: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_127, [1, 4096])\\n        mm_default_98: f32[1, 4096] = torch.ops.aten.mm.default(view_default_337, t_default_98);  view_default_337 = t_default_98 = None\\n        view_default_338: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_98, [1, 1, 4096]);  mm_default_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant129 = self._param_constant129\\n        t_default_99: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant129);  _param_constant129 = None\\n        view_default_339: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_127, [1, 4096])\\n        mm_default_99: f32[1, 4096] = torch.ops.aten.mm.default(view_default_339, t_default_99);  view_default_339 = t_default_99 = None\\n        view_default_340: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_99, [1, 1, 4096]);  mm_default_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant130 = self._param_constant130\\n        t_default_100: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant130);  _param_constant130 = None\\n        view_default_341: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_127, [1, 4096]);  mul_tensor_127 = None\\n        mm_default_100: f32[1, 4096] = torch.ops.aten.mm.default(view_default_341, t_default_100);  view_default_341 = t_default_100 = None\\n        view_default_342: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_100, [1, 1, 4096]);  mm_default_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_343: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_338, [1, 1, 32, 128]);  view_default_338 = None\\n        transpose_int_70: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_343, 1, 2);  view_default_343 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_344: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_340, [1, 1, 32, 128]);  view_default_340 = None\\n        transpose_int_71: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_344, 1, 2);  view_default_344 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_345: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_342, [1, 1, 32, 128]);  view_default_342 = None\\n        transpose_int_72: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_345, 1, 2);  view_default_345 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant28 = self._tensor_constant28\\n        slice_tensor_142: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant28, 0, 0, 9223372036854775807);  _tensor_constant28 = None\\n        slice_tensor_143: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_142, 1, 0, 9223372036854775807);  slice_tensor_142 = None\\n        sym_size_42: Sym(s0) = torch.ops.aten.sym_size(arg29, 2)\\n        add_16: Sym(s0 + 1) = 1 + sym_size_42;  sym_size_42 = None\\n        slice_tensor_144: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_143, 2, 0, add_16);  slice_tensor_143 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant29 = self._tensor_constant29\\n        slice_tensor_145: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant29, 0, 0, 9223372036854775807);  _tensor_constant29 = None\\n        slice_tensor_146: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_145, 1, 0, 9223372036854775807);  slice_tensor_145 = None\\n        slice_tensor_147: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_146, 2, 0, add_16);  slice_tensor_146 = add_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_56: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_144, 1);  slice_tensor_144 = None\\n        squeeze_dim_57: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_56, 0);  squeeze_dim_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_58: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_147, 1);  slice_tensor_147 = None\\n        squeeze_dim_59: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_58, 0);  squeeze_dim_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_28: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_57, [view_default]);  squeeze_dim_57 = None\\n        unsqueeze_default_31: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_28, 1);  index_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_29: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_59, [view_default]);  squeeze_dim_59 = None\\n        unsqueeze_default_32: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_29, 1);  index_tensor_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_128: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_70, unsqueeze_default_31)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_148: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_149: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 64, 9223372036854775807);  transpose_int_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_28: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_149);  slice_tensor_149 = None\\n        cat_default_56: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_28, slice_tensor_148], -1);  neg_default_28 = slice_tensor_148 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_129: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_56, unsqueeze_default_32);  cat_default_56 = None\\n        add_tensor_99: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_128, mul_tensor_129);  mul_tensor_128 = mul_tensor_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_130: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_71, unsqueeze_default_31);  unsqueeze_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_150: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_151: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 64, 9223372036854775807);  transpose_int_71 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_29: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_151);  slice_tensor_151 = None\\n        cat_default_57: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_29, slice_tensor_150], -1);  neg_default_29 = slice_tensor_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_131: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_57, unsqueeze_default_32);  cat_default_57 = unsqueeze_default_32 = None\\n        add_tensor_100: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_130, mul_tensor_131);  mul_tensor_130 = mul_tensor_131 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_58: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg29, add_tensor_100], 2);  arg29 = add_tensor_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_59: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg30, transpose_int_72], 2);  arg30 = transpose_int_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_73: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_58, 2, 3)\\n        expand_default_57: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_99, [1, 32, 1, 128]);  add_tensor_99 = None\\n        view_default_346: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_57, [32, 1, 128]);  expand_default_57 = None\\n        sym_size_43: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_58, 2)\\n        expand_default_58: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_73, [1, 32, 128, sym_size_43]);  transpose_int_73 = None\\n        view_default_347: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_58, [32, 128, sym_size_43]);  expand_default_58 = None\\n        bmm_default_28: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_346, view_default_347);  view_default_346 = view_default_347 = None\\n        view_default_348: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_28, [1, 32, 1, sym_size_43]);  bmm_default_28 = None\\n        div_tensor_14: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_348, 11.313708498984761);  view_default_348 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_101: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_14, masked_fill_scalar);  div_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_14: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_101, -1, False);  add_tensor_101 = None\\n        detach_default_43: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_14)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_59: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_14, [1, 32, 1, sym_size_43]);  _softmax_default_14 = None\\n        view_default_349: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_59, [32, 1, sym_size_43]);  expand_default_59 = sym_size_43 = None\\n        sym_size_44: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_59, 2)\\n        expand_default_60: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_59, [1, 32, sym_size_44, 128])\\n        view_default_350: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_60, [32, sym_size_44, 128]);  expand_default_60 = sym_size_44 = None\\n        bmm_default_29: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_349, view_default_350);  view_default_349 = view_default_350 = None\\n        view_default_351: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_29, [1, 32, 1, 128]);  bmm_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_74: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_351, 1, 2);  view_default_351 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_352: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_74, [1, 1, 4096]);  transpose_int_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant131 = self._param_constant131\\n        t_default_101: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant131);  _param_constant131 = None\\n        view_default_353: f32[1, 4096] = torch.ops.aten.view.default(view_default_352, [1, 4096]);  view_default_352 = None\\n        mm_default_101: f32[1, 4096] = torch.ops.aten.mm.default(view_default_353, t_default_101);  view_default_353 = t_default_101 = None\\n        view_default_354: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_101, [1, 1, 4096]);  mm_default_101 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_102: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_97, view_default_354);  add_tensor_97 = view_default_354 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_29: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_102, 2)\\n        mean_dim_29: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_29, [-1], True);  pow_tensor_scalar_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_103: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_29, 1e-06);  mean_dim_29 = None\\n        rsqrt_default_29: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_103);  add_tensor_103 = None\\n        detach_default_44: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_29)\\n        mul_tensor_132: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_102, rsqrt_default_29);  rsqrt_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant132 = self._param_constant132\\n        mul_tensor_133: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant132, mul_tensor_132);  _param_constant132 = mul_tensor_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant133 = self._param_constant133\\n        t_default_102: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant133);  _param_constant133 = None\\n        view_default_355: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_133, [1, 4096])\\n        mm_default_102: f32[1, 11008] = torch.ops.aten.mm.default(view_default_355, t_default_102);  view_default_355 = t_default_102 = None\\n        view_default_356: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_102, [1, 1, 11008]);  mm_default_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_14: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_356);  view_default_356 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant134 = self._param_constant134\\n        t_default_103: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant134);  _param_constant134 = None\\n        view_default_357: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_133, [1, 4096]);  mul_tensor_133 = None\\n        mm_default_103: f32[1, 11008] = torch.ops.aten.mm.default(view_default_357, t_default_103);  view_default_357 = t_default_103 = None\\n        view_default_358: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_103, [1, 1, 11008]);  mm_default_103 = None\\n        mul_tensor_134: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_14, view_default_358);  silu_default_14 = view_default_358 = None\\n        _param_constant135 = self._param_constant135\\n        t_default_104: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant135);  _param_constant135 = None\\n        view_default_359: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_134, [1, 11008]);  mul_tensor_134 = None\\n        mm_default_104: f32[1, 4096] = torch.ops.aten.mm.default(view_default_359, t_default_104);  view_default_359 = t_default_104 = None\\n        view_default_360: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_104, [1, 1, 4096]);  mm_default_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_104: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_102, view_default_360);  add_tensor_102 = view_default_360 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_30: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_104, 2)\\n        mean_dim_30: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_30, [-1], True);  pow_tensor_scalar_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_105: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_30, 1e-06);  mean_dim_30 = None\\n        rsqrt_default_30: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_105);  add_tensor_105 = None\\n        detach_default_45: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_30)\\n        mul_tensor_135: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_104, rsqrt_default_30);  rsqrt_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant136 = self._param_constant136\\n        mul_tensor_136: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant136, mul_tensor_135);  _param_constant136 = mul_tensor_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant137 = self._param_constant137\\n        t_default_105: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant137);  _param_constant137 = None\\n        view_default_361: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_136, [1, 4096])\\n        mm_default_105: f32[1, 4096] = torch.ops.aten.mm.default(view_default_361, t_default_105);  view_default_361 = t_default_105 = None\\n        view_default_362: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_105, [1, 1, 4096]);  mm_default_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant138 = self._param_constant138\\n        t_default_106: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant138);  _param_constant138 = None\\n        view_default_363: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_136, [1, 4096])\\n        mm_default_106: f32[1, 4096] = torch.ops.aten.mm.default(view_default_363, t_default_106);  view_default_363 = t_default_106 = None\\n        view_default_364: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_106, [1, 1, 4096]);  mm_default_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant139 = self._param_constant139\\n        t_default_107: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant139);  _param_constant139 = None\\n        view_default_365: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_136, [1, 4096]);  mul_tensor_136 = None\\n        mm_default_107: f32[1, 4096] = torch.ops.aten.mm.default(view_default_365, t_default_107);  view_default_365 = t_default_107 = None\\n        view_default_366: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_107, [1, 1, 4096]);  mm_default_107 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_367: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_362, [1, 1, 32, 128]);  view_default_362 = None\\n        transpose_int_75: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_367, 1, 2);  view_default_367 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_368: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_364, [1, 1, 32, 128]);  view_default_364 = None\\n        transpose_int_76: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_368, 1, 2);  view_default_368 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_369: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_366, [1, 1, 32, 128]);  view_default_366 = None\\n        transpose_int_77: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_369, 1, 2);  view_default_369 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant30 = self._tensor_constant30\\n        slice_tensor_152: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant30, 0, 0, 9223372036854775807);  _tensor_constant30 = None\\n        slice_tensor_153: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_152, 1, 0, 9223372036854775807);  slice_tensor_152 = None\\n        sym_size_45: Sym(s0) = torch.ops.aten.sym_size(arg31, 2)\\n        add_17: Sym(s0 + 1) = 1 + sym_size_45;  sym_size_45 = None\\n        slice_tensor_154: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_153, 2, 0, add_17);  slice_tensor_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant31 = self._tensor_constant31\\n        slice_tensor_155: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant31, 0, 0, 9223372036854775807);  _tensor_constant31 = None\\n        slice_tensor_156: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_155, 1, 0, 9223372036854775807);  slice_tensor_155 = None\\n        slice_tensor_157: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_156, 2, 0, add_17);  slice_tensor_156 = add_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_60: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_154, 1);  slice_tensor_154 = None\\n        squeeze_dim_61: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_60, 0);  squeeze_dim_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_62: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_157, 1);  slice_tensor_157 = None\\n        squeeze_dim_63: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_62, 0);  squeeze_dim_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_30: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_61, [view_default]);  squeeze_dim_61 = None\\n        unsqueeze_default_33: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_30, 1);  index_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_31: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_63, [view_default]);  squeeze_dim_63 = None\\n        unsqueeze_default_34: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_31, 1);  index_tensor_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_137: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_75, unsqueeze_default_33)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_158: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_159: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 64, 9223372036854775807);  transpose_int_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_30: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_159);  slice_tensor_159 = None\\n        cat_default_60: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_30, slice_tensor_158], -1);  neg_default_30 = slice_tensor_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_138: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_60, unsqueeze_default_34);  cat_default_60 = None\\n        add_tensor_106: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_137, mul_tensor_138);  mul_tensor_137 = mul_tensor_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_139: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_76, unsqueeze_default_33);  unsqueeze_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_160: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_161: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 64, 9223372036854775807);  transpose_int_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_31: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_161);  slice_tensor_161 = None\\n        cat_default_61: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_31, slice_tensor_160], -1);  neg_default_31 = slice_tensor_160 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_140: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_61, unsqueeze_default_34);  cat_default_61 = unsqueeze_default_34 = None\\n        add_tensor_107: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_139, mul_tensor_140);  mul_tensor_139 = mul_tensor_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_62: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg31, add_tensor_107], 2);  arg31 = add_tensor_107 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_63: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg32, transpose_int_77], 2);  arg32 = transpose_int_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_78: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_62, 2, 3)\\n        expand_default_61: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_106, [1, 32, 1, 128]);  add_tensor_106 = None\\n        view_default_370: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_61, [32, 1, 128]);  expand_default_61 = None\\n        sym_size_46: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_62, 2)\\n        expand_default_62: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_78, [1, 32, 128, sym_size_46]);  transpose_int_78 = None\\n        view_default_371: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_62, [32, 128, sym_size_46]);  expand_default_62 = None\\n        bmm_default_30: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_370, view_default_371);  view_default_370 = view_default_371 = None\\n        view_default_372: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_30, [1, 32, 1, sym_size_46]);  bmm_default_30 = None\\n        div_tensor_15: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_372, 11.313708498984761);  view_default_372 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_108: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_15, masked_fill_scalar);  div_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_15: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_108, -1, False);  add_tensor_108 = None\\n        detach_default_46: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_15)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_63: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_15, [1, 32, 1, sym_size_46]);  _softmax_default_15 = None\\n        view_default_373: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_63, [32, 1, sym_size_46]);  expand_default_63 = sym_size_46 = None\\n        sym_size_47: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_63, 2)\\n        expand_default_64: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_63, [1, 32, sym_size_47, 128])\\n        view_default_374: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_64, [32, sym_size_47, 128]);  expand_default_64 = sym_size_47 = None\\n        bmm_default_31: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_373, view_default_374);  view_default_373 = view_default_374 = None\\n        view_default_375: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_31, [1, 32, 1, 128]);  bmm_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_79: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_375, 1, 2);  view_default_375 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_376: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_79, [1, 1, 4096]);  transpose_int_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant140 = self._param_constant140\\n        t_default_108: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant140);  _param_constant140 = None\\n        view_default_377: f32[1, 4096] = torch.ops.aten.view.default(view_default_376, [1, 4096]);  view_default_376 = None\\n        mm_default_108: f32[1, 4096] = torch.ops.aten.mm.default(view_default_377, t_default_108);  view_default_377 = t_default_108 = None\\n        view_default_378: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_108, [1, 1, 4096]);  mm_default_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_109: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_104, view_default_378);  add_tensor_104 = view_default_378 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_31: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_109, 2)\\n        mean_dim_31: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_31, [-1], True);  pow_tensor_scalar_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_110: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_31, 1e-06);  mean_dim_31 = None\\n        rsqrt_default_31: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_110);  add_tensor_110 = None\\n        detach_default_47: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_31)\\n        mul_tensor_141: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_109, rsqrt_default_31);  rsqrt_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant141 = self._param_constant141\\n        mul_tensor_142: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant141, mul_tensor_141);  _param_constant141 = mul_tensor_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant142 = self._param_constant142\\n        t_default_109: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant142);  _param_constant142 = None\\n        view_default_379: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_142, [1, 4096])\\n        mm_default_109: f32[1, 11008] = torch.ops.aten.mm.default(view_default_379, t_default_109);  view_default_379 = t_default_109 = None\\n        view_default_380: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_109, [1, 1, 11008]);  mm_default_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_15: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_380);  view_default_380 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant143 = self._param_constant143\\n        t_default_110: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant143);  _param_constant143 = None\\n        view_default_381: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_142, [1, 4096]);  mul_tensor_142 = None\\n        mm_default_110: f32[1, 11008] = torch.ops.aten.mm.default(view_default_381, t_default_110);  view_default_381 = t_default_110 = None\\n        view_default_382: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_110, [1, 1, 11008]);  mm_default_110 = None\\n        mul_tensor_143: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_15, view_default_382);  silu_default_15 = view_default_382 = None\\n        _param_constant144 = self._param_constant144\\n        t_default_111: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant144);  _param_constant144 = None\\n        view_default_383: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_143, [1, 11008]);  mul_tensor_143 = None\\n        mm_default_111: f32[1, 4096] = torch.ops.aten.mm.default(view_default_383, t_default_111);  view_default_383 = t_default_111 = None\\n        view_default_384: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_111, [1, 1, 4096]);  mm_default_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_111: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_109, view_default_384);  add_tensor_109 = view_default_384 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_32: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_111, 2)\\n        mean_dim_32: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_32, [-1], True);  pow_tensor_scalar_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_112: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_32, 1e-06);  mean_dim_32 = None\\n        rsqrt_default_32: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_112);  add_tensor_112 = None\\n        detach_default_48: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_32)\\n        mul_tensor_144: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_111, rsqrt_default_32);  rsqrt_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant145 = self._param_constant145\\n        mul_tensor_145: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant145, mul_tensor_144);  _param_constant145 = mul_tensor_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant146 = self._param_constant146\\n        t_default_112: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant146);  _param_constant146 = None\\n        view_default_385: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_145, [1, 4096])\\n        mm_default_112: f32[1, 4096] = torch.ops.aten.mm.default(view_default_385, t_default_112);  view_default_385 = t_default_112 = None\\n        view_default_386: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_112, [1, 1, 4096]);  mm_default_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant147 = self._param_constant147\\n        t_default_113: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant147);  _param_constant147 = None\\n        view_default_387: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_145, [1, 4096])\\n        mm_default_113: f32[1, 4096] = torch.ops.aten.mm.default(view_default_387, t_default_113);  view_default_387 = t_default_113 = None\\n        view_default_388: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_113, [1, 1, 4096]);  mm_default_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant148 = self._param_constant148\\n        t_default_114: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant148);  _param_constant148 = None\\n        view_default_389: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_145, [1, 4096]);  mul_tensor_145 = None\\n        mm_default_114: f32[1, 4096] = torch.ops.aten.mm.default(view_default_389, t_default_114);  view_default_389 = t_default_114 = None\\n        view_default_390: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_114, [1, 1, 4096]);  mm_default_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_391: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_386, [1, 1, 32, 128]);  view_default_386 = None\\n        transpose_int_80: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_391, 1, 2);  view_default_391 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_392: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_388, [1, 1, 32, 128]);  view_default_388 = None\\n        transpose_int_81: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_392, 1, 2);  view_default_392 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_393: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_390, [1, 1, 32, 128]);  view_default_390 = None\\n        transpose_int_82: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_393, 1, 2);  view_default_393 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant32 = self._tensor_constant32\\n        slice_tensor_162: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant32, 0, 0, 9223372036854775807);  _tensor_constant32 = None\\n        slice_tensor_163: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_162, 1, 0, 9223372036854775807);  slice_tensor_162 = None\\n        sym_size_48: Sym(s0) = torch.ops.aten.sym_size(arg33, 2)\\n        add_18: Sym(s0 + 1) = 1 + sym_size_48;  sym_size_48 = None\\n        slice_tensor_164: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_163, 2, 0, add_18);  slice_tensor_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant33 = self._tensor_constant33\\n        slice_tensor_165: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant33, 0, 0, 9223372036854775807);  _tensor_constant33 = None\\n        slice_tensor_166: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_165, 1, 0, 9223372036854775807);  slice_tensor_165 = None\\n        slice_tensor_167: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_166, 2, 0, add_18);  slice_tensor_166 = add_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_64: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_164, 1);  slice_tensor_164 = None\\n        squeeze_dim_65: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_64, 0);  squeeze_dim_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_66: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_167, 1);  slice_tensor_167 = None\\n        squeeze_dim_67: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_66, 0);  squeeze_dim_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_32: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_65, [view_default]);  squeeze_dim_65 = None\\n        unsqueeze_default_35: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_32, 1);  index_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_33: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_67, [view_default]);  squeeze_dim_67 = None\\n        unsqueeze_default_36: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_33, 1);  index_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_146: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_80, unsqueeze_default_35)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_168: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_169: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 64, 9223372036854775807);  transpose_int_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_32: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_169);  slice_tensor_169 = None\\n        cat_default_64: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_32, slice_tensor_168], -1);  neg_default_32 = slice_tensor_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_147: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_64, unsqueeze_default_36);  cat_default_64 = None\\n        add_tensor_113: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_146, mul_tensor_147);  mul_tensor_146 = mul_tensor_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_148: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_81, unsqueeze_default_35);  unsqueeze_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_170: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_171: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 64, 9223372036854775807);  transpose_int_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_33: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_171);  slice_tensor_171 = None\\n        cat_default_65: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_33, slice_tensor_170], -1);  neg_default_33 = slice_tensor_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_149: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_65, unsqueeze_default_36);  cat_default_65 = unsqueeze_default_36 = None\\n        add_tensor_114: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_148, mul_tensor_149);  mul_tensor_148 = mul_tensor_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_66: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg33, add_tensor_114], 2);  arg33 = add_tensor_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_67: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg34, transpose_int_82], 2);  arg34 = transpose_int_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_83: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_66, 2, 3)\\n        expand_default_65: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_113, [1, 32, 1, 128]);  add_tensor_113 = None\\n        view_default_394: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_65, [32, 1, 128]);  expand_default_65 = None\\n        sym_size_49: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_66, 2)\\n        expand_default_66: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_83, [1, 32, 128, sym_size_49]);  transpose_int_83 = None\\n        view_default_395: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_66, [32, 128, sym_size_49]);  expand_default_66 = None\\n        bmm_default_32: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_394, view_default_395);  view_default_394 = view_default_395 = None\\n        view_default_396: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_32, [1, 32, 1, sym_size_49]);  bmm_default_32 = None\\n        div_tensor_16: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_396, 11.313708498984761);  view_default_396 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_115: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_16, masked_fill_scalar);  div_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_16: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_115, -1, False);  add_tensor_115 = None\\n        detach_default_49: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_16)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_67: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_16, [1, 32, 1, sym_size_49]);  _softmax_default_16 = None\\n        view_default_397: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_67, [32, 1, sym_size_49]);  expand_default_67 = sym_size_49 = None\\n        sym_size_50: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_67, 2)\\n        expand_default_68: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_67, [1, 32, sym_size_50, 128])\\n        view_default_398: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_68, [32, sym_size_50, 128]);  expand_default_68 = sym_size_50 = None\\n        bmm_default_33: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_397, view_default_398);  view_default_397 = view_default_398 = None\\n        view_default_399: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_33, [1, 32, 1, 128]);  bmm_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_84: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_399, 1, 2);  view_default_399 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_400: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_84, [1, 1, 4096]);  transpose_int_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant149 = self._param_constant149\\n        t_default_115: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant149);  _param_constant149 = None\\n        view_default_401: f32[1, 4096] = torch.ops.aten.view.default(view_default_400, [1, 4096]);  view_default_400 = None\\n        mm_default_115: f32[1, 4096] = torch.ops.aten.mm.default(view_default_401, t_default_115);  view_default_401 = t_default_115 = None\\n        view_default_402: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_115, [1, 1, 4096]);  mm_default_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_116: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_111, view_default_402);  add_tensor_111 = view_default_402 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_33: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_116, 2)\\n        mean_dim_33: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_33, [-1], True);  pow_tensor_scalar_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_117: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_33, 1e-06);  mean_dim_33 = None\\n        rsqrt_default_33: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_117);  add_tensor_117 = None\\n        detach_default_50: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_33)\\n        mul_tensor_150: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_116, rsqrt_default_33);  rsqrt_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant150 = self._param_constant150\\n        mul_tensor_151: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant150, mul_tensor_150);  _param_constant150 = mul_tensor_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant151 = self._param_constant151\\n        t_default_116: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant151);  _param_constant151 = None\\n        view_default_403: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_151, [1, 4096])\\n        mm_default_116: f32[1, 11008] = torch.ops.aten.mm.default(view_default_403, t_default_116);  view_default_403 = t_default_116 = None\\n        view_default_404: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_116, [1, 1, 11008]);  mm_default_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_16: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_404);  view_default_404 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant152 = self._param_constant152\\n        t_default_117: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant152);  _param_constant152 = None\\n        view_default_405: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_151, [1, 4096]);  mul_tensor_151 = None\\n        mm_default_117: f32[1, 11008] = torch.ops.aten.mm.default(view_default_405, t_default_117);  view_default_405 = t_default_117 = None\\n        view_default_406: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_117, [1, 1, 11008]);  mm_default_117 = None\\n        mul_tensor_152: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_16, view_default_406);  silu_default_16 = view_default_406 = None\\n        _param_constant153 = self._param_constant153\\n        t_default_118: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant153);  _param_constant153 = None\\n        view_default_407: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_152, [1, 11008]);  mul_tensor_152 = None\\n        mm_default_118: f32[1, 4096] = torch.ops.aten.mm.default(view_default_407, t_default_118);  view_default_407 = t_default_118 = None\\n        view_default_408: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_118, [1, 1, 4096]);  mm_default_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_118: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_116, view_default_408);  add_tensor_116 = view_default_408 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_34: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_118, 2)\\n        mean_dim_34: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_34, [-1], True);  pow_tensor_scalar_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_119: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_34, 1e-06);  mean_dim_34 = None\\n        rsqrt_default_34: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_119);  add_tensor_119 = None\\n        detach_default_51: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_34)\\n        mul_tensor_153: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_118, rsqrt_default_34);  rsqrt_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant154 = self._param_constant154\\n        mul_tensor_154: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant154, mul_tensor_153);  _param_constant154 = mul_tensor_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant155 = self._param_constant155\\n        t_default_119: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant155);  _param_constant155 = None\\n        view_default_409: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_154, [1, 4096])\\n        mm_default_119: f32[1, 4096] = torch.ops.aten.mm.default(view_default_409, t_default_119);  view_default_409 = t_default_119 = None\\n        view_default_410: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_119, [1, 1, 4096]);  mm_default_119 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant156 = self._param_constant156\\n        t_default_120: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant156);  _param_constant156 = None\\n        view_default_411: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_154, [1, 4096])\\n        mm_default_120: f32[1, 4096] = torch.ops.aten.mm.default(view_default_411, t_default_120);  view_default_411 = t_default_120 = None\\n        view_default_412: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_120, [1, 1, 4096]);  mm_default_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant157 = self._param_constant157\\n        t_default_121: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant157);  _param_constant157 = None\\n        view_default_413: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_154, [1, 4096]);  mul_tensor_154 = None\\n        mm_default_121: f32[1, 4096] = torch.ops.aten.mm.default(view_default_413, t_default_121);  view_default_413 = t_default_121 = None\\n        view_default_414: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_121, [1, 1, 4096]);  mm_default_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_415: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_410, [1, 1, 32, 128]);  view_default_410 = None\\n        transpose_int_85: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_415, 1, 2);  view_default_415 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_416: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_412, [1, 1, 32, 128]);  view_default_412 = None\\n        transpose_int_86: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_416, 1, 2);  view_default_416 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_417: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_414, [1, 1, 32, 128]);  view_default_414 = None\\n        transpose_int_87: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_417, 1, 2);  view_default_417 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant34 = self._tensor_constant34\\n        slice_tensor_172: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant34, 0, 0, 9223372036854775807);  _tensor_constant34 = None\\n        slice_tensor_173: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_172, 1, 0, 9223372036854775807);  slice_tensor_172 = None\\n        sym_size_51: Sym(s0) = torch.ops.aten.sym_size(arg35, 2)\\n        add_19: Sym(s0 + 1) = 1 + sym_size_51;  sym_size_51 = None\\n        slice_tensor_174: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_173, 2, 0, add_19);  slice_tensor_173 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant35 = self._tensor_constant35\\n        slice_tensor_175: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant35, 0, 0, 9223372036854775807);  _tensor_constant35 = None\\n        slice_tensor_176: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_175, 1, 0, 9223372036854775807);  slice_tensor_175 = None\\n        slice_tensor_177: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_176, 2, 0, add_19);  slice_tensor_176 = add_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_68: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_174, 1);  slice_tensor_174 = None\\n        squeeze_dim_69: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_68, 0);  squeeze_dim_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_70: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_177, 1);  slice_tensor_177 = None\\n        squeeze_dim_71: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_70, 0);  squeeze_dim_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_34: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_69, [view_default]);  squeeze_dim_69 = None\\n        unsqueeze_default_37: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_34, 1);  index_tensor_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_35: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_71, [view_default]);  squeeze_dim_71 = None\\n        unsqueeze_default_38: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_35, 1);  index_tensor_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_155: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_85, unsqueeze_default_37)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_178: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_179: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 64, 9223372036854775807);  transpose_int_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_34: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_179);  slice_tensor_179 = None\\n        cat_default_68: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_34, slice_tensor_178], -1);  neg_default_34 = slice_tensor_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_156: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_68, unsqueeze_default_38);  cat_default_68 = None\\n        add_tensor_120: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_155, mul_tensor_156);  mul_tensor_155 = mul_tensor_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_157: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_86, unsqueeze_default_37);  unsqueeze_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_180: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_181: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 64, 9223372036854775807);  transpose_int_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_35: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_181);  slice_tensor_181 = None\\n        cat_default_69: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_35, slice_tensor_180], -1);  neg_default_35 = slice_tensor_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_158: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_69, unsqueeze_default_38);  cat_default_69 = unsqueeze_default_38 = None\\n        add_tensor_121: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_157, mul_tensor_158);  mul_tensor_157 = mul_tensor_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_70: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg35, add_tensor_121], 2);  arg35 = add_tensor_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_71: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg36, transpose_int_87], 2);  arg36 = transpose_int_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_88: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_70, 2, 3)\\n        expand_default_69: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_120, [1, 32, 1, 128]);  add_tensor_120 = None\\n        view_default_418: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_69, [32, 1, 128]);  expand_default_69 = None\\n        sym_size_52: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_70, 2)\\n        expand_default_70: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_88, [1, 32, 128, sym_size_52]);  transpose_int_88 = None\\n        view_default_419: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_70, [32, 128, sym_size_52]);  expand_default_70 = None\\n        bmm_default_34: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_418, view_default_419);  view_default_418 = view_default_419 = None\\n        view_default_420: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_34, [1, 32, 1, sym_size_52]);  bmm_default_34 = None\\n        div_tensor_17: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_420, 11.313708498984761);  view_default_420 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_122: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_17, masked_fill_scalar);  div_tensor_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_17: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_122, -1, False);  add_tensor_122 = None\\n        detach_default_52: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_17)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_71: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_17, [1, 32, 1, sym_size_52]);  _softmax_default_17 = None\\n        view_default_421: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_71, [32, 1, sym_size_52]);  expand_default_71 = sym_size_52 = None\\n        sym_size_53: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_71, 2)\\n        expand_default_72: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_71, [1, 32, sym_size_53, 128])\\n        view_default_422: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_72, [32, sym_size_53, 128]);  expand_default_72 = sym_size_53 = None\\n        bmm_default_35: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_421, view_default_422);  view_default_421 = view_default_422 = None\\n        view_default_423: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_35, [1, 32, 1, 128]);  bmm_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_89: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_423, 1, 2);  view_default_423 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_424: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_89, [1, 1, 4096]);  transpose_int_89 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant158 = self._param_constant158\\n        t_default_122: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant158);  _param_constant158 = None\\n        view_default_425: f32[1, 4096] = torch.ops.aten.view.default(view_default_424, [1, 4096]);  view_default_424 = None\\n        mm_default_122: f32[1, 4096] = torch.ops.aten.mm.default(view_default_425, t_default_122);  view_default_425 = t_default_122 = None\\n        view_default_426: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_122, [1, 1, 4096]);  mm_default_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_123: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_118, view_default_426);  add_tensor_118 = view_default_426 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_35: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_123, 2)\\n        mean_dim_35: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_35, [-1], True);  pow_tensor_scalar_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_124: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_35, 1e-06);  mean_dim_35 = None\\n        rsqrt_default_35: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_124);  add_tensor_124 = None\\n        detach_default_53: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_35)\\n        mul_tensor_159: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_123, rsqrt_default_35);  rsqrt_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant159 = self._param_constant159\\n        mul_tensor_160: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant159, mul_tensor_159);  _param_constant159 = mul_tensor_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant160 = self._param_constant160\\n        t_default_123: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant160);  _param_constant160 = None\\n        view_default_427: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_160, [1, 4096])\\n        mm_default_123: f32[1, 11008] = torch.ops.aten.mm.default(view_default_427, t_default_123);  view_default_427 = t_default_123 = None\\n        view_default_428: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_123, [1, 1, 11008]);  mm_default_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_17: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_428);  view_default_428 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant161 = self._param_constant161\\n        t_default_124: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant161);  _param_constant161 = None\\n        view_default_429: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_160, [1, 4096]);  mul_tensor_160 = None\\n        mm_default_124: f32[1, 11008] = torch.ops.aten.mm.default(view_default_429, t_default_124);  view_default_429 = t_default_124 = None\\n        view_default_430: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_124, [1, 1, 11008]);  mm_default_124 = None\\n        mul_tensor_161: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_17, view_default_430);  silu_default_17 = view_default_430 = None\\n        _param_constant162 = self._param_constant162\\n        t_default_125: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant162);  _param_constant162 = None\\n        view_default_431: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_161, [1, 11008]);  mul_tensor_161 = None\\n        mm_default_125: f32[1, 4096] = torch.ops.aten.mm.default(view_default_431, t_default_125);  view_default_431 = t_default_125 = None\\n        view_default_432: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_125, [1, 1, 4096]);  mm_default_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_125: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_123, view_default_432);  add_tensor_123 = view_default_432 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_36: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_125, 2)\\n        mean_dim_36: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_36, [-1], True);  pow_tensor_scalar_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_126: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_36, 1e-06);  mean_dim_36 = None\\n        rsqrt_default_36: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_126);  add_tensor_126 = None\\n        detach_default_54: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_36)\\n        mul_tensor_162: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_125, rsqrt_default_36);  rsqrt_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant163 = self._param_constant163\\n        mul_tensor_163: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant163, mul_tensor_162);  _param_constant163 = mul_tensor_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant164 = self._param_constant164\\n        t_default_126: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant164);  _param_constant164 = None\\n        view_default_433: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_163, [1, 4096])\\n        mm_default_126: f32[1, 4096] = torch.ops.aten.mm.default(view_default_433, t_default_126);  view_default_433 = t_default_126 = None\\n        view_default_434: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_126, [1, 1, 4096]);  mm_default_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant165 = self._param_constant165\\n        t_default_127: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant165);  _param_constant165 = None\\n        view_default_435: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_163, [1, 4096])\\n        mm_default_127: f32[1, 4096] = torch.ops.aten.mm.default(view_default_435, t_default_127);  view_default_435 = t_default_127 = None\\n        view_default_436: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_127, [1, 1, 4096]);  mm_default_127 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant166 = self._param_constant166\\n        t_default_128: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant166);  _param_constant166 = None\\n        view_default_437: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_163, [1, 4096]);  mul_tensor_163 = None\\n        mm_default_128: f32[1, 4096] = torch.ops.aten.mm.default(view_default_437, t_default_128);  view_default_437 = t_default_128 = None\\n        view_default_438: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_128, [1, 1, 4096]);  mm_default_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_439: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_434, [1, 1, 32, 128]);  view_default_434 = None\\n        transpose_int_90: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_439, 1, 2);  view_default_439 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_440: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_436, [1, 1, 32, 128]);  view_default_436 = None\\n        transpose_int_91: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_440, 1, 2);  view_default_440 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_441: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_438, [1, 1, 32, 128]);  view_default_438 = None\\n        transpose_int_92: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_441, 1, 2);  view_default_441 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant36 = self._tensor_constant36\\n        slice_tensor_182: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant36, 0, 0, 9223372036854775807);  _tensor_constant36 = None\\n        slice_tensor_183: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_182, 1, 0, 9223372036854775807);  slice_tensor_182 = None\\n        sym_size_54: Sym(s0) = torch.ops.aten.sym_size(arg37, 2)\\n        add_20: Sym(s0 + 1) = 1 + sym_size_54;  sym_size_54 = None\\n        slice_tensor_184: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_183, 2, 0, add_20);  slice_tensor_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant37 = self._tensor_constant37\\n        slice_tensor_185: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant37, 0, 0, 9223372036854775807);  _tensor_constant37 = None\\n        slice_tensor_186: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_185, 1, 0, 9223372036854775807);  slice_tensor_185 = None\\n        slice_tensor_187: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_186, 2, 0, add_20);  slice_tensor_186 = add_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_72: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_184, 1);  slice_tensor_184 = None\\n        squeeze_dim_73: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_72, 0);  squeeze_dim_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_74: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_187, 1);  slice_tensor_187 = None\\n        squeeze_dim_75: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_74, 0);  squeeze_dim_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_36: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_73, [view_default]);  squeeze_dim_73 = None\\n        unsqueeze_default_39: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_36, 1);  index_tensor_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_37: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_75, [view_default]);  squeeze_dim_75 = None\\n        unsqueeze_default_40: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_37, 1);  index_tensor_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_164: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_90, unsqueeze_default_39)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_188: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_189: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 64, 9223372036854775807);  transpose_int_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_36: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_189);  slice_tensor_189 = None\\n        cat_default_72: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_36, slice_tensor_188], -1);  neg_default_36 = slice_tensor_188 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_165: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_72, unsqueeze_default_40);  cat_default_72 = None\\n        add_tensor_127: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_164, mul_tensor_165);  mul_tensor_164 = mul_tensor_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_166: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_91, unsqueeze_default_39);  unsqueeze_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_190: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_191: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 64, 9223372036854775807);  transpose_int_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_37: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_191);  slice_tensor_191 = None\\n        cat_default_73: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_37, slice_tensor_190], -1);  neg_default_37 = slice_tensor_190 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_167: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_73, unsqueeze_default_40);  cat_default_73 = unsqueeze_default_40 = None\\n        add_tensor_128: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_166, mul_tensor_167);  mul_tensor_166 = mul_tensor_167 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_74: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg37, add_tensor_128], 2);  arg37 = add_tensor_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_75: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg38, transpose_int_92], 2);  arg38 = transpose_int_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_93: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_74, 2, 3)\\n        expand_default_73: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_127, [1, 32, 1, 128]);  add_tensor_127 = None\\n        view_default_442: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_73, [32, 1, 128]);  expand_default_73 = None\\n        sym_size_55: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_74, 2)\\n        expand_default_74: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_93, [1, 32, 128, sym_size_55]);  transpose_int_93 = None\\n        view_default_443: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_74, [32, 128, sym_size_55]);  expand_default_74 = None\\n        bmm_default_36: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_442, view_default_443);  view_default_442 = view_default_443 = None\\n        view_default_444: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_36, [1, 32, 1, sym_size_55]);  bmm_default_36 = None\\n        div_tensor_18: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_444, 11.313708498984761);  view_default_444 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_129: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_18, masked_fill_scalar);  div_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_18: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_129, -1, False);  add_tensor_129 = None\\n        detach_default_55: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_18)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_75: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_18, [1, 32, 1, sym_size_55]);  _softmax_default_18 = None\\n        view_default_445: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_75, [32, 1, sym_size_55]);  expand_default_75 = sym_size_55 = None\\n        sym_size_56: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_75, 2)\\n        expand_default_76: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_75, [1, 32, sym_size_56, 128])\\n        view_default_446: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_76, [32, sym_size_56, 128]);  expand_default_76 = sym_size_56 = None\\n        bmm_default_37: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_445, view_default_446);  view_default_445 = view_default_446 = None\\n        view_default_447: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_37, [1, 32, 1, 128]);  bmm_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_94: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_447, 1, 2);  view_default_447 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_448: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_94, [1, 1, 4096]);  transpose_int_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant167 = self._param_constant167\\n        t_default_129: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant167);  _param_constant167 = None\\n        view_default_449: f32[1, 4096] = torch.ops.aten.view.default(view_default_448, [1, 4096]);  view_default_448 = None\\n        mm_default_129: f32[1, 4096] = torch.ops.aten.mm.default(view_default_449, t_default_129);  view_default_449 = t_default_129 = None\\n        view_default_450: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_129, [1, 1, 4096]);  mm_default_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_130: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_125, view_default_450);  add_tensor_125 = view_default_450 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_37: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_130, 2)\\n        mean_dim_37: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_37, [-1], True);  pow_tensor_scalar_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_131: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_37, 1e-06);  mean_dim_37 = None\\n        rsqrt_default_37: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_131);  add_tensor_131 = None\\n        detach_default_56: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_37)\\n        mul_tensor_168: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_130, rsqrt_default_37);  rsqrt_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant168 = self._param_constant168\\n        mul_tensor_169: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant168, mul_tensor_168);  _param_constant168 = mul_tensor_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant169 = self._param_constant169\\n        t_default_130: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant169);  _param_constant169 = None\\n        view_default_451: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_169, [1, 4096])\\n        mm_default_130: f32[1, 11008] = torch.ops.aten.mm.default(view_default_451, t_default_130);  view_default_451 = t_default_130 = None\\n        view_default_452: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_130, [1, 1, 11008]);  mm_default_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_18: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_452);  view_default_452 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant170 = self._param_constant170\\n        t_default_131: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant170);  _param_constant170 = None\\n        view_default_453: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_169, [1, 4096]);  mul_tensor_169 = None\\n        mm_default_131: f32[1, 11008] = torch.ops.aten.mm.default(view_default_453, t_default_131);  view_default_453 = t_default_131 = None\\n        view_default_454: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_131, [1, 1, 11008]);  mm_default_131 = None\\n        mul_tensor_170: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_18, view_default_454);  silu_default_18 = view_default_454 = None\\n        _param_constant171 = self._param_constant171\\n        t_default_132: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant171);  _param_constant171 = None\\n        view_default_455: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_170, [1, 11008]);  mul_tensor_170 = None\\n        mm_default_132: f32[1, 4096] = torch.ops.aten.mm.default(view_default_455, t_default_132);  view_default_455 = t_default_132 = None\\n        view_default_456: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_132, [1, 1, 4096]);  mm_default_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_132: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_130, view_default_456);  add_tensor_130 = view_default_456 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_38: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_132, 2)\\n        mean_dim_38: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_38, [-1], True);  pow_tensor_scalar_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_133: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_38, 1e-06);  mean_dim_38 = None\\n        rsqrt_default_38: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_133);  add_tensor_133 = None\\n        detach_default_57: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_38)\\n        mul_tensor_171: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_132, rsqrt_default_38);  rsqrt_default_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant172 = self._param_constant172\\n        mul_tensor_172: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant172, mul_tensor_171);  _param_constant172 = mul_tensor_171 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant173 = self._param_constant173\\n        t_default_133: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant173);  _param_constant173 = None\\n        view_default_457: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_172, [1, 4096])\\n        mm_default_133: f32[1, 4096] = torch.ops.aten.mm.default(view_default_457, t_default_133);  view_default_457 = t_default_133 = None\\n        view_default_458: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_133, [1, 1, 4096]);  mm_default_133 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant174 = self._param_constant174\\n        t_default_134: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant174);  _param_constant174 = None\\n        view_default_459: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_172, [1, 4096])\\n        mm_default_134: f32[1, 4096] = torch.ops.aten.mm.default(view_default_459, t_default_134);  view_default_459 = t_default_134 = None\\n        view_default_460: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_134, [1, 1, 4096]);  mm_default_134 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant175 = self._param_constant175\\n        t_default_135: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant175);  _param_constant175 = None\\n        view_default_461: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_172, [1, 4096]);  mul_tensor_172 = None\\n        mm_default_135: f32[1, 4096] = torch.ops.aten.mm.default(view_default_461, t_default_135);  view_default_461 = t_default_135 = None\\n        view_default_462: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_135, [1, 1, 4096]);  mm_default_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_463: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_458, [1, 1, 32, 128]);  view_default_458 = None\\n        transpose_int_95: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_463, 1, 2);  view_default_463 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_464: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_460, [1, 1, 32, 128]);  view_default_460 = None\\n        transpose_int_96: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_464, 1, 2);  view_default_464 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_465: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_462, [1, 1, 32, 128]);  view_default_462 = None\\n        transpose_int_97: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_465, 1, 2);  view_default_465 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant38 = self._tensor_constant38\\n        slice_tensor_192: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant38, 0, 0, 9223372036854775807);  _tensor_constant38 = None\\n        slice_tensor_193: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_192, 1, 0, 9223372036854775807);  slice_tensor_192 = None\\n        sym_size_57: Sym(s0) = torch.ops.aten.sym_size(arg39, 2)\\n        add_21: Sym(s0 + 1) = 1 + sym_size_57;  sym_size_57 = None\\n        slice_tensor_194: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_193, 2, 0, add_21);  slice_tensor_193 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant39 = self._tensor_constant39\\n        slice_tensor_195: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant39, 0, 0, 9223372036854775807);  _tensor_constant39 = None\\n        slice_tensor_196: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_195, 1, 0, 9223372036854775807);  slice_tensor_195 = None\\n        slice_tensor_197: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_196, 2, 0, add_21);  slice_tensor_196 = add_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_76: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_194, 1);  slice_tensor_194 = None\\n        squeeze_dim_77: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_76, 0);  squeeze_dim_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_78: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_197, 1);  slice_tensor_197 = None\\n        squeeze_dim_79: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_78, 0);  squeeze_dim_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_38: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_77, [view_default]);  squeeze_dim_77 = None\\n        unsqueeze_default_41: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_38, 1);  index_tensor_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_39: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_79, [view_default]);  squeeze_dim_79 = None\\n        unsqueeze_default_42: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_39, 1);  index_tensor_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_173: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_95, unsqueeze_default_41)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_198: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_199: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 64, 9223372036854775807);  transpose_int_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_38: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_199);  slice_tensor_199 = None\\n        cat_default_76: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_38, slice_tensor_198], -1);  neg_default_38 = slice_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_174: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_76, unsqueeze_default_42);  cat_default_76 = None\\n        add_tensor_134: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_173, mul_tensor_174);  mul_tensor_173 = mul_tensor_174 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_175: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_96, unsqueeze_default_41);  unsqueeze_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_200: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_201: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 64, 9223372036854775807);  transpose_int_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_39: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_201);  slice_tensor_201 = None\\n        cat_default_77: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_39, slice_tensor_200], -1);  neg_default_39 = slice_tensor_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_176: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_77, unsqueeze_default_42);  cat_default_77 = unsqueeze_default_42 = None\\n        add_tensor_135: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_175, mul_tensor_176);  mul_tensor_175 = mul_tensor_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_78: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg39, add_tensor_135], 2);  arg39 = add_tensor_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_79: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg40, transpose_int_97], 2);  arg40 = transpose_int_97 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_98: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_78, 2, 3)\\n        expand_default_77: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_134, [1, 32, 1, 128]);  add_tensor_134 = None\\n        view_default_466: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_77, [32, 1, 128]);  expand_default_77 = None\\n        sym_size_58: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_78, 2)\\n        expand_default_78: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_98, [1, 32, 128, sym_size_58]);  transpose_int_98 = None\\n        view_default_467: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_78, [32, 128, sym_size_58]);  expand_default_78 = None\\n        bmm_default_38: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_466, view_default_467);  view_default_466 = view_default_467 = None\\n        view_default_468: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_38, [1, 32, 1, sym_size_58]);  bmm_default_38 = None\\n        div_tensor_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_468, 11.313708498984761);  view_default_468 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_136: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_19, masked_fill_scalar);  div_tensor_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_19: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_136, -1, False);  add_tensor_136 = None\\n        detach_default_58: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_19)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_79: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_19, [1, 32, 1, sym_size_58]);  _softmax_default_19 = None\\n        view_default_469: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_79, [32, 1, sym_size_58]);  expand_default_79 = sym_size_58 = None\\n        sym_size_59: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_79, 2)\\n        expand_default_80: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_79, [1, 32, sym_size_59, 128])\\n        view_default_470: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_80, [32, sym_size_59, 128]);  expand_default_80 = sym_size_59 = None\\n        bmm_default_39: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_469, view_default_470);  view_default_469 = view_default_470 = None\\n        view_default_471: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_39, [1, 32, 1, 128]);  bmm_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_99: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_471, 1, 2);  view_default_471 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_472: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_99, [1, 1, 4096]);  transpose_int_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant176 = self._param_constant176\\n        t_default_136: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant176);  _param_constant176 = None\\n        view_default_473: f32[1, 4096] = torch.ops.aten.view.default(view_default_472, [1, 4096]);  view_default_472 = None\\n        mm_default_136: f32[1, 4096] = torch.ops.aten.mm.default(view_default_473, t_default_136);  view_default_473 = t_default_136 = None\\n        view_default_474: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_136, [1, 1, 4096]);  mm_default_136 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_137: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_132, view_default_474);  add_tensor_132 = view_default_474 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_39: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_137, 2)\\n        mean_dim_39: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_39, [-1], True);  pow_tensor_scalar_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_138: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_39, 1e-06);  mean_dim_39 = None\\n        rsqrt_default_39: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_138);  add_tensor_138 = None\\n        detach_default_59: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_39)\\n        mul_tensor_177: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_137, rsqrt_default_39);  rsqrt_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant177 = self._param_constant177\\n        mul_tensor_178: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant177, mul_tensor_177);  _param_constant177 = mul_tensor_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant178 = self._param_constant178\\n        t_default_137: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant178);  _param_constant178 = None\\n        view_default_475: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_178, [1, 4096])\\n        mm_default_137: f32[1, 11008] = torch.ops.aten.mm.default(view_default_475, t_default_137);  view_default_475 = t_default_137 = None\\n        view_default_476: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_137, [1, 1, 11008]);  mm_default_137 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_19: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_476);  view_default_476 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant179 = self._param_constant179\\n        t_default_138: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant179);  _param_constant179 = None\\n        view_default_477: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_178, [1, 4096]);  mul_tensor_178 = None\\n        mm_default_138: f32[1, 11008] = torch.ops.aten.mm.default(view_default_477, t_default_138);  view_default_477 = t_default_138 = None\\n        view_default_478: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_138, [1, 1, 11008]);  mm_default_138 = None\\n        mul_tensor_179: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_19, view_default_478);  silu_default_19 = view_default_478 = None\\n        _param_constant180 = self._param_constant180\\n        t_default_139: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant180);  _param_constant180 = None\\n        view_default_479: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_179, [1, 11008]);  mul_tensor_179 = None\\n        mm_default_139: f32[1, 4096] = torch.ops.aten.mm.default(view_default_479, t_default_139);  view_default_479 = t_default_139 = None\\n        view_default_480: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_139, [1, 1, 4096]);  mm_default_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_139: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_137, view_default_480);  add_tensor_137 = view_default_480 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_40: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_139, 2)\\n        mean_dim_40: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_40, [-1], True);  pow_tensor_scalar_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_140: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_40, 1e-06);  mean_dim_40 = None\\n        rsqrt_default_40: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_140);  add_tensor_140 = None\\n        detach_default_60: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_40)\\n        mul_tensor_180: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_139, rsqrt_default_40);  rsqrt_default_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant181 = self._param_constant181\\n        mul_tensor_181: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant181, mul_tensor_180);  _param_constant181 = mul_tensor_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant182 = self._param_constant182\\n        t_default_140: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant182);  _param_constant182 = None\\n        view_default_481: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_181, [1, 4096])\\n        mm_default_140: f32[1, 4096] = torch.ops.aten.mm.default(view_default_481, t_default_140);  view_default_481 = t_default_140 = None\\n        view_default_482: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_140, [1, 1, 4096]);  mm_default_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant183 = self._param_constant183\\n        t_default_141: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant183);  _param_constant183 = None\\n        view_default_483: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_181, [1, 4096])\\n        mm_default_141: f32[1, 4096] = torch.ops.aten.mm.default(view_default_483, t_default_141);  view_default_483 = t_default_141 = None\\n        view_default_484: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_141, [1, 1, 4096]);  mm_default_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant184 = self._param_constant184\\n        t_default_142: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant184);  _param_constant184 = None\\n        view_default_485: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_181, [1, 4096]);  mul_tensor_181 = None\\n        mm_default_142: f32[1, 4096] = torch.ops.aten.mm.default(view_default_485, t_default_142);  view_default_485 = t_default_142 = None\\n        view_default_486: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_142, [1, 1, 4096]);  mm_default_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_487: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_482, [1, 1, 32, 128]);  view_default_482 = None\\n        transpose_int_100: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_487, 1, 2);  view_default_487 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_488: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_484, [1, 1, 32, 128]);  view_default_484 = None\\n        transpose_int_101: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_488, 1, 2);  view_default_488 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_489: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_486, [1, 1, 32, 128]);  view_default_486 = None\\n        transpose_int_102: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_489, 1, 2);  view_default_489 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant40 = self._tensor_constant40\\n        slice_tensor_202: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant40, 0, 0, 9223372036854775807);  _tensor_constant40 = None\\n        slice_tensor_203: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_202, 1, 0, 9223372036854775807);  slice_tensor_202 = None\\n        sym_size_60: Sym(s0) = torch.ops.aten.sym_size(arg41, 2)\\n        add_22: Sym(s0 + 1) = 1 + sym_size_60;  sym_size_60 = None\\n        slice_tensor_204: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_203, 2, 0, add_22);  slice_tensor_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant41 = self._tensor_constant41\\n        slice_tensor_205: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant41, 0, 0, 9223372036854775807);  _tensor_constant41 = None\\n        slice_tensor_206: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_205, 1, 0, 9223372036854775807);  slice_tensor_205 = None\\n        slice_tensor_207: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_206, 2, 0, add_22);  slice_tensor_206 = add_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_80: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_204, 1);  slice_tensor_204 = None\\n        squeeze_dim_81: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_80, 0);  squeeze_dim_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_82: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_207, 1);  slice_tensor_207 = None\\n        squeeze_dim_83: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_82, 0);  squeeze_dim_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_40: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_81, [view_default]);  squeeze_dim_81 = None\\n        unsqueeze_default_43: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_40, 1);  index_tensor_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_41: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_83, [view_default]);  squeeze_dim_83 = None\\n        unsqueeze_default_44: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_41, 1);  index_tensor_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_182: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_100, unsqueeze_default_43)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_208: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_209: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 64, 9223372036854775807);  transpose_int_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_40: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_209);  slice_tensor_209 = None\\n        cat_default_80: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_40, slice_tensor_208], -1);  neg_default_40 = slice_tensor_208 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_183: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_80, unsqueeze_default_44);  cat_default_80 = None\\n        add_tensor_141: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_182, mul_tensor_183);  mul_tensor_182 = mul_tensor_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_184: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_101, unsqueeze_default_43);  unsqueeze_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_210: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_211: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 64, 9223372036854775807);  transpose_int_101 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_41: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_211);  slice_tensor_211 = None\\n        cat_default_81: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_41, slice_tensor_210], -1);  neg_default_41 = slice_tensor_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_185: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_81, unsqueeze_default_44);  cat_default_81 = unsqueeze_default_44 = None\\n        add_tensor_142: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_184, mul_tensor_185);  mul_tensor_184 = mul_tensor_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_82: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg41, add_tensor_142], 2);  arg41 = add_tensor_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_83: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg42, transpose_int_102], 2);  arg42 = transpose_int_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_103: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_82, 2, 3)\\n        expand_default_81: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_141, [1, 32, 1, 128]);  add_tensor_141 = None\\n        view_default_490: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_81, [32, 1, 128]);  expand_default_81 = None\\n        sym_size_61: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_82, 2)\\n        expand_default_82: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_103, [1, 32, 128, sym_size_61]);  transpose_int_103 = None\\n        view_default_491: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_82, [32, 128, sym_size_61]);  expand_default_82 = None\\n        bmm_default_40: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_490, view_default_491);  view_default_490 = view_default_491 = None\\n        view_default_492: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_40, [1, 32, 1, sym_size_61]);  bmm_default_40 = None\\n        div_tensor_20: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_492, 11.313708498984761);  view_default_492 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_143: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_20, masked_fill_scalar);  div_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_20: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_143, -1, False);  add_tensor_143 = None\\n        detach_default_61: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_20)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_83: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_20, [1, 32, 1, sym_size_61]);  _softmax_default_20 = None\\n        view_default_493: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_83, [32, 1, sym_size_61]);  expand_default_83 = sym_size_61 = None\\n        sym_size_62: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_83, 2)\\n        expand_default_84: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_83, [1, 32, sym_size_62, 128])\\n        view_default_494: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_84, [32, sym_size_62, 128]);  expand_default_84 = sym_size_62 = None\\n        bmm_default_41: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_493, view_default_494);  view_default_493 = view_default_494 = None\\n        view_default_495: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_41, [1, 32, 1, 128]);  bmm_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_104: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_495, 1, 2);  view_default_495 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_496: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_104, [1, 1, 4096]);  transpose_int_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant185 = self._param_constant185\\n        t_default_143: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant185);  _param_constant185 = None\\n        view_default_497: f32[1, 4096] = torch.ops.aten.view.default(view_default_496, [1, 4096]);  view_default_496 = None\\n        mm_default_143: f32[1, 4096] = torch.ops.aten.mm.default(view_default_497, t_default_143);  view_default_497 = t_default_143 = None\\n        view_default_498: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_143, [1, 1, 4096]);  mm_default_143 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_144: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_139, view_default_498);  add_tensor_139 = view_default_498 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_41: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_144, 2)\\n        mean_dim_41: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_41, [-1], True);  pow_tensor_scalar_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_145: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_41, 1e-06);  mean_dim_41 = None\\n        rsqrt_default_41: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_145);  add_tensor_145 = None\\n        detach_default_62: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_41)\\n        mul_tensor_186: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_144, rsqrt_default_41);  rsqrt_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant186 = self._param_constant186\\n        mul_tensor_187: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant186, mul_tensor_186);  _param_constant186 = mul_tensor_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant187 = self._param_constant187\\n        t_default_144: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant187);  _param_constant187 = None\\n        view_default_499: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_187, [1, 4096])\\n        mm_default_144: f32[1, 11008] = torch.ops.aten.mm.default(view_default_499, t_default_144);  view_default_499 = t_default_144 = None\\n        view_default_500: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_144, [1, 1, 11008]);  mm_default_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_20: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_500);  view_default_500 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant188 = self._param_constant188\\n        t_default_145: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant188);  _param_constant188 = None\\n        view_default_501: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_187, [1, 4096]);  mul_tensor_187 = None\\n        mm_default_145: f32[1, 11008] = torch.ops.aten.mm.default(view_default_501, t_default_145);  view_default_501 = t_default_145 = None\\n        view_default_502: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_145, [1, 1, 11008]);  mm_default_145 = None\\n        mul_tensor_188: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_20, view_default_502);  silu_default_20 = view_default_502 = None\\n        _param_constant189 = self._param_constant189\\n        t_default_146: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant189);  _param_constant189 = None\\n        view_default_503: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_188, [1, 11008]);  mul_tensor_188 = None\\n        mm_default_146: f32[1, 4096] = torch.ops.aten.mm.default(view_default_503, t_default_146);  view_default_503 = t_default_146 = None\\n        view_default_504: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_146, [1, 1, 4096]);  mm_default_146 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_146: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_144, view_default_504);  add_tensor_144 = view_default_504 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_42: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_146, 2)\\n        mean_dim_42: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_42, [-1], True);  pow_tensor_scalar_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_147: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_42, 1e-06);  mean_dim_42 = None\\n        rsqrt_default_42: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_147);  add_tensor_147 = None\\n        detach_default_63: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_42)\\n        mul_tensor_189: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_146, rsqrt_default_42);  rsqrt_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant190 = self._param_constant190\\n        mul_tensor_190: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant190, mul_tensor_189);  _param_constant190 = mul_tensor_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant191 = self._param_constant191\\n        t_default_147: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant191);  _param_constant191 = None\\n        view_default_505: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_190, [1, 4096])\\n        mm_default_147: f32[1, 4096] = torch.ops.aten.mm.default(view_default_505, t_default_147);  view_default_505 = t_default_147 = None\\n        view_default_506: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_147, [1, 1, 4096]);  mm_default_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant192 = self._param_constant192\\n        t_default_148: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant192);  _param_constant192 = None\\n        view_default_507: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_190, [1, 4096])\\n        mm_default_148: f32[1, 4096] = torch.ops.aten.mm.default(view_default_507, t_default_148);  view_default_507 = t_default_148 = None\\n        view_default_508: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_148, [1, 1, 4096]);  mm_default_148 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant193 = self._param_constant193\\n        t_default_149: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant193);  _param_constant193 = None\\n        view_default_509: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_190, [1, 4096]);  mul_tensor_190 = None\\n        mm_default_149: f32[1, 4096] = torch.ops.aten.mm.default(view_default_509, t_default_149);  view_default_509 = t_default_149 = None\\n        view_default_510: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_149, [1, 1, 4096]);  mm_default_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_511: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_506, [1, 1, 32, 128]);  view_default_506 = None\\n        transpose_int_105: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_511, 1, 2);  view_default_511 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_512: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_508, [1, 1, 32, 128]);  view_default_508 = None\\n        transpose_int_106: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_512, 1, 2);  view_default_512 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_513: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_510, [1, 1, 32, 128]);  view_default_510 = None\\n        transpose_int_107: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_513, 1, 2);  view_default_513 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant42 = self._tensor_constant42\\n        slice_tensor_212: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant42, 0, 0, 9223372036854775807);  _tensor_constant42 = None\\n        slice_tensor_213: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_212, 1, 0, 9223372036854775807);  slice_tensor_212 = None\\n        sym_size_63: Sym(s0) = torch.ops.aten.sym_size(arg43, 2)\\n        add_23: Sym(s0 + 1) = 1 + sym_size_63;  sym_size_63 = None\\n        slice_tensor_214: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_213, 2, 0, add_23);  slice_tensor_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant43 = self._tensor_constant43\\n        slice_tensor_215: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant43, 0, 0, 9223372036854775807);  _tensor_constant43 = None\\n        slice_tensor_216: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_215, 1, 0, 9223372036854775807);  slice_tensor_215 = None\\n        slice_tensor_217: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_216, 2, 0, add_23);  slice_tensor_216 = add_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_84: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_214, 1);  slice_tensor_214 = None\\n        squeeze_dim_85: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_84, 0);  squeeze_dim_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_86: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_217, 1);  slice_tensor_217 = None\\n        squeeze_dim_87: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_86, 0);  squeeze_dim_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_42: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_85, [view_default]);  squeeze_dim_85 = None\\n        unsqueeze_default_45: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_42, 1);  index_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_43: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_87, [view_default]);  squeeze_dim_87 = None\\n        unsqueeze_default_46: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_43, 1);  index_tensor_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_191: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_105, unsqueeze_default_45)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_218: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_219: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 64, 9223372036854775807);  transpose_int_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_42: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_219);  slice_tensor_219 = None\\n        cat_default_84: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_42, slice_tensor_218], -1);  neg_default_42 = slice_tensor_218 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_192: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_84, unsqueeze_default_46);  cat_default_84 = None\\n        add_tensor_148: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_191, mul_tensor_192);  mul_tensor_191 = mul_tensor_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_193: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_106, unsqueeze_default_45);  unsqueeze_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_220: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_221: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 64, 9223372036854775807);  transpose_int_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_43: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_221);  slice_tensor_221 = None\\n        cat_default_85: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_43, slice_tensor_220], -1);  neg_default_43 = slice_tensor_220 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_194: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_85, unsqueeze_default_46);  cat_default_85 = unsqueeze_default_46 = None\\n        add_tensor_149: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_193, mul_tensor_194);  mul_tensor_193 = mul_tensor_194 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_86: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg43, add_tensor_149], 2);  arg43 = add_tensor_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_87: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg44, transpose_int_107], 2);  arg44 = transpose_int_107 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_108: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_86, 2, 3)\\n        expand_default_85: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_148, [1, 32, 1, 128]);  add_tensor_148 = None\\n        view_default_514: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_85, [32, 1, 128]);  expand_default_85 = None\\n        sym_size_64: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_86, 2)\\n        expand_default_86: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_108, [1, 32, 128, sym_size_64]);  transpose_int_108 = None\\n        view_default_515: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_86, [32, 128, sym_size_64]);  expand_default_86 = None\\n        bmm_default_42: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_514, view_default_515);  view_default_514 = view_default_515 = None\\n        view_default_516: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_42, [1, 32, 1, sym_size_64]);  bmm_default_42 = None\\n        div_tensor_21: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_516, 11.313708498984761);  view_default_516 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_150: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_21, masked_fill_scalar);  div_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_21: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_150, -1, False);  add_tensor_150 = None\\n        detach_default_64: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_87: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_21, [1, 32, 1, sym_size_64]);  _softmax_default_21 = None\\n        view_default_517: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_87, [32, 1, sym_size_64]);  expand_default_87 = sym_size_64 = None\\n        sym_size_65: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_87, 2)\\n        expand_default_88: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_87, [1, 32, sym_size_65, 128])\\n        view_default_518: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_88, [32, sym_size_65, 128]);  expand_default_88 = sym_size_65 = None\\n        bmm_default_43: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_517, view_default_518);  view_default_517 = view_default_518 = None\\n        view_default_519: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_43, [1, 32, 1, 128]);  bmm_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_109: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_519, 1, 2);  view_default_519 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_520: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_109, [1, 1, 4096]);  transpose_int_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant194 = self._param_constant194\\n        t_default_150: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant194);  _param_constant194 = None\\n        view_default_521: f32[1, 4096] = torch.ops.aten.view.default(view_default_520, [1, 4096]);  view_default_520 = None\\n        mm_default_150: f32[1, 4096] = torch.ops.aten.mm.default(view_default_521, t_default_150);  view_default_521 = t_default_150 = None\\n        view_default_522: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_150, [1, 1, 4096]);  mm_default_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_151: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_146, view_default_522);  add_tensor_146 = view_default_522 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_43: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_151, 2)\\n        mean_dim_43: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_43, [-1], True);  pow_tensor_scalar_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_152: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_43, 1e-06);  mean_dim_43 = None\\n        rsqrt_default_43: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_152);  add_tensor_152 = None\\n        detach_default_65: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_43)\\n        mul_tensor_195: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_151, rsqrt_default_43);  rsqrt_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant195 = self._param_constant195\\n        mul_tensor_196: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant195, mul_tensor_195);  _param_constant195 = mul_tensor_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant196 = self._param_constant196\\n        t_default_151: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant196);  _param_constant196 = None\\n        view_default_523: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_196, [1, 4096])\\n        mm_default_151: f32[1, 11008] = torch.ops.aten.mm.default(view_default_523, t_default_151);  view_default_523 = t_default_151 = None\\n        view_default_524: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_151, [1, 1, 11008]);  mm_default_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_21: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_524);  view_default_524 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant197 = self._param_constant197\\n        t_default_152: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant197);  _param_constant197 = None\\n        view_default_525: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_196, [1, 4096]);  mul_tensor_196 = None\\n        mm_default_152: f32[1, 11008] = torch.ops.aten.mm.default(view_default_525, t_default_152);  view_default_525 = t_default_152 = None\\n        view_default_526: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_152, [1, 1, 11008]);  mm_default_152 = None\\n        mul_tensor_197: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_21, view_default_526);  silu_default_21 = view_default_526 = None\\n        _param_constant198 = self._param_constant198\\n        t_default_153: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant198);  _param_constant198 = None\\n        view_default_527: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_197, [1, 11008]);  mul_tensor_197 = None\\n        mm_default_153: f32[1, 4096] = torch.ops.aten.mm.default(view_default_527, t_default_153);  view_default_527 = t_default_153 = None\\n        view_default_528: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_153, [1, 1, 4096]);  mm_default_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_153: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_151, view_default_528);  add_tensor_151 = view_default_528 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_44: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_153, 2)\\n        mean_dim_44: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_44, [-1], True);  pow_tensor_scalar_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_154: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_44, 1e-06);  mean_dim_44 = None\\n        rsqrt_default_44: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_154);  add_tensor_154 = None\\n        detach_default_66: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_44)\\n        mul_tensor_198: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_153, rsqrt_default_44);  rsqrt_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant199 = self._param_constant199\\n        mul_tensor_199: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant199, mul_tensor_198);  _param_constant199 = mul_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant200 = self._param_constant200\\n        t_default_154: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant200);  _param_constant200 = None\\n        view_default_529: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_199, [1, 4096])\\n        mm_default_154: f32[1, 4096] = torch.ops.aten.mm.default(view_default_529, t_default_154);  view_default_529 = t_default_154 = None\\n        view_default_530: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_154, [1, 1, 4096]);  mm_default_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant201 = self._param_constant201\\n        t_default_155: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant201);  _param_constant201 = None\\n        view_default_531: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_199, [1, 4096])\\n        mm_default_155: f32[1, 4096] = torch.ops.aten.mm.default(view_default_531, t_default_155);  view_default_531 = t_default_155 = None\\n        view_default_532: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_155, [1, 1, 4096]);  mm_default_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant202 = self._param_constant202\\n        t_default_156: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant202);  _param_constant202 = None\\n        view_default_533: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_199, [1, 4096]);  mul_tensor_199 = None\\n        mm_default_156: f32[1, 4096] = torch.ops.aten.mm.default(view_default_533, t_default_156);  view_default_533 = t_default_156 = None\\n        view_default_534: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_156, [1, 1, 4096]);  mm_default_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_535: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_530, [1, 1, 32, 128]);  view_default_530 = None\\n        transpose_int_110: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_535, 1, 2);  view_default_535 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_536: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_532, [1, 1, 32, 128]);  view_default_532 = None\\n        transpose_int_111: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_536, 1, 2);  view_default_536 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_537: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_534, [1, 1, 32, 128]);  view_default_534 = None\\n        transpose_int_112: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_537, 1, 2);  view_default_537 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant44 = self._tensor_constant44\\n        slice_tensor_222: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant44, 0, 0, 9223372036854775807);  _tensor_constant44 = None\\n        slice_tensor_223: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_222, 1, 0, 9223372036854775807);  slice_tensor_222 = None\\n        sym_size_66: Sym(s0) = torch.ops.aten.sym_size(arg45, 2)\\n        add_24: Sym(s0 + 1) = 1 + sym_size_66;  sym_size_66 = None\\n        slice_tensor_224: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_223, 2, 0, add_24);  slice_tensor_223 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant45 = self._tensor_constant45\\n        slice_tensor_225: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant45, 0, 0, 9223372036854775807);  _tensor_constant45 = None\\n        slice_tensor_226: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_225, 1, 0, 9223372036854775807);  slice_tensor_225 = None\\n        slice_tensor_227: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_226, 2, 0, add_24);  slice_tensor_226 = add_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_88: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_224, 1);  slice_tensor_224 = None\\n        squeeze_dim_89: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_88, 0);  squeeze_dim_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_90: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_227, 1);  slice_tensor_227 = None\\n        squeeze_dim_91: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_90, 0);  squeeze_dim_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_44: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_89, [view_default]);  squeeze_dim_89 = None\\n        unsqueeze_default_47: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_44, 1);  index_tensor_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_45: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_91, [view_default]);  squeeze_dim_91 = None\\n        unsqueeze_default_48: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_45, 1);  index_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_200: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_110, unsqueeze_default_47)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_228: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_229: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 64, 9223372036854775807);  transpose_int_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_44: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_229);  slice_tensor_229 = None\\n        cat_default_88: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_44, slice_tensor_228], -1);  neg_default_44 = slice_tensor_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_201: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_88, unsqueeze_default_48);  cat_default_88 = None\\n        add_tensor_155: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_200, mul_tensor_201);  mul_tensor_200 = mul_tensor_201 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_202: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_111, unsqueeze_default_47);  unsqueeze_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_230: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_231: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 64, 9223372036854775807);  transpose_int_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_45: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_231);  slice_tensor_231 = None\\n        cat_default_89: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_45, slice_tensor_230], -1);  neg_default_45 = slice_tensor_230 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_203: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_89, unsqueeze_default_48);  cat_default_89 = unsqueeze_default_48 = None\\n        add_tensor_156: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_202, mul_tensor_203);  mul_tensor_202 = mul_tensor_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_90: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg45, add_tensor_156], 2);  arg45 = add_tensor_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_91: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg46, transpose_int_112], 2);  arg46 = transpose_int_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_113: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_90, 2, 3)\\n        expand_default_89: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_155, [1, 32, 1, 128]);  add_tensor_155 = None\\n        view_default_538: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_89, [32, 1, 128]);  expand_default_89 = None\\n        sym_size_67: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_90, 2)\\n        expand_default_90: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_113, [1, 32, 128, sym_size_67]);  transpose_int_113 = None\\n        view_default_539: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_90, [32, 128, sym_size_67]);  expand_default_90 = None\\n        bmm_default_44: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_538, view_default_539);  view_default_538 = view_default_539 = None\\n        view_default_540: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_44, [1, 32, 1, sym_size_67]);  bmm_default_44 = None\\n        div_tensor_22: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_540, 11.313708498984761);  view_default_540 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_157: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_22, masked_fill_scalar);  div_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_22: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_157, -1, False);  add_tensor_157 = None\\n        detach_default_67: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_22)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_91: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_22, [1, 32, 1, sym_size_67]);  _softmax_default_22 = None\\n        view_default_541: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_91, [32, 1, sym_size_67]);  expand_default_91 = sym_size_67 = None\\n        sym_size_68: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_91, 2)\\n        expand_default_92: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_91, [1, 32, sym_size_68, 128])\\n        view_default_542: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_92, [32, sym_size_68, 128]);  expand_default_92 = sym_size_68 = None\\n        bmm_default_45: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_541, view_default_542);  view_default_541 = view_default_542 = None\\n        view_default_543: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_45, [1, 32, 1, 128]);  bmm_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_114: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_543, 1, 2);  view_default_543 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_544: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_114, [1, 1, 4096]);  transpose_int_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant203 = self._param_constant203\\n        t_default_157: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant203);  _param_constant203 = None\\n        view_default_545: f32[1, 4096] = torch.ops.aten.view.default(view_default_544, [1, 4096]);  view_default_544 = None\\n        mm_default_157: f32[1, 4096] = torch.ops.aten.mm.default(view_default_545, t_default_157);  view_default_545 = t_default_157 = None\\n        view_default_546: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_157, [1, 1, 4096]);  mm_default_157 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_158: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_153, view_default_546);  add_tensor_153 = view_default_546 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_45: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_158, 2)\\n        mean_dim_45: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_45, [-1], True);  pow_tensor_scalar_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_159: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_45, 1e-06);  mean_dim_45 = None\\n        rsqrt_default_45: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_159);  add_tensor_159 = None\\n        detach_default_68: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_45)\\n        mul_tensor_204: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_158, rsqrt_default_45);  rsqrt_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant204 = self._param_constant204\\n        mul_tensor_205: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant204, mul_tensor_204);  _param_constant204 = mul_tensor_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant205 = self._param_constant205\\n        t_default_158: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant205);  _param_constant205 = None\\n        view_default_547: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_205, [1, 4096])\\n        mm_default_158: f32[1, 11008] = torch.ops.aten.mm.default(view_default_547, t_default_158);  view_default_547 = t_default_158 = None\\n        view_default_548: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_158, [1, 1, 11008]);  mm_default_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_22: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_548);  view_default_548 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant206 = self._param_constant206\\n        t_default_159: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant206);  _param_constant206 = None\\n        view_default_549: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_205, [1, 4096]);  mul_tensor_205 = None\\n        mm_default_159: f32[1, 11008] = torch.ops.aten.mm.default(view_default_549, t_default_159);  view_default_549 = t_default_159 = None\\n        view_default_550: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_159, [1, 1, 11008]);  mm_default_159 = None\\n        mul_tensor_206: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_22, view_default_550);  silu_default_22 = view_default_550 = None\\n        _param_constant207 = self._param_constant207\\n        t_default_160: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant207);  _param_constant207 = None\\n        view_default_551: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_206, [1, 11008]);  mul_tensor_206 = None\\n        mm_default_160: f32[1, 4096] = torch.ops.aten.mm.default(view_default_551, t_default_160);  view_default_551 = t_default_160 = None\\n        view_default_552: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_160, [1, 1, 4096]);  mm_default_160 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_160: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_158, view_default_552);  add_tensor_158 = view_default_552 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_46: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_160, 2)\\n        mean_dim_46: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_46, [-1], True);  pow_tensor_scalar_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_161: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_46, 1e-06);  mean_dim_46 = None\\n        rsqrt_default_46: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_161);  add_tensor_161 = None\\n        detach_default_69: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_46)\\n        mul_tensor_207: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_160, rsqrt_default_46);  rsqrt_default_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant208 = self._param_constant208\\n        mul_tensor_208: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant208, mul_tensor_207);  _param_constant208 = mul_tensor_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant209 = self._param_constant209\\n        t_default_161: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant209);  _param_constant209 = None\\n        view_default_553: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_208, [1, 4096])\\n        mm_default_161: f32[1, 4096] = torch.ops.aten.mm.default(view_default_553, t_default_161);  view_default_553 = t_default_161 = None\\n        view_default_554: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_161, [1, 1, 4096]);  mm_default_161 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant210 = self._param_constant210\\n        t_default_162: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant210);  _param_constant210 = None\\n        view_default_555: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_208, [1, 4096])\\n        mm_default_162: f32[1, 4096] = torch.ops.aten.mm.default(view_default_555, t_default_162);  view_default_555 = t_default_162 = None\\n        view_default_556: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_162, [1, 1, 4096]);  mm_default_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant211 = self._param_constant211\\n        t_default_163: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant211);  _param_constant211 = None\\n        view_default_557: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_208, [1, 4096]);  mul_tensor_208 = None\\n        mm_default_163: f32[1, 4096] = torch.ops.aten.mm.default(view_default_557, t_default_163);  view_default_557 = t_default_163 = None\\n        view_default_558: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_163, [1, 1, 4096]);  mm_default_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_559: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_554, [1, 1, 32, 128]);  view_default_554 = None\\n        transpose_int_115: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_559, 1, 2);  view_default_559 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_560: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_556, [1, 1, 32, 128]);  view_default_556 = None\\n        transpose_int_116: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_560, 1, 2);  view_default_560 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_561: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_558, [1, 1, 32, 128]);  view_default_558 = None\\n        transpose_int_117: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_561, 1, 2);  view_default_561 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant46 = self._tensor_constant46\\n        slice_tensor_232: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant46, 0, 0, 9223372036854775807);  _tensor_constant46 = None\\n        slice_tensor_233: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_232, 1, 0, 9223372036854775807);  slice_tensor_232 = None\\n        sym_size_69: Sym(s0) = torch.ops.aten.sym_size(arg47, 2)\\n        add_25: Sym(s0 + 1) = 1 + sym_size_69;  sym_size_69 = None\\n        slice_tensor_234: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_233, 2, 0, add_25);  slice_tensor_233 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant47 = self._tensor_constant47\\n        slice_tensor_235: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant47, 0, 0, 9223372036854775807);  _tensor_constant47 = None\\n        slice_tensor_236: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_235, 1, 0, 9223372036854775807);  slice_tensor_235 = None\\n        slice_tensor_237: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_236, 2, 0, add_25);  slice_tensor_236 = add_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_92: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_234, 1);  slice_tensor_234 = None\\n        squeeze_dim_93: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_92, 0);  squeeze_dim_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_94: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_237, 1);  slice_tensor_237 = None\\n        squeeze_dim_95: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_94, 0);  squeeze_dim_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_46: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_93, [view_default]);  squeeze_dim_93 = None\\n        unsqueeze_default_49: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_46, 1);  index_tensor_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_47: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_95, [view_default]);  squeeze_dim_95 = None\\n        unsqueeze_default_50: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_47, 1);  index_tensor_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_209: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_115, unsqueeze_default_49)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_238: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_239: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 64, 9223372036854775807);  transpose_int_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_46: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_239);  slice_tensor_239 = None\\n        cat_default_92: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_46, slice_tensor_238], -1);  neg_default_46 = slice_tensor_238 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_210: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_92, unsqueeze_default_50);  cat_default_92 = None\\n        add_tensor_162: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_209, mul_tensor_210);  mul_tensor_209 = mul_tensor_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_211: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_116, unsqueeze_default_49);  unsqueeze_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_240: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_241: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 64, 9223372036854775807);  transpose_int_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_47: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_241);  slice_tensor_241 = None\\n        cat_default_93: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_47, slice_tensor_240], -1);  neg_default_47 = slice_tensor_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_212: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_93, unsqueeze_default_50);  cat_default_93 = unsqueeze_default_50 = None\\n        add_tensor_163: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_211, mul_tensor_212);  mul_tensor_211 = mul_tensor_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_94: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg47, add_tensor_163], 2);  arg47 = add_tensor_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_95: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg48, transpose_int_117], 2);  arg48 = transpose_int_117 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_118: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_94, 2, 3)\\n        expand_default_93: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_162, [1, 32, 1, 128]);  add_tensor_162 = None\\n        view_default_562: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_93, [32, 1, 128]);  expand_default_93 = None\\n        sym_size_70: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_94, 2)\\n        expand_default_94: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_118, [1, 32, 128, sym_size_70]);  transpose_int_118 = None\\n        view_default_563: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_94, [32, 128, sym_size_70]);  expand_default_94 = None\\n        bmm_default_46: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_562, view_default_563);  view_default_562 = view_default_563 = None\\n        view_default_564: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_46, [1, 32, 1, sym_size_70]);  bmm_default_46 = None\\n        div_tensor_23: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_564, 11.313708498984761);  view_default_564 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_164: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_23, masked_fill_scalar);  div_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_23: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_164, -1, False);  add_tensor_164 = None\\n        detach_default_70: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_23)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_95: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_23, [1, 32, 1, sym_size_70]);  _softmax_default_23 = None\\n        view_default_565: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_95, [32, 1, sym_size_70]);  expand_default_95 = sym_size_70 = None\\n        sym_size_71: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_95, 2)\\n        expand_default_96: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_95, [1, 32, sym_size_71, 128])\\n        view_default_566: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_96, [32, sym_size_71, 128]);  expand_default_96 = sym_size_71 = None\\n        bmm_default_47: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_565, view_default_566);  view_default_565 = view_default_566 = None\\n        view_default_567: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_47, [1, 32, 1, 128]);  bmm_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_119: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_567, 1, 2);  view_default_567 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_568: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_119, [1, 1, 4096]);  transpose_int_119 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant212 = self._param_constant212\\n        t_default_164: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant212);  _param_constant212 = None\\n        view_default_569: f32[1, 4096] = torch.ops.aten.view.default(view_default_568, [1, 4096]);  view_default_568 = None\\n        mm_default_164: f32[1, 4096] = torch.ops.aten.mm.default(view_default_569, t_default_164);  view_default_569 = t_default_164 = None\\n        view_default_570: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_164, [1, 1, 4096]);  mm_default_164 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_165: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_160, view_default_570);  add_tensor_160 = view_default_570 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_47: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_165, 2)\\n        mean_dim_47: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_47, [-1], True);  pow_tensor_scalar_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_166: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_47, 1e-06);  mean_dim_47 = None\\n        rsqrt_default_47: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_166);  add_tensor_166 = None\\n        detach_default_71: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_47)\\n        mul_tensor_213: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_165, rsqrt_default_47);  rsqrt_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant213 = self._param_constant213\\n        mul_tensor_214: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant213, mul_tensor_213);  _param_constant213 = mul_tensor_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant214 = self._param_constant214\\n        t_default_165: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant214);  _param_constant214 = None\\n        view_default_571: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_214, [1, 4096])\\n        mm_default_165: f32[1, 11008] = torch.ops.aten.mm.default(view_default_571, t_default_165);  view_default_571 = t_default_165 = None\\n        view_default_572: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_165, [1, 1, 11008]);  mm_default_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_23: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_572);  view_default_572 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant215 = self._param_constant215\\n        t_default_166: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant215);  _param_constant215 = None\\n        view_default_573: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_214, [1, 4096]);  mul_tensor_214 = None\\n        mm_default_166: f32[1, 11008] = torch.ops.aten.mm.default(view_default_573, t_default_166);  view_default_573 = t_default_166 = None\\n        view_default_574: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_166, [1, 1, 11008]);  mm_default_166 = None\\n        mul_tensor_215: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_23, view_default_574);  silu_default_23 = view_default_574 = None\\n        _param_constant216 = self._param_constant216\\n        t_default_167: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant216);  _param_constant216 = None\\n        view_default_575: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_215, [1, 11008]);  mul_tensor_215 = None\\n        mm_default_167: f32[1, 4096] = torch.ops.aten.mm.default(view_default_575, t_default_167);  view_default_575 = t_default_167 = None\\n        view_default_576: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_167, [1, 1, 4096]);  mm_default_167 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_167: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_165, view_default_576);  add_tensor_165 = view_default_576 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_48: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_167, 2)\\n        mean_dim_48: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_48, [-1], True);  pow_tensor_scalar_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_168: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_48, 1e-06);  mean_dim_48 = None\\n        rsqrt_default_48: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_168);  add_tensor_168 = None\\n        detach_default_72: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_48)\\n        mul_tensor_216: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_167, rsqrt_default_48);  rsqrt_default_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant217 = self._param_constant217\\n        mul_tensor_217: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant217, mul_tensor_216);  _param_constant217 = mul_tensor_216 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant218 = self._param_constant218\\n        t_default_168: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant218);  _param_constant218 = None\\n        view_default_577: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_217, [1, 4096])\\n        mm_default_168: f32[1, 4096] = torch.ops.aten.mm.default(view_default_577, t_default_168);  view_default_577 = t_default_168 = None\\n        view_default_578: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_168, [1, 1, 4096]);  mm_default_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant219 = self._param_constant219\\n        t_default_169: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant219);  _param_constant219 = None\\n        view_default_579: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_217, [1, 4096])\\n        mm_default_169: f32[1, 4096] = torch.ops.aten.mm.default(view_default_579, t_default_169);  view_default_579 = t_default_169 = None\\n        view_default_580: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_169, [1, 1, 4096]);  mm_default_169 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant220 = self._param_constant220\\n        t_default_170: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant220);  _param_constant220 = None\\n        view_default_581: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_217, [1, 4096]);  mul_tensor_217 = None\\n        mm_default_170: f32[1, 4096] = torch.ops.aten.mm.default(view_default_581, t_default_170);  view_default_581 = t_default_170 = None\\n        view_default_582: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_170, [1, 1, 4096]);  mm_default_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_583: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_578, [1, 1, 32, 128]);  view_default_578 = None\\n        transpose_int_120: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_583, 1, 2);  view_default_583 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_584: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_580, [1, 1, 32, 128]);  view_default_580 = None\\n        transpose_int_121: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_584, 1, 2);  view_default_584 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_585: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_582, [1, 1, 32, 128]);  view_default_582 = None\\n        transpose_int_122: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_585, 1, 2);  view_default_585 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant48 = self._tensor_constant48\\n        slice_tensor_242: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant48, 0, 0, 9223372036854775807);  _tensor_constant48 = None\\n        slice_tensor_243: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_242, 1, 0, 9223372036854775807);  slice_tensor_242 = None\\n        sym_size_72: Sym(s0) = torch.ops.aten.sym_size(arg49, 2)\\n        add_26: Sym(s0 + 1) = 1 + sym_size_72;  sym_size_72 = None\\n        slice_tensor_244: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_243, 2, 0, add_26);  slice_tensor_243 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant49 = self._tensor_constant49\\n        slice_tensor_245: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant49, 0, 0, 9223372036854775807);  _tensor_constant49 = None\\n        slice_tensor_246: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_245, 1, 0, 9223372036854775807);  slice_tensor_245 = None\\n        slice_tensor_247: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_246, 2, 0, add_26);  slice_tensor_246 = add_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_96: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_244, 1);  slice_tensor_244 = None\\n        squeeze_dim_97: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_96, 0);  squeeze_dim_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_98: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_247, 1);  slice_tensor_247 = None\\n        squeeze_dim_99: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_98, 0);  squeeze_dim_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_48: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_97, [view_default]);  squeeze_dim_97 = None\\n        unsqueeze_default_51: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_48, 1);  index_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_49: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_99, [view_default]);  squeeze_dim_99 = None\\n        unsqueeze_default_52: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_49, 1);  index_tensor_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_218: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_120, unsqueeze_default_51)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_248: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_249: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 64, 9223372036854775807);  transpose_int_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_48: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_249);  slice_tensor_249 = None\\n        cat_default_96: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_48, slice_tensor_248], -1);  neg_default_48 = slice_tensor_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_219: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_96, unsqueeze_default_52);  cat_default_96 = None\\n        add_tensor_169: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_218, mul_tensor_219);  mul_tensor_218 = mul_tensor_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_220: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_121, unsqueeze_default_51);  unsqueeze_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_250: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_251: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 64, 9223372036854775807);  transpose_int_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_49: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_251);  slice_tensor_251 = None\\n        cat_default_97: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_49, slice_tensor_250], -1);  neg_default_49 = slice_tensor_250 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_221: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_97, unsqueeze_default_52);  cat_default_97 = unsqueeze_default_52 = None\\n        add_tensor_170: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_220, mul_tensor_221);  mul_tensor_220 = mul_tensor_221 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_98: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg49, add_tensor_170], 2);  arg49 = add_tensor_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_99: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg50, transpose_int_122], 2);  arg50 = transpose_int_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_123: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_98, 2, 3)\\n        expand_default_97: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_169, [1, 32, 1, 128]);  add_tensor_169 = None\\n        view_default_586: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_97, [32, 1, 128]);  expand_default_97 = None\\n        sym_size_73: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_98, 2)\\n        expand_default_98: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_123, [1, 32, 128, sym_size_73]);  transpose_int_123 = None\\n        view_default_587: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_98, [32, 128, sym_size_73]);  expand_default_98 = None\\n        bmm_default_48: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_586, view_default_587);  view_default_586 = view_default_587 = None\\n        view_default_588: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_48, [1, 32, 1, sym_size_73]);  bmm_default_48 = None\\n        div_tensor_24: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_588, 11.313708498984761);  view_default_588 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_171: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_24, masked_fill_scalar);  div_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_24: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_171, -1, False);  add_tensor_171 = None\\n        detach_default_73: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_24)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_99: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_24, [1, 32, 1, sym_size_73]);  _softmax_default_24 = None\\n        view_default_589: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_99, [32, 1, sym_size_73]);  expand_default_99 = sym_size_73 = None\\n        sym_size_74: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_99, 2)\\n        expand_default_100: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_99, [1, 32, sym_size_74, 128])\\n        view_default_590: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_100, [32, sym_size_74, 128]);  expand_default_100 = sym_size_74 = None\\n        bmm_default_49: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_589, view_default_590);  view_default_589 = view_default_590 = None\\n        view_default_591: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_49, [1, 32, 1, 128]);  bmm_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_124: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_591, 1, 2);  view_default_591 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_592: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_124, [1, 1, 4096]);  transpose_int_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant221 = self._param_constant221\\n        t_default_171: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant221);  _param_constant221 = None\\n        view_default_593: f32[1, 4096] = torch.ops.aten.view.default(view_default_592, [1, 4096]);  view_default_592 = None\\n        mm_default_171: f32[1, 4096] = torch.ops.aten.mm.default(view_default_593, t_default_171);  view_default_593 = t_default_171 = None\\n        view_default_594: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_171, [1, 1, 4096]);  mm_default_171 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_172: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_167, view_default_594);  add_tensor_167 = view_default_594 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_49: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_172, 2)\\n        mean_dim_49: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_49, [-1], True);  pow_tensor_scalar_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_173: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_49, 1e-06);  mean_dim_49 = None\\n        rsqrt_default_49: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_173);  add_tensor_173 = None\\n        detach_default_74: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_49)\\n        mul_tensor_222: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_172, rsqrt_default_49);  rsqrt_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant222 = self._param_constant222\\n        mul_tensor_223: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant222, mul_tensor_222);  _param_constant222 = mul_tensor_222 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant223 = self._param_constant223\\n        t_default_172: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant223);  _param_constant223 = None\\n        view_default_595: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_223, [1, 4096])\\n        mm_default_172: f32[1, 11008] = torch.ops.aten.mm.default(view_default_595, t_default_172);  view_default_595 = t_default_172 = None\\n        view_default_596: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_172, [1, 1, 11008]);  mm_default_172 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_24: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_596);  view_default_596 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant224 = self._param_constant224\\n        t_default_173: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant224);  _param_constant224 = None\\n        view_default_597: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_223, [1, 4096]);  mul_tensor_223 = None\\n        mm_default_173: f32[1, 11008] = torch.ops.aten.mm.default(view_default_597, t_default_173);  view_default_597 = t_default_173 = None\\n        view_default_598: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_173, [1, 1, 11008]);  mm_default_173 = None\\n        mul_tensor_224: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_24, view_default_598);  silu_default_24 = view_default_598 = None\\n        _param_constant225 = self._param_constant225\\n        t_default_174: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant225);  _param_constant225 = None\\n        view_default_599: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_224, [1, 11008]);  mul_tensor_224 = None\\n        mm_default_174: f32[1, 4096] = torch.ops.aten.mm.default(view_default_599, t_default_174);  view_default_599 = t_default_174 = None\\n        view_default_600: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_174, [1, 1, 4096]);  mm_default_174 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_174: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_172, view_default_600);  add_tensor_172 = view_default_600 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_50: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_174, 2)\\n        mean_dim_50: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_50, [-1], True);  pow_tensor_scalar_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_175: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_50, 1e-06);  mean_dim_50 = None\\n        rsqrt_default_50: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_175);  add_tensor_175 = None\\n        detach_default_75: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_50)\\n        mul_tensor_225: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_174, rsqrt_default_50);  rsqrt_default_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant226 = self._param_constant226\\n        mul_tensor_226: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant226, mul_tensor_225);  _param_constant226 = mul_tensor_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant227 = self._param_constant227\\n        t_default_175: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant227);  _param_constant227 = None\\n        view_default_601: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_226, [1, 4096])\\n        mm_default_175: f32[1, 4096] = torch.ops.aten.mm.default(view_default_601, t_default_175);  view_default_601 = t_default_175 = None\\n        view_default_602: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_175, [1, 1, 4096]);  mm_default_175 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant228 = self._param_constant228\\n        t_default_176: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant228);  _param_constant228 = None\\n        view_default_603: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_226, [1, 4096])\\n        mm_default_176: f32[1, 4096] = torch.ops.aten.mm.default(view_default_603, t_default_176);  view_default_603 = t_default_176 = None\\n        view_default_604: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_176, [1, 1, 4096]);  mm_default_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant229 = self._param_constant229\\n        t_default_177: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant229);  _param_constant229 = None\\n        view_default_605: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_226, [1, 4096]);  mul_tensor_226 = None\\n        mm_default_177: f32[1, 4096] = torch.ops.aten.mm.default(view_default_605, t_default_177);  view_default_605 = t_default_177 = None\\n        view_default_606: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_177, [1, 1, 4096]);  mm_default_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_607: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_602, [1, 1, 32, 128]);  view_default_602 = None\\n        transpose_int_125: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_607, 1, 2);  view_default_607 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_608: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_604, [1, 1, 32, 128]);  view_default_604 = None\\n        transpose_int_126: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_608, 1, 2);  view_default_608 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_609: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_606, [1, 1, 32, 128]);  view_default_606 = None\\n        transpose_int_127: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_609, 1, 2);  view_default_609 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant50 = self._tensor_constant50\\n        slice_tensor_252: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant50, 0, 0, 9223372036854775807);  _tensor_constant50 = None\\n        slice_tensor_253: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_252, 1, 0, 9223372036854775807);  slice_tensor_252 = None\\n        sym_size_75: Sym(s0) = torch.ops.aten.sym_size(arg51, 2)\\n        add_27: Sym(s0 + 1) = 1 + sym_size_75;  sym_size_75 = None\\n        slice_tensor_254: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_253, 2, 0, add_27);  slice_tensor_253 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant51 = self._tensor_constant51\\n        slice_tensor_255: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant51, 0, 0, 9223372036854775807);  _tensor_constant51 = None\\n        slice_tensor_256: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_255, 1, 0, 9223372036854775807);  slice_tensor_255 = None\\n        slice_tensor_257: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_256, 2, 0, add_27);  slice_tensor_256 = add_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_100: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_254, 1);  slice_tensor_254 = None\\n        squeeze_dim_101: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_100, 0);  squeeze_dim_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_102: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_257, 1);  slice_tensor_257 = None\\n        squeeze_dim_103: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_102, 0);  squeeze_dim_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_50: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_101, [view_default]);  squeeze_dim_101 = None\\n        unsqueeze_default_53: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_50, 1);  index_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_51: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_103, [view_default]);  squeeze_dim_103 = None\\n        unsqueeze_default_54: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_51, 1);  index_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_227: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_125, unsqueeze_default_53)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_258: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_259: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 64, 9223372036854775807);  transpose_int_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_50: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_259);  slice_tensor_259 = None\\n        cat_default_100: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_50, slice_tensor_258], -1);  neg_default_50 = slice_tensor_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_228: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_100, unsqueeze_default_54);  cat_default_100 = None\\n        add_tensor_176: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_227, mul_tensor_228);  mul_tensor_227 = mul_tensor_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_229: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_126, unsqueeze_default_53);  unsqueeze_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_260: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_261: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 64, 9223372036854775807);  transpose_int_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_51: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_261);  slice_tensor_261 = None\\n        cat_default_101: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_51, slice_tensor_260], -1);  neg_default_51 = slice_tensor_260 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_230: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_101, unsqueeze_default_54);  cat_default_101 = unsqueeze_default_54 = None\\n        add_tensor_177: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_229, mul_tensor_230);  mul_tensor_229 = mul_tensor_230 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_102: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg51, add_tensor_177], 2);  arg51 = add_tensor_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_103: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg52, transpose_int_127], 2);  arg52 = transpose_int_127 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_128: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_102, 2, 3)\\n        expand_default_101: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_176, [1, 32, 1, 128]);  add_tensor_176 = None\\n        view_default_610: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_101, [32, 1, 128]);  expand_default_101 = None\\n        sym_size_76: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_102, 2)\\n        expand_default_102: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_128, [1, 32, 128, sym_size_76]);  transpose_int_128 = None\\n        view_default_611: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_102, [32, 128, sym_size_76]);  expand_default_102 = None\\n        bmm_default_50: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_610, view_default_611);  view_default_610 = view_default_611 = None\\n        view_default_612: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_50, [1, 32, 1, sym_size_76]);  bmm_default_50 = None\\n        div_tensor_25: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_612, 11.313708498984761);  view_default_612 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_178: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_25, masked_fill_scalar);  div_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_25: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_178, -1, False);  add_tensor_178 = None\\n        detach_default_76: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_25)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_103: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_25, [1, 32, 1, sym_size_76]);  _softmax_default_25 = None\\n        view_default_613: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_103, [32, 1, sym_size_76]);  expand_default_103 = sym_size_76 = None\\n        sym_size_77: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_103, 2)\\n        expand_default_104: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_103, [1, 32, sym_size_77, 128])\\n        view_default_614: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_104, [32, sym_size_77, 128]);  expand_default_104 = sym_size_77 = None\\n        bmm_default_51: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_613, view_default_614);  view_default_613 = view_default_614 = None\\n        view_default_615: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_51, [1, 32, 1, 128]);  bmm_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_129: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_615, 1, 2);  view_default_615 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_616: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_129, [1, 1, 4096]);  transpose_int_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant230 = self._param_constant230\\n        t_default_178: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant230);  _param_constant230 = None\\n        view_default_617: f32[1, 4096] = torch.ops.aten.view.default(view_default_616, [1, 4096]);  view_default_616 = None\\n        mm_default_178: f32[1, 4096] = torch.ops.aten.mm.default(view_default_617, t_default_178);  view_default_617 = t_default_178 = None\\n        view_default_618: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_178, [1, 1, 4096]);  mm_default_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_179: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_174, view_default_618);  add_tensor_174 = view_default_618 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_51: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_179, 2)\\n        mean_dim_51: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_51, [-1], True);  pow_tensor_scalar_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_180: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_51, 1e-06);  mean_dim_51 = None\\n        rsqrt_default_51: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_180);  add_tensor_180 = None\\n        detach_default_77: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_51)\\n        mul_tensor_231: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_179, rsqrt_default_51);  rsqrt_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant231 = self._param_constant231\\n        mul_tensor_232: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant231, mul_tensor_231);  _param_constant231 = mul_tensor_231 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant232 = self._param_constant232\\n        t_default_179: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant232);  _param_constant232 = None\\n        view_default_619: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_232, [1, 4096])\\n        mm_default_179: f32[1, 11008] = torch.ops.aten.mm.default(view_default_619, t_default_179);  view_default_619 = t_default_179 = None\\n        view_default_620: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_179, [1, 1, 11008]);  mm_default_179 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_25: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_620);  view_default_620 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant233 = self._param_constant233\\n        t_default_180: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant233);  _param_constant233 = None\\n        view_default_621: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_232, [1, 4096]);  mul_tensor_232 = None\\n        mm_default_180: f32[1, 11008] = torch.ops.aten.mm.default(view_default_621, t_default_180);  view_default_621 = t_default_180 = None\\n        view_default_622: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_180, [1, 1, 11008]);  mm_default_180 = None\\n        mul_tensor_233: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_25, view_default_622);  silu_default_25 = view_default_622 = None\\n        _param_constant234 = self._param_constant234\\n        t_default_181: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant234);  _param_constant234 = None\\n        view_default_623: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_233, [1, 11008]);  mul_tensor_233 = None\\n        mm_default_181: f32[1, 4096] = torch.ops.aten.mm.default(view_default_623, t_default_181);  view_default_623 = t_default_181 = None\\n        view_default_624: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_181, [1, 1, 4096]);  mm_default_181 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_181: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_179, view_default_624);  add_tensor_179 = view_default_624 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_52: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_181, 2)\\n        mean_dim_52: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_52, [-1], True);  pow_tensor_scalar_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_182: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_52, 1e-06);  mean_dim_52 = None\\n        rsqrt_default_52: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_182);  add_tensor_182 = None\\n        detach_default_78: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_52)\\n        mul_tensor_234: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_181, rsqrt_default_52);  rsqrt_default_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant235 = self._param_constant235\\n        mul_tensor_235: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant235, mul_tensor_234);  _param_constant235 = mul_tensor_234 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant236 = self._param_constant236\\n        t_default_182: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant236);  _param_constant236 = None\\n        view_default_625: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_235, [1, 4096])\\n        mm_default_182: f32[1, 4096] = torch.ops.aten.mm.default(view_default_625, t_default_182);  view_default_625 = t_default_182 = None\\n        view_default_626: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_182, [1, 1, 4096]);  mm_default_182 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant237 = self._param_constant237\\n        t_default_183: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant237);  _param_constant237 = None\\n        view_default_627: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_235, [1, 4096])\\n        mm_default_183: f32[1, 4096] = torch.ops.aten.mm.default(view_default_627, t_default_183);  view_default_627 = t_default_183 = None\\n        view_default_628: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_183, [1, 1, 4096]);  mm_default_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant238 = self._param_constant238\\n        t_default_184: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant238);  _param_constant238 = None\\n        view_default_629: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_235, [1, 4096]);  mul_tensor_235 = None\\n        mm_default_184: f32[1, 4096] = torch.ops.aten.mm.default(view_default_629, t_default_184);  view_default_629 = t_default_184 = None\\n        view_default_630: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_184, [1, 1, 4096]);  mm_default_184 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_631: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_626, [1, 1, 32, 128]);  view_default_626 = None\\n        transpose_int_130: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_631, 1, 2);  view_default_631 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_632: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_628, [1, 1, 32, 128]);  view_default_628 = None\\n        transpose_int_131: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_632, 1, 2);  view_default_632 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_633: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_630, [1, 1, 32, 128]);  view_default_630 = None\\n        transpose_int_132: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_633, 1, 2);  view_default_633 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant52 = self._tensor_constant52\\n        slice_tensor_262: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant52, 0, 0, 9223372036854775807);  _tensor_constant52 = None\\n        slice_tensor_263: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_262, 1, 0, 9223372036854775807);  slice_tensor_262 = None\\n        sym_size_78: Sym(s0) = torch.ops.aten.sym_size(arg53, 2)\\n        add_28: Sym(s0 + 1) = 1 + sym_size_78;  sym_size_78 = None\\n        slice_tensor_264: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_263, 2, 0, add_28);  slice_tensor_263 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant53 = self._tensor_constant53\\n        slice_tensor_265: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant53, 0, 0, 9223372036854775807);  _tensor_constant53 = None\\n        slice_tensor_266: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_265, 1, 0, 9223372036854775807);  slice_tensor_265 = None\\n        slice_tensor_267: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_266, 2, 0, add_28);  slice_tensor_266 = add_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_104: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_264, 1);  slice_tensor_264 = None\\n        squeeze_dim_105: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_104, 0);  squeeze_dim_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_106: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_267, 1);  slice_tensor_267 = None\\n        squeeze_dim_107: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_106, 0);  squeeze_dim_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_52: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_105, [view_default]);  squeeze_dim_105 = None\\n        unsqueeze_default_55: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_52, 1);  index_tensor_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_53: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_107, [view_default]);  squeeze_dim_107 = None\\n        unsqueeze_default_56: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_53, 1);  index_tensor_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_236: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_130, unsqueeze_default_55)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_268: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_269: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 64, 9223372036854775807);  transpose_int_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_52: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_269);  slice_tensor_269 = None\\n        cat_default_104: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_52, slice_tensor_268], -1);  neg_default_52 = slice_tensor_268 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_237: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_104, unsqueeze_default_56);  cat_default_104 = None\\n        add_tensor_183: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_236, mul_tensor_237);  mul_tensor_236 = mul_tensor_237 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_238: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_131, unsqueeze_default_55);  unsqueeze_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_270: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_271: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 64, 9223372036854775807);  transpose_int_131 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_53: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_271);  slice_tensor_271 = None\\n        cat_default_105: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_53, slice_tensor_270], -1);  neg_default_53 = slice_tensor_270 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_239: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_105, unsqueeze_default_56);  cat_default_105 = unsqueeze_default_56 = None\\n        add_tensor_184: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_238, mul_tensor_239);  mul_tensor_238 = mul_tensor_239 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_106: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg53, add_tensor_184], 2);  arg53 = add_tensor_184 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_107: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg54, transpose_int_132], 2);  arg54 = transpose_int_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_133: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_106, 2, 3)\\n        expand_default_105: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_183, [1, 32, 1, 128]);  add_tensor_183 = None\\n        view_default_634: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_105, [32, 1, 128]);  expand_default_105 = None\\n        sym_size_79: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_106, 2)\\n        expand_default_106: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_133, [1, 32, 128, sym_size_79]);  transpose_int_133 = None\\n        view_default_635: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_106, [32, 128, sym_size_79]);  expand_default_106 = None\\n        bmm_default_52: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_634, view_default_635);  view_default_634 = view_default_635 = None\\n        view_default_636: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_52, [1, 32, 1, sym_size_79]);  bmm_default_52 = None\\n        div_tensor_26: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_636, 11.313708498984761);  view_default_636 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_185: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_26, masked_fill_scalar);  div_tensor_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_26: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_185, -1, False);  add_tensor_185 = None\\n        detach_default_79: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_26)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_107: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_26, [1, 32, 1, sym_size_79]);  _softmax_default_26 = None\\n        view_default_637: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_107, [32, 1, sym_size_79]);  expand_default_107 = sym_size_79 = None\\n        sym_size_80: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_107, 2)\\n        expand_default_108: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_107, [1, 32, sym_size_80, 128])\\n        view_default_638: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_108, [32, sym_size_80, 128]);  expand_default_108 = sym_size_80 = None\\n        bmm_default_53: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_637, view_default_638);  view_default_637 = view_default_638 = None\\n        view_default_639: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_53, [1, 32, 1, 128]);  bmm_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_134: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_639, 1, 2);  view_default_639 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_640: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_134, [1, 1, 4096]);  transpose_int_134 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant239 = self._param_constant239\\n        t_default_185: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant239);  _param_constant239 = None\\n        view_default_641: f32[1, 4096] = torch.ops.aten.view.default(view_default_640, [1, 4096]);  view_default_640 = None\\n        mm_default_185: f32[1, 4096] = torch.ops.aten.mm.default(view_default_641, t_default_185);  view_default_641 = t_default_185 = None\\n        view_default_642: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_185, [1, 1, 4096]);  mm_default_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_186: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_181, view_default_642);  add_tensor_181 = view_default_642 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_53: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_186, 2)\\n        mean_dim_53: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_53, [-1], True);  pow_tensor_scalar_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_187: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_53, 1e-06);  mean_dim_53 = None\\n        rsqrt_default_53: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_187);  add_tensor_187 = None\\n        detach_default_80: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_53)\\n        mul_tensor_240: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_186, rsqrt_default_53);  rsqrt_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant240 = self._param_constant240\\n        mul_tensor_241: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant240, mul_tensor_240);  _param_constant240 = mul_tensor_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant241 = self._param_constant241\\n        t_default_186: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant241);  _param_constant241 = None\\n        view_default_643: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_241, [1, 4096])\\n        mm_default_186: f32[1, 11008] = torch.ops.aten.mm.default(view_default_643, t_default_186);  view_default_643 = t_default_186 = None\\n        view_default_644: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_186, [1, 1, 11008]);  mm_default_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_26: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_644);  view_default_644 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant242 = self._param_constant242\\n        t_default_187: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant242);  _param_constant242 = None\\n        view_default_645: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_241, [1, 4096]);  mul_tensor_241 = None\\n        mm_default_187: f32[1, 11008] = torch.ops.aten.mm.default(view_default_645, t_default_187);  view_default_645 = t_default_187 = None\\n        view_default_646: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_187, [1, 1, 11008]);  mm_default_187 = None\\n        mul_tensor_242: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_26, view_default_646);  silu_default_26 = view_default_646 = None\\n        _param_constant243 = self._param_constant243\\n        t_default_188: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant243);  _param_constant243 = None\\n        view_default_647: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_242, [1, 11008]);  mul_tensor_242 = None\\n        mm_default_188: f32[1, 4096] = torch.ops.aten.mm.default(view_default_647, t_default_188);  view_default_647 = t_default_188 = None\\n        view_default_648: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_188, [1, 1, 4096]);  mm_default_188 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_188: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_186, view_default_648);  add_tensor_186 = view_default_648 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_54: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_188, 2)\\n        mean_dim_54: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_54, [-1], True);  pow_tensor_scalar_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_189: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_54, 1e-06);  mean_dim_54 = None\\n        rsqrt_default_54: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_189);  add_tensor_189 = None\\n        detach_default_81: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_54)\\n        mul_tensor_243: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_188, rsqrt_default_54);  rsqrt_default_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant244 = self._param_constant244\\n        mul_tensor_244: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant244, mul_tensor_243);  _param_constant244 = mul_tensor_243 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant245 = self._param_constant245\\n        t_default_189: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant245);  _param_constant245 = None\\n        view_default_649: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_244, [1, 4096])\\n        mm_default_189: f32[1, 4096] = torch.ops.aten.mm.default(view_default_649, t_default_189);  view_default_649 = t_default_189 = None\\n        view_default_650: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_189, [1, 1, 4096]);  mm_default_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant246 = self._param_constant246\\n        t_default_190: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant246);  _param_constant246 = None\\n        view_default_651: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_244, [1, 4096])\\n        mm_default_190: f32[1, 4096] = torch.ops.aten.mm.default(view_default_651, t_default_190);  view_default_651 = t_default_190 = None\\n        view_default_652: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_190, [1, 1, 4096]);  mm_default_190 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant247 = self._param_constant247\\n        t_default_191: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant247);  _param_constant247 = None\\n        view_default_653: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_244, [1, 4096]);  mul_tensor_244 = None\\n        mm_default_191: f32[1, 4096] = torch.ops.aten.mm.default(view_default_653, t_default_191);  view_default_653 = t_default_191 = None\\n        view_default_654: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_191, [1, 1, 4096]);  mm_default_191 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_655: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_650, [1, 1, 32, 128]);  view_default_650 = None\\n        transpose_int_135: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_655, 1, 2);  view_default_655 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_656: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_652, [1, 1, 32, 128]);  view_default_652 = None\\n        transpose_int_136: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_656, 1, 2);  view_default_656 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_657: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_654, [1, 1, 32, 128]);  view_default_654 = None\\n        transpose_int_137: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_657, 1, 2);  view_default_657 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant54 = self._tensor_constant54\\n        slice_tensor_272: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant54, 0, 0, 9223372036854775807);  _tensor_constant54 = None\\n        slice_tensor_273: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_272, 1, 0, 9223372036854775807);  slice_tensor_272 = None\\n        sym_size_81: Sym(s0) = torch.ops.aten.sym_size(arg55, 2)\\n        add_29: Sym(s0 + 1) = 1 + sym_size_81;  sym_size_81 = None\\n        slice_tensor_274: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_273, 2, 0, add_29);  slice_tensor_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant55 = self._tensor_constant55\\n        slice_tensor_275: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant55, 0, 0, 9223372036854775807);  _tensor_constant55 = None\\n        slice_tensor_276: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_275, 1, 0, 9223372036854775807);  slice_tensor_275 = None\\n        slice_tensor_277: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_276, 2, 0, add_29);  slice_tensor_276 = add_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_108: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_274, 1);  slice_tensor_274 = None\\n        squeeze_dim_109: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_108, 0);  squeeze_dim_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_110: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_277, 1);  slice_tensor_277 = None\\n        squeeze_dim_111: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_110, 0);  squeeze_dim_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_54: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_109, [view_default]);  squeeze_dim_109 = None\\n        unsqueeze_default_57: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_54, 1);  index_tensor_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_55: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_111, [view_default]);  squeeze_dim_111 = None\\n        unsqueeze_default_58: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_55, 1);  index_tensor_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_245: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_135, unsqueeze_default_57)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_278: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_279: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 64, 9223372036854775807);  transpose_int_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_54: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_279);  slice_tensor_279 = None\\n        cat_default_108: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_54, slice_tensor_278], -1);  neg_default_54 = slice_tensor_278 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_246: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_108, unsqueeze_default_58);  cat_default_108 = None\\n        add_tensor_190: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_245, mul_tensor_246);  mul_tensor_245 = mul_tensor_246 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_247: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_136, unsqueeze_default_57);  unsqueeze_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_280: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_281: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 64, 9223372036854775807);  transpose_int_136 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_55: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_281);  slice_tensor_281 = None\\n        cat_default_109: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_55, slice_tensor_280], -1);  neg_default_55 = slice_tensor_280 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_248: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_109, unsqueeze_default_58);  cat_default_109 = unsqueeze_default_58 = None\\n        add_tensor_191: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_247, mul_tensor_248);  mul_tensor_247 = mul_tensor_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_110: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg55, add_tensor_191], 2);  arg55 = add_tensor_191 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_111: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg56, transpose_int_137], 2);  arg56 = transpose_int_137 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_138: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_110, 2, 3)\\n        expand_default_109: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_190, [1, 32, 1, 128]);  add_tensor_190 = None\\n        view_default_658: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_109, [32, 1, 128]);  expand_default_109 = None\\n        sym_size_82: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_110, 2)\\n        expand_default_110: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_138, [1, 32, 128, sym_size_82]);  transpose_int_138 = None\\n        view_default_659: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_110, [32, 128, sym_size_82]);  expand_default_110 = None\\n        bmm_default_54: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_658, view_default_659);  view_default_658 = view_default_659 = None\\n        view_default_660: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_54, [1, 32, 1, sym_size_82]);  bmm_default_54 = None\\n        div_tensor_27: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_660, 11.313708498984761);  view_default_660 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_192: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_27, masked_fill_scalar);  div_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_27: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_192, -1, False);  add_tensor_192 = None\\n        detach_default_82: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_27)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_111: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_27, [1, 32, 1, sym_size_82]);  _softmax_default_27 = None\\n        view_default_661: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_111, [32, 1, sym_size_82]);  expand_default_111 = sym_size_82 = None\\n        sym_size_83: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_111, 2)\\n        expand_default_112: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_111, [1, 32, sym_size_83, 128])\\n        view_default_662: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_112, [32, sym_size_83, 128]);  expand_default_112 = sym_size_83 = None\\n        bmm_default_55: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_661, view_default_662);  view_default_661 = view_default_662 = None\\n        view_default_663: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_55, [1, 32, 1, 128]);  bmm_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_139: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_663, 1, 2);  view_default_663 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_664: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_139, [1, 1, 4096]);  transpose_int_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant248 = self._param_constant248\\n        t_default_192: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant248);  _param_constant248 = None\\n        view_default_665: f32[1, 4096] = torch.ops.aten.view.default(view_default_664, [1, 4096]);  view_default_664 = None\\n        mm_default_192: f32[1, 4096] = torch.ops.aten.mm.default(view_default_665, t_default_192);  view_default_665 = t_default_192 = None\\n        view_default_666: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_192, [1, 1, 4096]);  mm_default_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_193: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_188, view_default_666);  add_tensor_188 = view_default_666 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_55: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_193, 2)\\n        mean_dim_55: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_55, [-1], True);  pow_tensor_scalar_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_194: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_55, 1e-06);  mean_dim_55 = None\\n        rsqrt_default_55: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_194);  add_tensor_194 = None\\n        detach_default_83: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_55)\\n        mul_tensor_249: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_193, rsqrt_default_55);  rsqrt_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant249 = self._param_constant249\\n        mul_tensor_250: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant249, mul_tensor_249);  _param_constant249 = mul_tensor_249 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant250 = self._param_constant250\\n        t_default_193: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant250);  _param_constant250 = None\\n        view_default_667: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_250, [1, 4096])\\n        mm_default_193: f32[1, 11008] = torch.ops.aten.mm.default(view_default_667, t_default_193);  view_default_667 = t_default_193 = None\\n        view_default_668: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_193, [1, 1, 11008]);  mm_default_193 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_27: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_668);  view_default_668 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant251 = self._param_constant251\\n        t_default_194: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant251);  _param_constant251 = None\\n        view_default_669: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_250, [1, 4096]);  mul_tensor_250 = None\\n        mm_default_194: f32[1, 11008] = torch.ops.aten.mm.default(view_default_669, t_default_194);  view_default_669 = t_default_194 = None\\n        view_default_670: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_194, [1, 1, 11008]);  mm_default_194 = None\\n        mul_tensor_251: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_27, view_default_670);  silu_default_27 = view_default_670 = None\\n        _param_constant252 = self._param_constant252\\n        t_default_195: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant252);  _param_constant252 = None\\n        view_default_671: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_251, [1, 11008]);  mul_tensor_251 = None\\n        mm_default_195: f32[1, 4096] = torch.ops.aten.mm.default(view_default_671, t_default_195);  view_default_671 = t_default_195 = None\\n        view_default_672: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_195, [1, 1, 4096]);  mm_default_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_195: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_193, view_default_672);  add_tensor_193 = view_default_672 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_56: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_195, 2)\\n        mean_dim_56: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_56, [-1], True);  pow_tensor_scalar_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_196: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_56, 1e-06);  mean_dim_56 = None\\n        rsqrt_default_56: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_196);  add_tensor_196 = None\\n        detach_default_84: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_56)\\n        mul_tensor_252: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_195, rsqrt_default_56);  rsqrt_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant253 = self._param_constant253\\n        mul_tensor_253: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant253, mul_tensor_252);  _param_constant253 = mul_tensor_252 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant254 = self._param_constant254\\n        t_default_196: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant254);  _param_constant254 = None\\n        view_default_673: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_253, [1, 4096])\\n        mm_default_196: f32[1, 4096] = torch.ops.aten.mm.default(view_default_673, t_default_196);  view_default_673 = t_default_196 = None\\n        view_default_674: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_196, [1, 1, 4096]);  mm_default_196 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant255 = self._param_constant255\\n        t_default_197: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant255);  _param_constant255 = None\\n        view_default_675: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_253, [1, 4096])\\n        mm_default_197: f32[1, 4096] = torch.ops.aten.mm.default(view_default_675, t_default_197);  view_default_675 = t_default_197 = None\\n        view_default_676: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_197, [1, 1, 4096]);  mm_default_197 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant256 = self._param_constant256\\n        t_default_198: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant256);  _param_constant256 = None\\n        view_default_677: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_253, [1, 4096]);  mul_tensor_253 = None\\n        mm_default_198: f32[1, 4096] = torch.ops.aten.mm.default(view_default_677, t_default_198);  view_default_677 = t_default_198 = None\\n        view_default_678: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_198, [1, 1, 4096]);  mm_default_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_679: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_674, [1, 1, 32, 128]);  view_default_674 = None\\n        transpose_int_140: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_679, 1, 2);  view_default_679 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_680: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_676, [1, 1, 32, 128]);  view_default_676 = None\\n        transpose_int_141: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_680, 1, 2);  view_default_680 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_681: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_678, [1, 1, 32, 128]);  view_default_678 = None\\n        transpose_int_142: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_681, 1, 2);  view_default_681 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant56 = self._tensor_constant56\\n        slice_tensor_282: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant56, 0, 0, 9223372036854775807);  _tensor_constant56 = None\\n        slice_tensor_283: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_282, 1, 0, 9223372036854775807);  slice_tensor_282 = None\\n        sym_size_84: Sym(s0) = torch.ops.aten.sym_size(arg57, 2)\\n        add_30: Sym(s0 + 1) = 1 + sym_size_84;  sym_size_84 = None\\n        slice_tensor_284: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_283, 2, 0, add_30);  slice_tensor_283 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant57 = self._tensor_constant57\\n        slice_tensor_285: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant57, 0, 0, 9223372036854775807);  _tensor_constant57 = None\\n        slice_tensor_286: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_285, 1, 0, 9223372036854775807);  slice_tensor_285 = None\\n        slice_tensor_287: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_286, 2, 0, add_30);  slice_tensor_286 = add_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_112: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_284, 1);  slice_tensor_284 = None\\n        squeeze_dim_113: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_112, 0);  squeeze_dim_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_114: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_287, 1);  slice_tensor_287 = None\\n        squeeze_dim_115: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_114, 0);  squeeze_dim_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_56: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_113, [view_default]);  squeeze_dim_113 = None\\n        unsqueeze_default_59: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_56, 1);  index_tensor_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_57: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_115, [view_default]);  squeeze_dim_115 = None\\n        unsqueeze_default_60: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_57, 1);  index_tensor_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_254: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_140, unsqueeze_default_59)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_288: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_289: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 64, 9223372036854775807);  transpose_int_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_56: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_289);  slice_tensor_289 = None\\n        cat_default_112: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_56, slice_tensor_288], -1);  neg_default_56 = slice_tensor_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_255: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_112, unsqueeze_default_60);  cat_default_112 = None\\n        add_tensor_197: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_254, mul_tensor_255);  mul_tensor_254 = mul_tensor_255 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_256: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_141, unsqueeze_default_59);  unsqueeze_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_290: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_291: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 64, 9223372036854775807);  transpose_int_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_57: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_291);  slice_tensor_291 = None\\n        cat_default_113: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_57, slice_tensor_290], -1);  neg_default_57 = slice_tensor_290 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_257: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_113, unsqueeze_default_60);  cat_default_113 = unsqueeze_default_60 = None\\n        add_tensor_198: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_256, mul_tensor_257);  mul_tensor_256 = mul_tensor_257 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_114: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg57, add_tensor_198], 2);  arg57 = add_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_115: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg58, transpose_int_142], 2);  arg58 = transpose_int_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_143: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_114, 2, 3)\\n        expand_default_113: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_197, [1, 32, 1, 128]);  add_tensor_197 = None\\n        view_default_682: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_113, [32, 1, 128]);  expand_default_113 = None\\n        sym_size_85: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_114, 2)\\n        expand_default_114: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_143, [1, 32, 128, sym_size_85]);  transpose_int_143 = None\\n        view_default_683: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_114, [32, 128, sym_size_85]);  expand_default_114 = None\\n        bmm_default_56: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_682, view_default_683);  view_default_682 = view_default_683 = None\\n        view_default_684: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_56, [1, 32, 1, sym_size_85]);  bmm_default_56 = None\\n        div_tensor_28: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_684, 11.313708498984761);  view_default_684 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_199: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_28, masked_fill_scalar);  div_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_28: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_199, -1, False);  add_tensor_199 = None\\n        detach_default_85: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_28)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_115: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_28, [1, 32, 1, sym_size_85]);  _softmax_default_28 = None\\n        view_default_685: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_115, [32, 1, sym_size_85]);  expand_default_115 = sym_size_85 = None\\n        sym_size_86: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_115, 2)\\n        expand_default_116: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_115, [1, 32, sym_size_86, 128])\\n        view_default_686: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_116, [32, sym_size_86, 128]);  expand_default_116 = sym_size_86 = None\\n        bmm_default_57: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_685, view_default_686);  view_default_685 = view_default_686 = None\\n        view_default_687: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_57, [1, 32, 1, 128]);  bmm_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_144: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_687, 1, 2);  view_default_687 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_688: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_144, [1, 1, 4096]);  transpose_int_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant257 = self._param_constant257\\n        t_default_199: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant257);  _param_constant257 = None\\n        view_default_689: f32[1, 4096] = torch.ops.aten.view.default(view_default_688, [1, 4096]);  view_default_688 = None\\n        mm_default_199: f32[1, 4096] = torch.ops.aten.mm.default(view_default_689, t_default_199);  view_default_689 = t_default_199 = None\\n        view_default_690: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_199, [1, 1, 4096]);  mm_default_199 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_200: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_195, view_default_690);  add_tensor_195 = view_default_690 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_57: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_200, 2)\\n        mean_dim_57: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_57, [-1], True);  pow_tensor_scalar_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_201: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_57, 1e-06);  mean_dim_57 = None\\n        rsqrt_default_57: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_201);  add_tensor_201 = None\\n        detach_default_86: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_57)\\n        mul_tensor_258: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_200, rsqrt_default_57);  rsqrt_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant258 = self._param_constant258\\n        mul_tensor_259: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant258, mul_tensor_258);  _param_constant258 = mul_tensor_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant259 = self._param_constant259\\n        t_default_200: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant259);  _param_constant259 = None\\n        view_default_691: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_259, [1, 4096])\\n        mm_default_200: f32[1, 11008] = torch.ops.aten.mm.default(view_default_691, t_default_200);  view_default_691 = t_default_200 = None\\n        view_default_692: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_200, [1, 1, 11008]);  mm_default_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_28: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_692);  view_default_692 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant260 = self._param_constant260\\n        t_default_201: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant260);  _param_constant260 = None\\n        view_default_693: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_259, [1, 4096]);  mul_tensor_259 = None\\n        mm_default_201: f32[1, 11008] = torch.ops.aten.mm.default(view_default_693, t_default_201);  view_default_693 = t_default_201 = None\\n        view_default_694: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_201, [1, 1, 11008]);  mm_default_201 = None\\n        mul_tensor_260: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_28, view_default_694);  silu_default_28 = view_default_694 = None\\n        _param_constant261 = self._param_constant261\\n        t_default_202: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant261);  _param_constant261 = None\\n        view_default_695: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_260, [1, 11008]);  mul_tensor_260 = None\\n        mm_default_202: f32[1, 4096] = torch.ops.aten.mm.default(view_default_695, t_default_202);  view_default_695 = t_default_202 = None\\n        view_default_696: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_202, [1, 1, 4096]);  mm_default_202 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_202: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_200, view_default_696);  add_tensor_200 = view_default_696 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_58: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_202, 2)\\n        mean_dim_58: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_58, [-1], True);  pow_tensor_scalar_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_203: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_58, 1e-06);  mean_dim_58 = None\\n        rsqrt_default_58: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_203);  add_tensor_203 = None\\n        detach_default_87: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_58)\\n        mul_tensor_261: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_202, rsqrt_default_58);  rsqrt_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant262 = self._param_constant262\\n        mul_tensor_262: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant262, mul_tensor_261);  _param_constant262 = mul_tensor_261 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant263 = self._param_constant263\\n        t_default_203: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant263);  _param_constant263 = None\\n        view_default_697: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_262, [1, 4096])\\n        mm_default_203: f32[1, 4096] = torch.ops.aten.mm.default(view_default_697, t_default_203);  view_default_697 = t_default_203 = None\\n        view_default_698: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_203, [1, 1, 4096]);  mm_default_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant264 = self._param_constant264\\n        t_default_204: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant264);  _param_constant264 = None\\n        view_default_699: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_262, [1, 4096])\\n        mm_default_204: f32[1, 4096] = torch.ops.aten.mm.default(view_default_699, t_default_204);  view_default_699 = t_default_204 = None\\n        view_default_700: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_204, [1, 1, 4096]);  mm_default_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant265 = self._param_constant265\\n        t_default_205: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant265);  _param_constant265 = None\\n        view_default_701: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_262, [1, 4096]);  mul_tensor_262 = None\\n        mm_default_205: f32[1, 4096] = torch.ops.aten.mm.default(view_default_701, t_default_205);  view_default_701 = t_default_205 = None\\n        view_default_702: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_205, [1, 1, 4096]);  mm_default_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_703: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_698, [1, 1, 32, 128]);  view_default_698 = None\\n        transpose_int_145: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_703, 1, 2);  view_default_703 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_704: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_700, [1, 1, 32, 128]);  view_default_700 = None\\n        transpose_int_146: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_704, 1, 2);  view_default_704 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_705: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_702, [1, 1, 32, 128]);  view_default_702 = None\\n        transpose_int_147: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_705, 1, 2);  view_default_705 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant58 = self._tensor_constant58\\n        slice_tensor_292: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant58, 0, 0, 9223372036854775807);  _tensor_constant58 = None\\n        slice_tensor_293: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_292, 1, 0, 9223372036854775807);  slice_tensor_292 = None\\n        sym_size_87: Sym(s0) = torch.ops.aten.sym_size(arg59, 2)\\n        add_31: Sym(s0 + 1) = 1 + sym_size_87;  sym_size_87 = None\\n        slice_tensor_294: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_293, 2, 0, add_31);  slice_tensor_293 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant59 = self._tensor_constant59\\n        slice_tensor_295: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant59, 0, 0, 9223372036854775807);  _tensor_constant59 = None\\n        slice_tensor_296: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_295, 1, 0, 9223372036854775807);  slice_tensor_295 = None\\n        slice_tensor_297: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_296, 2, 0, add_31);  slice_tensor_296 = add_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_116: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_294, 1);  slice_tensor_294 = None\\n        squeeze_dim_117: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_116, 0);  squeeze_dim_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_118: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_297, 1);  slice_tensor_297 = None\\n        squeeze_dim_119: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_118, 0);  squeeze_dim_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_58: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_117, [view_default]);  squeeze_dim_117 = None\\n        unsqueeze_default_61: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_58, 1);  index_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_59: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_119, [view_default]);  squeeze_dim_119 = None\\n        unsqueeze_default_62: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_59, 1);  index_tensor_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_263: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_145, unsqueeze_default_61)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_298: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_299: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 64, 9223372036854775807);  transpose_int_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_58: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_299);  slice_tensor_299 = None\\n        cat_default_116: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_58, slice_tensor_298], -1);  neg_default_58 = slice_tensor_298 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_264: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_116, unsqueeze_default_62);  cat_default_116 = None\\n        add_tensor_204: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_263, mul_tensor_264);  mul_tensor_263 = mul_tensor_264 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_265: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_146, unsqueeze_default_61);  unsqueeze_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_300: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_301: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 64, 9223372036854775807);  transpose_int_146 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_59: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_301);  slice_tensor_301 = None\\n        cat_default_117: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_59, slice_tensor_300], -1);  neg_default_59 = slice_tensor_300 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_266: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_117, unsqueeze_default_62);  cat_default_117 = unsqueeze_default_62 = None\\n        add_tensor_205: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_265, mul_tensor_266);  mul_tensor_265 = mul_tensor_266 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_118: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg59, add_tensor_205], 2);  arg59 = add_tensor_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_119: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg60, transpose_int_147], 2);  arg60 = transpose_int_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_148: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_118, 2, 3)\\n        expand_default_117: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_204, [1, 32, 1, 128]);  add_tensor_204 = None\\n        view_default_706: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_117, [32, 1, 128]);  expand_default_117 = None\\n        sym_size_88: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_118, 2)\\n        expand_default_118: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_148, [1, 32, 128, sym_size_88]);  transpose_int_148 = None\\n        view_default_707: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_118, [32, 128, sym_size_88]);  expand_default_118 = None\\n        bmm_default_58: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_706, view_default_707);  view_default_706 = view_default_707 = None\\n        view_default_708: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_58, [1, 32, 1, sym_size_88]);  bmm_default_58 = None\\n        div_tensor_29: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_708, 11.313708498984761);  view_default_708 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_206: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_29, masked_fill_scalar);  div_tensor_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_29: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_206, -1, False);  add_tensor_206 = None\\n        detach_default_88: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_29)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_119: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_29, [1, 32, 1, sym_size_88]);  _softmax_default_29 = None\\n        view_default_709: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_119, [32, 1, sym_size_88]);  expand_default_119 = sym_size_88 = None\\n        sym_size_89: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_119, 2)\\n        expand_default_120: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_119, [1, 32, sym_size_89, 128])\\n        view_default_710: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_120, [32, sym_size_89, 128]);  expand_default_120 = sym_size_89 = None\\n        bmm_default_59: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_709, view_default_710);  view_default_709 = view_default_710 = None\\n        view_default_711: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_59, [1, 32, 1, 128]);  bmm_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_149: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_711, 1, 2);  view_default_711 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_712: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_149, [1, 1, 4096]);  transpose_int_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant266 = self._param_constant266\\n        t_default_206: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant266);  _param_constant266 = None\\n        view_default_713: f32[1, 4096] = torch.ops.aten.view.default(view_default_712, [1, 4096]);  view_default_712 = None\\n        mm_default_206: f32[1, 4096] = torch.ops.aten.mm.default(view_default_713, t_default_206);  view_default_713 = t_default_206 = None\\n        view_default_714: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_206, [1, 1, 4096]);  mm_default_206 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_207: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_202, view_default_714);  add_tensor_202 = view_default_714 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_59: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_207, 2)\\n        mean_dim_59: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_59, [-1], True);  pow_tensor_scalar_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_208: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_59, 1e-06);  mean_dim_59 = None\\n        rsqrt_default_59: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_208);  add_tensor_208 = None\\n        detach_default_89: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_59)\\n        mul_tensor_267: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_207, rsqrt_default_59);  rsqrt_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant267 = self._param_constant267\\n        mul_tensor_268: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant267, mul_tensor_267);  _param_constant267 = mul_tensor_267 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant268 = self._param_constant268\\n        t_default_207: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant268);  _param_constant268 = None\\n        view_default_715: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_268, [1, 4096])\\n        mm_default_207: f32[1, 11008] = torch.ops.aten.mm.default(view_default_715, t_default_207);  view_default_715 = t_default_207 = None\\n        view_default_716: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_207, [1, 1, 11008]);  mm_default_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_29: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_716);  view_default_716 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant269 = self._param_constant269\\n        t_default_208: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant269);  _param_constant269 = None\\n        view_default_717: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_268, [1, 4096]);  mul_tensor_268 = None\\n        mm_default_208: f32[1, 11008] = torch.ops.aten.mm.default(view_default_717, t_default_208);  view_default_717 = t_default_208 = None\\n        view_default_718: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_208, [1, 1, 11008]);  mm_default_208 = None\\n        mul_tensor_269: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_29, view_default_718);  silu_default_29 = view_default_718 = None\\n        _param_constant270 = self._param_constant270\\n        t_default_209: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant270);  _param_constant270 = None\\n        view_default_719: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_269, [1, 11008]);  mul_tensor_269 = None\\n        mm_default_209: f32[1, 4096] = torch.ops.aten.mm.default(view_default_719, t_default_209);  view_default_719 = t_default_209 = None\\n        view_default_720: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_209, [1, 1, 4096]);  mm_default_209 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_209: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_207, view_default_720);  add_tensor_207 = view_default_720 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_60: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_209, 2)\\n        mean_dim_60: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_60, [-1], True);  pow_tensor_scalar_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_210: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_60, 1e-06);  mean_dim_60 = None\\n        rsqrt_default_60: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_210);  add_tensor_210 = None\\n        detach_default_90: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_60)\\n        mul_tensor_270: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_209, rsqrt_default_60);  rsqrt_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant271 = self._param_constant271\\n        mul_tensor_271: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant271, mul_tensor_270);  _param_constant271 = mul_tensor_270 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant272 = self._param_constant272\\n        t_default_210: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant272);  _param_constant272 = None\\n        view_default_721: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_271, [1, 4096])\\n        mm_default_210: f32[1, 4096] = torch.ops.aten.mm.default(view_default_721, t_default_210);  view_default_721 = t_default_210 = None\\n        view_default_722: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_210, [1, 1, 4096]);  mm_default_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant273 = self._param_constant273\\n        t_default_211: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant273);  _param_constant273 = None\\n        view_default_723: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_271, [1, 4096])\\n        mm_default_211: f32[1, 4096] = torch.ops.aten.mm.default(view_default_723, t_default_211);  view_default_723 = t_default_211 = None\\n        view_default_724: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_211, [1, 1, 4096]);  mm_default_211 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant274 = self._param_constant274\\n        t_default_212: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant274);  _param_constant274 = None\\n        view_default_725: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_271, [1, 4096]);  mul_tensor_271 = None\\n        mm_default_212: f32[1, 4096] = torch.ops.aten.mm.default(view_default_725, t_default_212);  view_default_725 = t_default_212 = None\\n        view_default_726: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_212, [1, 1, 4096]);  mm_default_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_727: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_722, [1, 1, 32, 128]);  view_default_722 = None\\n        transpose_int_150: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_727, 1, 2);  view_default_727 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_728: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_724, [1, 1, 32, 128]);  view_default_724 = None\\n        transpose_int_151: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_728, 1, 2);  view_default_728 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_729: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_726, [1, 1, 32, 128]);  view_default_726 = None\\n        transpose_int_152: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_729, 1, 2);  view_default_729 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant60 = self._tensor_constant60\\n        slice_tensor_302: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant60, 0, 0, 9223372036854775807);  _tensor_constant60 = None\\n        slice_tensor_303: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_302, 1, 0, 9223372036854775807);  slice_tensor_302 = None\\n        sym_size_90: Sym(s0) = torch.ops.aten.sym_size(arg61, 2)\\n        add_32: Sym(s0 + 1) = 1 + sym_size_90;  sym_size_90 = None\\n        slice_tensor_304: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_303, 2, 0, add_32);  slice_tensor_303 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant61 = self._tensor_constant61\\n        slice_tensor_305: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant61, 0, 0, 9223372036854775807);  _tensor_constant61 = None\\n        slice_tensor_306: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_305, 1, 0, 9223372036854775807);  slice_tensor_305 = None\\n        slice_tensor_307: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_306, 2, 0, add_32);  slice_tensor_306 = add_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_120: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_304, 1);  slice_tensor_304 = None\\n        squeeze_dim_121: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_120, 0);  squeeze_dim_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_122: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_307, 1);  slice_tensor_307 = None\\n        squeeze_dim_123: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_122, 0);  squeeze_dim_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_60: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_121, [view_default]);  squeeze_dim_121 = None\\n        unsqueeze_default_63: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_60, 1);  index_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_61: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_123, [view_default]);  squeeze_dim_123 = None\\n        unsqueeze_default_64: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_61, 1);  index_tensor_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_272: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_150, unsqueeze_default_63)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_308: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_309: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 64, 9223372036854775807);  transpose_int_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_60: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_309);  slice_tensor_309 = None\\n        cat_default_120: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_60, slice_tensor_308], -1);  neg_default_60 = slice_tensor_308 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_273: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_120, unsqueeze_default_64);  cat_default_120 = None\\n        add_tensor_211: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_272, mul_tensor_273);  mul_tensor_272 = mul_tensor_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_274: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_151, unsqueeze_default_63);  unsqueeze_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_310: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_311: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 64, 9223372036854775807);  transpose_int_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_61: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_311);  slice_tensor_311 = None\\n        cat_default_121: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_61, slice_tensor_310], -1);  neg_default_61 = slice_tensor_310 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_275: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_121, unsqueeze_default_64);  cat_default_121 = unsqueeze_default_64 = None\\n        add_tensor_212: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_274, mul_tensor_275);  mul_tensor_274 = mul_tensor_275 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_122: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg61, add_tensor_212], 2);  arg61 = add_tensor_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_123: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg62, transpose_int_152], 2);  arg62 = transpose_int_152 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_153: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_122, 2, 3)\\n        expand_default_121: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_211, [1, 32, 1, 128]);  add_tensor_211 = None\\n        view_default_730: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_121, [32, 1, 128]);  expand_default_121 = None\\n        sym_size_91: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_122, 2)\\n        expand_default_122: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_153, [1, 32, 128, sym_size_91]);  transpose_int_153 = None\\n        view_default_731: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_122, [32, 128, sym_size_91]);  expand_default_122 = None\\n        bmm_default_60: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_730, view_default_731);  view_default_730 = view_default_731 = None\\n        view_default_732: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_60, [1, 32, 1, sym_size_91]);  bmm_default_60 = None\\n        div_tensor_30: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_732, 11.313708498984761);  view_default_732 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_213: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_30, masked_fill_scalar);  div_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_30: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_213, -1, False);  add_tensor_213 = None\\n        detach_default_91: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_30)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_123: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_30, [1, 32, 1, sym_size_91]);  _softmax_default_30 = None\\n        view_default_733: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_123, [32, 1, sym_size_91]);  expand_default_123 = sym_size_91 = None\\n        sym_size_92: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_123, 2)\\n        expand_default_124: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_123, [1, 32, sym_size_92, 128])\\n        view_default_734: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_124, [32, sym_size_92, 128]);  expand_default_124 = sym_size_92 = None\\n        bmm_default_61: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_733, view_default_734);  view_default_733 = view_default_734 = None\\n        view_default_735: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_61, [1, 32, 1, 128]);  bmm_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_154: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_735, 1, 2);  view_default_735 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_736: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_154, [1, 1, 4096]);  transpose_int_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant275 = self._param_constant275\\n        t_default_213: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant275);  _param_constant275 = None\\n        view_default_737: f32[1, 4096] = torch.ops.aten.view.default(view_default_736, [1, 4096]);  view_default_736 = None\\n        mm_default_213: f32[1, 4096] = torch.ops.aten.mm.default(view_default_737, t_default_213);  view_default_737 = t_default_213 = None\\n        view_default_738: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_213, [1, 1, 4096]);  mm_default_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_214: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_209, view_default_738);  add_tensor_209 = view_default_738 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_61: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_214, 2)\\n        mean_dim_61: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_61, [-1], True);  pow_tensor_scalar_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_215: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_61, 1e-06);  mean_dim_61 = None\\n        rsqrt_default_61: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_215);  add_tensor_215 = None\\n        detach_default_92: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_61)\\n        mul_tensor_276: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_214, rsqrt_default_61);  rsqrt_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant276 = self._param_constant276\\n        mul_tensor_277: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant276, mul_tensor_276);  _param_constant276 = mul_tensor_276 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant277 = self._param_constant277\\n        t_default_214: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant277);  _param_constant277 = None\\n        view_default_739: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_277, [1, 4096])\\n        mm_default_214: f32[1, 11008] = torch.ops.aten.mm.default(view_default_739, t_default_214);  view_default_739 = t_default_214 = None\\n        view_default_740: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_214, [1, 1, 11008]);  mm_default_214 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_30: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_740);  view_default_740 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant278 = self._param_constant278\\n        t_default_215: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant278);  _param_constant278 = None\\n        view_default_741: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_277, [1, 4096]);  mul_tensor_277 = None\\n        mm_default_215: f32[1, 11008] = torch.ops.aten.mm.default(view_default_741, t_default_215);  view_default_741 = t_default_215 = None\\n        view_default_742: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_215, [1, 1, 11008]);  mm_default_215 = None\\n        mul_tensor_278: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_30, view_default_742);  silu_default_30 = view_default_742 = None\\n        _param_constant279 = self._param_constant279\\n        t_default_216: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant279);  _param_constant279 = None\\n        view_default_743: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_278, [1, 11008]);  mul_tensor_278 = None\\n        mm_default_216: f32[1, 4096] = torch.ops.aten.mm.default(view_default_743, t_default_216);  view_default_743 = t_default_216 = None\\n        view_default_744: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_216, [1, 1, 4096]);  mm_default_216 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_216: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_214, view_default_744);  add_tensor_214 = view_default_744 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_62: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_216, 2)\\n        mean_dim_62: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_62, [-1], True);  pow_tensor_scalar_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_217: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_62, 1e-06);  mean_dim_62 = None\\n        rsqrt_default_62: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_217);  add_tensor_217 = None\\n        detach_default_93: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_62)\\n        mul_tensor_279: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_216, rsqrt_default_62);  rsqrt_default_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant280 = self._param_constant280\\n        mul_tensor_280: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant280, mul_tensor_279);  _param_constant280 = mul_tensor_279 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:321, code: query_states = self.q_proj(hidden_states)\\n        _param_constant281 = self._param_constant281\\n        t_default_217: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant281);  _param_constant281 = None\\n        view_default_745: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_280, [1, 4096])\\n        mm_default_217: f32[1, 4096] = torch.ops.aten.mm.default(view_default_745, t_default_217);  view_default_745 = t_default_217 = None\\n        view_default_746: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_217, [1, 1, 4096]);  mm_default_217 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:322, code: key_states = self.k_proj(hidden_states)\\n        _param_constant282 = self._param_constant282\\n        t_default_218: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant282);  _param_constant282 = None\\n        view_default_747: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_280, [1, 4096])\\n        mm_default_218: f32[1, 4096] = torch.ops.aten.mm.default(view_default_747, t_default_218);  view_default_747 = t_default_218 = None\\n        view_default_748: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_218, [1, 1, 4096]);  mm_default_218 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323, code: value_states = self.v_proj(hidden_states)\\n        _param_constant283 = self._param_constant283\\n        t_default_219: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant283);  _param_constant283 = None\\n        view_default_749: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_280, [1, 4096]);  mul_tensor_280 = None\\n        mm_default_219: f32[1, 4096] = torch.ops.aten.mm.default(view_default_749, t_default_219);  view_default_749 = t_default_219 = None\\n        view_default_750: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_219, [1, 1, 4096]);  mm_default_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_751: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_746, [1, 1, 32, 128]);  view_default_746 = None\\n        transpose_int_155: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_751, 1, 2);  view_default_751 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:326, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_752: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_748, [1, 1, 32, 128]);  view_default_748 = None\\n        transpose_int_156: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_752, 1, 2);  view_default_752 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:327, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_753: f32[1, 1, 32, 128] = torch.ops.aten.view.default(view_default_750, [1, 1, 32, 128]);  view_default_750 = None\\n        transpose_int_157: f32[1, 32, 1, 128] = torch.ops.aten.transpose.int(view_default_753, 1, 2);  view_default_753 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant62 = self._tensor_constant62\\n        slice_tensor_312: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant62, 0, 0, 9223372036854775807);  _tensor_constant62 = None\\n        slice_tensor_313: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_312, 1, 0, 9223372036854775807);  slice_tensor_312 = None\\n        sym_size_93: Sym(s0) = torch.ops.aten.sym_size(arg63, 2)\\n        add_33: Sym(s0 + 1) = 1 + sym_size_93;  sym_size_93 = None\\n        slice_tensor_314: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_313, 2, 0, add_33);  slice_tensor_313 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant63 = self._tensor_constant63\\n        slice_tensor_315: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant63, 0, 0, 9223372036854775807);  _tensor_constant63 = None\\n        slice_tensor_316: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_315, 1, 0, 9223372036854775807);  slice_tensor_315 = None\\n        slice_tensor_317: f32[1, 1, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_316, 2, 0, add_33);  slice_tensor_316 = add_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_124: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_314, 1);  slice_tensor_314 = None\\n        squeeze_dim_125: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_124, 0);  squeeze_dim_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_126: f32[1, s0 + 1, 128] = torch.ops.aten.squeeze.dim(slice_tensor_317, 1);  slice_tensor_317 = None\\n        squeeze_dim_127: f32[s0 + 1, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_126, 0);  squeeze_dim_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_62: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_125, [view_default]);  squeeze_dim_125 = None\\n        unsqueeze_default_65: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_62, 1);  index_tensor_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_63: f32[1, 1, 128] = torch.ops.aten.index.Tensor(squeeze_dim_127, [view_default]);  squeeze_dim_127 = view_default = None\\n        unsqueeze_default_66: f32[1, 1, 1, 128] = torch.ops.aten.unsqueeze.default(index_tensor_63, 1);  index_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_281: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_155, unsqueeze_default_65)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_318: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_319: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 64, 9223372036854775807);  transpose_int_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_62: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_319);  slice_tensor_319 = None\\n        cat_default_124: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_62, slice_tensor_318], -1);  neg_default_62 = slice_tensor_318 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_282: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_124, unsqueeze_default_66);  cat_default_124 = None\\n        add_tensor_218: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_281, mul_tensor_282);  mul_tensor_281 = mul_tensor_282 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_283: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(transpose_int_156, unsqueeze_default_65);  unsqueeze_default_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_320: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_321: f32[1, 32, 1, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 64, 9223372036854775807);  transpose_int_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_63: f32[1, 32, 1, 64] = torch.ops.aten.neg.default(slice_tensor_321);  slice_tensor_321 = None\\n        cat_default_125: f32[1, 32, 1, 128] = torch.ops.aten.cat.default([neg_default_63, slice_tensor_320], -1);  neg_default_63 = slice_tensor_320 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_284: f32[1, 32, 1, 128] = torch.ops.aten.mul.Tensor(cat_default_125, unsqueeze_default_66);  cat_default_125 = unsqueeze_default_66 = None\\n        add_tensor_219: f32[1, 32, 1, 128] = torch.ops.aten.add.Tensor(mul_tensor_283, mul_tensor_284);  mul_tensor_283 = mul_tensor_284 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:337, code: key_states = torch.cat([past_key_value[0], key_states], dim=2)\\n        cat_default_126: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg63, add_tensor_219], 2);  arg63 = add_tensor_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:338, code: value_states = torch.cat([past_key_value[1], value_states], dim=2)\\n        cat_default_127: f32[1, 32, s0 + 1, 128] = torch.ops.aten.cat.default([arg64, transpose_int_157], 2);  arg64 = transpose_int_157 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:346, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_158: f32[1, 32, 128, s0 + 1] = torch.ops.aten.transpose.int(cat_default_126, 2, 3)\\n        expand_default_125: f32[1, 32, 1, 128] = torch.ops.aten.expand.default(add_tensor_218, [1, 32, 1, 128]);  add_tensor_218 = None\\n        view_default_754: f32[32, 1, 128] = torch.ops.aten.view.default(expand_default_125, [32, 1, 128]);  expand_default_125 = None\\n        sym_size_94: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_126, 2)\\n        expand_default_126: f32[1, 32, 128, s0 + 1] = torch.ops.aten.expand.default(transpose_int_158, [1, 32, 128, sym_size_94]);  transpose_int_158 = None\\n        view_default_755: f32[32, 128, s0 + 1] = torch.ops.aten.view.default(expand_default_126, [32, 128, sym_size_94]);  expand_default_126 = None\\n        bmm_default_62: f32[32, 1, s0 + 1] = torch.ops.aten.bmm.default(view_default_754, view_default_755);  view_default_754 = view_default_755 = None\\n        view_default_756: f32[1, 32, 1, s0 + 1] = torch.ops.aten.view.default(bmm_default_62, [1, 32, 1, sym_size_94]);  bmm_default_62 = None\\n        div_tensor_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten.div.Tensor(view_default_756, 11.313708498984761);  view_default_756 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:359, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_220: f32[1, 32, 1, s0 + 1] = torch.ops.aten.add.Tensor(div_tensor_31, masked_fill_scalar);  div_tensor_31 = masked_fill_scalar = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_31: f32[1, 32, 1, s0 + 1] = torch.ops.aten._softmax.default(add_tensor_220, -1, False);  add_tensor_220 = None\\n        detach_default_94: f32[1, 32, 1, s0 + 1] = torch.ops.aten.detach.default(_softmax_default_31)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:363, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_127: f32[1, 32, 1, s0 + 1] = torch.ops.aten.expand.default(_softmax_default_31, [1, 32, 1, sym_size_94]);  _softmax_default_31 = None\\n        view_default_757: f32[32, 1, s0 + 1] = torch.ops.aten.view.default(expand_default_127, [32, 1, sym_size_94]);  expand_default_127 = sym_size_94 = None\\n        sym_size_95: Sym(s0 + 1) = torch.ops.aten.sym_size(cat_default_127, 2)\\n        expand_default_128: f32[1, 32, s0 + 1, 128] = torch.ops.aten.expand.default(cat_default_127, [1, 32, sym_size_95, 128])\\n        view_default_758: f32[32, s0 + 1, 128] = torch.ops.aten.view.default(expand_default_128, [32, sym_size_95, 128]);  expand_default_128 = sym_size_95 = None\\n        bmm_default_63: f32[32, 1, 128] = torch.ops.aten.bmm.default(view_default_757, view_default_758);  view_default_757 = view_default_758 = None\\n        view_default_759: f32[1, 32, 1, 128] = torch.ops.aten.view.default(bmm_default_63, [1, 32, 1, 128]);  bmm_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:371, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_159: f32[1, 1, 32, 128] = torch.ops.aten.transpose.int(view_default_759, 1, 2);  view_default_759 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:372, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_760: f32[1, 1, 4096] = torch.ops.aten.view.default(transpose_int_159, [1, 1, 4096]);  transpose_int_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:379, code: attn_output = self.o_proj(attn_output)\\n        _param_constant284 = self._param_constant284\\n        t_default_220: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant284);  _param_constant284 = None\\n        view_default_761: f32[1, 4096] = torch.ops.aten.view.default(view_default_760, [1, 4096]);  view_default_760 = None\\n        mm_default_220: f32[1, 4096] = torch.ops.aten.mm.default(view_default_761, t_default_220);  view_default_761 = t_default_220 = None\\n        view_default_762: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_220, [1, 1, 4096]);  mm_default_220 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:432, code: hidden_states = residual + hidden_states\\n        add_tensor_221: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_216, view_default_762);  add_tensor_216 = view_default_762 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_63: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_221, 2)\\n        mean_dim_63: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_63, [-1], True);  pow_tensor_scalar_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_222: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_63, 1e-06);  mean_dim_63 = None\\n        rsqrt_default_63: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_222);  add_tensor_222 = None\\n        detach_default_95: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_63)\\n        mul_tensor_285: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_221, rsqrt_default_63);  rsqrt_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant285 = self._param_constant285\\n        mul_tensor_286: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant285, mul_tensor_285);  _param_constant285 = mul_tensor_285 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant286 = self._param_constant286\\n        t_default_221: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant286);  _param_constant286 = None\\n        view_default_763: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_286, [1, 4096])\\n        mm_default_221: f32[1, 11008] = torch.ops.aten.mm.default(view_default_763, t_default_221);  view_default_763 = t_default_221 = None\\n        view_default_764: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_221, [1, 1, 11008]);  mm_default_221 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_31: f32[1, 1, 11008] = torch.ops.aten.silu.default(view_default_764);  view_default_764 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant287 = self._param_constant287\\n        t_default_222: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant287);  _param_constant287 = None\\n        view_default_765: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_286, [1, 4096]);  mul_tensor_286 = None\\n        mm_default_222: f32[1, 11008] = torch.ops.aten.mm.default(view_default_765, t_default_222);  view_default_765 = t_default_222 = None\\n        view_default_766: f32[1, 1, 11008] = torch.ops.aten.view.default(mm_default_222, [1, 1, 11008]);  mm_default_222 = None\\n        mul_tensor_287: f32[1, 1, 11008] = torch.ops.aten.mul.Tensor(silu_default_31, view_default_766);  silu_default_31 = view_default_766 = None\\n        _param_constant288 = self._param_constant288\\n        t_default_223: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant288);  _param_constant288 = None\\n        view_default_767: f32[1, 11008] = torch.ops.aten.view.default(mul_tensor_287, [1, 11008]);  mul_tensor_287 = None\\n        mm_default_223: f32[1, 4096] = torch.ops.aten.mm.default(view_default_767, t_default_223);  view_default_767 = t_default_223 = None\\n        view_default_768: f32[1, 1, 4096] = torch.ops.aten.view.default(mm_default_223, [1, 1, 4096]);  mm_default_223 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:438, code: hidden_states = residual + hidden_states\\n        add_tensor_223: f32[1, 1, 4096] = torch.ops.aten.add.Tensor(add_tensor_221, view_default_768);  add_tensor_221 = view_default_768 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_64: f32[1, 1, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_223, 2)\\n        mean_dim_64: f32[1, 1, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_64, [-1], True);  pow_tensor_scalar_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_224: f32[1, 1, 1] = torch.ops.aten.add.Tensor(mean_dim_64, 1e-06);  mean_dim_64 = None\\n        rsqrt_default_64: f32[1, 1, 1] = torch.ops.aten.rsqrt.default(add_tensor_224);  add_tensor_224 = None\\n        detach_default_96: f32[1, 1, 1] = torch.ops.aten.detach.default(rsqrt_default_64)\\n        mul_tensor_288: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(add_tensor_223, rsqrt_default_64);  add_tensor_223 = rsqrt_default_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant289 = self._param_constant289\\n        mul_tensor_289: f32[1, 1, 4096] = torch.ops.aten.mul.Tensor(_param_constant289, mul_tensor_288);  _param_constant289 = mul_tensor_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:838, code: logits = self.lm_head(hidden_states)\\n        _param_constant290 = self._param_constant290\\n        t_default_224: f32[4096, 32000] = torch.ops.aten.t.default(_param_constant290);  _param_constant290 = None\\n        view_default_769: f32[1, 4096] = torch.ops.aten.view.default(mul_tensor_289, [1, 4096]);  mul_tensor_289 = None\\n        mm_default_224: f32[1, 32000] = torch.ops.aten.mm.default(view_default_769, t_default_224);  view_default_769 = t_default_224 = None\\n        view_default_770: f32[1, 1, 32000] = torch.ops.aten.view.default(mm_default_224, [1, 1, 32000]);  mm_default_224 = None\\n        \\n        # File: /tmp/ipykernel_2064425/3922580714.py:45, code: state1_flat = [x[:, :, -2:-1, :] for x in state1_flat]\\n        slice_tensor_322: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_2, 0, 0, 9223372036854775807);  cat_default_2 = None\\n        slice_tensor_323: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_322, 1, 0, 9223372036854775807);  slice_tensor_322 = None\\n        slice_tensor_324: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_323, 2, -2, -1);  slice_tensor_323 = None\\n        slice_tensor_325: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_324, 3, 0, 9223372036854775807);  slice_tensor_324 = None\\n        slice_tensor_326: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_3, 0, 0, 9223372036854775807);  cat_default_3 = None\\n        slice_tensor_327: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_326, 1, 0, 9223372036854775807);  slice_tensor_326 = None\\n        slice_tensor_328: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_327, 2, -2, -1);  slice_tensor_327 = None\\n        slice_tensor_329: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_328, 3, 0, 9223372036854775807);  slice_tensor_328 = None\\n        slice_tensor_330: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_6, 0, 0, 9223372036854775807);  cat_default_6 = None\\n        slice_tensor_331: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_330, 1, 0, 9223372036854775807);  slice_tensor_330 = None\\n        slice_tensor_332: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_331, 2, -2, -1);  slice_tensor_331 = None\\n        slice_tensor_333: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_332, 3, 0, 9223372036854775807);  slice_tensor_332 = None\\n        slice_tensor_334: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_7, 0, 0, 9223372036854775807);  cat_default_7 = None\\n        slice_tensor_335: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_334, 1, 0, 9223372036854775807);  slice_tensor_334 = None\\n        slice_tensor_336: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_335, 2, -2, -1);  slice_tensor_335 = None\\n        slice_tensor_337: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_336, 3, 0, 9223372036854775807);  slice_tensor_336 = None\\n        slice_tensor_338: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_10, 0, 0, 9223372036854775807);  cat_default_10 = None\\n        slice_tensor_339: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_338, 1, 0, 9223372036854775807);  slice_tensor_338 = None\\n        slice_tensor_340: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_339, 2, -2, -1);  slice_tensor_339 = None\\n        slice_tensor_341: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_340, 3, 0, 9223372036854775807);  slice_tensor_340 = None\\n        slice_tensor_342: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_11, 0, 0, 9223372036854775807);  cat_default_11 = None\\n        slice_tensor_343: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_342, 1, 0, 9223372036854775807);  slice_tensor_342 = None\\n        slice_tensor_344: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_343, 2, -2, -1);  slice_tensor_343 = None\\n        slice_tensor_345: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_344, 3, 0, 9223372036854775807);  slice_tensor_344 = None\\n        slice_tensor_346: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_14, 0, 0, 9223372036854775807);  cat_default_14 = None\\n        slice_tensor_347: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_346, 1, 0, 9223372036854775807);  slice_tensor_346 = None\\n        slice_tensor_348: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_347, 2, -2, -1);  slice_tensor_347 = None\\n        slice_tensor_349: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_348, 3, 0, 9223372036854775807);  slice_tensor_348 = None\\n        slice_tensor_350: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_15, 0, 0, 9223372036854775807);  cat_default_15 = None\\n        slice_tensor_351: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_350, 1, 0, 9223372036854775807);  slice_tensor_350 = None\\n        slice_tensor_352: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_351, 2, -2, -1);  slice_tensor_351 = None\\n        slice_tensor_353: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_352, 3, 0, 9223372036854775807);  slice_tensor_352 = None\\n        slice_tensor_354: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_18, 0, 0, 9223372036854775807);  cat_default_18 = None\\n        slice_tensor_355: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_354, 1, 0, 9223372036854775807);  slice_tensor_354 = None\\n        slice_tensor_356: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_355, 2, -2, -1);  slice_tensor_355 = None\\n        slice_tensor_357: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_356, 3, 0, 9223372036854775807);  slice_tensor_356 = None\\n        slice_tensor_358: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_19, 0, 0, 9223372036854775807);  cat_default_19 = None\\n        slice_tensor_359: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_358, 1, 0, 9223372036854775807);  slice_tensor_358 = None\\n        slice_tensor_360: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_359, 2, -2, -1);  slice_tensor_359 = None\\n        slice_tensor_361: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_360, 3, 0, 9223372036854775807);  slice_tensor_360 = None\\n        slice_tensor_362: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_22, 0, 0, 9223372036854775807);  cat_default_22 = None\\n        slice_tensor_363: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_362, 1, 0, 9223372036854775807);  slice_tensor_362 = None\\n        slice_tensor_364: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_363, 2, -2, -1);  slice_tensor_363 = None\\n        slice_tensor_365: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_364, 3, 0, 9223372036854775807);  slice_tensor_364 = None\\n        slice_tensor_366: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_23, 0, 0, 9223372036854775807);  cat_default_23 = None\\n        slice_tensor_367: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_366, 1, 0, 9223372036854775807);  slice_tensor_366 = None\\n        slice_tensor_368: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_367, 2, -2, -1);  slice_tensor_367 = None\\n        slice_tensor_369: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_368, 3, 0, 9223372036854775807);  slice_tensor_368 = None\\n        slice_tensor_370: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_26, 0, 0, 9223372036854775807);  cat_default_26 = None\\n        slice_tensor_371: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_370, 1, 0, 9223372036854775807);  slice_tensor_370 = None\\n        slice_tensor_372: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_371, 2, -2, -1);  slice_tensor_371 = None\\n        slice_tensor_373: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_372, 3, 0, 9223372036854775807);  slice_tensor_372 = None\\n        slice_tensor_374: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_27, 0, 0, 9223372036854775807);  cat_default_27 = None\\n        slice_tensor_375: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_374, 1, 0, 9223372036854775807);  slice_tensor_374 = None\\n        slice_tensor_376: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_375, 2, -2, -1);  slice_tensor_375 = None\\n        slice_tensor_377: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_376, 3, 0, 9223372036854775807);  slice_tensor_376 = None\\n        slice_tensor_378: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_30, 0, 0, 9223372036854775807);  cat_default_30 = None\\n        slice_tensor_379: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_378, 1, 0, 9223372036854775807);  slice_tensor_378 = None\\n        slice_tensor_380: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_379, 2, -2, -1);  slice_tensor_379 = None\\n        slice_tensor_381: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_380, 3, 0, 9223372036854775807);  slice_tensor_380 = None\\n        slice_tensor_382: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_31, 0, 0, 9223372036854775807);  cat_default_31 = None\\n        slice_tensor_383: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_382, 1, 0, 9223372036854775807);  slice_tensor_382 = None\\n        slice_tensor_384: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_383, 2, -2, -1);  slice_tensor_383 = None\\n        slice_tensor_385: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_384, 3, 0, 9223372036854775807);  slice_tensor_384 = None\\n        slice_tensor_386: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_34, 0, 0, 9223372036854775807);  cat_default_34 = None\\n        slice_tensor_387: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_386, 1, 0, 9223372036854775807);  slice_tensor_386 = None\\n        slice_tensor_388: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_387, 2, -2, -1);  slice_tensor_387 = None\\n        slice_tensor_389: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_388, 3, 0, 9223372036854775807);  slice_tensor_388 = None\\n        slice_tensor_390: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_35, 0, 0, 9223372036854775807);  cat_default_35 = None\\n        slice_tensor_391: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_390, 1, 0, 9223372036854775807);  slice_tensor_390 = None\\n        slice_tensor_392: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_391, 2, -2, -1);  slice_tensor_391 = None\\n        slice_tensor_393: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_392, 3, 0, 9223372036854775807);  slice_tensor_392 = None\\n        slice_tensor_394: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_38, 0, 0, 9223372036854775807);  cat_default_38 = None\\n        slice_tensor_395: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_394, 1, 0, 9223372036854775807);  slice_tensor_394 = None\\n        slice_tensor_396: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_395, 2, -2, -1);  slice_tensor_395 = None\\n        slice_tensor_397: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_396, 3, 0, 9223372036854775807);  slice_tensor_396 = None\\n        slice_tensor_398: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_39, 0, 0, 9223372036854775807);  cat_default_39 = None\\n        slice_tensor_399: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_398, 1, 0, 9223372036854775807);  slice_tensor_398 = None\\n        slice_tensor_400: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_399, 2, -2, -1);  slice_tensor_399 = None\\n        slice_tensor_401: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_400, 3, 0, 9223372036854775807);  slice_tensor_400 = None\\n        slice_tensor_402: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_42, 0, 0, 9223372036854775807);  cat_default_42 = None\\n        slice_tensor_403: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_402, 1, 0, 9223372036854775807);  slice_tensor_402 = None\\n        slice_tensor_404: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_403, 2, -2, -1);  slice_tensor_403 = None\\n        slice_tensor_405: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_404, 3, 0, 9223372036854775807);  slice_tensor_404 = None\\n        slice_tensor_406: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_43, 0, 0, 9223372036854775807);  cat_default_43 = None\\n        slice_tensor_407: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_406, 1, 0, 9223372036854775807);  slice_tensor_406 = None\\n        slice_tensor_408: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_407, 2, -2, -1);  slice_tensor_407 = None\\n        slice_tensor_409: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_408, 3, 0, 9223372036854775807);  slice_tensor_408 = None\\n        slice_tensor_410: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_46, 0, 0, 9223372036854775807);  cat_default_46 = None\\n        slice_tensor_411: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_410, 1, 0, 9223372036854775807);  slice_tensor_410 = None\\n        slice_tensor_412: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_411, 2, -2, -1);  slice_tensor_411 = None\\n        slice_tensor_413: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_412, 3, 0, 9223372036854775807);  slice_tensor_412 = None\\n        slice_tensor_414: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_47, 0, 0, 9223372036854775807);  cat_default_47 = None\\n        slice_tensor_415: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_414, 1, 0, 9223372036854775807);  slice_tensor_414 = None\\n        slice_tensor_416: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_415, 2, -2, -1);  slice_tensor_415 = None\\n        slice_tensor_417: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_416, 3, 0, 9223372036854775807);  slice_tensor_416 = None\\n        slice_tensor_418: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_50, 0, 0, 9223372036854775807);  cat_default_50 = None\\n        slice_tensor_419: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_418, 1, 0, 9223372036854775807);  slice_tensor_418 = None\\n        slice_tensor_420: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_419, 2, -2, -1);  slice_tensor_419 = None\\n        slice_tensor_421: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_420, 3, 0, 9223372036854775807);  slice_tensor_420 = None\\n        slice_tensor_422: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_51, 0, 0, 9223372036854775807);  cat_default_51 = None\\n        slice_tensor_423: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_422, 1, 0, 9223372036854775807);  slice_tensor_422 = None\\n        slice_tensor_424: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_423, 2, -2, -1);  slice_tensor_423 = None\\n        slice_tensor_425: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_424, 3, 0, 9223372036854775807);  slice_tensor_424 = None\\n        slice_tensor_426: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_54, 0, 0, 9223372036854775807);  cat_default_54 = None\\n        slice_tensor_427: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_426, 1, 0, 9223372036854775807);  slice_tensor_426 = None\\n        slice_tensor_428: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_427, 2, -2, -1);  slice_tensor_427 = None\\n        slice_tensor_429: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_428, 3, 0, 9223372036854775807);  slice_tensor_428 = None\\n        slice_tensor_430: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_55, 0, 0, 9223372036854775807);  cat_default_55 = None\\n        slice_tensor_431: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_430, 1, 0, 9223372036854775807);  slice_tensor_430 = None\\n        slice_tensor_432: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_431, 2, -2, -1);  slice_tensor_431 = None\\n        slice_tensor_433: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_432, 3, 0, 9223372036854775807);  slice_tensor_432 = None\\n        slice_tensor_434: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_58, 0, 0, 9223372036854775807);  cat_default_58 = None\\n        slice_tensor_435: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_434, 1, 0, 9223372036854775807);  slice_tensor_434 = None\\n        slice_tensor_436: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_435, 2, -2, -1);  slice_tensor_435 = None\\n        slice_tensor_437: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_436, 3, 0, 9223372036854775807);  slice_tensor_436 = None\\n        slice_tensor_438: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_59, 0, 0, 9223372036854775807);  cat_default_59 = None\\n        slice_tensor_439: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_438, 1, 0, 9223372036854775807);  slice_tensor_438 = None\\n        slice_tensor_440: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_439, 2, -2, -1);  slice_tensor_439 = None\\n        slice_tensor_441: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_440, 3, 0, 9223372036854775807);  slice_tensor_440 = None\\n        slice_tensor_442: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_62, 0, 0, 9223372036854775807);  cat_default_62 = None\\n        slice_tensor_443: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_442, 1, 0, 9223372036854775807);  slice_tensor_442 = None\\n        slice_tensor_444: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_443, 2, -2, -1);  slice_tensor_443 = None\\n        slice_tensor_445: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_444, 3, 0, 9223372036854775807);  slice_tensor_444 = None\\n        slice_tensor_446: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_63, 0, 0, 9223372036854775807);  cat_default_63 = None\\n        slice_tensor_447: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_446, 1, 0, 9223372036854775807);  slice_tensor_446 = None\\n        slice_tensor_448: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_447, 2, -2, -1);  slice_tensor_447 = None\\n        slice_tensor_449: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_448, 3, 0, 9223372036854775807);  slice_tensor_448 = None\\n        slice_tensor_450: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_66, 0, 0, 9223372036854775807);  cat_default_66 = None\\n        slice_tensor_451: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_450, 1, 0, 9223372036854775807);  slice_tensor_450 = None\\n        slice_tensor_452: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_451, 2, -2, -1);  slice_tensor_451 = None\\n        slice_tensor_453: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_452, 3, 0, 9223372036854775807);  slice_tensor_452 = None\\n        slice_tensor_454: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_67, 0, 0, 9223372036854775807);  cat_default_67 = None\\n        slice_tensor_455: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_454, 1, 0, 9223372036854775807);  slice_tensor_454 = None\\n        slice_tensor_456: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_455, 2, -2, -1);  slice_tensor_455 = None\\n        slice_tensor_457: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_456, 3, 0, 9223372036854775807);  slice_tensor_456 = None\\n        slice_tensor_458: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_70, 0, 0, 9223372036854775807);  cat_default_70 = None\\n        slice_tensor_459: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_458, 1, 0, 9223372036854775807);  slice_tensor_458 = None\\n        slice_tensor_460: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_459, 2, -2, -1);  slice_tensor_459 = None\\n        slice_tensor_461: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_460, 3, 0, 9223372036854775807);  slice_tensor_460 = None\\n        slice_tensor_462: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_71, 0, 0, 9223372036854775807);  cat_default_71 = None\\n        slice_tensor_463: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_462, 1, 0, 9223372036854775807);  slice_tensor_462 = None\\n        slice_tensor_464: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_463, 2, -2, -1);  slice_tensor_463 = None\\n        slice_tensor_465: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_464, 3, 0, 9223372036854775807);  slice_tensor_464 = None\\n        slice_tensor_466: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_74, 0, 0, 9223372036854775807);  cat_default_74 = None\\n        slice_tensor_467: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_466, 1, 0, 9223372036854775807);  slice_tensor_466 = None\\n        slice_tensor_468: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_467, 2, -2, -1);  slice_tensor_467 = None\\n        slice_tensor_469: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_468, 3, 0, 9223372036854775807);  slice_tensor_468 = None\\n        slice_tensor_470: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_75, 0, 0, 9223372036854775807);  cat_default_75 = None\\n        slice_tensor_471: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_470, 1, 0, 9223372036854775807);  slice_tensor_470 = None\\n        slice_tensor_472: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_471, 2, -2, -1);  slice_tensor_471 = None\\n        slice_tensor_473: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_472, 3, 0, 9223372036854775807);  slice_tensor_472 = None\\n        slice_tensor_474: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_78, 0, 0, 9223372036854775807);  cat_default_78 = None\\n        slice_tensor_475: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_474, 1, 0, 9223372036854775807);  slice_tensor_474 = None\\n        slice_tensor_476: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_475, 2, -2, -1);  slice_tensor_475 = None\\n        slice_tensor_477: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_476, 3, 0, 9223372036854775807);  slice_tensor_476 = None\\n        slice_tensor_478: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_79, 0, 0, 9223372036854775807);  cat_default_79 = None\\n        slice_tensor_479: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_478, 1, 0, 9223372036854775807);  slice_tensor_478 = None\\n        slice_tensor_480: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_479, 2, -2, -1);  slice_tensor_479 = None\\n        slice_tensor_481: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_480, 3, 0, 9223372036854775807);  slice_tensor_480 = None\\n        slice_tensor_482: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_82, 0, 0, 9223372036854775807);  cat_default_82 = None\\n        slice_tensor_483: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_482, 1, 0, 9223372036854775807);  slice_tensor_482 = None\\n        slice_tensor_484: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_483, 2, -2, -1);  slice_tensor_483 = None\\n        slice_tensor_485: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_484, 3, 0, 9223372036854775807);  slice_tensor_484 = None\\n        slice_tensor_486: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_83, 0, 0, 9223372036854775807);  cat_default_83 = None\\n        slice_tensor_487: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_486, 1, 0, 9223372036854775807);  slice_tensor_486 = None\\n        slice_tensor_488: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_487, 2, -2, -1);  slice_tensor_487 = None\\n        slice_tensor_489: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_488, 3, 0, 9223372036854775807);  slice_tensor_488 = None\\n        slice_tensor_490: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_86, 0, 0, 9223372036854775807);  cat_default_86 = None\\n        slice_tensor_491: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_490, 1, 0, 9223372036854775807);  slice_tensor_490 = None\\n        slice_tensor_492: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_491, 2, -2, -1);  slice_tensor_491 = None\\n        slice_tensor_493: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_492, 3, 0, 9223372036854775807);  slice_tensor_492 = None\\n        slice_tensor_494: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_87, 0, 0, 9223372036854775807);  cat_default_87 = None\\n        slice_tensor_495: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_494, 1, 0, 9223372036854775807);  slice_tensor_494 = None\\n        slice_tensor_496: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_495, 2, -2, -1);  slice_tensor_495 = None\\n        slice_tensor_497: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_496, 3, 0, 9223372036854775807);  slice_tensor_496 = None\\n        slice_tensor_498: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_90, 0, 0, 9223372036854775807);  cat_default_90 = None\\n        slice_tensor_499: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_498, 1, 0, 9223372036854775807);  slice_tensor_498 = None\\n        slice_tensor_500: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_499, 2, -2, -1);  slice_tensor_499 = None\\n        slice_tensor_501: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_500, 3, 0, 9223372036854775807);  slice_tensor_500 = None\\n        slice_tensor_502: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_91, 0, 0, 9223372036854775807);  cat_default_91 = None\\n        slice_tensor_503: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_502, 1, 0, 9223372036854775807);  slice_tensor_502 = None\\n        slice_tensor_504: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_503, 2, -2, -1);  slice_tensor_503 = None\\n        slice_tensor_505: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_504, 3, 0, 9223372036854775807);  slice_tensor_504 = None\\n        slice_tensor_506: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_94, 0, 0, 9223372036854775807);  cat_default_94 = None\\n        slice_tensor_507: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_506, 1, 0, 9223372036854775807);  slice_tensor_506 = None\\n        slice_tensor_508: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_507, 2, -2, -1);  slice_tensor_507 = None\\n        slice_tensor_509: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_508, 3, 0, 9223372036854775807);  slice_tensor_508 = None\\n        slice_tensor_510: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_95, 0, 0, 9223372036854775807);  cat_default_95 = None\\n        slice_tensor_511: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_510, 1, 0, 9223372036854775807);  slice_tensor_510 = None\\n        slice_tensor_512: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_511, 2, -2, -1);  slice_tensor_511 = None\\n        slice_tensor_513: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_512, 3, 0, 9223372036854775807);  slice_tensor_512 = None\\n        slice_tensor_514: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_98, 0, 0, 9223372036854775807);  cat_default_98 = None\\n        slice_tensor_515: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_514, 1, 0, 9223372036854775807);  slice_tensor_514 = None\\n        slice_tensor_516: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_515, 2, -2, -1);  slice_tensor_515 = None\\n        slice_tensor_517: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_516, 3, 0, 9223372036854775807);  slice_tensor_516 = None\\n        slice_tensor_518: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_99, 0, 0, 9223372036854775807);  cat_default_99 = None\\n        slice_tensor_519: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_518, 1, 0, 9223372036854775807);  slice_tensor_518 = None\\n        slice_tensor_520: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_519, 2, -2, -1);  slice_tensor_519 = None\\n        slice_tensor_521: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_520, 3, 0, 9223372036854775807);  slice_tensor_520 = None\\n        slice_tensor_522: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_102, 0, 0, 9223372036854775807);  cat_default_102 = None\\n        slice_tensor_523: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_522, 1, 0, 9223372036854775807);  slice_tensor_522 = None\\n        slice_tensor_524: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_523, 2, -2, -1);  slice_tensor_523 = None\\n        slice_tensor_525: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_524, 3, 0, 9223372036854775807);  slice_tensor_524 = None\\n        slice_tensor_526: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_103, 0, 0, 9223372036854775807);  cat_default_103 = None\\n        slice_tensor_527: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_526, 1, 0, 9223372036854775807);  slice_tensor_526 = None\\n        slice_tensor_528: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_527, 2, -2, -1);  slice_tensor_527 = None\\n        slice_tensor_529: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_528, 3, 0, 9223372036854775807);  slice_tensor_528 = None\\n        slice_tensor_530: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_106, 0, 0, 9223372036854775807);  cat_default_106 = None\\n        slice_tensor_531: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_530, 1, 0, 9223372036854775807);  slice_tensor_530 = None\\n        slice_tensor_532: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_531, 2, -2, -1);  slice_tensor_531 = None\\n        slice_tensor_533: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_532, 3, 0, 9223372036854775807);  slice_tensor_532 = None\\n        slice_tensor_534: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_107, 0, 0, 9223372036854775807);  cat_default_107 = None\\n        slice_tensor_535: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_534, 1, 0, 9223372036854775807);  slice_tensor_534 = None\\n        slice_tensor_536: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_535, 2, -2, -1);  slice_tensor_535 = None\\n        slice_tensor_537: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_536, 3, 0, 9223372036854775807);  slice_tensor_536 = None\\n        slice_tensor_538: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_110, 0, 0, 9223372036854775807);  cat_default_110 = None\\n        slice_tensor_539: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_538, 1, 0, 9223372036854775807);  slice_tensor_538 = None\\n        slice_tensor_540: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_539, 2, -2, -1);  slice_tensor_539 = None\\n        slice_tensor_541: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_540, 3, 0, 9223372036854775807);  slice_tensor_540 = None\\n        slice_tensor_542: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_111, 0, 0, 9223372036854775807);  cat_default_111 = None\\n        slice_tensor_543: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_542, 1, 0, 9223372036854775807);  slice_tensor_542 = None\\n        slice_tensor_544: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_543, 2, -2, -1);  slice_tensor_543 = None\\n        slice_tensor_545: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_544, 3, 0, 9223372036854775807);  slice_tensor_544 = None\\n        slice_tensor_546: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_114, 0, 0, 9223372036854775807);  cat_default_114 = None\\n        slice_tensor_547: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_546, 1, 0, 9223372036854775807);  slice_tensor_546 = None\\n        slice_tensor_548: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_547, 2, -2, -1);  slice_tensor_547 = None\\n        slice_tensor_549: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_548, 3, 0, 9223372036854775807);  slice_tensor_548 = None\\n        slice_tensor_550: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_115, 0, 0, 9223372036854775807);  cat_default_115 = None\\n        slice_tensor_551: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_550, 1, 0, 9223372036854775807);  slice_tensor_550 = None\\n        slice_tensor_552: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_551, 2, -2, -1);  slice_tensor_551 = None\\n        slice_tensor_553: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_552, 3, 0, 9223372036854775807);  slice_tensor_552 = None\\n        slice_tensor_554: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_118, 0, 0, 9223372036854775807);  cat_default_118 = None\\n        slice_tensor_555: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_554, 1, 0, 9223372036854775807);  slice_tensor_554 = None\\n        slice_tensor_556: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_555, 2, -2, -1);  slice_tensor_555 = None\\n        slice_tensor_557: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_556, 3, 0, 9223372036854775807);  slice_tensor_556 = None\\n        slice_tensor_558: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_119, 0, 0, 9223372036854775807);  cat_default_119 = None\\n        slice_tensor_559: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_558, 1, 0, 9223372036854775807);  slice_tensor_558 = None\\n        slice_tensor_560: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_559, 2, -2, -1);  slice_tensor_559 = None\\n        slice_tensor_561: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_560, 3, 0, 9223372036854775807);  slice_tensor_560 = None\\n        slice_tensor_562: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_122, 0, 0, 9223372036854775807);  cat_default_122 = None\\n        slice_tensor_563: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_562, 1, 0, 9223372036854775807);  slice_tensor_562 = None\\n        slice_tensor_564: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_563, 2, -2, -1);  slice_tensor_563 = None\\n        slice_tensor_565: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_564, 3, 0, 9223372036854775807);  slice_tensor_564 = None\\n        slice_tensor_566: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_123, 0, 0, 9223372036854775807);  cat_default_123 = None\\n        slice_tensor_567: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_566, 1, 0, 9223372036854775807);  slice_tensor_566 = None\\n        slice_tensor_568: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_567, 2, -2, -1);  slice_tensor_567 = None\\n        slice_tensor_569: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_568, 3, 0, 9223372036854775807);  slice_tensor_568 = None\\n        slice_tensor_570: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_126, 0, 0, 9223372036854775807);  cat_default_126 = None\\n        slice_tensor_571: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_570, 1, 0, 9223372036854775807);  slice_tensor_570 = None\\n        slice_tensor_572: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_571, 2, -2, -1);  slice_tensor_571 = None\\n        slice_tensor_573: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_572, 3, 0, 9223372036854775807);  slice_tensor_572 = None\\n        slice_tensor_574: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(cat_default_127, 0, 0, 9223372036854775807);  cat_default_127 = None\\n        slice_tensor_575: f32[1, 32, s0 + 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_574, 1, 0, 9223372036854775807);  slice_tensor_574 = None\\n        slice_tensor_576: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_575, 2, -2, -1);  slice_tensor_575 = None\\n        slice_tensor_577: f32[1, 32, 1, 128] = torch.ops.aten.slice.Tensor(slice_tensor_576, 3, 0, 9223372036854775807);  slice_tensor_576 = None\\n        \\n        # File: /tmp/ipykernel_2064425/3922580714.py:46, code: token1 = torch.argmax(result.logits[:, -1, :], dim=1)\\n        slice_tensor_578: f32[1, 1, 32000] = torch.ops.aten.slice.Tensor(view_default_770, 0, 0, 9223372036854775807);  view_default_770 = None\\n        select_int: f32[1, 32000] = torch.ops.aten.select.int(slice_tensor_578, 1, -1);  slice_tensor_578 = None\\n        slice_tensor_579: f32[1, 32000] = torch.ops.aten.slice.Tensor(select_int, 1, 0, 9223372036854775807);  select_int = None\\n        argmax_default: i64[1] = torch.ops.aten.argmax.default(slice_tensor_579, 1);  slice_tensor_579 = None\\n        return pytree.tree_unflatten([argmax_default, slice_tensor_325, slice_tensor_329, slice_tensor_333, slice_tensor_337, slice_tensor_341, slice_tensor_345, slice_tensor_349, slice_tensor_353, slice_tensor_357, slice_tensor_361, slice_tensor_365, slice_tensor_369, slice_tensor_373, slice_tensor_377, slice_tensor_381, slice_tensor_385, slice_tensor_389, slice_tensor_393, slice_tensor_397, slice_tensor_401, slice_tensor_405, slice_tensor_409, slice_tensor_413, slice_tensor_417, slice_tensor_421, slice_tensor_425, slice_tensor_429, slice_tensor_433, slice_tensor_437, slice_tensor_441, slice_tensor_445, slice_tensor_449, slice_tensor_453, slice_tensor_457, slice_tensor_461, slice_tensor_465, slice_tensor_469, slice_tensor_473, slice_tensor_477, slice_tensor_481, slice_tensor_485, slice_tensor_489, slice_tensor_493, slice_tensor_497, slice_tensor_501, slice_tensor_505, slice_tensor_509, slice_tensor_513, slice_tensor_517, slice_tensor_521, slice_tensor_525, slice_tensor_529, slice_tensor_533, slice_tensor_537, slice_tensor_541, slice_tensor_545, slice_tensor_549, slice_tensor_553, slice_tensor_557, slice_tensor_561, slice_tensor_565, slice_tensor_569, slice_tensor_573, slice_tensor_577], self._out_spec)\\n        \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export forward\n",
    "exp_forward = dynamo.export(\n",
    "    sm.forward, \n",
    "    aten_graph=True,\n",
    "    assume_static_by_default=True,\n",
    "    # Constrain the first state dim and then form an equality\n",
    "    # on all of the others. If we don't specify sufficient constraints\n",
    "    # for these, Dynamo will print two pages of a copy-pastable version\n",
    "    # of basically this based on what it found in the graph but wants\n",
    "    # you to be explicit about.\n",
    "    constraints= [\n",
    "        dynamic_dim(example_state0[0], 2) < 4095\n",
    "    ] + [\n",
    "        (dynamic_dim(x, 2) == (dynamic_dim(example_state0[0], 2))) for x in example_state0[1:]\n",
    "    ],\n",
    ")\n",
    "g, guards = exp_forward(example_token0, *example_state0)\n",
    "g.print_readable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
