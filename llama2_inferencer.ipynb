{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-4.24.2-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "Requirement already satisfied: requests in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.8.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: fsspec in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stella/src/venv/Turbine/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, tqdm, regex, pyyaml, protobuf, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 protobuf-4.24.2 pyyaml-6.0.1 regex-2023.8.8 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.32.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece protobuf transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils import _pytree as pytree\n",
    "import textwrap\n",
    "AUTH_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.float,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    use_fast=False,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results):\n",
    "    past_key_values, _ = pytree.tree_flatten(results.past_key_values)\n",
    "    print(\"Logits:\", pytree.tree_map(lambda x: x.shape, results.logits))\n",
    "    print(f\"PKV (len={len(past_key_values)}):\")\n",
    "    count = 0\n",
    "    prev = \"\"\n",
    "    for s in pytree.tree_map(lambda x: repr(x.shape), past_key_values):\n",
    "        if s == prev:\n",
    "            count += 1\n",
    "            continue\n",
    "        elif count:\n",
    "            print(\" \", s, f\"* {count+1}\" if count else \"\")\n",
    "            count = 0\n",
    "        prev = s\n",
    "    if count:\n",
    "        print(\" \", s, f\"* {count+1}\" if count else \"\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: {'input_ids': tensor([[    1,  2184, 29901,   887,   526,   263,  8444, 29892,  3390,  1319,\n",
      "           322, 15993, 20255, 29889, 29849,  1234,   408,  1371,  3730,   408,\n",
      "          1950, 29892,  1550,  1641,  9109, 29889, 29871,  3575,  6089,   881,\n",
      "           451,  3160,   738, 10311,  1319, 29892,   443,   621,   936, 29892,\n",
      "         11021,   391, 29892,  7916,   391, 29892,   304, 27375, 29892, 18215,\n",
      "         29892,   470, 27302,  2793, 29889,  3529,  9801,   393,   596, 20890,\n",
      "           526,  5374,   635,   443,  5365,  1463,   322,  6374,   297,  5469,\n",
      "         29889,   960,   263,  1139,   947,   451,  1207,   738,  4060, 29892,\n",
      "           470,   338,   451,  2114,  1474, 16165,   261,   296, 29892,  5649,\n",
      "          2020,  2012,   310, 22862,  1554,   451,  1959, 29889,   960,   366,\n",
      "          1016, 29915, 29873,  1073,   278,  1234,   304,   263,  1139, 29892,\n",
      "          3113,  1016, 29915, 29873,  6232,  2089,  2472, 19423, 29989, 11889,\n",
      "         29989, 29958, 26857,   350, 16926, 27105,  1460,   505,  6077,  2175,\n",
      "           472,   838,  2423,  7808,   802, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "  Shape: torch.Size([1, 136])\n",
      "Logits: torch.Size([1, 136, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 136, 128]) * 64\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "        \"System: You are a helpful, respectful and honest assistant. Always answer \"\n",
    "        \"as helpfully as possible, while being safe.  Your answers should not \"\n",
    "        \"include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal \"\n",
    "        \"content. Please ensure that your responses are socially unbiased and positive \"\n",
    "        \"in nature. If a question does not make any sense, or is not factually coherent, \"\n",
    "        \"explain why instead of answering something not correct. If you don't know the \"\n",
    "        \"answer to a question, please don't share false information.\"\n",
    "    )\n",
    "conversation = prompt + \"<|USER|>Should Bugs Bunny have turned left at Albuquerque?\"\n",
    "\n",
    "initial_input = tokenizer(conversation, return_tensors=\"pt\")\n",
    "print(\"Example input:\", initial_input)\n",
    "print(\"  Shape:\", initial_input.input_ids.shape)\n",
    "initial_results = mdl.forward(initial_input.input_ids)\n",
    "summarize_results(initial_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: torch.Size([1, 136, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '</' (tensor([829]))\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "all_detoks = []\n",
    "def decode_token(results, index=-1, store=True):\n",
    "    print(\"Logits:\", results.logits.shape)\n",
    "    print(\"Logits reshaped:\", results.logits[:, index, :].shape)\n",
    "    token = torch.argmax(results.logits[:, index, :], dim=1)\n",
    "    detok = tokenizer.decode(token, skip_special_tokens=False)\n",
    "    print(f\"--> Decoded: '{detok}' ({token})\")\n",
    "    if store:\n",
    "        all_tokens.append(token[0])\n",
    "        all_detoks.append(detok)\n",
    "    return token, detok\n",
    "\n",
    "# Decode initial token\n",
    "# for i in range(initial_results.logits.shape[1]):\n",
    "#     token, detok = decode_token(initial_results, index=i)\n",
    "token, detok = decode_token(initial_results, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next input token: tensor([[829]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 137, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'user' (tensor([1792]))\n",
      "Next input token: tensor([[1792]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 138, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '>' (tensor([29958]))\n",
      "Next input token: tensor([[29958]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 139, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '' (tensor([29871]))\n",
      "Next input token: tensor([[29871]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 140, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'I' (tensor([306]))\n",
      "Next input token: tensor([[306]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 141, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: ''' (tensor([29915]))\n",
      "Next input token: tensor([[29915]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 142, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'm' (tensor([29885]))\n",
      "Next input token: tensor([[29885]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 143, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'just' (tensor([925]))\n",
      "Next input token: tensor([[925]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 144, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'an' (tensor([385]))\n",
      "Next input token: tensor([[385]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 145, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'A' (tensor([319]))\n",
      "Next input token: tensor([[319]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 146, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'I' (tensor([29902]))\n",
      "Next input token: tensor([[29902]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 147, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: ',' (tensor([29892]))\n",
      "Next input token: tensor([[29892]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 148, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'I' (tensor([306]))\n",
      "Next input token: tensor([[306]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 149, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'don' (tensor([1016]))\n",
      "Next input token: tensor([[1016]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 150, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: ''' (tensor([29915]))\n",
      "Next input token: tensor([[29915]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 151, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 't' (tensor([29873]))\n",
      "Next input token: tensor([[29873]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 152, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'have' (tensor([505]))\n",
      "Next input token: tensor([[505]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 153, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'access' (tensor([2130]))\n",
      "Next input token: tensor([[2130]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 154, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'to' (tensor([304]))\n",
      "Next input token: tensor([[304]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 155, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'the' (tensor([278]))\n",
      "Next input token: tensor([[278]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 156, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'personal' (tensor([7333]))\n",
      "Next input token: tensor([[7333]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 157, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'information' (tensor([2472]))\n",
      "Next input token: tensor([[2472]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 158, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'or' (tensor([470]))\n",
      "Next input token: tensor([[470]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 159, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'context' (tensor([3030]))\n",
      "Next input token: tensor([[3030]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 160, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'of' (tensor([310]))\n",
      "Next input token: tensor([[310]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 161, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'B' (tensor([350]))\n",
      "Next input token: tensor([[350]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 162, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ugs' (tensor([16926]))\n",
      "Next input token: tensor([[16926]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 163, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'Bun' (tensor([27105]))\n",
      "Next input token: tensor([[27105]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 164, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ny' (tensor([1460]))\n",
      "Next input token: tensor([[1460]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 165, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'or' (tensor([470]))\n",
      "Next input token: tensor([[470]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 166, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'his' (tensor([670]))\n",
      "Next input token: tensor([[670]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 167, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'travel' (tensor([9850]))\n",
      "Next input token: tensor([[9850]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 168, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'plans' (tensor([13900]))\n",
      "Next input token: tensor([[13900]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 169, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: ',' (tensor([29892]))\n",
      "Next input token: tensor([[29892]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 170, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'so' (tensor([577]))\n",
      "Next input token: tensor([[577]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 171, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'I' (tensor([306]))\n",
      "Next input token: tensor([[306]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 172, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'cannot' (tensor([2609]))\n",
      "Next input token: tensor([[2609]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 173, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'provide' (tensor([3867]))\n",
      "Next input token: tensor([[3867]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 174, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'a' (tensor([263]))\n",
      "Next input token: tensor([[263]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 175, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'definit' (tensor([8422]))\n",
      "Next input token: tensor([[8422]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 176, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ive' (tensor([573]))\n",
      "Next input token: tensor([[573]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 177, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'answer' (tensor([1234]))\n",
      "Next input token: tensor([[1234]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 178, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'to' (tensor([304]))\n",
      "Next input token: tensor([[304]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 179, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'whether' (tensor([3692]))\n",
      "Next input token: tensor([[3692]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 180, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'he' (tensor([540]))\n",
      "Next input token: tensor([[540]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 181, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'should' (tensor([881]))\n",
      "Next input token: tensor([[881]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 182, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'have' (tensor([505]))\n",
      "Next input token: tensor([[505]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 183, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'turned' (tensor([6077]))\n",
      "Next input token: tensor([[6077]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 184, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'left' (tensor([2175]))\n",
      "Next input token: tensor([[2175]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 185, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'at' (tensor([472]))\n",
      "Next input token: tensor([[472]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 186, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'Al' (tensor([838]))\n",
      "Next input token: tensor([[838]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 187, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'bu' (tensor([2423]))\n",
      "Next input token: tensor([[2423]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 188, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'quer' (tensor([7808]))\n",
      "Next input token: tensor([[7808]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 189, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'que' (tensor([802]))\n",
      "Next input token: tensor([[802]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 190, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'or' (tensor([470]))\n",
      "Next input token: tensor([[470]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 191, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'not' (tensor([451]))\n",
      "Next input token: tensor([[451]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 192, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '.' (tensor([29889]))\n",
      "Next input token: tensor([[29889]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 193, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'Additionally' (tensor([19814]))\n",
      "Next input token: tensor([[19814]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 194, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: ',' (tensor([29892]))\n",
      "Next input token: tensor([[29892]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 195, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'it' (tensor([372]))\n",
      "Next input token: tensor([[372]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 196, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'is' (tensor([338]))\n",
      "Next input token: tensor([[338]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 197, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'important' (tensor([4100]))\n",
      "Next input token: tensor([[4100]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 198, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'to' (tensor([304]))\n",
      "Next input token: tensor([[304]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 199, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'note' (tensor([4443]))\n",
      "Next input token: tensor([[4443]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 200, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'that' (tensor([393]))\n",
      "Next input token: tensor([[393]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 201, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'B' (tensor([350]))\n",
      "Next input token: tensor([[350]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 202, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ugs' (tensor([16926]))\n",
      "Next input token: tensor([[16926]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 203, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'Bun' (tensor([27105]))\n",
      "Next input token: tensor([[27105]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 204, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ny' (tensor([1460]))\n",
      "Next input token: tensor([[1460]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 205, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'is' (tensor([338]))\n",
      "Next input token: tensor([[338]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 206, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'a' (tensor([263]))\n",
      "Next input token: tensor([[263]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 207, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'fict' (tensor([26797]))\n",
      "Next input token: tensor([[26797]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 208, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ional' (tensor([1848]))\n",
      "Next input token: tensor([[1848]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 209, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'character' (tensor([2931]))\n",
      "Next input token: tensor([[2931]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 210, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'and' (tensor([322]))\n",
      "Next input token: tensor([[322]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 211, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'his' (tensor([670]))\n",
      "Next input token: tensor([[670]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 212, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'actions' (tensor([8820]))\n",
      "Next input token: tensor([[8820]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 213, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'are' (tensor([526]))\n",
      "Next input token: tensor([[526]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 214, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'not' (tensor([451]))\n",
      "Next input token: tensor([[451]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 215, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'meant' (tensor([6839]))\n",
      "Next input token: tensor([[6839]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 216, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'to' (tensor([304]))\n",
      "Next input token: tensor([[304]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 217, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'be' (tensor([367]))\n",
      "Next input token: tensor([[367]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 218, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'taken' (tensor([4586]))\n",
      "Next input token: tensor([[4586]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 219, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'as' (tensor([408]))\n",
      "Next input token: tensor([[408]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 220, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'real' (tensor([1855]))\n",
      "Next input token: tensor([[1855]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 221, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '-' (tensor([29899]))\n",
      "Next input token: tensor([[29899]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 222, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'world' (tensor([11526]))\n",
      "Next input token: tensor([[11526]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 223, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'advice' (tensor([9848]))\n",
      "Next input token: tensor([[9848]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 224, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: '.' (tensor([29889]))\n",
      "Next input token: tensor([[29889]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 225, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'It' (tensor([739]))\n",
      "Next input token: tensor([[739]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 226, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'is' (tensor([338]))\n",
      "Next input token: tensor([[338]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 227, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'always' (tensor([2337]))\n",
      "Next input token: tensor([[2337]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 228, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'best' (tensor([1900]))\n",
      "Next input token: tensor([[1900]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 229, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'to' (tensor([304]))\n",
      "Next input token: tensor([[304]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 230, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'rely' (tensor([19104]))\n",
      "Next input token: tensor([[19104]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 231, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'on' (tensor([373]))\n",
      "Next input token: tensor([[373]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 232, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'fact' (tensor([2114]))\n",
      "Next input token: tensor([[2114]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 233, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'ual' (tensor([950]))\n",
      "Next input token: tensor([[950]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 234, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'information' (tensor([2472]))\n",
      "Next input token: tensor([[2472]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 235, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'and' (tensor([322]))\n",
      "Next input token: tensor([[322]])\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "PKV (len=64):\n",
      "  torch.Size([1, 32, 236, 128]) * 64\n",
      "Logits: torch.Size([1, 1, 32000])\n",
      "Logits reshaped: torch.Size([1, 32000])\n",
      "--> Decoded: 'safe' (tensor([9109]))\n",
      "All tokens: [tensor(829), tensor(1792), tensor(29958), tensor(29871), tensor(306), tensor(29915), tensor(29885), tensor(925), tensor(385), tensor(319), tensor(29902), tensor(29892), tensor(306), tensor(1016), tensor(29915), tensor(29873), tensor(505), tensor(2130), tensor(304), tensor(278), tensor(7333), tensor(2472), tensor(470), tensor(3030), tensor(310), tensor(350), tensor(16926), tensor(27105), tensor(1460), tensor(470), tensor(670), tensor(9850), tensor(13900), tensor(29892), tensor(577), tensor(306), tensor(2609), tensor(3867), tensor(263), tensor(8422), tensor(573), tensor(1234), tensor(304), tensor(3692), tensor(540), tensor(881), tensor(505), tensor(6077), tensor(2175), tensor(472), tensor(838), tensor(2423), tensor(7808), tensor(802), tensor(470), tensor(451), tensor(29889), tensor(19814), tensor(29892), tensor(372), tensor(338), tensor(4100), tensor(304), tensor(4443), tensor(393), tensor(350), tensor(16926), tensor(27105), tensor(1460), tensor(338), tensor(263), tensor(26797), tensor(1848), tensor(2931), tensor(322), tensor(670), tensor(8820), tensor(526), tensor(451), tensor(6839), tensor(304), tensor(367), tensor(4586), tensor(408), tensor(1855), tensor(29899), tensor(11526), tensor(9848), tensor(29889), tensor(739), tensor(338), tensor(2337), tensor(1900), tensor(304), tensor(19104), tensor(373), tensor(2114), tensor(950), tensor(2472), tensor(322), tensor(9109)]\n",
      "All detoks: ['</', 'user', '>', '', 'I', \"'\", 'm', 'just', 'an', 'A', 'I', ',', 'I', 'don', \"'\", 't', 'have', 'access', 'to', 'the', 'personal', 'information', 'or', 'context', 'of', 'B', 'ugs', 'Bun', 'ny', 'or', 'his', 'travel', 'plans', ',', 'so', 'I', 'cannot', 'provide', 'a', 'definit', 'ive', 'answer', 'to', 'whether', 'he', 'should', 'have', 'turned', 'left', 'at', 'Al', 'bu', 'quer', 'que', 'or', 'not', '.', 'Additionally', ',', 'it', 'is', 'important', 'to', 'note', 'that', 'B', 'ugs', 'Bun', 'ny', 'is', 'a', 'fict', 'ional', 'character', 'and', 'his', 'actions', 'are', 'not', 'meant', 'to', 'be', 'taken', 'as', 'real', '-', 'world', 'advice', '.', 'It', 'is', 'always', 'best', 'to', 'rely', 'on', 'fact', 'ual', 'information', 'and', 'safe']\n",
      "System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|USER|>Should Bugs Bunny have turned left at Albuquerque?\n",
      "</user>  I'm just an AI, I don't have access to the personal information or context of Bugs Bunny or his travel plans, so I cannot provide a definitive answer to whether he should have turned left at Albuquerque or not. Additionally, it is important to note that Bugs Bunny is a fictional character and his actions are not meant to be taken as real-world advice. It is always best to rely on factual information and safe\n"
     ]
    }
   ],
   "source": [
    "# Decode loop for subsequent tokens.\n",
    "current_results = initial_results\n",
    "for _ in range(100):\n",
    "    prior_pkvs, _ = pytree.tree_flatten(current_results.past_key_values)\n",
    "    next_input_token = torch.reshape(token, [1, 1])\n",
    "    print(\"Next input token:\", next_input_token)\n",
    "    step_results = mdl.forward(next_input_token, past_key_values=current_results.past_key_values)\n",
    "    summarize_results(step_results)\n",
    "    token, detok = decode_token(step_results)\n",
    "    if token[0] == 2:\n",
    "        break\n",
    "    current_results = step_results\n",
    "\n",
    "    current_pkvs, _ = pytree.tree_flatten(current_results.past_key_values)\n",
    "    pkv_len = prior_pkvs[0].shape[2]\n",
    "    for check_step in range(pkv_len):\n",
    "        for left, right in zip(prior_pkvs, current_pkvs):\n",
    "            if not torch.equal(left[:, :, check_step, :], right[:, :, check_step, :]):\n",
    "                print(f\"PKVS MISMATCH AT STEP {check_step}!\")\n",
    "\n",
    "print(\"All tokens:\", all_tokens)\n",
    "print(\"All detoks:\", all_detoks)\n",
    "\n",
    "print(conversation)\n",
    "print(tokenizer.decode(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to FX Trace the initialization graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2226,  0.0299,  0.2729,  ...,  1.4124,  1.9937,  0.7167],\n",
       "          [-7.0586, -6.1635, -1.1156,  ..., -3.6511, -4.9533, -6.6091],\n",
       "          [-5.2646, -5.7587, -0.9062,  ..., -4.5840, -3.2360, -5.7636],\n",
       "          ...,\n",
       "          [-4.6406, -1.7202, 11.8724,  ..., -1.1916, -3.8464, -3.2414],\n",
       "          [ 0.0742,  0.8039, 13.5591,  ..., -1.0009,  1.9355, -0.8619],\n",
       "          [-5.0538, -8.4153, 13.4058,  ..., -4.3091, -0.8193, -1.8111]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " 136)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch._export.constraints import constrain_as_size, constrain_as_value\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "MAX_STEP_SEQ = 512\n",
    "empty_states = pytree.tree_map(\n",
    "    lambda x: torch.zeros(\n",
    "        BATCH_SIZE, x.shape[1], MAX_STEP_SEQ, x.shape[3], \n",
    "        dtype=x.dtype), initial_results.past_key_values)\n",
    "\n",
    "\n",
    "class StatefulModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.decoder_states = empty_states\n",
    "        self.step_seq = torch.zeros(1, dtype=torch.int32)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        torch.sym_constrain_range(input_ids.shape[1], min=2, max=4095)\n",
    "        results = self.base_model.forward(input_ids)\n",
    "        pkvs = results.past_key_values\n",
    "        pkv_example = pkvs[0][0]\n",
    "        seq_length = pkv_example.shape[2]\n",
    "        constrain_as_value(seq_length)\n",
    "        self._update_states(pkvs, seq_length) \n",
    "        return results.logits, seq_length\n",
    "\n",
    "    def _update_states(self, update_states, seq_length):\n",
    "        states_flat, _ = pytree.tree_flatten(self.decoder_states)\n",
    "        updates_flat, _ = pytree.tree_flatten(update_states)\n",
    "        for state, update in zip(states_flat, updates_flat):\n",
    "            state[:, :, 0:seq_length, :] = update[:, :, :, :]\n",
    "\n",
    "sm = StatefulModel(mdl)\n",
    "input_ids = initial_input.input_ids\n",
    "_, seq_length = input_ids.shape\n",
    "sm.forward(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 136])\n",
      "<function export.<locals>.inner at 0x7f78affe6ca0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-30 17:20:22,482] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard Eq(512, s0) == False, this could result in accuracy problems\n",
      "[2023-08-30 17:20:22,484] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard s0 > 512 == False, this could result in accuracy problems\n",
      "[2023-08-30 17:20:22,485] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard Eq(65536, 128*s0) == False, this could result in accuracy problems\n",
      "[2023-08-30 17:20:22,486] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard Ne(65536, 128*s0) == True, this could result in accuracy problems\n",
      "[2023-08-30 17:20:29,965] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard Eq(s0, 512) == False, this could result in accuracy problems\n",
      "[2023-08-30 17:20:29,967] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard Eq(128*s0, 65536) == False, this could result in accuracy problems\n",
      "[2023-08-30 17:20:29,968] torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard Ne(128*s0, 65536) == True, this could result in accuracy problems\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, input_ids):\n",
      "        arg0: i64[1, s0], = fx_pytree.tree_flatten_spec(([input_ids], {}), self._in_spec)\n",
      "        # File: /tmp/ipykernel_881114/4291870228.py:32, code: torch.sym_constrain_range(input_ids.shape[1], min=2, max=4095)\n",
      "        sym_size: Sym(s0) = torch.ops.aten.sym_size(arg0, 1)\n",
      "        sym_constrain_range_default = torch.ops.aten.sym_constrain_range.default(sym_size, min = 2, max = 4095)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:643, code: position_ids = torch.arange(\n",
      "        add: Sym(s0) = sym_size + 0\n",
      "        arange_start: i64[s0] = torch.ops.aten.arange.start(0, add, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  add = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:646, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        unsqueeze_default: i64[1, s0] = torch.ops.aten.unsqueeze.default(arange_start, 0);  arange_start = None\n",
      "        view_default: i64[1, s0] = torch.ops.aten.view.default(unsqueeze_default, [-1, sym_size]);  unsqueeze_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:651, code: inputs_embeds = self.embed_tokens(input_ids)\n",
      "        _param_constant0 = self._param_constant0\n",
      "        embedding_default: f32[1, s0, 4096] = torch.ops.aten.embedding.default(_param_constant0, arg0);  _param_constant0 = arg0 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:654, code: attention_mask = torch.ones(\n",
      "        ones_default: b8[1, s0] = torch.ops.aten.ones.default([1, sym_size], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:50, code: mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
      "        full_default: f32[s0, s0] = torch.ops.aten.full.default([sym_size, sym_size], -3.4028234663852886e+38, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:51, code: mask_cond = torch.arange(mask.size(-1), device=device)\n",
      "        arange_default: i64[s0] = torch.ops.aten.arange.default(sym_size, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:52, code: mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "        add_tensor: i64[s0] = torch.ops.aten.add.Tensor(arange_default, 1)\n",
      "        view_default_1: i64[s0, 1] = torch.ops.aten.view.default(add_tensor, [sym_size, 1]);  add_tensor = None\n",
      "        lt_tensor: b8[s0, s0] = torch.ops.aten.lt.Tensor(arange_default, view_default_1);  arange_default = view_default_1 = None\n",
      "        masked_fill__scalar: f32[s0, s0] = torch.ops.aten.masked_fill_.Scalar(full_default, lt_tensor, 0);  full_default = lt_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:57, code: return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
      "        unsqueeze_default_1: f32[1, s0, s0] = torch.ops.aten.unsqueeze.default(masked_fill__scalar, 0);  masked_fill__scalar = None\n",
      "        unsqueeze_default_2: f32[1, 1, s0, s0] = torch.ops.aten.unsqueeze.default(unsqueeze_default_1, 1);  unsqueeze_default_1 = None\n",
      "        slice_tensor: f32[1, 1, s0, s0] = torch.ops.aten.slice.Tensor(unsqueeze_default_2, 2, 0, 9223372036854775807);  unsqueeze_default_2 = None\n",
      "        slice_tensor_1: f32[1, 1, s0, s0] = torch.ops.aten.slice.Tensor(slice_tensor, 3, 0, 9223372036854775807);  slice_tensor = None\n",
      "        add_1: Sym(s0) = sym_size + 0\n",
      "        expand_default: f32[1, 1, s0, s0] = torch.ops.aten.expand.default(slice_tensor_1, [1, 1, sym_size, add_1]);  slice_tensor_1 = add_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:68, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "        slice_tensor_2: b8[1, s0] = torch.ops.aten.slice.Tensor(ones_default, 0, 0, 9223372036854775807);  ones_default = None\n",
      "        unsqueeze_default_3: b8[1, 1, s0] = torch.ops.aten.unsqueeze.default(slice_tensor_2, 1);  slice_tensor_2 = None\n",
      "        unsqueeze_default_4: b8[1, 1, 1, s0] = torch.ops.aten.unsqueeze.default(unsqueeze_default_3, 2);  unsqueeze_default_3 = None\n",
      "        slice_tensor_3: b8[1, 1, 1, s0] = torch.ops.aten.slice.Tensor(unsqueeze_default_4, 3, 0, 9223372036854775807);  unsqueeze_default_4 = None\n",
      "        expand_default_1: b8[1, 1, s0, s0] = torch.ops.aten.expand.default(slice_tensor_3, [1, 1, sym_size, sym_size]);  slice_tensor_3 = None\n",
      "        _to_copy_default: f32[1, 1, s0, s0] = torch.ops.aten._to_copy.default(expand_default_1, dtype = torch.float32);  expand_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70, code: inverted_mask = 1.0 - expanded_mask\n",
      "        rsub_scalar: f32[1, 1, s0, s0] = torch.ops.aten.rsub.Scalar(_to_copy_default, 1.0);  _to_copy_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "        _to_copy_default_1: b8[1, 1, s0, s0] = torch.ops.aten._to_copy.default(rsub_scalar, dtype = torch.bool)\n",
      "        masked_fill_scalar: f32[1, 1, s0, s0] = torch.ops.aten.masked_fill.Scalar(rsub_scalar, _to_copy_default_1, -3.4028234663852886e+38);  rsub_scalar = _to_copy_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:598, code: expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
      "        add_tensor_1: f32[1, 1, s0, s0] = torch.ops.aten.add.Tensor(masked_fill_scalar, expand_default);  masked_fill_scalar = expand_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(embedding_default, 2)\n",
      "        mean_dim: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar, [-1], True);  pow_tensor_scalar = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_2: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim, 1e-06);  mean_dim = None\n",
      "        rsqrt_default: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_2);  add_tensor_2 = None\n",
      "        detach_default: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default)\n",
      "        mul_tensor: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(embedding_default, rsqrt_default);  rsqrt_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant1 = self._param_constant1\n",
      "        mul_tensor_1: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant1, mul_tensor);  _param_constant1 = mul_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant2 = self._param_constant2\n",
      "        t_default: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant2);  _param_constant2 = None\n",
      "        view_default_2: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_1, [sym_size, 4096])\n",
      "        mm_default: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_2, t_default);  view_default_2 = t_default = None\n",
      "        view_default_3: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default, [1, sym_size, 4096]);  mm_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant3 = self._param_constant3\n",
      "        t_default_1: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant3);  _param_constant3 = None\n",
      "        view_default_4: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_1, [sym_size, 4096])\n",
      "        mm_default_1: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_4, t_default_1);  view_default_4 = t_default_1 = None\n",
      "        view_default_5: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_1, [1, sym_size, 4096]);  mm_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant4 = self._param_constant4\n",
      "        t_default_2: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant4);  _param_constant4 = None\n",
      "        view_default_6: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_1, [sym_size, 4096]);  mul_tensor_1 = None\n",
      "        mm_default_2: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_6, t_default_2);  view_default_6 = t_default_2 = None\n",
      "        view_default_7: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_2, [1, sym_size, 4096]);  mm_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_8: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_3, [1, sym_size, 32, 128])\n",
      "        transpose_int: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_8, 1, 2);  view_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_9: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_5, [1, sym_size, 32, 128])\n",
      "        transpose_int_1: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_9, 1, 2);  view_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_10: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_7, [1, sym_size, 32, 128])\n",
      "        transpose_int_2: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_10, 1, 2);  view_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant0 = self._tensor_constant0\n",
      "        slice_tensor_4: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant0, 0, 0, 9223372036854775807);  _tensor_constant0 = None\n",
      "        slice_tensor_5: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_4, 1, 0, 9223372036854775807);  slice_tensor_4 = None\n",
      "        sym_size_1: Sym(s0) = torch.ops.aten.sym_size(view_default_5, 1);  view_default_5 = None\n",
      "        slice_tensor_6: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_5, 2, 0, sym_size_1);  slice_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant1 = self._tensor_constant1\n",
      "        slice_tensor_7: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant1, 0, 0, 9223372036854775807);  _tensor_constant1 = None\n",
      "        slice_tensor_8: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_7, 1, 0, 9223372036854775807);  slice_tensor_7 = None\n",
      "        slice_tensor_9: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_8, 2, 0, sym_size_1);  slice_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_6, 1);  slice_tensor_6 = None\n",
      "        squeeze_dim_1: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim, 0);  squeeze_dim = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_2: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_9, 1);  slice_tensor_9 = None\n",
      "        squeeze_dim_3: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_2, 0);  squeeze_dim_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_1, [view_default]);  squeeze_dim_1 = None\n",
      "        unsqueeze_default_5: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor, 1);  index_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_1: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_3, [view_default]);  squeeze_dim_3 = None\n",
      "        unsqueeze_default_6: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_1, 1);  index_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_2: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int, unsqueeze_default_5)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_10: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_11: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 64, 9223372036854775807);  transpose_int = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_11);  slice_tensor_11 = None\n",
      "        cat_default: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default, slice_tensor_10], -1);  neg_default = slice_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_3: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default, unsqueeze_default_6);  cat_default = None\n",
      "        add_tensor_3: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_2, mul_tensor_3);  mul_tensor_2 = mul_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_4: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_1, unsqueeze_default_5);  unsqueeze_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_12: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_13: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 64, 9223372036854775807);  transpose_int_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_1: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_13);  slice_tensor_13 = None\n",
      "        cat_default_1: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_1, slice_tensor_12], -1);  neg_default_1 = slice_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_5: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_1, unsqueeze_default_6);  cat_default_1 = unsqueeze_default_6 = None\n",
      "        add_tensor_4: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_4, mul_tensor_5);  mul_tensor_4 = mul_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_3: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_4, 2, 3)\n",
      "        sym_size_2: Sym(s0) = torch.ops.aten.sym_size(view_default_3, 1);  view_default_3 = None\n",
      "        expand_default_2: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_3, [1, 32, sym_size_2, 128]);  add_tensor_3 = None\n",
      "        view_default_11: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_2, [32, sym_size_2, 128]);  expand_default_2 = None\n",
      "        expand_default_3: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_3, [1, 32, 128, sym_size_1]);  transpose_int_3 = None\n",
      "        view_default_12: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_3, [32, 128, sym_size_1]);  expand_default_3 = None\n",
      "        bmm_default: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_11, view_default_12);  view_default_11 = view_default_12 = None\n",
      "        view_default_13: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default, [1, 32, sym_size_2, sym_size_1]);  bmm_default = None\n",
      "        div_tensor: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_13, 11.313708498984761);  view_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_5: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor, add_tensor_1);  div_tensor = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_5, -1, False);  add_tensor_5 = None\n",
      "        detach_default_1: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_4: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default, [1, 32, sym_size_2, sym_size_1]);  _softmax_default = None\n",
      "        view_default_14: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_4, [32, sym_size_2, sym_size_1]);  expand_default_4 = None\n",
      "        sym_size_3: Sym(s0) = torch.ops.aten.sym_size(view_default_7, 1);  view_default_7 = None\n",
      "        expand_default_5: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_2, [1, 32, sym_size_3, 128])\n",
      "        view_default_15: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_5, [32, sym_size_3, 128]);  expand_default_5 = sym_size_3 = None\n",
      "        bmm_default_1: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_14, view_default_15);  view_default_14 = view_default_15 = None\n",
      "        view_default_16: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_1, [1, 32, sym_size_2, 128]);  bmm_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_4: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_16, 1, 2);  view_default_16 = None\n",
      "        clone_default: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_4, memory_format = torch.contiguous_format);  transpose_int_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_17: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default, [1, sym_size, 4096]);  clone_default = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant5 = self._param_constant5\n",
      "        t_default_3: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant5);  _param_constant5 = None\n",
      "        view_default_18: f32[s0, 4096] = torch.ops.aten.view.default(view_default_17, [sym_size_2, 4096]);  view_default_17 = None\n",
      "        mm_default_3: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_18, t_default_3);  view_default_18 = t_default_3 = None\n",
      "        view_default_19: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_3, [1, sym_size_2, 4096]);  mm_default_3 = sym_size_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_6: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(embedding_default, view_default_19);  embedding_default = view_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_1: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_6, 2)\n",
      "        mean_dim_1: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_1, [-1], True);  pow_tensor_scalar_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_7: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_1, 1e-06);  mean_dim_1 = None\n",
      "        rsqrt_default_1: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_7);  add_tensor_7 = None\n",
      "        detach_default_2: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_1)\n",
      "        mul_tensor_6: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_6, rsqrt_default_1);  rsqrt_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant6 = self._param_constant6\n",
      "        mul_tensor_7: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant6, mul_tensor_6);  _param_constant6 = mul_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant7 = self._param_constant7\n",
      "        t_default_4: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant7);  _param_constant7 = None\n",
      "        view_default_20: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_7, [sym_size, 4096])\n",
      "        mm_default_4: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_20, t_default_4);  view_default_20 = t_default_4 = None\n",
      "        view_default_21: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_4, [1, sym_size, 11008]);  mm_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant8 = self._param_constant8\n",
      "        t_default_5: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant8);  _param_constant8 = None\n",
      "        view_default_22: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_7, [sym_size, 4096]);  mul_tensor_7 = None\n",
      "        mm_default_5: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_22, t_default_5);  view_default_22 = t_default_5 = None\n",
      "        view_default_23: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_5, [1, sym_size, 11008]);  mm_default_5 = None\n",
      "        mul_tensor_8: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default, view_default_23);  silu_default = view_default_23 = None\n",
      "        _param_constant9 = self._param_constant9\n",
      "        t_default_6: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant9);  _param_constant9 = None\n",
      "        sym_size_4: Sym(s0) = torch.ops.aten.sym_size(view_default_21, 1);  view_default_21 = None\n",
      "        view_default_24: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_8, [sym_size_4, 11008]);  mul_tensor_8 = None\n",
      "        mm_default_6: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_24, t_default_6);  view_default_24 = t_default_6 = None\n",
      "        view_default_25: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_6, [1, sym_size_4, 4096]);  mm_default_6 = sym_size_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_8: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_6, view_default_25);  add_tensor_6 = view_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_2: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_8, 2)\n",
      "        mean_dim_2: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_2, [-1], True);  pow_tensor_scalar_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_9: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_2, 1e-06);  mean_dim_2 = None\n",
      "        rsqrt_default_2: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_9);  add_tensor_9 = None\n",
      "        detach_default_3: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_2)\n",
      "        mul_tensor_9: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_8, rsqrt_default_2);  rsqrt_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant10 = self._param_constant10\n",
      "        mul_tensor_10: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant10, mul_tensor_9);  _param_constant10 = mul_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant11 = self._param_constant11\n",
      "        t_default_7: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant11);  _param_constant11 = None\n",
      "        view_default_26: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_10, [sym_size, 4096])\n",
      "        mm_default_7: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_26, t_default_7);  view_default_26 = t_default_7 = None\n",
      "        view_default_27: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_7, [1, sym_size, 4096]);  mm_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant12 = self._param_constant12\n",
      "        t_default_8: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant12);  _param_constant12 = None\n",
      "        view_default_28: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_10, [sym_size, 4096])\n",
      "        mm_default_8: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_28, t_default_8);  view_default_28 = t_default_8 = None\n",
      "        view_default_29: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_8, [1, sym_size, 4096]);  mm_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant13 = self._param_constant13\n",
      "        t_default_9: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant13);  _param_constant13 = None\n",
      "        view_default_30: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_10, [sym_size, 4096]);  mul_tensor_10 = None\n",
      "        mm_default_9: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_30, t_default_9);  view_default_30 = t_default_9 = None\n",
      "        view_default_31: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_9, [1, sym_size, 4096]);  mm_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_32: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_27, [1, sym_size, 32, 128])\n",
      "        transpose_int_5: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_32, 1, 2);  view_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_33: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_29, [1, sym_size, 32, 128])\n",
      "        transpose_int_6: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_33, 1, 2);  view_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_34: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_31, [1, sym_size, 32, 128])\n",
      "        transpose_int_7: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_34, 1, 2);  view_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant2 = self._tensor_constant2\n",
      "        slice_tensor_14: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant2, 0, 0, 9223372036854775807);  _tensor_constant2 = None\n",
      "        slice_tensor_15: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_14, 1, 0, 9223372036854775807);  slice_tensor_14 = None\n",
      "        sym_size_5: Sym(s0) = torch.ops.aten.sym_size(view_default_29, 1);  view_default_29 = None\n",
      "        slice_tensor_16: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_15, 2, 0, sym_size_5);  slice_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant3 = self._tensor_constant3\n",
      "        slice_tensor_17: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant3, 0, 0, 9223372036854775807);  _tensor_constant3 = None\n",
      "        slice_tensor_18: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_17, 1, 0, 9223372036854775807);  slice_tensor_17 = None\n",
      "        slice_tensor_19: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_18, 2, 0, sym_size_5);  slice_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_4: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_16, 1);  slice_tensor_16 = None\n",
      "        squeeze_dim_5: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_4, 0);  squeeze_dim_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_6: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_19, 1);  slice_tensor_19 = None\n",
      "        squeeze_dim_7: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_6, 0);  squeeze_dim_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_2: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_5, [view_default]);  squeeze_dim_5 = None\n",
      "        unsqueeze_default_7: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_2, 1);  index_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_3: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_7, [view_default]);  squeeze_dim_7 = None\n",
      "        unsqueeze_default_8: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_3, 1);  index_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_11: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_5, unsqueeze_default_7)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_20: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_21: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 64, 9223372036854775807);  transpose_int_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_2: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_21);  slice_tensor_21 = None\n",
      "        cat_default_2: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_2, slice_tensor_20], -1);  neg_default_2 = slice_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_12: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_2, unsqueeze_default_8);  cat_default_2 = None\n",
      "        add_tensor_10: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_11, mul_tensor_12);  mul_tensor_11 = mul_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_13: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_6, unsqueeze_default_7);  unsqueeze_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_22: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_23: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 64, 9223372036854775807);  transpose_int_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_3: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_23);  slice_tensor_23 = None\n",
      "        cat_default_3: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_3, slice_tensor_22], -1);  neg_default_3 = slice_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_14: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_3, unsqueeze_default_8);  cat_default_3 = unsqueeze_default_8 = None\n",
      "        add_tensor_11: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_13, mul_tensor_14);  mul_tensor_13 = mul_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_8: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_11, 2, 3)\n",
      "        sym_size_6: Sym(s0) = torch.ops.aten.sym_size(view_default_27, 1);  view_default_27 = None\n",
      "        expand_default_6: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_10, [1, 32, sym_size_6, 128]);  add_tensor_10 = None\n",
      "        view_default_35: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_6, [32, sym_size_6, 128]);  expand_default_6 = None\n",
      "        expand_default_7: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_8, [1, 32, 128, sym_size_5]);  transpose_int_8 = None\n",
      "        view_default_36: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_7, [32, 128, sym_size_5]);  expand_default_7 = None\n",
      "        bmm_default_2: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_35, view_default_36);  view_default_35 = view_default_36 = None\n",
      "        view_default_37: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_2, [1, 32, sym_size_6, sym_size_5]);  bmm_default_2 = None\n",
      "        div_tensor_1: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_37, 11.313708498984761);  view_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_12: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_1, add_tensor_1);  div_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_1: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_12, -1, False);  add_tensor_12 = None\n",
      "        detach_default_4: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_1)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_8: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_1, [1, 32, sym_size_6, sym_size_5]);  _softmax_default_1 = None\n",
      "        view_default_38: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_8, [32, sym_size_6, sym_size_5]);  expand_default_8 = sym_size_5 = None\n",
      "        sym_size_7: Sym(s0) = torch.ops.aten.sym_size(view_default_31, 1);  view_default_31 = None\n",
      "        expand_default_9: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_7, [1, 32, sym_size_7, 128])\n",
      "        view_default_39: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_9, [32, sym_size_7, 128]);  expand_default_9 = sym_size_7 = None\n",
      "        bmm_default_3: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_38, view_default_39);  view_default_38 = view_default_39 = None\n",
      "        view_default_40: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_3, [1, 32, sym_size_6, 128]);  bmm_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_9: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_40, 1, 2);  view_default_40 = None\n",
      "        clone_default_1: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_9, memory_format = torch.contiguous_format);  transpose_int_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_41: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_1, [1, sym_size, 4096]);  clone_default_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant14 = self._param_constant14\n",
      "        t_default_10: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant14);  _param_constant14 = None\n",
      "        view_default_42: f32[s0, 4096] = torch.ops.aten.view.default(view_default_41, [sym_size_6, 4096]);  view_default_41 = None\n",
      "        mm_default_10: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_42, t_default_10);  view_default_42 = t_default_10 = None\n",
      "        view_default_43: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_10, [1, sym_size_6, 4096]);  mm_default_10 = sym_size_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_13: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_8, view_default_43);  add_tensor_8 = view_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_3: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_13, 2)\n",
      "        mean_dim_3: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_3, [-1], True);  pow_tensor_scalar_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_14: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_3, 1e-06);  mean_dim_3 = None\n",
      "        rsqrt_default_3: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_14);  add_tensor_14 = None\n",
      "        detach_default_5: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_3)\n",
      "        mul_tensor_15: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_13, rsqrt_default_3);  rsqrt_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant15 = self._param_constant15\n",
      "        mul_tensor_16: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant15, mul_tensor_15);  _param_constant15 = mul_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant16 = self._param_constant16\n",
      "        t_default_11: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant16);  _param_constant16 = None\n",
      "        view_default_44: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_16, [sym_size, 4096])\n",
      "        mm_default_11: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_44, t_default_11);  view_default_44 = t_default_11 = None\n",
      "        view_default_45: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_11, [1, sym_size, 11008]);  mm_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_1: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_45)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant17 = self._param_constant17\n",
      "        t_default_12: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant17);  _param_constant17 = None\n",
      "        view_default_46: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_16, [sym_size, 4096]);  mul_tensor_16 = None\n",
      "        mm_default_12: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_46, t_default_12);  view_default_46 = t_default_12 = None\n",
      "        view_default_47: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_12, [1, sym_size, 11008]);  mm_default_12 = None\n",
      "        mul_tensor_17: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_1, view_default_47);  silu_default_1 = view_default_47 = None\n",
      "        _param_constant18 = self._param_constant18\n",
      "        t_default_13: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant18);  _param_constant18 = None\n",
      "        sym_size_8: Sym(s0) = torch.ops.aten.sym_size(view_default_45, 1);  view_default_45 = None\n",
      "        view_default_48: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_17, [sym_size_8, 11008]);  mul_tensor_17 = None\n",
      "        mm_default_13: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_48, t_default_13);  view_default_48 = t_default_13 = None\n",
      "        view_default_49: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_13, [1, sym_size_8, 4096]);  mm_default_13 = sym_size_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_15: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_13, view_default_49);  add_tensor_13 = view_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_4: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_15, 2)\n",
      "        mean_dim_4: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_4, [-1], True);  pow_tensor_scalar_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_16: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_4, 1e-06);  mean_dim_4 = None\n",
      "        rsqrt_default_4: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_16);  add_tensor_16 = None\n",
      "        detach_default_6: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_4)\n",
      "        mul_tensor_18: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_15, rsqrt_default_4);  rsqrt_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant19 = self._param_constant19\n",
      "        mul_tensor_19: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant19, mul_tensor_18);  _param_constant19 = mul_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant20 = self._param_constant20\n",
      "        t_default_14: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant20);  _param_constant20 = None\n",
      "        view_default_50: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_19, [sym_size, 4096])\n",
      "        mm_default_14: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_50, t_default_14);  view_default_50 = t_default_14 = None\n",
      "        view_default_51: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_14, [1, sym_size, 4096]);  mm_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant21 = self._param_constant21\n",
      "        t_default_15: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant21);  _param_constant21 = None\n",
      "        view_default_52: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_19, [sym_size, 4096])\n",
      "        mm_default_15: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_52, t_default_15);  view_default_52 = t_default_15 = None\n",
      "        view_default_53: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_15, [1, sym_size, 4096]);  mm_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant22 = self._param_constant22\n",
      "        t_default_16: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant22);  _param_constant22 = None\n",
      "        view_default_54: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_19, [sym_size, 4096]);  mul_tensor_19 = None\n",
      "        mm_default_16: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_54, t_default_16);  view_default_54 = t_default_16 = None\n",
      "        view_default_55: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_16, [1, sym_size, 4096]);  mm_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_56: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_51, [1, sym_size, 32, 128])\n",
      "        transpose_int_10: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_56, 1, 2);  view_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_57: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_53, [1, sym_size, 32, 128])\n",
      "        transpose_int_11: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_57, 1, 2);  view_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_58: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_55, [1, sym_size, 32, 128])\n",
      "        transpose_int_12: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_58, 1, 2);  view_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant4 = self._tensor_constant4\n",
      "        slice_tensor_24: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant4, 0, 0, 9223372036854775807);  _tensor_constant4 = None\n",
      "        slice_tensor_25: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_24, 1, 0, 9223372036854775807);  slice_tensor_24 = None\n",
      "        sym_size_9: Sym(s0) = torch.ops.aten.sym_size(view_default_53, 1);  view_default_53 = None\n",
      "        slice_tensor_26: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_25, 2, 0, sym_size_9);  slice_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant5 = self._tensor_constant5\n",
      "        slice_tensor_27: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant5, 0, 0, 9223372036854775807);  _tensor_constant5 = None\n",
      "        slice_tensor_28: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_27, 1, 0, 9223372036854775807);  slice_tensor_27 = None\n",
      "        slice_tensor_29: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_28, 2, 0, sym_size_9);  slice_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_8: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_26, 1);  slice_tensor_26 = None\n",
      "        squeeze_dim_9: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_8, 0);  squeeze_dim_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_10: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_29, 1);  slice_tensor_29 = None\n",
      "        squeeze_dim_11: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_10, 0);  squeeze_dim_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_4: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_9, [view_default]);  squeeze_dim_9 = None\n",
      "        unsqueeze_default_9: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_4, 1);  index_tensor_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_5: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_11, [view_default]);  squeeze_dim_11 = None\n",
      "        unsqueeze_default_10: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_5, 1);  index_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_20: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_10, unsqueeze_default_9)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_30: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_31: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 64, 9223372036854775807);  transpose_int_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_4: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_31);  slice_tensor_31 = None\n",
      "        cat_default_4: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_4, slice_tensor_30], -1);  neg_default_4 = slice_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_21: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_4, unsqueeze_default_10);  cat_default_4 = None\n",
      "        add_tensor_17: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_20, mul_tensor_21);  mul_tensor_20 = mul_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_22: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_11, unsqueeze_default_9);  unsqueeze_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_32: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_33: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 64, 9223372036854775807);  transpose_int_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_5: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_33);  slice_tensor_33 = None\n",
      "        cat_default_5: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_5, slice_tensor_32], -1);  neg_default_5 = slice_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_23: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_5, unsqueeze_default_10);  cat_default_5 = unsqueeze_default_10 = None\n",
      "        add_tensor_18: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_22, mul_tensor_23);  mul_tensor_22 = mul_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_13: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_18, 2, 3)\n",
      "        sym_size_10: Sym(s0) = torch.ops.aten.sym_size(view_default_51, 1);  view_default_51 = None\n",
      "        expand_default_10: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_17, [1, 32, sym_size_10, 128]);  add_tensor_17 = None\n",
      "        view_default_59: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_10, [32, sym_size_10, 128]);  expand_default_10 = None\n",
      "        expand_default_11: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_13, [1, 32, 128, sym_size_9]);  transpose_int_13 = None\n",
      "        view_default_60: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_11, [32, 128, sym_size_9]);  expand_default_11 = None\n",
      "        bmm_default_4: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_59, view_default_60);  view_default_59 = view_default_60 = None\n",
      "        view_default_61: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_4, [1, 32, sym_size_10, sym_size_9]);  bmm_default_4 = None\n",
      "        div_tensor_2: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_61, 11.313708498984761);  view_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_19: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_2, add_tensor_1);  div_tensor_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_2: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_19, -1, False);  add_tensor_19 = None\n",
      "        detach_default_7: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_2)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_12: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_2, [1, 32, sym_size_10, sym_size_9]);  _softmax_default_2 = None\n",
      "        view_default_62: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_12, [32, sym_size_10, sym_size_9]);  expand_default_12 = sym_size_9 = None\n",
      "        sym_size_11: Sym(s0) = torch.ops.aten.sym_size(view_default_55, 1);  view_default_55 = None\n",
      "        expand_default_13: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_12, [1, 32, sym_size_11, 128])\n",
      "        view_default_63: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_13, [32, sym_size_11, 128]);  expand_default_13 = sym_size_11 = None\n",
      "        bmm_default_5: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_62, view_default_63);  view_default_62 = view_default_63 = None\n",
      "        view_default_64: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_5, [1, 32, sym_size_10, 128]);  bmm_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_14: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_64, 1, 2);  view_default_64 = None\n",
      "        clone_default_2: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_14, memory_format = torch.contiguous_format);  transpose_int_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_65: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_2, [1, sym_size, 4096]);  clone_default_2 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant23 = self._param_constant23\n",
      "        t_default_17: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant23);  _param_constant23 = None\n",
      "        view_default_66: f32[s0, 4096] = torch.ops.aten.view.default(view_default_65, [sym_size_10, 4096]);  view_default_65 = None\n",
      "        mm_default_17: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_66, t_default_17);  view_default_66 = t_default_17 = None\n",
      "        view_default_67: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_17, [1, sym_size_10, 4096]);  mm_default_17 = sym_size_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_20: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_15, view_default_67);  add_tensor_15 = view_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_5: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_20, 2)\n",
      "        mean_dim_5: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_5, [-1], True);  pow_tensor_scalar_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_21: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_5, 1e-06);  mean_dim_5 = None\n",
      "        rsqrt_default_5: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_21);  add_tensor_21 = None\n",
      "        detach_default_8: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_5)\n",
      "        mul_tensor_24: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_20, rsqrt_default_5);  rsqrt_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant24 = self._param_constant24\n",
      "        mul_tensor_25: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant24, mul_tensor_24);  _param_constant24 = mul_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant25 = self._param_constant25\n",
      "        t_default_18: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant25);  _param_constant25 = None\n",
      "        view_default_68: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_25, [sym_size, 4096])\n",
      "        mm_default_18: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_68, t_default_18);  view_default_68 = t_default_18 = None\n",
      "        view_default_69: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_18, [1, sym_size, 11008]);  mm_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_2: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_69)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant26 = self._param_constant26\n",
      "        t_default_19: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant26);  _param_constant26 = None\n",
      "        view_default_70: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_25, [sym_size, 4096]);  mul_tensor_25 = None\n",
      "        mm_default_19: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_70, t_default_19);  view_default_70 = t_default_19 = None\n",
      "        view_default_71: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_19, [1, sym_size, 11008]);  mm_default_19 = None\n",
      "        mul_tensor_26: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_2, view_default_71);  silu_default_2 = view_default_71 = None\n",
      "        _param_constant27 = self._param_constant27\n",
      "        t_default_20: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant27);  _param_constant27 = None\n",
      "        sym_size_12: Sym(s0) = torch.ops.aten.sym_size(view_default_69, 1);  view_default_69 = None\n",
      "        view_default_72: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_26, [sym_size_12, 11008]);  mul_tensor_26 = None\n",
      "        mm_default_20: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_72, t_default_20);  view_default_72 = t_default_20 = None\n",
      "        view_default_73: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_20, [1, sym_size_12, 4096]);  mm_default_20 = sym_size_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_22: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_20, view_default_73);  add_tensor_20 = view_default_73 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_6: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_22, 2)\n",
      "        mean_dim_6: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_6, [-1], True);  pow_tensor_scalar_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_23: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_6, 1e-06);  mean_dim_6 = None\n",
      "        rsqrt_default_6: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_23);  add_tensor_23 = None\n",
      "        detach_default_9: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_6)\n",
      "        mul_tensor_27: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_22, rsqrt_default_6);  rsqrt_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant28 = self._param_constant28\n",
      "        mul_tensor_28: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant28, mul_tensor_27);  _param_constant28 = mul_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant29 = self._param_constant29\n",
      "        t_default_21: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant29);  _param_constant29 = None\n",
      "        view_default_74: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_28, [sym_size, 4096])\n",
      "        mm_default_21: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_74, t_default_21);  view_default_74 = t_default_21 = None\n",
      "        view_default_75: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_21, [1, sym_size, 4096]);  mm_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant30 = self._param_constant30\n",
      "        t_default_22: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant30);  _param_constant30 = None\n",
      "        view_default_76: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_28, [sym_size, 4096])\n",
      "        mm_default_22: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_76, t_default_22);  view_default_76 = t_default_22 = None\n",
      "        view_default_77: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_22, [1, sym_size, 4096]);  mm_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant31 = self._param_constant31\n",
      "        t_default_23: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant31);  _param_constant31 = None\n",
      "        view_default_78: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_28, [sym_size, 4096]);  mul_tensor_28 = None\n",
      "        mm_default_23: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_78, t_default_23);  view_default_78 = t_default_23 = None\n",
      "        view_default_79: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_23, [1, sym_size, 4096]);  mm_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_80: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_75, [1, sym_size, 32, 128])\n",
      "        transpose_int_15: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_80, 1, 2);  view_default_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_81: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_77, [1, sym_size, 32, 128])\n",
      "        transpose_int_16: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_81, 1, 2);  view_default_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_82: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_79, [1, sym_size, 32, 128])\n",
      "        transpose_int_17: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_82, 1, 2);  view_default_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant6 = self._tensor_constant6\n",
      "        slice_tensor_34: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant6, 0, 0, 9223372036854775807);  _tensor_constant6 = None\n",
      "        slice_tensor_35: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_34, 1, 0, 9223372036854775807);  slice_tensor_34 = None\n",
      "        sym_size_13: Sym(s0) = torch.ops.aten.sym_size(view_default_77, 1);  view_default_77 = None\n",
      "        slice_tensor_36: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_35, 2, 0, sym_size_13);  slice_tensor_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant7 = self._tensor_constant7\n",
      "        slice_tensor_37: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant7, 0, 0, 9223372036854775807);  _tensor_constant7 = None\n",
      "        slice_tensor_38: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_37, 1, 0, 9223372036854775807);  slice_tensor_37 = None\n",
      "        slice_tensor_39: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_38, 2, 0, sym_size_13);  slice_tensor_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_12: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_36, 1);  slice_tensor_36 = None\n",
      "        squeeze_dim_13: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_12, 0);  squeeze_dim_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_14: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_39, 1);  slice_tensor_39 = None\n",
      "        squeeze_dim_15: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_14, 0);  squeeze_dim_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_6: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_13, [view_default]);  squeeze_dim_13 = None\n",
      "        unsqueeze_default_11: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_6, 1);  index_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_7: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_15, [view_default]);  squeeze_dim_15 = None\n",
      "        unsqueeze_default_12: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_7, 1);  index_tensor_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_29: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_15, unsqueeze_default_11)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_40: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_41: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 64, 9223372036854775807);  transpose_int_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_6: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_41);  slice_tensor_41 = None\n",
      "        cat_default_6: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_6, slice_tensor_40], -1);  neg_default_6 = slice_tensor_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_30: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_6, unsqueeze_default_12);  cat_default_6 = None\n",
      "        add_tensor_24: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_29, mul_tensor_30);  mul_tensor_29 = mul_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_31: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_16, unsqueeze_default_11);  unsqueeze_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_42: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_43: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 64, 9223372036854775807);  transpose_int_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_7: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_43);  slice_tensor_43 = None\n",
      "        cat_default_7: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_7, slice_tensor_42], -1);  neg_default_7 = slice_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_32: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_7, unsqueeze_default_12);  cat_default_7 = unsqueeze_default_12 = None\n",
      "        add_tensor_25: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_31, mul_tensor_32);  mul_tensor_31 = mul_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_18: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_25, 2, 3)\n",
      "        sym_size_14: Sym(s0) = torch.ops.aten.sym_size(view_default_75, 1);  view_default_75 = None\n",
      "        expand_default_14: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_24, [1, 32, sym_size_14, 128]);  add_tensor_24 = None\n",
      "        view_default_83: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_14, [32, sym_size_14, 128]);  expand_default_14 = None\n",
      "        expand_default_15: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_18, [1, 32, 128, sym_size_13]);  transpose_int_18 = None\n",
      "        view_default_84: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_15, [32, 128, sym_size_13]);  expand_default_15 = None\n",
      "        bmm_default_6: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_83, view_default_84);  view_default_83 = view_default_84 = None\n",
      "        view_default_85: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_6, [1, 32, sym_size_14, sym_size_13]);  bmm_default_6 = None\n",
      "        div_tensor_3: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_85, 11.313708498984761);  view_default_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_26: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_3, add_tensor_1);  div_tensor_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_3: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_26, -1, False);  add_tensor_26 = None\n",
      "        detach_default_10: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_3)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_16: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_3, [1, 32, sym_size_14, sym_size_13]);  _softmax_default_3 = None\n",
      "        view_default_86: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_16, [32, sym_size_14, sym_size_13]);  expand_default_16 = sym_size_13 = None\n",
      "        sym_size_15: Sym(s0) = torch.ops.aten.sym_size(view_default_79, 1);  view_default_79 = None\n",
      "        expand_default_17: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_17, [1, 32, sym_size_15, 128])\n",
      "        view_default_87: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_17, [32, sym_size_15, 128]);  expand_default_17 = sym_size_15 = None\n",
      "        bmm_default_7: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_86, view_default_87);  view_default_86 = view_default_87 = None\n",
      "        view_default_88: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_7, [1, 32, sym_size_14, 128]);  bmm_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_19: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_88, 1, 2);  view_default_88 = None\n",
      "        clone_default_3: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_19, memory_format = torch.contiguous_format);  transpose_int_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_89: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_3, [1, sym_size, 4096]);  clone_default_3 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant32 = self._param_constant32\n",
      "        t_default_24: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant32);  _param_constant32 = None\n",
      "        view_default_90: f32[s0, 4096] = torch.ops.aten.view.default(view_default_89, [sym_size_14, 4096]);  view_default_89 = None\n",
      "        mm_default_24: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_90, t_default_24);  view_default_90 = t_default_24 = None\n",
      "        view_default_91: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_24, [1, sym_size_14, 4096]);  mm_default_24 = sym_size_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_27: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_22, view_default_91);  add_tensor_22 = view_default_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_7: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_27, 2)\n",
      "        mean_dim_7: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_7, [-1], True);  pow_tensor_scalar_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_28: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_7, 1e-06);  mean_dim_7 = None\n",
      "        rsqrt_default_7: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_28);  add_tensor_28 = None\n",
      "        detach_default_11: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_7)\n",
      "        mul_tensor_33: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_27, rsqrt_default_7);  rsqrt_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant33 = self._param_constant33\n",
      "        mul_tensor_34: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant33, mul_tensor_33);  _param_constant33 = mul_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant34 = self._param_constant34\n",
      "        t_default_25: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant34);  _param_constant34 = None\n",
      "        view_default_92: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_34, [sym_size, 4096])\n",
      "        mm_default_25: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_92, t_default_25);  view_default_92 = t_default_25 = None\n",
      "        view_default_93: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_25, [1, sym_size, 11008]);  mm_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_3: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_93)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant35 = self._param_constant35\n",
      "        t_default_26: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant35);  _param_constant35 = None\n",
      "        view_default_94: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_34, [sym_size, 4096]);  mul_tensor_34 = None\n",
      "        mm_default_26: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_94, t_default_26);  view_default_94 = t_default_26 = None\n",
      "        view_default_95: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_26, [1, sym_size, 11008]);  mm_default_26 = None\n",
      "        mul_tensor_35: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_3, view_default_95);  silu_default_3 = view_default_95 = None\n",
      "        _param_constant36 = self._param_constant36\n",
      "        t_default_27: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant36);  _param_constant36 = None\n",
      "        sym_size_16: Sym(s0) = torch.ops.aten.sym_size(view_default_93, 1);  view_default_93 = None\n",
      "        view_default_96: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_35, [sym_size_16, 11008]);  mul_tensor_35 = None\n",
      "        mm_default_27: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_96, t_default_27);  view_default_96 = t_default_27 = None\n",
      "        view_default_97: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_27, [1, sym_size_16, 4096]);  mm_default_27 = sym_size_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_29: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_27, view_default_97);  add_tensor_27 = view_default_97 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_8: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_29, 2)\n",
      "        mean_dim_8: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_8, [-1], True);  pow_tensor_scalar_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_30: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_8, 1e-06);  mean_dim_8 = None\n",
      "        rsqrt_default_8: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_30);  add_tensor_30 = None\n",
      "        detach_default_12: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_8)\n",
      "        mul_tensor_36: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_29, rsqrt_default_8);  rsqrt_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant37 = self._param_constant37\n",
      "        mul_tensor_37: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant37, mul_tensor_36);  _param_constant37 = mul_tensor_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant38 = self._param_constant38\n",
      "        t_default_28: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant38);  _param_constant38 = None\n",
      "        view_default_98: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_37, [sym_size, 4096])\n",
      "        mm_default_28: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_98, t_default_28);  view_default_98 = t_default_28 = None\n",
      "        view_default_99: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_28, [1, sym_size, 4096]);  mm_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant39 = self._param_constant39\n",
      "        t_default_29: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant39);  _param_constant39 = None\n",
      "        view_default_100: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_37, [sym_size, 4096])\n",
      "        mm_default_29: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_100, t_default_29);  view_default_100 = t_default_29 = None\n",
      "        view_default_101: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_29, [1, sym_size, 4096]);  mm_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant40 = self._param_constant40\n",
      "        t_default_30: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant40);  _param_constant40 = None\n",
      "        view_default_102: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_37, [sym_size, 4096]);  mul_tensor_37 = None\n",
      "        mm_default_30: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_102, t_default_30);  view_default_102 = t_default_30 = None\n",
      "        view_default_103: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_30, [1, sym_size, 4096]);  mm_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_104: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_99, [1, sym_size, 32, 128])\n",
      "        transpose_int_20: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_104, 1, 2);  view_default_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_105: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_101, [1, sym_size, 32, 128])\n",
      "        transpose_int_21: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_105, 1, 2);  view_default_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_106: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_103, [1, sym_size, 32, 128])\n",
      "        transpose_int_22: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_106, 1, 2);  view_default_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant8 = self._tensor_constant8\n",
      "        slice_tensor_44: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant8, 0, 0, 9223372036854775807);  _tensor_constant8 = None\n",
      "        slice_tensor_45: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_44, 1, 0, 9223372036854775807);  slice_tensor_44 = None\n",
      "        sym_size_17: Sym(s0) = torch.ops.aten.sym_size(view_default_101, 1);  view_default_101 = None\n",
      "        slice_tensor_46: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_45, 2, 0, sym_size_17);  slice_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant9 = self._tensor_constant9\n",
      "        slice_tensor_47: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant9, 0, 0, 9223372036854775807);  _tensor_constant9 = None\n",
      "        slice_tensor_48: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_47, 1, 0, 9223372036854775807);  slice_tensor_47 = None\n",
      "        slice_tensor_49: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_48, 2, 0, sym_size_17);  slice_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_16: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_46, 1);  slice_tensor_46 = None\n",
      "        squeeze_dim_17: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_16, 0);  squeeze_dim_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_18: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_49, 1);  slice_tensor_49 = None\n",
      "        squeeze_dim_19: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_18, 0);  squeeze_dim_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_8: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_17, [view_default]);  squeeze_dim_17 = None\n",
      "        unsqueeze_default_13: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_8, 1);  index_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_9: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_19, [view_default]);  squeeze_dim_19 = None\n",
      "        unsqueeze_default_14: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_9, 1);  index_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_38: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_20, unsqueeze_default_13)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_50: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_51: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 64, 9223372036854775807);  transpose_int_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_8: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_51);  slice_tensor_51 = None\n",
      "        cat_default_8: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_8, slice_tensor_50], -1);  neg_default_8 = slice_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_39: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_8, unsqueeze_default_14);  cat_default_8 = None\n",
      "        add_tensor_31: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_38, mul_tensor_39);  mul_tensor_38 = mul_tensor_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_40: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_21, unsqueeze_default_13);  unsqueeze_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_52: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_53: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 64, 9223372036854775807);  transpose_int_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_9: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_53);  slice_tensor_53 = None\n",
      "        cat_default_9: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_9, slice_tensor_52], -1);  neg_default_9 = slice_tensor_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_41: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_9, unsqueeze_default_14);  cat_default_9 = unsqueeze_default_14 = None\n",
      "        add_tensor_32: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_40, mul_tensor_41);  mul_tensor_40 = mul_tensor_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_23: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_32, 2, 3)\n",
      "        sym_size_18: Sym(s0) = torch.ops.aten.sym_size(view_default_99, 1);  view_default_99 = None\n",
      "        expand_default_18: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_31, [1, 32, sym_size_18, 128]);  add_tensor_31 = None\n",
      "        view_default_107: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_18, [32, sym_size_18, 128]);  expand_default_18 = None\n",
      "        expand_default_19: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_23, [1, 32, 128, sym_size_17]);  transpose_int_23 = None\n",
      "        view_default_108: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_19, [32, 128, sym_size_17]);  expand_default_19 = None\n",
      "        bmm_default_8: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_107, view_default_108);  view_default_107 = view_default_108 = None\n",
      "        view_default_109: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_8, [1, 32, sym_size_18, sym_size_17]);  bmm_default_8 = None\n",
      "        div_tensor_4: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_109, 11.313708498984761);  view_default_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_33: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_4, add_tensor_1);  div_tensor_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_4: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_33, -1, False);  add_tensor_33 = None\n",
      "        detach_default_13: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_4)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_20: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_4, [1, 32, sym_size_18, sym_size_17]);  _softmax_default_4 = None\n",
      "        view_default_110: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_20, [32, sym_size_18, sym_size_17]);  expand_default_20 = sym_size_17 = None\n",
      "        sym_size_19: Sym(s0) = torch.ops.aten.sym_size(view_default_103, 1);  view_default_103 = None\n",
      "        expand_default_21: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_22, [1, 32, sym_size_19, 128])\n",
      "        view_default_111: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_21, [32, sym_size_19, 128]);  expand_default_21 = sym_size_19 = None\n",
      "        bmm_default_9: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_110, view_default_111);  view_default_110 = view_default_111 = None\n",
      "        view_default_112: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_9, [1, 32, sym_size_18, 128]);  bmm_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_24: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_112, 1, 2);  view_default_112 = None\n",
      "        clone_default_4: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_24, memory_format = torch.contiguous_format);  transpose_int_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_113: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_4, [1, sym_size, 4096]);  clone_default_4 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant41 = self._param_constant41\n",
      "        t_default_31: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant41);  _param_constant41 = None\n",
      "        view_default_114: f32[s0, 4096] = torch.ops.aten.view.default(view_default_113, [sym_size_18, 4096]);  view_default_113 = None\n",
      "        mm_default_31: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_114, t_default_31);  view_default_114 = t_default_31 = None\n",
      "        view_default_115: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_31, [1, sym_size_18, 4096]);  mm_default_31 = sym_size_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_34: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_29, view_default_115);  add_tensor_29 = view_default_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_9: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_34, 2)\n",
      "        mean_dim_9: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_9, [-1], True);  pow_tensor_scalar_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_35: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_9, 1e-06);  mean_dim_9 = None\n",
      "        rsqrt_default_9: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_35);  add_tensor_35 = None\n",
      "        detach_default_14: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_9)\n",
      "        mul_tensor_42: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_34, rsqrt_default_9);  rsqrt_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant42 = self._param_constant42\n",
      "        mul_tensor_43: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant42, mul_tensor_42);  _param_constant42 = mul_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant43 = self._param_constant43\n",
      "        t_default_32: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant43);  _param_constant43 = None\n",
      "        view_default_116: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_43, [sym_size, 4096])\n",
      "        mm_default_32: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_116, t_default_32);  view_default_116 = t_default_32 = None\n",
      "        view_default_117: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_32, [1, sym_size, 11008]);  mm_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_4: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_117)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant44 = self._param_constant44\n",
      "        t_default_33: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant44);  _param_constant44 = None\n",
      "        view_default_118: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_43, [sym_size, 4096]);  mul_tensor_43 = None\n",
      "        mm_default_33: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_118, t_default_33);  view_default_118 = t_default_33 = None\n",
      "        view_default_119: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_33, [1, sym_size, 11008]);  mm_default_33 = None\n",
      "        mul_tensor_44: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_4, view_default_119);  silu_default_4 = view_default_119 = None\n",
      "        _param_constant45 = self._param_constant45\n",
      "        t_default_34: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant45);  _param_constant45 = None\n",
      "        sym_size_20: Sym(s0) = torch.ops.aten.sym_size(view_default_117, 1);  view_default_117 = None\n",
      "        view_default_120: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_44, [sym_size_20, 11008]);  mul_tensor_44 = None\n",
      "        mm_default_34: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_120, t_default_34);  view_default_120 = t_default_34 = None\n",
      "        view_default_121: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_34, [1, sym_size_20, 4096]);  mm_default_34 = sym_size_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_36: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_34, view_default_121);  add_tensor_34 = view_default_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_10: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_36, 2)\n",
      "        mean_dim_10: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_10, [-1], True);  pow_tensor_scalar_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_37: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_10, 1e-06);  mean_dim_10 = None\n",
      "        rsqrt_default_10: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_37);  add_tensor_37 = None\n",
      "        detach_default_15: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_10)\n",
      "        mul_tensor_45: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_36, rsqrt_default_10);  rsqrt_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant46 = self._param_constant46\n",
      "        mul_tensor_46: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant46, mul_tensor_45);  _param_constant46 = mul_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant47 = self._param_constant47\n",
      "        t_default_35: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant47);  _param_constant47 = None\n",
      "        view_default_122: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_46, [sym_size, 4096])\n",
      "        mm_default_35: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_122, t_default_35);  view_default_122 = t_default_35 = None\n",
      "        view_default_123: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_35, [1, sym_size, 4096]);  mm_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant48 = self._param_constant48\n",
      "        t_default_36: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant48);  _param_constant48 = None\n",
      "        view_default_124: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_46, [sym_size, 4096])\n",
      "        mm_default_36: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_124, t_default_36);  view_default_124 = t_default_36 = None\n",
      "        view_default_125: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_36, [1, sym_size, 4096]);  mm_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant49 = self._param_constant49\n",
      "        t_default_37: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant49);  _param_constant49 = None\n",
      "        view_default_126: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_46, [sym_size, 4096]);  mul_tensor_46 = None\n",
      "        mm_default_37: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_126, t_default_37);  view_default_126 = t_default_37 = None\n",
      "        view_default_127: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_37, [1, sym_size, 4096]);  mm_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_128: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_123, [1, sym_size, 32, 128])\n",
      "        transpose_int_25: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_128, 1, 2);  view_default_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_129: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_125, [1, sym_size, 32, 128])\n",
      "        transpose_int_26: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_129, 1, 2);  view_default_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_130: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_127, [1, sym_size, 32, 128])\n",
      "        transpose_int_27: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_130, 1, 2);  view_default_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant10 = self._tensor_constant10\n",
      "        slice_tensor_54: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant10, 0, 0, 9223372036854775807);  _tensor_constant10 = None\n",
      "        slice_tensor_55: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_54, 1, 0, 9223372036854775807);  slice_tensor_54 = None\n",
      "        sym_size_21: Sym(s0) = torch.ops.aten.sym_size(view_default_125, 1);  view_default_125 = None\n",
      "        slice_tensor_56: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_55, 2, 0, sym_size_21);  slice_tensor_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant11 = self._tensor_constant11\n",
      "        slice_tensor_57: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant11, 0, 0, 9223372036854775807);  _tensor_constant11 = None\n",
      "        slice_tensor_58: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_57, 1, 0, 9223372036854775807);  slice_tensor_57 = None\n",
      "        slice_tensor_59: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_58, 2, 0, sym_size_21);  slice_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_20: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_56, 1);  slice_tensor_56 = None\n",
      "        squeeze_dim_21: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_20, 0);  squeeze_dim_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_22: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_59, 1);  slice_tensor_59 = None\n",
      "        squeeze_dim_23: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_22, 0);  squeeze_dim_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_10: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_21, [view_default]);  squeeze_dim_21 = None\n",
      "        unsqueeze_default_15: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_10, 1);  index_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_11: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_23, [view_default]);  squeeze_dim_23 = None\n",
      "        unsqueeze_default_16: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_11, 1);  index_tensor_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_47: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_25, unsqueeze_default_15)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_60: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_61: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 64, 9223372036854775807);  transpose_int_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_10: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_61);  slice_tensor_61 = None\n",
      "        cat_default_10: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_10, slice_tensor_60], -1);  neg_default_10 = slice_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_48: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_10, unsqueeze_default_16);  cat_default_10 = None\n",
      "        add_tensor_38: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_47, mul_tensor_48);  mul_tensor_47 = mul_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_49: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_26, unsqueeze_default_15);  unsqueeze_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_62: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_63: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 64, 9223372036854775807);  transpose_int_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_11: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_63);  slice_tensor_63 = None\n",
      "        cat_default_11: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_11, slice_tensor_62], -1);  neg_default_11 = slice_tensor_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_50: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_11, unsqueeze_default_16);  cat_default_11 = unsqueeze_default_16 = None\n",
      "        add_tensor_39: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_49, mul_tensor_50);  mul_tensor_49 = mul_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_28: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_39, 2, 3)\n",
      "        sym_size_22: Sym(s0) = torch.ops.aten.sym_size(view_default_123, 1);  view_default_123 = None\n",
      "        expand_default_22: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_38, [1, 32, sym_size_22, 128]);  add_tensor_38 = None\n",
      "        view_default_131: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_22, [32, sym_size_22, 128]);  expand_default_22 = None\n",
      "        expand_default_23: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_28, [1, 32, 128, sym_size_21]);  transpose_int_28 = None\n",
      "        view_default_132: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_23, [32, 128, sym_size_21]);  expand_default_23 = None\n",
      "        bmm_default_10: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_131, view_default_132);  view_default_131 = view_default_132 = None\n",
      "        view_default_133: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_10, [1, 32, sym_size_22, sym_size_21]);  bmm_default_10 = None\n",
      "        div_tensor_5: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_133, 11.313708498984761);  view_default_133 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_40: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_5, add_tensor_1);  div_tensor_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_5: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_40, -1, False);  add_tensor_40 = None\n",
      "        detach_default_16: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_5)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_24: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_5, [1, 32, sym_size_22, sym_size_21]);  _softmax_default_5 = None\n",
      "        view_default_134: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_24, [32, sym_size_22, sym_size_21]);  expand_default_24 = sym_size_21 = None\n",
      "        sym_size_23: Sym(s0) = torch.ops.aten.sym_size(view_default_127, 1);  view_default_127 = None\n",
      "        expand_default_25: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_27, [1, 32, sym_size_23, 128])\n",
      "        view_default_135: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_25, [32, sym_size_23, 128]);  expand_default_25 = sym_size_23 = None\n",
      "        bmm_default_11: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_134, view_default_135);  view_default_134 = view_default_135 = None\n",
      "        view_default_136: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_11, [1, 32, sym_size_22, 128]);  bmm_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_29: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_136, 1, 2);  view_default_136 = None\n",
      "        clone_default_5: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_29, memory_format = torch.contiguous_format);  transpose_int_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_137: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_5, [1, sym_size, 4096]);  clone_default_5 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant50 = self._param_constant50\n",
      "        t_default_38: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant50);  _param_constant50 = None\n",
      "        view_default_138: f32[s0, 4096] = torch.ops.aten.view.default(view_default_137, [sym_size_22, 4096]);  view_default_137 = None\n",
      "        mm_default_38: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_138, t_default_38);  view_default_138 = t_default_38 = None\n",
      "        view_default_139: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_38, [1, sym_size_22, 4096]);  mm_default_38 = sym_size_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_41: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_36, view_default_139);  add_tensor_36 = view_default_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_11: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_41, 2)\n",
      "        mean_dim_11: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_11, [-1], True);  pow_tensor_scalar_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_42: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_11, 1e-06);  mean_dim_11 = None\n",
      "        rsqrt_default_11: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_42);  add_tensor_42 = None\n",
      "        detach_default_17: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_11)\n",
      "        mul_tensor_51: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_41, rsqrt_default_11);  rsqrt_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant51 = self._param_constant51\n",
      "        mul_tensor_52: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant51, mul_tensor_51);  _param_constant51 = mul_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant52 = self._param_constant52\n",
      "        t_default_39: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant52);  _param_constant52 = None\n",
      "        view_default_140: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_52, [sym_size, 4096])\n",
      "        mm_default_39: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_140, t_default_39);  view_default_140 = t_default_39 = None\n",
      "        view_default_141: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_39, [1, sym_size, 11008]);  mm_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_5: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_141)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant53 = self._param_constant53\n",
      "        t_default_40: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant53);  _param_constant53 = None\n",
      "        view_default_142: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_52, [sym_size, 4096]);  mul_tensor_52 = None\n",
      "        mm_default_40: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_142, t_default_40);  view_default_142 = t_default_40 = None\n",
      "        view_default_143: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_40, [1, sym_size, 11008]);  mm_default_40 = None\n",
      "        mul_tensor_53: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_5, view_default_143);  silu_default_5 = view_default_143 = None\n",
      "        _param_constant54 = self._param_constant54\n",
      "        t_default_41: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant54);  _param_constant54 = None\n",
      "        sym_size_24: Sym(s0) = torch.ops.aten.sym_size(view_default_141, 1);  view_default_141 = None\n",
      "        view_default_144: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_53, [sym_size_24, 11008]);  mul_tensor_53 = None\n",
      "        mm_default_41: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_144, t_default_41);  view_default_144 = t_default_41 = None\n",
      "        view_default_145: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_41, [1, sym_size_24, 4096]);  mm_default_41 = sym_size_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_43: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_41, view_default_145);  add_tensor_41 = view_default_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_12: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_43, 2)\n",
      "        mean_dim_12: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_12, [-1], True);  pow_tensor_scalar_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_44: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_12, 1e-06);  mean_dim_12 = None\n",
      "        rsqrt_default_12: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_44);  add_tensor_44 = None\n",
      "        detach_default_18: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_12)\n",
      "        mul_tensor_54: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_43, rsqrt_default_12);  rsqrt_default_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant55 = self._param_constant55\n",
      "        mul_tensor_55: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant55, mul_tensor_54);  _param_constant55 = mul_tensor_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant56 = self._param_constant56\n",
      "        t_default_42: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant56);  _param_constant56 = None\n",
      "        view_default_146: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_55, [sym_size, 4096])\n",
      "        mm_default_42: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_146, t_default_42);  view_default_146 = t_default_42 = None\n",
      "        view_default_147: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_42, [1, sym_size, 4096]);  mm_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant57 = self._param_constant57\n",
      "        t_default_43: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant57);  _param_constant57 = None\n",
      "        view_default_148: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_55, [sym_size, 4096])\n",
      "        mm_default_43: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_148, t_default_43);  view_default_148 = t_default_43 = None\n",
      "        view_default_149: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_43, [1, sym_size, 4096]);  mm_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant58 = self._param_constant58\n",
      "        t_default_44: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant58);  _param_constant58 = None\n",
      "        view_default_150: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_55, [sym_size, 4096]);  mul_tensor_55 = None\n",
      "        mm_default_44: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_150, t_default_44);  view_default_150 = t_default_44 = None\n",
      "        view_default_151: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_44, [1, sym_size, 4096]);  mm_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_152: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_147, [1, sym_size, 32, 128])\n",
      "        transpose_int_30: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_152, 1, 2);  view_default_152 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_153: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_149, [1, sym_size, 32, 128])\n",
      "        transpose_int_31: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_153, 1, 2);  view_default_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_154: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_151, [1, sym_size, 32, 128])\n",
      "        transpose_int_32: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_154, 1, 2);  view_default_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant12 = self._tensor_constant12\n",
      "        slice_tensor_64: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant12, 0, 0, 9223372036854775807);  _tensor_constant12 = None\n",
      "        slice_tensor_65: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_64, 1, 0, 9223372036854775807);  slice_tensor_64 = None\n",
      "        sym_size_25: Sym(s0) = torch.ops.aten.sym_size(view_default_149, 1);  view_default_149 = None\n",
      "        slice_tensor_66: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_65, 2, 0, sym_size_25);  slice_tensor_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant13 = self._tensor_constant13\n",
      "        slice_tensor_67: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant13, 0, 0, 9223372036854775807);  _tensor_constant13 = None\n",
      "        slice_tensor_68: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_67, 1, 0, 9223372036854775807);  slice_tensor_67 = None\n",
      "        slice_tensor_69: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_68, 2, 0, sym_size_25);  slice_tensor_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_24: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_66, 1);  slice_tensor_66 = None\n",
      "        squeeze_dim_25: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_24, 0);  squeeze_dim_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_26: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_69, 1);  slice_tensor_69 = None\n",
      "        squeeze_dim_27: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_26, 0);  squeeze_dim_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_12: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_25, [view_default]);  squeeze_dim_25 = None\n",
      "        unsqueeze_default_17: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_12, 1);  index_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_13: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_27, [view_default]);  squeeze_dim_27 = None\n",
      "        unsqueeze_default_18: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_13, 1);  index_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_56: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_30, unsqueeze_default_17)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_70: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_71: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 64, 9223372036854775807);  transpose_int_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_12: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_71);  slice_tensor_71 = None\n",
      "        cat_default_12: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_12, slice_tensor_70], -1);  neg_default_12 = slice_tensor_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_57: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_12, unsqueeze_default_18);  cat_default_12 = None\n",
      "        add_tensor_45: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_56, mul_tensor_57);  mul_tensor_56 = mul_tensor_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_58: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_31, unsqueeze_default_17);  unsqueeze_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_72: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_73: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 64, 9223372036854775807);  transpose_int_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_13: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_73);  slice_tensor_73 = None\n",
      "        cat_default_13: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_13, slice_tensor_72], -1);  neg_default_13 = slice_tensor_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_59: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_13, unsqueeze_default_18);  cat_default_13 = unsqueeze_default_18 = None\n",
      "        add_tensor_46: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_58, mul_tensor_59);  mul_tensor_58 = mul_tensor_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_33: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_46, 2, 3)\n",
      "        sym_size_26: Sym(s0) = torch.ops.aten.sym_size(view_default_147, 1);  view_default_147 = None\n",
      "        expand_default_26: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_45, [1, 32, sym_size_26, 128]);  add_tensor_45 = None\n",
      "        view_default_155: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_26, [32, sym_size_26, 128]);  expand_default_26 = None\n",
      "        expand_default_27: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_33, [1, 32, 128, sym_size_25]);  transpose_int_33 = None\n",
      "        view_default_156: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_27, [32, 128, sym_size_25]);  expand_default_27 = None\n",
      "        bmm_default_12: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_155, view_default_156);  view_default_155 = view_default_156 = None\n",
      "        view_default_157: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_12, [1, 32, sym_size_26, sym_size_25]);  bmm_default_12 = None\n",
      "        div_tensor_6: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_157, 11.313708498984761);  view_default_157 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_47: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_6, add_tensor_1);  div_tensor_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_6: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_47, -1, False);  add_tensor_47 = None\n",
      "        detach_default_19: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_6)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_28: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_6, [1, 32, sym_size_26, sym_size_25]);  _softmax_default_6 = None\n",
      "        view_default_158: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_28, [32, sym_size_26, sym_size_25]);  expand_default_28 = sym_size_25 = None\n",
      "        sym_size_27: Sym(s0) = torch.ops.aten.sym_size(view_default_151, 1);  view_default_151 = None\n",
      "        expand_default_29: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_32, [1, 32, sym_size_27, 128])\n",
      "        view_default_159: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_29, [32, sym_size_27, 128]);  expand_default_29 = sym_size_27 = None\n",
      "        bmm_default_13: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_158, view_default_159);  view_default_158 = view_default_159 = None\n",
      "        view_default_160: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_13, [1, 32, sym_size_26, 128]);  bmm_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_34: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_160, 1, 2);  view_default_160 = None\n",
      "        clone_default_6: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_34, memory_format = torch.contiguous_format);  transpose_int_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_161: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_6, [1, sym_size, 4096]);  clone_default_6 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant59 = self._param_constant59\n",
      "        t_default_45: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant59);  _param_constant59 = None\n",
      "        view_default_162: f32[s0, 4096] = torch.ops.aten.view.default(view_default_161, [sym_size_26, 4096]);  view_default_161 = None\n",
      "        mm_default_45: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_162, t_default_45);  view_default_162 = t_default_45 = None\n",
      "        view_default_163: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_45, [1, sym_size_26, 4096]);  mm_default_45 = sym_size_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_48: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_43, view_default_163);  add_tensor_43 = view_default_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_13: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_48, 2)\n",
      "        mean_dim_13: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_13, [-1], True);  pow_tensor_scalar_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_49: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_13, 1e-06);  mean_dim_13 = None\n",
      "        rsqrt_default_13: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_49);  add_tensor_49 = None\n",
      "        detach_default_20: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_13)\n",
      "        mul_tensor_60: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_48, rsqrt_default_13);  rsqrt_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant60 = self._param_constant60\n",
      "        mul_tensor_61: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant60, mul_tensor_60);  _param_constant60 = mul_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant61 = self._param_constant61\n",
      "        t_default_46: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant61);  _param_constant61 = None\n",
      "        view_default_164: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_61, [sym_size, 4096])\n",
      "        mm_default_46: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_164, t_default_46);  view_default_164 = t_default_46 = None\n",
      "        view_default_165: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_46, [1, sym_size, 11008]);  mm_default_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_6: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_165)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant62 = self._param_constant62\n",
      "        t_default_47: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant62);  _param_constant62 = None\n",
      "        view_default_166: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_61, [sym_size, 4096]);  mul_tensor_61 = None\n",
      "        mm_default_47: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_166, t_default_47);  view_default_166 = t_default_47 = None\n",
      "        view_default_167: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_47, [1, sym_size, 11008]);  mm_default_47 = None\n",
      "        mul_tensor_62: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_6, view_default_167);  silu_default_6 = view_default_167 = None\n",
      "        _param_constant63 = self._param_constant63\n",
      "        t_default_48: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant63);  _param_constant63 = None\n",
      "        sym_size_28: Sym(s0) = torch.ops.aten.sym_size(view_default_165, 1);  view_default_165 = None\n",
      "        view_default_168: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_62, [sym_size_28, 11008]);  mul_tensor_62 = None\n",
      "        mm_default_48: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_168, t_default_48);  view_default_168 = t_default_48 = None\n",
      "        view_default_169: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_48, [1, sym_size_28, 4096]);  mm_default_48 = sym_size_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_50: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_48, view_default_169);  add_tensor_48 = view_default_169 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_14: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_50, 2)\n",
      "        mean_dim_14: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_14, [-1], True);  pow_tensor_scalar_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_51: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_14, 1e-06);  mean_dim_14 = None\n",
      "        rsqrt_default_14: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_51);  add_tensor_51 = None\n",
      "        detach_default_21: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_14)\n",
      "        mul_tensor_63: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_50, rsqrt_default_14);  rsqrt_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant64 = self._param_constant64\n",
      "        mul_tensor_64: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant64, mul_tensor_63);  _param_constant64 = mul_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant65 = self._param_constant65\n",
      "        t_default_49: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant65);  _param_constant65 = None\n",
      "        view_default_170: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_64, [sym_size, 4096])\n",
      "        mm_default_49: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_170, t_default_49);  view_default_170 = t_default_49 = None\n",
      "        view_default_171: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_49, [1, sym_size, 4096]);  mm_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant66 = self._param_constant66\n",
      "        t_default_50: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant66);  _param_constant66 = None\n",
      "        view_default_172: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_64, [sym_size, 4096])\n",
      "        mm_default_50: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_172, t_default_50);  view_default_172 = t_default_50 = None\n",
      "        view_default_173: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_50, [1, sym_size, 4096]);  mm_default_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant67 = self._param_constant67\n",
      "        t_default_51: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant67);  _param_constant67 = None\n",
      "        view_default_174: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_64, [sym_size, 4096]);  mul_tensor_64 = None\n",
      "        mm_default_51: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_174, t_default_51);  view_default_174 = t_default_51 = None\n",
      "        view_default_175: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_51, [1, sym_size, 4096]);  mm_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_176: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_171, [1, sym_size, 32, 128])\n",
      "        transpose_int_35: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_176, 1, 2);  view_default_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_177: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_173, [1, sym_size, 32, 128])\n",
      "        transpose_int_36: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_177, 1, 2);  view_default_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_178: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_175, [1, sym_size, 32, 128])\n",
      "        transpose_int_37: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_178, 1, 2);  view_default_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant14 = self._tensor_constant14\n",
      "        slice_tensor_74: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant14, 0, 0, 9223372036854775807);  _tensor_constant14 = None\n",
      "        slice_tensor_75: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_74, 1, 0, 9223372036854775807);  slice_tensor_74 = None\n",
      "        sym_size_29: Sym(s0) = torch.ops.aten.sym_size(view_default_173, 1);  view_default_173 = None\n",
      "        slice_tensor_76: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_75, 2, 0, sym_size_29);  slice_tensor_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant15 = self._tensor_constant15\n",
      "        slice_tensor_77: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant15, 0, 0, 9223372036854775807);  _tensor_constant15 = None\n",
      "        slice_tensor_78: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_77, 1, 0, 9223372036854775807);  slice_tensor_77 = None\n",
      "        slice_tensor_79: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_78, 2, 0, sym_size_29);  slice_tensor_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_28: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_76, 1);  slice_tensor_76 = None\n",
      "        squeeze_dim_29: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_28, 0);  squeeze_dim_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_30: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_79, 1);  slice_tensor_79 = None\n",
      "        squeeze_dim_31: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_30, 0);  squeeze_dim_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_14: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_29, [view_default]);  squeeze_dim_29 = None\n",
      "        unsqueeze_default_19: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_14, 1);  index_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_15: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_31, [view_default]);  squeeze_dim_31 = None\n",
      "        unsqueeze_default_20: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_15, 1);  index_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_65: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_35, unsqueeze_default_19)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_80: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_81: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 64, 9223372036854775807);  transpose_int_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_14: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_81);  slice_tensor_81 = None\n",
      "        cat_default_14: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_14, slice_tensor_80], -1);  neg_default_14 = slice_tensor_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_66: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_14, unsqueeze_default_20);  cat_default_14 = None\n",
      "        add_tensor_52: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_65, mul_tensor_66);  mul_tensor_65 = mul_tensor_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_67: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_36, unsqueeze_default_19);  unsqueeze_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_82: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_83: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 64, 9223372036854775807);  transpose_int_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_15: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_83);  slice_tensor_83 = None\n",
      "        cat_default_15: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_15, slice_tensor_82], -1);  neg_default_15 = slice_tensor_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_68: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_15, unsqueeze_default_20);  cat_default_15 = unsqueeze_default_20 = None\n",
      "        add_tensor_53: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_67, mul_tensor_68);  mul_tensor_67 = mul_tensor_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_38: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_53, 2, 3)\n",
      "        sym_size_30: Sym(s0) = torch.ops.aten.sym_size(view_default_171, 1);  view_default_171 = None\n",
      "        expand_default_30: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_52, [1, 32, sym_size_30, 128]);  add_tensor_52 = None\n",
      "        view_default_179: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_30, [32, sym_size_30, 128]);  expand_default_30 = None\n",
      "        expand_default_31: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_38, [1, 32, 128, sym_size_29]);  transpose_int_38 = None\n",
      "        view_default_180: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_31, [32, 128, sym_size_29]);  expand_default_31 = None\n",
      "        bmm_default_14: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_179, view_default_180);  view_default_179 = view_default_180 = None\n",
      "        view_default_181: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_14, [1, 32, sym_size_30, sym_size_29]);  bmm_default_14 = None\n",
      "        div_tensor_7: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_181, 11.313708498984761);  view_default_181 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_54: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_7, add_tensor_1);  div_tensor_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_7: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_54, -1, False);  add_tensor_54 = None\n",
      "        detach_default_22: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_7)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_32: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_7, [1, 32, sym_size_30, sym_size_29]);  _softmax_default_7 = None\n",
      "        view_default_182: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_32, [32, sym_size_30, sym_size_29]);  expand_default_32 = sym_size_29 = None\n",
      "        sym_size_31: Sym(s0) = torch.ops.aten.sym_size(view_default_175, 1);  view_default_175 = None\n",
      "        expand_default_33: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_37, [1, 32, sym_size_31, 128])\n",
      "        view_default_183: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_33, [32, sym_size_31, 128]);  expand_default_33 = sym_size_31 = None\n",
      "        bmm_default_15: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_182, view_default_183);  view_default_182 = view_default_183 = None\n",
      "        view_default_184: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_15, [1, 32, sym_size_30, 128]);  bmm_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_39: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_184, 1, 2);  view_default_184 = None\n",
      "        clone_default_7: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_39, memory_format = torch.contiguous_format);  transpose_int_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_185: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_7, [1, sym_size, 4096]);  clone_default_7 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant68 = self._param_constant68\n",
      "        t_default_52: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant68);  _param_constant68 = None\n",
      "        view_default_186: f32[s0, 4096] = torch.ops.aten.view.default(view_default_185, [sym_size_30, 4096]);  view_default_185 = None\n",
      "        mm_default_52: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_186, t_default_52);  view_default_186 = t_default_52 = None\n",
      "        view_default_187: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_52, [1, sym_size_30, 4096]);  mm_default_52 = sym_size_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_55: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_50, view_default_187);  add_tensor_50 = view_default_187 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_15: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_55, 2)\n",
      "        mean_dim_15: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_15, [-1], True);  pow_tensor_scalar_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_56: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_15, 1e-06);  mean_dim_15 = None\n",
      "        rsqrt_default_15: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_56);  add_tensor_56 = None\n",
      "        detach_default_23: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_15)\n",
      "        mul_tensor_69: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_55, rsqrt_default_15);  rsqrt_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant69 = self._param_constant69\n",
      "        mul_tensor_70: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant69, mul_tensor_69);  _param_constant69 = mul_tensor_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant70 = self._param_constant70\n",
      "        t_default_53: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant70);  _param_constant70 = None\n",
      "        view_default_188: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_70, [sym_size, 4096])\n",
      "        mm_default_53: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_188, t_default_53);  view_default_188 = t_default_53 = None\n",
      "        view_default_189: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_53, [1, sym_size, 11008]);  mm_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_7: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_189)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant71 = self._param_constant71\n",
      "        t_default_54: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant71);  _param_constant71 = None\n",
      "        view_default_190: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_70, [sym_size, 4096]);  mul_tensor_70 = None\n",
      "        mm_default_54: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_190, t_default_54);  view_default_190 = t_default_54 = None\n",
      "        view_default_191: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_54, [1, sym_size, 11008]);  mm_default_54 = None\n",
      "        mul_tensor_71: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_7, view_default_191);  silu_default_7 = view_default_191 = None\n",
      "        _param_constant72 = self._param_constant72\n",
      "        t_default_55: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant72);  _param_constant72 = None\n",
      "        sym_size_32: Sym(s0) = torch.ops.aten.sym_size(view_default_189, 1);  view_default_189 = None\n",
      "        view_default_192: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_71, [sym_size_32, 11008]);  mul_tensor_71 = None\n",
      "        mm_default_55: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_192, t_default_55);  view_default_192 = t_default_55 = None\n",
      "        view_default_193: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_55, [1, sym_size_32, 4096]);  mm_default_55 = sym_size_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_57: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_55, view_default_193);  add_tensor_55 = view_default_193 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_16: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_57, 2)\n",
      "        mean_dim_16: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_16, [-1], True);  pow_tensor_scalar_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_58: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_16, 1e-06);  mean_dim_16 = None\n",
      "        rsqrt_default_16: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_58);  add_tensor_58 = None\n",
      "        detach_default_24: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_16)\n",
      "        mul_tensor_72: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_57, rsqrt_default_16);  rsqrt_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant73 = self._param_constant73\n",
      "        mul_tensor_73: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant73, mul_tensor_72);  _param_constant73 = mul_tensor_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant74 = self._param_constant74\n",
      "        t_default_56: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant74);  _param_constant74 = None\n",
      "        view_default_194: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_73, [sym_size, 4096])\n",
      "        mm_default_56: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_194, t_default_56);  view_default_194 = t_default_56 = None\n",
      "        view_default_195: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_56, [1, sym_size, 4096]);  mm_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant75 = self._param_constant75\n",
      "        t_default_57: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant75);  _param_constant75 = None\n",
      "        view_default_196: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_73, [sym_size, 4096])\n",
      "        mm_default_57: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_196, t_default_57);  view_default_196 = t_default_57 = None\n",
      "        view_default_197: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_57, [1, sym_size, 4096]);  mm_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant76 = self._param_constant76\n",
      "        t_default_58: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant76);  _param_constant76 = None\n",
      "        view_default_198: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_73, [sym_size, 4096]);  mul_tensor_73 = None\n",
      "        mm_default_58: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_198, t_default_58);  view_default_198 = t_default_58 = None\n",
      "        view_default_199: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_58, [1, sym_size, 4096]);  mm_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_200: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_195, [1, sym_size, 32, 128])\n",
      "        transpose_int_40: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_200, 1, 2);  view_default_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_201: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_197, [1, sym_size, 32, 128])\n",
      "        transpose_int_41: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_201, 1, 2);  view_default_201 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_202: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_199, [1, sym_size, 32, 128])\n",
      "        transpose_int_42: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_202, 1, 2);  view_default_202 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant16 = self._tensor_constant16\n",
      "        slice_tensor_84: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant16, 0, 0, 9223372036854775807);  _tensor_constant16 = None\n",
      "        slice_tensor_85: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_84, 1, 0, 9223372036854775807);  slice_tensor_84 = None\n",
      "        sym_size_33: Sym(s0) = torch.ops.aten.sym_size(view_default_197, 1);  view_default_197 = None\n",
      "        slice_tensor_86: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_85, 2, 0, sym_size_33);  slice_tensor_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant17 = self._tensor_constant17\n",
      "        slice_tensor_87: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant17, 0, 0, 9223372036854775807);  _tensor_constant17 = None\n",
      "        slice_tensor_88: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_87, 1, 0, 9223372036854775807);  slice_tensor_87 = None\n",
      "        slice_tensor_89: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_88, 2, 0, sym_size_33);  slice_tensor_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_32: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_86, 1);  slice_tensor_86 = None\n",
      "        squeeze_dim_33: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_32, 0);  squeeze_dim_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_34: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_89, 1);  slice_tensor_89 = None\n",
      "        squeeze_dim_35: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_34, 0);  squeeze_dim_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_16: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_33, [view_default]);  squeeze_dim_33 = None\n",
      "        unsqueeze_default_21: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_16, 1);  index_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_17: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_35, [view_default]);  squeeze_dim_35 = None\n",
      "        unsqueeze_default_22: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_17, 1);  index_tensor_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_74: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_40, unsqueeze_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_90: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_91: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 64, 9223372036854775807);  transpose_int_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_16: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_91);  slice_tensor_91 = None\n",
      "        cat_default_16: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_16, slice_tensor_90], -1);  neg_default_16 = slice_tensor_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_75: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_16, unsqueeze_default_22);  cat_default_16 = None\n",
      "        add_tensor_59: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_74, mul_tensor_75);  mul_tensor_74 = mul_tensor_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_76: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_41, unsqueeze_default_21);  unsqueeze_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_92: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_93: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 64, 9223372036854775807);  transpose_int_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_17: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_93);  slice_tensor_93 = None\n",
      "        cat_default_17: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_17, slice_tensor_92], -1);  neg_default_17 = slice_tensor_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_77: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_17, unsqueeze_default_22);  cat_default_17 = unsqueeze_default_22 = None\n",
      "        add_tensor_60: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_76, mul_tensor_77);  mul_tensor_76 = mul_tensor_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_43: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_60, 2, 3)\n",
      "        sym_size_34: Sym(s0) = torch.ops.aten.sym_size(view_default_195, 1);  view_default_195 = None\n",
      "        expand_default_34: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_59, [1, 32, sym_size_34, 128]);  add_tensor_59 = None\n",
      "        view_default_203: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_34, [32, sym_size_34, 128]);  expand_default_34 = None\n",
      "        expand_default_35: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_43, [1, 32, 128, sym_size_33]);  transpose_int_43 = None\n",
      "        view_default_204: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_35, [32, 128, sym_size_33]);  expand_default_35 = None\n",
      "        bmm_default_16: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_203, view_default_204);  view_default_203 = view_default_204 = None\n",
      "        view_default_205: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_16, [1, 32, sym_size_34, sym_size_33]);  bmm_default_16 = None\n",
      "        div_tensor_8: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_205, 11.313708498984761);  view_default_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_61: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_8, add_tensor_1);  div_tensor_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_8: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_61, -1, False);  add_tensor_61 = None\n",
      "        detach_default_25: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_8)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_36: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_8, [1, 32, sym_size_34, sym_size_33]);  _softmax_default_8 = None\n",
      "        view_default_206: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_36, [32, sym_size_34, sym_size_33]);  expand_default_36 = sym_size_33 = None\n",
      "        sym_size_35: Sym(s0) = torch.ops.aten.sym_size(view_default_199, 1);  view_default_199 = None\n",
      "        expand_default_37: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_42, [1, 32, sym_size_35, 128])\n",
      "        view_default_207: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_37, [32, sym_size_35, 128]);  expand_default_37 = sym_size_35 = None\n",
      "        bmm_default_17: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_206, view_default_207);  view_default_206 = view_default_207 = None\n",
      "        view_default_208: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_17, [1, 32, sym_size_34, 128]);  bmm_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_44: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_208, 1, 2);  view_default_208 = None\n",
      "        clone_default_8: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_44, memory_format = torch.contiguous_format);  transpose_int_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_209: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_8, [1, sym_size, 4096]);  clone_default_8 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant77 = self._param_constant77\n",
      "        t_default_59: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant77);  _param_constant77 = None\n",
      "        view_default_210: f32[s0, 4096] = torch.ops.aten.view.default(view_default_209, [sym_size_34, 4096]);  view_default_209 = None\n",
      "        mm_default_59: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_210, t_default_59);  view_default_210 = t_default_59 = None\n",
      "        view_default_211: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_59, [1, sym_size_34, 4096]);  mm_default_59 = sym_size_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_62: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_57, view_default_211);  add_tensor_57 = view_default_211 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_17: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_62, 2)\n",
      "        mean_dim_17: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_17, [-1], True);  pow_tensor_scalar_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_63: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_17, 1e-06);  mean_dim_17 = None\n",
      "        rsqrt_default_17: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_63);  add_tensor_63 = None\n",
      "        detach_default_26: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_17)\n",
      "        mul_tensor_78: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_62, rsqrt_default_17);  rsqrt_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant78 = self._param_constant78\n",
      "        mul_tensor_79: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant78, mul_tensor_78);  _param_constant78 = mul_tensor_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant79 = self._param_constant79\n",
      "        t_default_60: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant79);  _param_constant79 = None\n",
      "        view_default_212: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_79, [sym_size, 4096])\n",
      "        mm_default_60: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_212, t_default_60);  view_default_212 = t_default_60 = None\n",
      "        view_default_213: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_60, [1, sym_size, 11008]);  mm_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_8: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_213)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant80 = self._param_constant80\n",
      "        t_default_61: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant80);  _param_constant80 = None\n",
      "        view_default_214: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_79, [sym_size, 4096]);  mul_tensor_79 = None\n",
      "        mm_default_61: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_214, t_default_61);  view_default_214 = t_default_61 = None\n",
      "        view_default_215: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_61, [1, sym_size, 11008]);  mm_default_61 = None\n",
      "        mul_tensor_80: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_8, view_default_215);  silu_default_8 = view_default_215 = None\n",
      "        _param_constant81 = self._param_constant81\n",
      "        t_default_62: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant81);  _param_constant81 = None\n",
      "        sym_size_36: Sym(s0) = torch.ops.aten.sym_size(view_default_213, 1);  view_default_213 = None\n",
      "        view_default_216: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_80, [sym_size_36, 11008]);  mul_tensor_80 = None\n",
      "        mm_default_62: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_216, t_default_62);  view_default_216 = t_default_62 = None\n",
      "        view_default_217: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_62, [1, sym_size_36, 4096]);  mm_default_62 = sym_size_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_64: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_62, view_default_217);  add_tensor_62 = view_default_217 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_18: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_64, 2)\n",
      "        mean_dim_18: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_18, [-1], True);  pow_tensor_scalar_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_65: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_18, 1e-06);  mean_dim_18 = None\n",
      "        rsqrt_default_18: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_65);  add_tensor_65 = None\n",
      "        detach_default_27: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_18)\n",
      "        mul_tensor_81: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_64, rsqrt_default_18);  rsqrt_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant82 = self._param_constant82\n",
      "        mul_tensor_82: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant82, mul_tensor_81);  _param_constant82 = mul_tensor_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant83 = self._param_constant83\n",
      "        t_default_63: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant83);  _param_constant83 = None\n",
      "        view_default_218: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_82, [sym_size, 4096])\n",
      "        mm_default_63: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_218, t_default_63);  view_default_218 = t_default_63 = None\n",
      "        view_default_219: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_63, [1, sym_size, 4096]);  mm_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant84 = self._param_constant84\n",
      "        t_default_64: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant84);  _param_constant84 = None\n",
      "        view_default_220: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_82, [sym_size, 4096])\n",
      "        mm_default_64: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_220, t_default_64);  view_default_220 = t_default_64 = None\n",
      "        view_default_221: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_64, [1, sym_size, 4096]);  mm_default_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant85 = self._param_constant85\n",
      "        t_default_65: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant85);  _param_constant85 = None\n",
      "        view_default_222: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_82, [sym_size, 4096]);  mul_tensor_82 = None\n",
      "        mm_default_65: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_222, t_default_65);  view_default_222 = t_default_65 = None\n",
      "        view_default_223: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_65, [1, sym_size, 4096]);  mm_default_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_224: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_219, [1, sym_size, 32, 128])\n",
      "        transpose_int_45: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_224, 1, 2);  view_default_224 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_225: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_221, [1, sym_size, 32, 128])\n",
      "        transpose_int_46: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_225, 1, 2);  view_default_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_226: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_223, [1, sym_size, 32, 128])\n",
      "        transpose_int_47: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_226, 1, 2);  view_default_226 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant18 = self._tensor_constant18\n",
      "        slice_tensor_94: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant18, 0, 0, 9223372036854775807);  _tensor_constant18 = None\n",
      "        slice_tensor_95: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_94, 1, 0, 9223372036854775807);  slice_tensor_94 = None\n",
      "        sym_size_37: Sym(s0) = torch.ops.aten.sym_size(view_default_221, 1);  view_default_221 = None\n",
      "        slice_tensor_96: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_95, 2, 0, sym_size_37);  slice_tensor_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant19 = self._tensor_constant19\n",
      "        slice_tensor_97: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant19, 0, 0, 9223372036854775807);  _tensor_constant19 = None\n",
      "        slice_tensor_98: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_97, 1, 0, 9223372036854775807);  slice_tensor_97 = None\n",
      "        slice_tensor_99: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_98, 2, 0, sym_size_37);  slice_tensor_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_36: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_96, 1);  slice_tensor_96 = None\n",
      "        squeeze_dim_37: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_36, 0);  squeeze_dim_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_38: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_99, 1);  slice_tensor_99 = None\n",
      "        squeeze_dim_39: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_38, 0);  squeeze_dim_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_18: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_37, [view_default]);  squeeze_dim_37 = None\n",
      "        unsqueeze_default_23: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_18, 1);  index_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_19: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_39, [view_default]);  squeeze_dim_39 = None\n",
      "        unsqueeze_default_24: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_19, 1);  index_tensor_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_83: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_45, unsqueeze_default_23)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_100: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_101: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 64, 9223372036854775807);  transpose_int_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_18: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_101);  slice_tensor_101 = None\n",
      "        cat_default_18: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_18, slice_tensor_100], -1);  neg_default_18 = slice_tensor_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_84: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_18, unsqueeze_default_24);  cat_default_18 = None\n",
      "        add_tensor_66: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_83, mul_tensor_84);  mul_tensor_83 = mul_tensor_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_85: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_46, unsqueeze_default_23);  unsqueeze_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_102: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_103: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 64, 9223372036854775807);  transpose_int_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_19: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_103);  slice_tensor_103 = None\n",
      "        cat_default_19: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_19, slice_tensor_102], -1);  neg_default_19 = slice_tensor_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_86: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_19, unsqueeze_default_24);  cat_default_19 = unsqueeze_default_24 = None\n",
      "        add_tensor_67: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_85, mul_tensor_86);  mul_tensor_85 = mul_tensor_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_48: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_67, 2, 3)\n",
      "        sym_size_38: Sym(s0) = torch.ops.aten.sym_size(view_default_219, 1);  view_default_219 = None\n",
      "        expand_default_38: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_66, [1, 32, sym_size_38, 128]);  add_tensor_66 = None\n",
      "        view_default_227: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_38, [32, sym_size_38, 128]);  expand_default_38 = None\n",
      "        expand_default_39: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_48, [1, 32, 128, sym_size_37]);  transpose_int_48 = None\n",
      "        view_default_228: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_39, [32, 128, sym_size_37]);  expand_default_39 = None\n",
      "        bmm_default_18: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_227, view_default_228);  view_default_227 = view_default_228 = None\n",
      "        view_default_229: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_18, [1, 32, sym_size_38, sym_size_37]);  bmm_default_18 = None\n",
      "        div_tensor_9: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_229, 11.313708498984761);  view_default_229 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_68: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_9, add_tensor_1);  div_tensor_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_9: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_68, -1, False);  add_tensor_68 = None\n",
      "        detach_default_28: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_9)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_40: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_9, [1, 32, sym_size_38, sym_size_37]);  _softmax_default_9 = None\n",
      "        view_default_230: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_40, [32, sym_size_38, sym_size_37]);  expand_default_40 = sym_size_37 = None\n",
      "        sym_size_39: Sym(s0) = torch.ops.aten.sym_size(view_default_223, 1);  view_default_223 = None\n",
      "        expand_default_41: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_47, [1, 32, sym_size_39, 128])\n",
      "        view_default_231: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_41, [32, sym_size_39, 128]);  expand_default_41 = sym_size_39 = None\n",
      "        bmm_default_19: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_230, view_default_231);  view_default_230 = view_default_231 = None\n",
      "        view_default_232: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_19, [1, 32, sym_size_38, 128]);  bmm_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_49: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_232, 1, 2);  view_default_232 = None\n",
      "        clone_default_9: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_49, memory_format = torch.contiguous_format);  transpose_int_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_233: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_9, [1, sym_size, 4096]);  clone_default_9 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant86 = self._param_constant86\n",
      "        t_default_66: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant86);  _param_constant86 = None\n",
      "        view_default_234: f32[s0, 4096] = torch.ops.aten.view.default(view_default_233, [sym_size_38, 4096]);  view_default_233 = None\n",
      "        mm_default_66: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_234, t_default_66);  view_default_234 = t_default_66 = None\n",
      "        view_default_235: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_66, [1, sym_size_38, 4096]);  mm_default_66 = sym_size_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_69: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_64, view_default_235);  add_tensor_64 = view_default_235 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_19: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_69, 2)\n",
      "        mean_dim_19: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_19, [-1], True);  pow_tensor_scalar_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_70: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_19, 1e-06);  mean_dim_19 = None\n",
      "        rsqrt_default_19: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_70);  add_tensor_70 = None\n",
      "        detach_default_29: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_19)\n",
      "        mul_tensor_87: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_69, rsqrt_default_19);  rsqrt_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant87 = self._param_constant87\n",
      "        mul_tensor_88: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant87, mul_tensor_87);  _param_constant87 = mul_tensor_87 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant88 = self._param_constant88\n",
      "        t_default_67: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant88);  _param_constant88 = None\n",
      "        view_default_236: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_88, [sym_size, 4096])\n",
      "        mm_default_67: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_236, t_default_67);  view_default_236 = t_default_67 = None\n",
      "        view_default_237: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_67, [1, sym_size, 11008]);  mm_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_9: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_237)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant89 = self._param_constant89\n",
      "        t_default_68: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant89);  _param_constant89 = None\n",
      "        view_default_238: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_88, [sym_size, 4096]);  mul_tensor_88 = None\n",
      "        mm_default_68: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_238, t_default_68);  view_default_238 = t_default_68 = None\n",
      "        view_default_239: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_68, [1, sym_size, 11008]);  mm_default_68 = None\n",
      "        mul_tensor_89: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_9, view_default_239);  silu_default_9 = view_default_239 = None\n",
      "        _param_constant90 = self._param_constant90\n",
      "        t_default_69: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant90);  _param_constant90 = None\n",
      "        sym_size_40: Sym(s0) = torch.ops.aten.sym_size(view_default_237, 1);  view_default_237 = None\n",
      "        view_default_240: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_89, [sym_size_40, 11008]);  mul_tensor_89 = None\n",
      "        mm_default_69: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_240, t_default_69);  view_default_240 = t_default_69 = None\n",
      "        view_default_241: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_69, [1, sym_size_40, 4096]);  mm_default_69 = sym_size_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_71: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_69, view_default_241);  add_tensor_69 = view_default_241 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_20: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_71, 2)\n",
      "        mean_dim_20: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_20, [-1], True);  pow_tensor_scalar_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_72: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_20, 1e-06);  mean_dim_20 = None\n",
      "        rsqrt_default_20: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_72);  add_tensor_72 = None\n",
      "        detach_default_30: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_20)\n",
      "        mul_tensor_90: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_71, rsqrt_default_20);  rsqrt_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant91 = self._param_constant91\n",
      "        mul_tensor_91: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant91, mul_tensor_90);  _param_constant91 = mul_tensor_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant92 = self._param_constant92\n",
      "        t_default_70: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant92);  _param_constant92 = None\n",
      "        view_default_242: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_91, [sym_size, 4096])\n",
      "        mm_default_70: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_242, t_default_70);  view_default_242 = t_default_70 = None\n",
      "        view_default_243: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_70, [1, sym_size, 4096]);  mm_default_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant93 = self._param_constant93\n",
      "        t_default_71: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant93);  _param_constant93 = None\n",
      "        view_default_244: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_91, [sym_size, 4096])\n",
      "        mm_default_71: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_244, t_default_71);  view_default_244 = t_default_71 = None\n",
      "        view_default_245: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_71, [1, sym_size, 4096]);  mm_default_71 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant94 = self._param_constant94\n",
      "        t_default_72: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant94);  _param_constant94 = None\n",
      "        view_default_246: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_91, [sym_size, 4096]);  mul_tensor_91 = None\n",
      "        mm_default_72: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_246, t_default_72);  view_default_246 = t_default_72 = None\n",
      "        view_default_247: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_72, [1, sym_size, 4096]);  mm_default_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_248: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_243, [1, sym_size, 32, 128])\n",
      "        transpose_int_50: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_248, 1, 2);  view_default_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_249: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_245, [1, sym_size, 32, 128])\n",
      "        transpose_int_51: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_249, 1, 2);  view_default_249 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_250: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_247, [1, sym_size, 32, 128])\n",
      "        transpose_int_52: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_250, 1, 2);  view_default_250 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant20 = self._tensor_constant20\n",
      "        slice_tensor_104: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant20, 0, 0, 9223372036854775807);  _tensor_constant20 = None\n",
      "        slice_tensor_105: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_104, 1, 0, 9223372036854775807);  slice_tensor_104 = None\n",
      "        sym_size_41: Sym(s0) = torch.ops.aten.sym_size(view_default_245, 1);  view_default_245 = None\n",
      "        slice_tensor_106: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_105, 2, 0, sym_size_41);  slice_tensor_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant21 = self._tensor_constant21\n",
      "        slice_tensor_107: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant21, 0, 0, 9223372036854775807);  _tensor_constant21 = None\n",
      "        slice_tensor_108: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_107, 1, 0, 9223372036854775807);  slice_tensor_107 = None\n",
      "        slice_tensor_109: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_108, 2, 0, sym_size_41);  slice_tensor_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_40: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_106, 1);  slice_tensor_106 = None\n",
      "        squeeze_dim_41: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_40, 0);  squeeze_dim_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_42: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_109, 1);  slice_tensor_109 = None\n",
      "        squeeze_dim_43: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_42, 0);  squeeze_dim_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_20: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_41, [view_default]);  squeeze_dim_41 = None\n",
      "        unsqueeze_default_25: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_20, 1);  index_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_21: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_43, [view_default]);  squeeze_dim_43 = None\n",
      "        unsqueeze_default_26: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_21, 1);  index_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_92: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_50, unsqueeze_default_25)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_110: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_111: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 64, 9223372036854775807);  transpose_int_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_20: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_111);  slice_tensor_111 = None\n",
      "        cat_default_20: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_20, slice_tensor_110], -1);  neg_default_20 = slice_tensor_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_93: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_20, unsqueeze_default_26);  cat_default_20 = None\n",
      "        add_tensor_73: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_92, mul_tensor_93);  mul_tensor_92 = mul_tensor_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_94: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_51, unsqueeze_default_25);  unsqueeze_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_112: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_113: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 64, 9223372036854775807);  transpose_int_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_21: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_113);  slice_tensor_113 = None\n",
      "        cat_default_21: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_21, slice_tensor_112], -1);  neg_default_21 = slice_tensor_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_95: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_21, unsqueeze_default_26);  cat_default_21 = unsqueeze_default_26 = None\n",
      "        add_tensor_74: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_94, mul_tensor_95);  mul_tensor_94 = mul_tensor_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_53: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_74, 2, 3)\n",
      "        sym_size_42: Sym(s0) = torch.ops.aten.sym_size(view_default_243, 1);  view_default_243 = None\n",
      "        expand_default_42: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_73, [1, 32, sym_size_42, 128]);  add_tensor_73 = None\n",
      "        view_default_251: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_42, [32, sym_size_42, 128]);  expand_default_42 = None\n",
      "        expand_default_43: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_53, [1, 32, 128, sym_size_41]);  transpose_int_53 = None\n",
      "        view_default_252: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_43, [32, 128, sym_size_41]);  expand_default_43 = None\n",
      "        bmm_default_20: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_251, view_default_252);  view_default_251 = view_default_252 = None\n",
      "        view_default_253: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_20, [1, 32, sym_size_42, sym_size_41]);  bmm_default_20 = None\n",
      "        div_tensor_10: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_253, 11.313708498984761);  view_default_253 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_75: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_10, add_tensor_1);  div_tensor_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_10: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_75, -1, False);  add_tensor_75 = None\n",
      "        detach_default_31: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_10)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_44: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_10, [1, 32, sym_size_42, sym_size_41]);  _softmax_default_10 = None\n",
      "        view_default_254: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_44, [32, sym_size_42, sym_size_41]);  expand_default_44 = sym_size_41 = None\n",
      "        sym_size_43: Sym(s0) = torch.ops.aten.sym_size(view_default_247, 1);  view_default_247 = None\n",
      "        expand_default_45: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_52, [1, 32, sym_size_43, 128])\n",
      "        view_default_255: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_45, [32, sym_size_43, 128]);  expand_default_45 = sym_size_43 = None\n",
      "        bmm_default_21: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_254, view_default_255);  view_default_254 = view_default_255 = None\n",
      "        view_default_256: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_21, [1, 32, sym_size_42, 128]);  bmm_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_54: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_256, 1, 2);  view_default_256 = None\n",
      "        clone_default_10: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_54, memory_format = torch.contiguous_format);  transpose_int_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_257: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_10, [1, sym_size, 4096]);  clone_default_10 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant95 = self._param_constant95\n",
      "        t_default_73: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant95);  _param_constant95 = None\n",
      "        view_default_258: f32[s0, 4096] = torch.ops.aten.view.default(view_default_257, [sym_size_42, 4096]);  view_default_257 = None\n",
      "        mm_default_73: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_258, t_default_73);  view_default_258 = t_default_73 = None\n",
      "        view_default_259: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_73, [1, sym_size_42, 4096]);  mm_default_73 = sym_size_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_76: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_71, view_default_259);  add_tensor_71 = view_default_259 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_21: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_76, 2)\n",
      "        mean_dim_21: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_21, [-1], True);  pow_tensor_scalar_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_77: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_21, 1e-06);  mean_dim_21 = None\n",
      "        rsqrt_default_21: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_77);  add_tensor_77 = None\n",
      "        detach_default_32: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_21)\n",
      "        mul_tensor_96: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_76, rsqrt_default_21);  rsqrt_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant96 = self._param_constant96\n",
      "        mul_tensor_97: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant96, mul_tensor_96);  _param_constant96 = mul_tensor_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant97 = self._param_constant97\n",
      "        t_default_74: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant97);  _param_constant97 = None\n",
      "        view_default_260: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_97, [sym_size, 4096])\n",
      "        mm_default_74: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_260, t_default_74);  view_default_260 = t_default_74 = None\n",
      "        view_default_261: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_74, [1, sym_size, 11008]);  mm_default_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_10: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_261)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant98 = self._param_constant98\n",
      "        t_default_75: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant98);  _param_constant98 = None\n",
      "        view_default_262: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_97, [sym_size, 4096]);  mul_tensor_97 = None\n",
      "        mm_default_75: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_262, t_default_75);  view_default_262 = t_default_75 = None\n",
      "        view_default_263: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_75, [1, sym_size, 11008]);  mm_default_75 = None\n",
      "        mul_tensor_98: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_10, view_default_263);  silu_default_10 = view_default_263 = None\n",
      "        _param_constant99 = self._param_constant99\n",
      "        t_default_76: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant99);  _param_constant99 = None\n",
      "        sym_size_44: Sym(s0) = torch.ops.aten.sym_size(view_default_261, 1);  view_default_261 = None\n",
      "        view_default_264: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_98, [sym_size_44, 11008]);  mul_tensor_98 = None\n",
      "        mm_default_76: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_264, t_default_76);  view_default_264 = t_default_76 = None\n",
      "        view_default_265: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_76, [1, sym_size_44, 4096]);  mm_default_76 = sym_size_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_78: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_76, view_default_265);  add_tensor_76 = view_default_265 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_22: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_78, 2)\n",
      "        mean_dim_22: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_22, [-1], True);  pow_tensor_scalar_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_79: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_22, 1e-06);  mean_dim_22 = None\n",
      "        rsqrt_default_22: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_79);  add_tensor_79 = None\n",
      "        detach_default_33: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_22)\n",
      "        mul_tensor_99: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_78, rsqrt_default_22);  rsqrt_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant100 = self._param_constant100\n",
      "        mul_tensor_100: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant100, mul_tensor_99);  _param_constant100 = mul_tensor_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant101 = self._param_constant101\n",
      "        t_default_77: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant101);  _param_constant101 = None\n",
      "        view_default_266: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_100, [sym_size, 4096])\n",
      "        mm_default_77: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_266, t_default_77);  view_default_266 = t_default_77 = None\n",
      "        view_default_267: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_77, [1, sym_size, 4096]);  mm_default_77 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant102 = self._param_constant102\n",
      "        t_default_78: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant102);  _param_constant102 = None\n",
      "        view_default_268: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_100, [sym_size, 4096])\n",
      "        mm_default_78: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_268, t_default_78);  view_default_268 = t_default_78 = None\n",
      "        view_default_269: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_78, [1, sym_size, 4096]);  mm_default_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant103 = self._param_constant103\n",
      "        t_default_79: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant103);  _param_constant103 = None\n",
      "        view_default_270: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_100, [sym_size, 4096]);  mul_tensor_100 = None\n",
      "        mm_default_79: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_270, t_default_79);  view_default_270 = t_default_79 = None\n",
      "        view_default_271: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_79, [1, sym_size, 4096]);  mm_default_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_272: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_267, [1, sym_size, 32, 128])\n",
      "        transpose_int_55: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_272, 1, 2);  view_default_272 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_273: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_269, [1, sym_size, 32, 128])\n",
      "        transpose_int_56: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_273, 1, 2);  view_default_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_274: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_271, [1, sym_size, 32, 128])\n",
      "        transpose_int_57: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_274, 1, 2);  view_default_274 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant22 = self._tensor_constant22\n",
      "        slice_tensor_114: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant22, 0, 0, 9223372036854775807);  _tensor_constant22 = None\n",
      "        slice_tensor_115: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_114, 1, 0, 9223372036854775807);  slice_tensor_114 = None\n",
      "        sym_size_45: Sym(s0) = torch.ops.aten.sym_size(view_default_269, 1);  view_default_269 = None\n",
      "        slice_tensor_116: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_115, 2, 0, sym_size_45);  slice_tensor_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant23 = self._tensor_constant23\n",
      "        slice_tensor_117: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant23, 0, 0, 9223372036854775807);  _tensor_constant23 = None\n",
      "        slice_tensor_118: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_117, 1, 0, 9223372036854775807);  slice_tensor_117 = None\n",
      "        slice_tensor_119: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_118, 2, 0, sym_size_45);  slice_tensor_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_44: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_116, 1);  slice_tensor_116 = None\n",
      "        squeeze_dim_45: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_44, 0);  squeeze_dim_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_46: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_119, 1);  slice_tensor_119 = None\n",
      "        squeeze_dim_47: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_46, 0);  squeeze_dim_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_22: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_45, [view_default]);  squeeze_dim_45 = None\n",
      "        unsqueeze_default_27: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_22, 1);  index_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_23: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_47, [view_default]);  squeeze_dim_47 = None\n",
      "        unsqueeze_default_28: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_23, 1);  index_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_101: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_55, unsqueeze_default_27)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_120: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_121: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 64, 9223372036854775807);  transpose_int_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_22: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_121);  slice_tensor_121 = None\n",
      "        cat_default_22: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_22, slice_tensor_120], -1);  neg_default_22 = slice_tensor_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_102: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_22, unsqueeze_default_28);  cat_default_22 = None\n",
      "        add_tensor_80: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_101, mul_tensor_102);  mul_tensor_101 = mul_tensor_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_103: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_56, unsqueeze_default_27);  unsqueeze_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_122: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_123: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 64, 9223372036854775807);  transpose_int_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_23: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_123);  slice_tensor_123 = None\n",
      "        cat_default_23: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_23, slice_tensor_122], -1);  neg_default_23 = slice_tensor_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_104: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_23, unsqueeze_default_28);  cat_default_23 = unsqueeze_default_28 = None\n",
      "        add_tensor_81: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_103, mul_tensor_104);  mul_tensor_103 = mul_tensor_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_58: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_81, 2, 3)\n",
      "        sym_size_46: Sym(s0) = torch.ops.aten.sym_size(view_default_267, 1);  view_default_267 = None\n",
      "        expand_default_46: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_80, [1, 32, sym_size_46, 128]);  add_tensor_80 = None\n",
      "        view_default_275: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_46, [32, sym_size_46, 128]);  expand_default_46 = None\n",
      "        expand_default_47: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_58, [1, 32, 128, sym_size_45]);  transpose_int_58 = None\n",
      "        view_default_276: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_47, [32, 128, sym_size_45]);  expand_default_47 = None\n",
      "        bmm_default_22: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_275, view_default_276);  view_default_275 = view_default_276 = None\n",
      "        view_default_277: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_22, [1, 32, sym_size_46, sym_size_45]);  bmm_default_22 = None\n",
      "        div_tensor_11: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_277, 11.313708498984761);  view_default_277 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_82: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_11, add_tensor_1);  div_tensor_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_11: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_82, -1, False);  add_tensor_82 = None\n",
      "        detach_default_34: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_11)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_48: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_11, [1, 32, sym_size_46, sym_size_45]);  _softmax_default_11 = None\n",
      "        view_default_278: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_48, [32, sym_size_46, sym_size_45]);  expand_default_48 = sym_size_45 = None\n",
      "        sym_size_47: Sym(s0) = torch.ops.aten.sym_size(view_default_271, 1);  view_default_271 = None\n",
      "        expand_default_49: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_57, [1, 32, sym_size_47, 128])\n",
      "        view_default_279: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_49, [32, sym_size_47, 128]);  expand_default_49 = sym_size_47 = None\n",
      "        bmm_default_23: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_278, view_default_279);  view_default_278 = view_default_279 = None\n",
      "        view_default_280: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_23, [1, 32, sym_size_46, 128]);  bmm_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_59: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_280, 1, 2);  view_default_280 = None\n",
      "        clone_default_11: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_59, memory_format = torch.contiguous_format);  transpose_int_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_281: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_11, [1, sym_size, 4096]);  clone_default_11 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant104 = self._param_constant104\n",
      "        t_default_80: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant104);  _param_constant104 = None\n",
      "        view_default_282: f32[s0, 4096] = torch.ops.aten.view.default(view_default_281, [sym_size_46, 4096]);  view_default_281 = None\n",
      "        mm_default_80: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_282, t_default_80);  view_default_282 = t_default_80 = None\n",
      "        view_default_283: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_80, [1, sym_size_46, 4096]);  mm_default_80 = sym_size_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_83: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_78, view_default_283);  add_tensor_78 = view_default_283 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_23: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_83, 2)\n",
      "        mean_dim_23: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_23, [-1], True);  pow_tensor_scalar_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_84: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_23, 1e-06);  mean_dim_23 = None\n",
      "        rsqrt_default_23: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_84);  add_tensor_84 = None\n",
      "        detach_default_35: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_23)\n",
      "        mul_tensor_105: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_83, rsqrt_default_23);  rsqrt_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant105 = self._param_constant105\n",
      "        mul_tensor_106: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant105, mul_tensor_105);  _param_constant105 = mul_tensor_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant106 = self._param_constant106\n",
      "        t_default_81: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant106);  _param_constant106 = None\n",
      "        view_default_284: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_106, [sym_size, 4096])\n",
      "        mm_default_81: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_284, t_default_81);  view_default_284 = t_default_81 = None\n",
      "        view_default_285: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_81, [1, sym_size, 11008]);  mm_default_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_11: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_285)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant107 = self._param_constant107\n",
      "        t_default_82: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant107);  _param_constant107 = None\n",
      "        view_default_286: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_106, [sym_size, 4096]);  mul_tensor_106 = None\n",
      "        mm_default_82: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_286, t_default_82);  view_default_286 = t_default_82 = None\n",
      "        view_default_287: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_82, [1, sym_size, 11008]);  mm_default_82 = None\n",
      "        mul_tensor_107: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_11, view_default_287);  silu_default_11 = view_default_287 = None\n",
      "        _param_constant108 = self._param_constant108\n",
      "        t_default_83: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant108);  _param_constant108 = None\n",
      "        sym_size_48: Sym(s0) = torch.ops.aten.sym_size(view_default_285, 1);  view_default_285 = None\n",
      "        view_default_288: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_107, [sym_size_48, 11008]);  mul_tensor_107 = None\n",
      "        mm_default_83: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_288, t_default_83);  view_default_288 = t_default_83 = None\n",
      "        view_default_289: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_83, [1, sym_size_48, 4096]);  mm_default_83 = sym_size_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_85: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_83, view_default_289);  add_tensor_83 = view_default_289 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_24: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_85, 2)\n",
      "        mean_dim_24: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_24, [-1], True);  pow_tensor_scalar_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_86: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_24, 1e-06);  mean_dim_24 = None\n",
      "        rsqrt_default_24: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_86);  add_tensor_86 = None\n",
      "        detach_default_36: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_24)\n",
      "        mul_tensor_108: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_85, rsqrt_default_24);  rsqrt_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant109 = self._param_constant109\n",
      "        mul_tensor_109: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant109, mul_tensor_108);  _param_constant109 = mul_tensor_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant110 = self._param_constant110\n",
      "        t_default_84: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant110);  _param_constant110 = None\n",
      "        view_default_290: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_109, [sym_size, 4096])\n",
      "        mm_default_84: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_290, t_default_84);  view_default_290 = t_default_84 = None\n",
      "        view_default_291: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_84, [1, sym_size, 4096]);  mm_default_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant111 = self._param_constant111\n",
      "        t_default_85: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant111);  _param_constant111 = None\n",
      "        view_default_292: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_109, [sym_size, 4096])\n",
      "        mm_default_85: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_292, t_default_85);  view_default_292 = t_default_85 = None\n",
      "        view_default_293: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_85, [1, sym_size, 4096]);  mm_default_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant112 = self._param_constant112\n",
      "        t_default_86: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant112);  _param_constant112 = None\n",
      "        view_default_294: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_109, [sym_size, 4096]);  mul_tensor_109 = None\n",
      "        mm_default_86: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_294, t_default_86);  view_default_294 = t_default_86 = None\n",
      "        view_default_295: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_86, [1, sym_size, 4096]);  mm_default_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_296: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_291, [1, sym_size, 32, 128])\n",
      "        transpose_int_60: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_296, 1, 2);  view_default_296 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_297: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_293, [1, sym_size, 32, 128])\n",
      "        transpose_int_61: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_297, 1, 2);  view_default_297 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_298: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_295, [1, sym_size, 32, 128])\n",
      "        transpose_int_62: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_298, 1, 2);  view_default_298 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant24 = self._tensor_constant24\n",
      "        slice_tensor_124: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant24, 0, 0, 9223372036854775807);  _tensor_constant24 = None\n",
      "        slice_tensor_125: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_124, 1, 0, 9223372036854775807);  slice_tensor_124 = None\n",
      "        sym_size_49: Sym(s0) = torch.ops.aten.sym_size(view_default_293, 1);  view_default_293 = None\n",
      "        slice_tensor_126: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_125, 2, 0, sym_size_49);  slice_tensor_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant25 = self._tensor_constant25\n",
      "        slice_tensor_127: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant25, 0, 0, 9223372036854775807);  _tensor_constant25 = None\n",
      "        slice_tensor_128: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_127, 1, 0, 9223372036854775807);  slice_tensor_127 = None\n",
      "        slice_tensor_129: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_128, 2, 0, sym_size_49);  slice_tensor_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_48: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_126, 1);  slice_tensor_126 = None\n",
      "        squeeze_dim_49: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_48, 0);  squeeze_dim_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_50: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_129, 1);  slice_tensor_129 = None\n",
      "        squeeze_dim_51: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_50, 0);  squeeze_dim_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_24: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_49, [view_default]);  squeeze_dim_49 = None\n",
      "        unsqueeze_default_29: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_24, 1);  index_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_25: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_51, [view_default]);  squeeze_dim_51 = None\n",
      "        unsqueeze_default_30: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_25, 1);  index_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_110: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_60, unsqueeze_default_29)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_130: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_131: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 64, 9223372036854775807);  transpose_int_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_24: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_131);  slice_tensor_131 = None\n",
      "        cat_default_24: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_24, slice_tensor_130], -1);  neg_default_24 = slice_tensor_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_111: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_24, unsqueeze_default_30);  cat_default_24 = None\n",
      "        add_tensor_87: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_110, mul_tensor_111);  mul_tensor_110 = mul_tensor_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_112: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_61, unsqueeze_default_29);  unsqueeze_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_132: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_133: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 64, 9223372036854775807);  transpose_int_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_25: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_133);  slice_tensor_133 = None\n",
      "        cat_default_25: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_25, slice_tensor_132], -1);  neg_default_25 = slice_tensor_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_113: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_25, unsqueeze_default_30);  cat_default_25 = unsqueeze_default_30 = None\n",
      "        add_tensor_88: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_112, mul_tensor_113);  mul_tensor_112 = mul_tensor_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_63: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_88, 2, 3)\n",
      "        sym_size_50: Sym(s0) = torch.ops.aten.sym_size(view_default_291, 1);  view_default_291 = None\n",
      "        expand_default_50: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_87, [1, 32, sym_size_50, 128]);  add_tensor_87 = None\n",
      "        view_default_299: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_50, [32, sym_size_50, 128]);  expand_default_50 = None\n",
      "        expand_default_51: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_63, [1, 32, 128, sym_size_49]);  transpose_int_63 = None\n",
      "        view_default_300: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_51, [32, 128, sym_size_49]);  expand_default_51 = None\n",
      "        bmm_default_24: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_299, view_default_300);  view_default_299 = view_default_300 = None\n",
      "        view_default_301: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_24, [1, 32, sym_size_50, sym_size_49]);  bmm_default_24 = None\n",
      "        div_tensor_12: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_301, 11.313708498984761);  view_default_301 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_89: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_12, add_tensor_1);  div_tensor_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_12: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_89, -1, False);  add_tensor_89 = None\n",
      "        detach_default_37: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_12)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_52: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_12, [1, 32, sym_size_50, sym_size_49]);  _softmax_default_12 = None\n",
      "        view_default_302: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_52, [32, sym_size_50, sym_size_49]);  expand_default_52 = sym_size_49 = None\n",
      "        sym_size_51: Sym(s0) = torch.ops.aten.sym_size(view_default_295, 1);  view_default_295 = None\n",
      "        expand_default_53: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_62, [1, 32, sym_size_51, 128])\n",
      "        view_default_303: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_53, [32, sym_size_51, 128]);  expand_default_53 = sym_size_51 = None\n",
      "        bmm_default_25: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_302, view_default_303);  view_default_302 = view_default_303 = None\n",
      "        view_default_304: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_25, [1, 32, sym_size_50, 128]);  bmm_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_64: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_304, 1, 2);  view_default_304 = None\n",
      "        clone_default_12: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_64, memory_format = torch.contiguous_format);  transpose_int_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_305: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_12, [1, sym_size, 4096]);  clone_default_12 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant113 = self._param_constant113\n",
      "        t_default_87: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant113);  _param_constant113 = None\n",
      "        view_default_306: f32[s0, 4096] = torch.ops.aten.view.default(view_default_305, [sym_size_50, 4096]);  view_default_305 = None\n",
      "        mm_default_87: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_306, t_default_87);  view_default_306 = t_default_87 = None\n",
      "        view_default_307: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_87, [1, sym_size_50, 4096]);  mm_default_87 = sym_size_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_90: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_85, view_default_307);  add_tensor_85 = view_default_307 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_25: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_90, 2)\n",
      "        mean_dim_25: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_25, [-1], True);  pow_tensor_scalar_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_91: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_25, 1e-06);  mean_dim_25 = None\n",
      "        rsqrt_default_25: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_91);  add_tensor_91 = None\n",
      "        detach_default_38: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_25)\n",
      "        mul_tensor_114: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_90, rsqrt_default_25);  rsqrt_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant114 = self._param_constant114\n",
      "        mul_tensor_115: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant114, mul_tensor_114);  _param_constant114 = mul_tensor_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant115 = self._param_constant115\n",
      "        t_default_88: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant115);  _param_constant115 = None\n",
      "        view_default_308: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_115, [sym_size, 4096])\n",
      "        mm_default_88: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_308, t_default_88);  view_default_308 = t_default_88 = None\n",
      "        view_default_309: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_88, [1, sym_size, 11008]);  mm_default_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_12: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_309)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant116 = self._param_constant116\n",
      "        t_default_89: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant116);  _param_constant116 = None\n",
      "        view_default_310: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_115, [sym_size, 4096]);  mul_tensor_115 = None\n",
      "        mm_default_89: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_310, t_default_89);  view_default_310 = t_default_89 = None\n",
      "        view_default_311: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_89, [1, sym_size, 11008]);  mm_default_89 = None\n",
      "        mul_tensor_116: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_12, view_default_311);  silu_default_12 = view_default_311 = None\n",
      "        _param_constant117 = self._param_constant117\n",
      "        t_default_90: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant117);  _param_constant117 = None\n",
      "        sym_size_52: Sym(s0) = torch.ops.aten.sym_size(view_default_309, 1);  view_default_309 = None\n",
      "        view_default_312: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_116, [sym_size_52, 11008]);  mul_tensor_116 = None\n",
      "        mm_default_90: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_312, t_default_90);  view_default_312 = t_default_90 = None\n",
      "        view_default_313: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_90, [1, sym_size_52, 4096]);  mm_default_90 = sym_size_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_92: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_90, view_default_313);  add_tensor_90 = view_default_313 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_26: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_92, 2)\n",
      "        mean_dim_26: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_26, [-1], True);  pow_tensor_scalar_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_93: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_26, 1e-06);  mean_dim_26 = None\n",
      "        rsqrt_default_26: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_93);  add_tensor_93 = None\n",
      "        detach_default_39: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_26)\n",
      "        mul_tensor_117: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_92, rsqrt_default_26);  rsqrt_default_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant118 = self._param_constant118\n",
      "        mul_tensor_118: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant118, mul_tensor_117);  _param_constant118 = mul_tensor_117 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant119 = self._param_constant119\n",
      "        t_default_91: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant119);  _param_constant119 = None\n",
      "        view_default_314: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_118, [sym_size, 4096])\n",
      "        mm_default_91: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_314, t_default_91);  view_default_314 = t_default_91 = None\n",
      "        view_default_315: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_91, [1, sym_size, 4096]);  mm_default_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant120 = self._param_constant120\n",
      "        t_default_92: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant120);  _param_constant120 = None\n",
      "        view_default_316: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_118, [sym_size, 4096])\n",
      "        mm_default_92: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_316, t_default_92);  view_default_316 = t_default_92 = None\n",
      "        view_default_317: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_92, [1, sym_size, 4096]);  mm_default_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant121 = self._param_constant121\n",
      "        t_default_93: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant121);  _param_constant121 = None\n",
      "        view_default_318: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_118, [sym_size, 4096]);  mul_tensor_118 = None\n",
      "        mm_default_93: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_318, t_default_93);  view_default_318 = t_default_93 = None\n",
      "        view_default_319: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_93, [1, sym_size, 4096]);  mm_default_93 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_320: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_315, [1, sym_size, 32, 128])\n",
      "        transpose_int_65: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_320, 1, 2);  view_default_320 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_321: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_317, [1, sym_size, 32, 128])\n",
      "        transpose_int_66: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_321, 1, 2);  view_default_321 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_322: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_319, [1, sym_size, 32, 128])\n",
      "        transpose_int_67: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_322, 1, 2);  view_default_322 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant26 = self._tensor_constant26\n",
      "        slice_tensor_134: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant26, 0, 0, 9223372036854775807);  _tensor_constant26 = None\n",
      "        slice_tensor_135: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_134, 1, 0, 9223372036854775807);  slice_tensor_134 = None\n",
      "        sym_size_53: Sym(s0) = torch.ops.aten.sym_size(view_default_317, 1);  view_default_317 = None\n",
      "        slice_tensor_136: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_135, 2, 0, sym_size_53);  slice_tensor_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant27 = self._tensor_constant27\n",
      "        slice_tensor_137: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant27, 0, 0, 9223372036854775807);  _tensor_constant27 = None\n",
      "        slice_tensor_138: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_137, 1, 0, 9223372036854775807);  slice_tensor_137 = None\n",
      "        slice_tensor_139: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_138, 2, 0, sym_size_53);  slice_tensor_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_52: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_136, 1);  slice_tensor_136 = None\n",
      "        squeeze_dim_53: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_52, 0);  squeeze_dim_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_54: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_139, 1);  slice_tensor_139 = None\n",
      "        squeeze_dim_55: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_54, 0);  squeeze_dim_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_26: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_53, [view_default]);  squeeze_dim_53 = None\n",
      "        unsqueeze_default_31: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_26, 1);  index_tensor_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_27: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_55, [view_default]);  squeeze_dim_55 = None\n",
      "        unsqueeze_default_32: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_27, 1);  index_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_119: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_65, unsqueeze_default_31)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_140: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_141: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 64, 9223372036854775807);  transpose_int_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_26: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_141);  slice_tensor_141 = None\n",
      "        cat_default_26: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_26, slice_tensor_140], -1);  neg_default_26 = slice_tensor_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_120: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_26, unsqueeze_default_32);  cat_default_26 = None\n",
      "        add_tensor_94: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_119, mul_tensor_120);  mul_tensor_119 = mul_tensor_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_121: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_66, unsqueeze_default_31);  unsqueeze_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_142: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_143: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 64, 9223372036854775807);  transpose_int_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_27: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_143);  slice_tensor_143 = None\n",
      "        cat_default_27: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_27, slice_tensor_142], -1);  neg_default_27 = slice_tensor_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_122: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_27, unsqueeze_default_32);  cat_default_27 = unsqueeze_default_32 = None\n",
      "        add_tensor_95: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_121, mul_tensor_122);  mul_tensor_121 = mul_tensor_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_68: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_95, 2, 3)\n",
      "        sym_size_54: Sym(s0) = torch.ops.aten.sym_size(view_default_315, 1);  view_default_315 = None\n",
      "        expand_default_54: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_94, [1, 32, sym_size_54, 128]);  add_tensor_94 = None\n",
      "        view_default_323: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_54, [32, sym_size_54, 128]);  expand_default_54 = None\n",
      "        expand_default_55: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_68, [1, 32, 128, sym_size_53]);  transpose_int_68 = None\n",
      "        view_default_324: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_55, [32, 128, sym_size_53]);  expand_default_55 = None\n",
      "        bmm_default_26: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_323, view_default_324);  view_default_323 = view_default_324 = None\n",
      "        view_default_325: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_26, [1, 32, sym_size_54, sym_size_53]);  bmm_default_26 = None\n",
      "        div_tensor_13: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_325, 11.313708498984761);  view_default_325 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_96: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_13, add_tensor_1);  div_tensor_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_13: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_96, -1, False);  add_tensor_96 = None\n",
      "        detach_default_40: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_13)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_56: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_13, [1, 32, sym_size_54, sym_size_53]);  _softmax_default_13 = None\n",
      "        view_default_326: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_56, [32, sym_size_54, sym_size_53]);  expand_default_56 = sym_size_53 = None\n",
      "        sym_size_55: Sym(s0) = torch.ops.aten.sym_size(view_default_319, 1);  view_default_319 = None\n",
      "        expand_default_57: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_67, [1, 32, sym_size_55, 128])\n",
      "        view_default_327: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_57, [32, sym_size_55, 128]);  expand_default_57 = sym_size_55 = None\n",
      "        bmm_default_27: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_326, view_default_327);  view_default_326 = view_default_327 = None\n",
      "        view_default_328: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_27, [1, 32, sym_size_54, 128]);  bmm_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_69: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_328, 1, 2);  view_default_328 = None\n",
      "        clone_default_13: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_69, memory_format = torch.contiguous_format);  transpose_int_69 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_329: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_13, [1, sym_size, 4096]);  clone_default_13 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant122 = self._param_constant122\n",
      "        t_default_94: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant122);  _param_constant122 = None\n",
      "        view_default_330: f32[s0, 4096] = torch.ops.aten.view.default(view_default_329, [sym_size_54, 4096]);  view_default_329 = None\n",
      "        mm_default_94: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_330, t_default_94);  view_default_330 = t_default_94 = None\n",
      "        view_default_331: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_94, [1, sym_size_54, 4096]);  mm_default_94 = sym_size_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_97: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_92, view_default_331);  add_tensor_92 = view_default_331 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_27: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_97, 2)\n",
      "        mean_dim_27: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_27, [-1], True);  pow_tensor_scalar_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_98: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_27, 1e-06);  mean_dim_27 = None\n",
      "        rsqrt_default_27: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_98);  add_tensor_98 = None\n",
      "        detach_default_41: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_27)\n",
      "        mul_tensor_123: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_97, rsqrt_default_27);  rsqrt_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant123 = self._param_constant123\n",
      "        mul_tensor_124: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant123, mul_tensor_123);  _param_constant123 = mul_tensor_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant124 = self._param_constant124\n",
      "        t_default_95: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant124);  _param_constant124 = None\n",
      "        view_default_332: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_124, [sym_size, 4096])\n",
      "        mm_default_95: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_332, t_default_95);  view_default_332 = t_default_95 = None\n",
      "        view_default_333: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_95, [1, sym_size, 11008]);  mm_default_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_13: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_333)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant125 = self._param_constant125\n",
      "        t_default_96: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant125);  _param_constant125 = None\n",
      "        view_default_334: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_124, [sym_size, 4096]);  mul_tensor_124 = None\n",
      "        mm_default_96: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_334, t_default_96);  view_default_334 = t_default_96 = None\n",
      "        view_default_335: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_96, [1, sym_size, 11008]);  mm_default_96 = None\n",
      "        mul_tensor_125: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_13, view_default_335);  silu_default_13 = view_default_335 = None\n",
      "        _param_constant126 = self._param_constant126\n",
      "        t_default_97: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant126);  _param_constant126 = None\n",
      "        sym_size_56: Sym(s0) = torch.ops.aten.sym_size(view_default_333, 1);  view_default_333 = None\n",
      "        view_default_336: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_125, [sym_size_56, 11008]);  mul_tensor_125 = None\n",
      "        mm_default_97: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_336, t_default_97);  view_default_336 = t_default_97 = None\n",
      "        view_default_337: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_97, [1, sym_size_56, 4096]);  mm_default_97 = sym_size_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_99: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_97, view_default_337);  add_tensor_97 = view_default_337 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_28: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_99, 2)\n",
      "        mean_dim_28: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_28, [-1], True);  pow_tensor_scalar_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_100: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_28, 1e-06);  mean_dim_28 = None\n",
      "        rsqrt_default_28: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_100);  add_tensor_100 = None\n",
      "        detach_default_42: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_28)\n",
      "        mul_tensor_126: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_99, rsqrt_default_28);  rsqrt_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant127 = self._param_constant127\n",
      "        mul_tensor_127: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant127, mul_tensor_126);  _param_constant127 = mul_tensor_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant128 = self._param_constant128\n",
      "        t_default_98: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant128);  _param_constant128 = None\n",
      "        view_default_338: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_127, [sym_size, 4096])\n",
      "        mm_default_98: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_338, t_default_98);  view_default_338 = t_default_98 = None\n",
      "        view_default_339: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_98, [1, sym_size, 4096]);  mm_default_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant129 = self._param_constant129\n",
      "        t_default_99: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant129);  _param_constant129 = None\n",
      "        view_default_340: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_127, [sym_size, 4096])\n",
      "        mm_default_99: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_340, t_default_99);  view_default_340 = t_default_99 = None\n",
      "        view_default_341: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_99, [1, sym_size, 4096]);  mm_default_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant130 = self._param_constant130\n",
      "        t_default_100: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant130);  _param_constant130 = None\n",
      "        view_default_342: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_127, [sym_size, 4096]);  mul_tensor_127 = None\n",
      "        mm_default_100: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_342, t_default_100);  view_default_342 = t_default_100 = None\n",
      "        view_default_343: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_100, [1, sym_size, 4096]);  mm_default_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_344: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_339, [1, sym_size, 32, 128])\n",
      "        transpose_int_70: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_344, 1, 2);  view_default_344 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_345: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_341, [1, sym_size, 32, 128])\n",
      "        transpose_int_71: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_345, 1, 2);  view_default_345 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_346: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_343, [1, sym_size, 32, 128])\n",
      "        transpose_int_72: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_346, 1, 2);  view_default_346 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant28 = self._tensor_constant28\n",
      "        slice_tensor_144: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant28, 0, 0, 9223372036854775807);  _tensor_constant28 = None\n",
      "        slice_tensor_145: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_144, 1, 0, 9223372036854775807);  slice_tensor_144 = None\n",
      "        sym_size_57: Sym(s0) = torch.ops.aten.sym_size(view_default_341, 1);  view_default_341 = None\n",
      "        slice_tensor_146: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_145, 2, 0, sym_size_57);  slice_tensor_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant29 = self._tensor_constant29\n",
      "        slice_tensor_147: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant29, 0, 0, 9223372036854775807);  _tensor_constant29 = None\n",
      "        slice_tensor_148: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_147, 1, 0, 9223372036854775807);  slice_tensor_147 = None\n",
      "        slice_tensor_149: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_148, 2, 0, sym_size_57);  slice_tensor_148 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_56: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_146, 1);  slice_tensor_146 = None\n",
      "        squeeze_dim_57: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_56, 0);  squeeze_dim_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_58: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_149, 1);  slice_tensor_149 = None\n",
      "        squeeze_dim_59: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_58, 0);  squeeze_dim_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_28: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_57, [view_default]);  squeeze_dim_57 = None\n",
      "        unsqueeze_default_33: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_28, 1);  index_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_29: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_59, [view_default]);  squeeze_dim_59 = None\n",
      "        unsqueeze_default_34: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_29, 1);  index_tensor_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_128: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_70, unsqueeze_default_33)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_150: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_151: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 64, 9223372036854775807);  transpose_int_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_28: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_151);  slice_tensor_151 = None\n",
      "        cat_default_28: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_28, slice_tensor_150], -1);  neg_default_28 = slice_tensor_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_129: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_28, unsqueeze_default_34);  cat_default_28 = None\n",
      "        add_tensor_101: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_128, mul_tensor_129);  mul_tensor_128 = mul_tensor_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_130: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_71, unsqueeze_default_33);  unsqueeze_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_152: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_153: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 64, 9223372036854775807);  transpose_int_71 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_29: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_153);  slice_tensor_153 = None\n",
      "        cat_default_29: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_29, slice_tensor_152], -1);  neg_default_29 = slice_tensor_152 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_131: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_29, unsqueeze_default_34);  cat_default_29 = unsqueeze_default_34 = None\n",
      "        add_tensor_102: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_130, mul_tensor_131);  mul_tensor_130 = mul_tensor_131 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_73: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_102, 2, 3)\n",
      "        sym_size_58: Sym(s0) = torch.ops.aten.sym_size(view_default_339, 1);  view_default_339 = None\n",
      "        expand_default_58: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_101, [1, 32, sym_size_58, 128]);  add_tensor_101 = None\n",
      "        view_default_347: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_58, [32, sym_size_58, 128]);  expand_default_58 = None\n",
      "        expand_default_59: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_73, [1, 32, 128, sym_size_57]);  transpose_int_73 = None\n",
      "        view_default_348: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_59, [32, 128, sym_size_57]);  expand_default_59 = None\n",
      "        bmm_default_28: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_347, view_default_348);  view_default_347 = view_default_348 = None\n",
      "        view_default_349: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_28, [1, 32, sym_size_58, sym_size_57]);  bmm_default_28 = None\n",
      "        div_tensor_14: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_349, 11.313708498984761);  view_default_349 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_103: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_14, add_tensor_1);  div_tensor_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_14: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_103, -1, False);  add_tensor_103 = None\n",
      "        detach_default_43: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_14)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_60: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_14, [1, 32, sym_size_58, sym_size_57]);  _softmax_default_14 = None\n",
      "        view_default_350: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_60, [32, sym_size_58, sym_size_57]);  expand_default_60 = sym_size_57 = None\n",
      "        sym_size_59: Sym(s0) = torch.ops.aten.sym_size(view_default_343, 1);  view_default_343 = None\n",
      "        expand_default_61: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_72, [1, 32, sym_size_59, 128])\n",
      "        view_default_351: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_61, [32, sym_size_59, 128]);  expand_default_61 = sym_size_59 = None\n",
      "        bmm_default_29: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_350, view_default_351);  view_default_350 = view_default_351 = None\n",
      "        view_default_352: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_29, [1, 32, sym_size_58, 128]);  bmm_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_74: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_352, 1, 2);  view_default_352 = None\n",
      "        clone_default_14: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_74, memory_format = torch.contiguous_format);  transpose_int_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_353: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_14, [1, sym_size, 4096]);  clone_default_14 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant131 = self._param_constant131\n",
      "        t_default_101: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant131);  _param_constant131 = None\n",
      "        view_default_354: f32[s0, 4096] = torch.ops.aten.view.default(view_default_353, [sym_size_58, 4096]);  view_default_353 = None\n",
      "        mm_default_101: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_354, t_default_101);  view_default_354 = t_default_101 = None\n",
      "        view_default_355: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_101, [1, sym_size_58, 4096]);  mm_default_101 = sym_size_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_104: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_99, view_default_355);  add_tensor_99 = view_default_355 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_29: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_104, 2)\n",
      "        mean_dim_29: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_29, [-1], True);  pow_tensor_scalar_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_105: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_29, 1e-06);  mean_dim_29 = None\n",
      "        rsqrt_default_29: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_105);  add_tensor_105 = None\n",
      "        detach_default_44: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_29)\n",
      "        mul_tensor_132: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_104, rsqrt_default_29);  rsqrt_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant132 = self._param_constant132\n",
      "        mul_tensor_133: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant132, mul_tensor_132);  _param_constant132 = mul_tensor_132 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant133 = self._param_constant133\n",
      "        t_default_102: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant133);  _param_constant133 = None\n",
      "        view_default_356: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_133, [sym_size, 4096])\n",
      "        mm_default_102: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_356, t_default_102);  view_default_356 = t_default_102 = None\n",
      "        view_default_357: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_102, [1, sym_size, 11008]);  mm_default_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_14: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_357)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant134 = self._param_constant134\n",
      "        t_default_103: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant134);  _param_constant134 = None\n",
      "        view_default_358: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_133, [sym_size, 4096]);  mul_tensor_133 = None\n",
      "        mm_default_103: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_358, t_default_103);  view_default_358 = t_default_103 = None\n",
      "        view_default_359: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_103, [1, sym_size, 11008]);  mm_default_103 = None\n",
      "        mul_tensor_134: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_14, view_default_359);  silu_default_14 = view_default_359 = None\n",
      "        _param_constant135 = self._param_constant135\n",
      "        t_default_104: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant135);  _param_constant135 = None\n",
      "        sym_size_60: Sym(s0) = torch.ops.aten.sym_size(view_default_357, 1);  view_default_357 = None\n",
      "        view_default_360: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_134, [sym_size_60, 11008]);  mul_tensor_134 = None\n",
      "        mm_default_104: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_360, t_default_104);  view_default_360 = t_default_104 = None\n",
      "        view_default_361: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_104, [1, sym_size_60, 4096]);  mm_default_104 = sym_size_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_106: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_104, view_default_361);  add_tensor_104 = view_default_361 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_30: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_106, 2)\n",
      "        mean_dim_30: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_30, [-1], True);  pow_tensor_scalar_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_107: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_30, 1e-06);  mean_dim_30 = None\n",
      "        rsqrt_default_30: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_107);  add_tensor_107 = None\n",
      "        detach_default_45: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_30)\n",
      "        mul_tensor_135: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_106, rsqrt_default_30);  rsqrt_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant136 = self._param_constant136\n",
      "        mul_tensor_136: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant136, mul_tensor_135);  _param_constant136 = mul_tensor_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant137 = self._param_constant137\n",
      "        t_default_105: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant137);  _param_constant137 = None\n",
      "        view_default_362: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_136, [sym_size, 4096])\n",
      "        mm_default_105: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_362, t_default_105);  view_default_362 = t_default_105 = None\n",
      "        view_default_363: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_105, [1, sym_size, 4096]);  mm_default_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant138 = self._param_constant138\n",
      "        t_default_106: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant138);  _param_constant138 = None\n",
      "        view_default_364: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_136, [sym_size, 4096])\n",
      "        mm_default_106: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_364, t_default_106);  view_default_364 = t_default_106 = None\n",
      "        view_default_365: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_106, [1, sym_size, 4096]);  mm_default_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant139 = self._param_constant139\n",
      "        t_default_107: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant139);  _param_constant139 = None\n",
      "        view_default_366: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_136, [sym_size, 4096]);  mul_tensor_136 = None\n",
      "        mm_default_107: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_366, t_default_107);  view_default_366 = t_default_107 = None\n",
      "        view_default_367: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_107, [1, sym_size, 4096]);  mm_default_107 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_368: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_363, [1, sym_size, 32, 128])\n",
      "        transpose_int_75: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_368, 1, 2);  view_default_368 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_369: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_365, [1, sym_size, 32, 128])\n",
      "        transpose_int_76: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_369, 1, 2);  view_default_369 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_370: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_367, [1, sym_size, 32, 128])\n",
      "        transpose_int_77: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_370, 1, 2);  view_default_370 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant30 = self._tensor_constant30\n",
      "        slice_tensor_154: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant30, 0, 0, 9223372036854775807);  _tensor_constant30 = None\n",
      "        slice_tensor_155: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_154, 1, 0, 9223372036854775807);  slice_tensor_154 = None\n",
      "        sym_size_61: Sym(s0) = torch.ops.aten.sym_size(view_default_365, 1);  view_default_365 = None\n",
      "        slice_tensor_156: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_155, 2, 0, sym_size_61);  slice_tensor_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant31 = self._tensor_constant31\n",
      "        slice_tensor_157: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant31, 0, 0, 9223372036854775807);  _tensor_constant31 = None\n",
      "        slice_tensor_158: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_157, 1, 0, 9223372036854775807);  slice_tensor_157 = None\n",
      "        slice_tensor_159: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_158, 2, 0, sym_size_61);  slice_tensor_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_60: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_156, 1);  slice_tensor_156 = None\n",
      "        squeeze_dim_61: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_60, 0);  squeeze_dim_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_62: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_159, 1);  slice_tensor_159 = None\n",
      "        squeeze_dim_63: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_62, 0);  squeeze_dim_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_30: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_61, [view_default]);  squeeze_dim_61 = None\n",
      "        unsqueeze_default_35: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_30, 1);  index_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_31: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_63, [view_default]);  squeeze_dim_63 = None\n",
      "        unsqueeze_default_36: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_31, 1);  index_tensor_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_137: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_75, unsqueeze_default_35)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_160: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_161: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 64, 9223372036854775807);  transpose_int_75 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_30: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_161);  slice_tensor_161 = None\n",
      "        cat_default_30: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_30, slice_tensor_160], -1);  neg_default_30 = slice_tensor_160 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_138: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_30, unsqueeze_default_36);  cat_default_30 = None\n",
      "        add_tensor_108: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_137, mul_tensor_138);  mul_tensor_137 = mul_tensor_138 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_139: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_76, unsqueeze_default_35);  unsqueeze_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_162: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_163: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 64, 9223372036854775807);  transpose_int_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_31: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_163);  slice_tensor_163 = None\n",
      "        cat_default_31: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_31, slice_tensor_162], -1);  neg_default_31 = slice_tensor_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_140: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_31, unsqueeze_default_36);  cat_default_31 = unsqueeze_default_36 = None\n",
      "        add_tensor_109: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_139, mul_tensor_140);  mul_tensor_139 = mul_tensor_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_78: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_109, 2, 3)\n",
      "        sym_size_62: Sym(s0) = torch.ops.aten.sym_size(view_default_363, 1);  view_default_363 = None\n",
      "        expand_default_62: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_108, [1, 32, sym_size_62, 128]);  add_tensor_108 = None\n",
      "        view_default_371: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_62, [32, sym_size_62, 128]);  expand_default_62 = None\n",
      "        expand_default_63: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_78, [1, 32, 128, sym_size_61]);  transpose_int_78 = None\n",
      "        view_default_372: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_63, [32, 128, sym_size_61]);  expand_default_63 = None\n",
      "        bmm_default_30: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_371, view_default_372);  view_default_371 = view_default_372 = None\n",
      "        view_default_373: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_30, [1, 32, sym_size_62, sym_size_61]);  bmm_default_30 = None\n",
      "        div_tensor_15: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_373, 11.313708498984761);  view_default_373 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_110: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_15, add_tensor_1);  div_tensor_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_15: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_110, -1, False);  add_tensor_110 = None\n",
      "        detach_default_46: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_15)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_64: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_15, [1, 32, sym_size_62, sym_size_61]);  _softmax_default_15 = None\n",
      "        view_default_374: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_64, [32, sym_size_62, sym_size_61]);  expand_default_64 = sym_size_61 = None\n",
      "        sym_size_63: Sym(s0) = torch.ops.aten.sym_size(view_default_367, 1);  view_default_367 = None\n",
      "        expand_default_65: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_77, [1, 32, sym_size_63, 128])\n",
      "        view_default_375: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_65, [32, sym_size_63, 128]);  expand_default_65 = sym_size_63 = None\n",
      "        bmm_default_31: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_374, view_default_375);  view_default_374 = view_default_375 = None\n",
      "        view_default_376: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_31, [1, 32, sym_size_62, 128]);  bmm_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_79: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_376, 1, 2);  view_default_376 = None\n",
      "        clone_default_15: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_79, memory_format = torch.contiguous_format);  transpose_int_79 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_377: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_15, [1, sym_size, 4096]);  clone_default_15 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant140 = self._param_constant140\n",
      "        t_default_108: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant140);  _param_constant140 = None\n",
      "        view_default_378: f32[s0, 4096] = torch.ops.aten.view.default(view_default_377, [sym_size_62, 4096]);  view_default_377 = None\n",
      "        mm_default_108: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_378, t_default_108);  view_default_378 = t_default_108 = None\n",
      "        view_default_379: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_108, [1, sym_size_62, 4096]);  mm_default_108 = sym_size_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_111: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_106, view_default_379);  add_tensor_106 = view_default_379 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_31: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_111, 2)\n",
      "        mean_dim_31: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_31, [-1], True);  pow_tensor_scalar_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_112: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_31, 1e-06);  mean_dim_31 = None\n",
      "        rsqrt_default_31: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_112);  add_tensor_112 = None\n",
      "        detach_default_47: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_31)\n",
      "        mul_tensor_141: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_111, rsqrt_default_31);  rsqrt_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant141 = self._param_constant141\n",
      "        mul_tensor_142: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant141, mul_tensor_141);  _param_constant141 = mul_tensor_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant142 = self._param_constant142\n",
      "        t_default_109: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant142);  _param_constant142 = None\n",
      "        view_default_380: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_142, [sym_size, 4096])\n",
      "        mm_default_109: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_380, t_default_109);  view_default_380 = t_default_109 = None\n",
      "        view_default_381: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_109, [1, sym_size, 11008]);  mm_default_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_15: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_381)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant143 = self._param_constant143\n",
      "        t_default_110: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant143);  _param_constant143 = None\n",
      "        view_default_382: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_142, [sym_size, 4096]);  mul_tensor_142 = None\n",
      "        mm_default_110: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_382, t_default_110);  view_default_382 = t_default_110 = None\n",
      "        view_default_383: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_110, [1, sym_size, 11008]);  mm_default_110 = None\n",
      "        mul_tensor_143: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_15, view_default_383);  silu_default_15 = view_default_383 = None\n",
      "        _param_constant144 = self._param_constant144\n",
      "        t_default_111: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant144);  _param_constant144 = None\n",
      "        sym_size_64: Sym(s0) = torch.ops.aten.sym_size(view_default_381, 1);  view_default_381 = None\n",
      "        view_default_384: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_143, [sym_size_64, 11008]);  mul_tensor_143 = None\n",
      "        mm_default_111: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_384, t_default_111);  view_default_384 = t_default_111 = None\n",
      "        view_default_385: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_111, [1, sym_size_64, 4096]);  mm_default_111 = sym_size_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_113: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_111, view_default_385);  add_tensor_111 = view_default_385 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_32: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_113, 2)\n",
      "        mean_dim_32: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_32, [-1], True);  pow_tensor_scalar_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_114: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_32, 1e-06);  mean_dim_32 = None\n",
      "        rsqrt_default_32: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_114);  add_tensor_114 = None\n",
      "        detach_default_48: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_32)\n",
      "        mul_tensor_144: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_113, rsqrt_default_32);  rsqrt_default_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant145 = self._param_constant145\n",
      "        mul_tensor_145: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant145, mul_tensor_144);  _param_constant145 = mul_tensor_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant146 = self._param_constant146\n",
      "        t_default_112: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant146);  _param_constant146 = None\n",
      "        view_default_386: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_145, [sym_size, 4096])\n",
      "        mm_default_112: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_386, t_default_112);  view_default_386 = t_default_112 = None\n",
      "        view_default_387: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_112, [1, sym_size, 4096]);  mm_default_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant147 = self._param_constant147\n",
      "        t_default_113: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant147);  _param_constant147 = None\n",
      "        view_default_388: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_145, [sym_size, 4096])\n",
      "        mm_default_113: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_388, t_default_113);  view_default_388 = t_default_113 = None\n",
      "        view_default_389: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_113, [1, sym_size, 4096]);  mm_default_113 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant148 = self._param_constant148\n",
      "        t_default_114: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant148);  _param_constant148 = None\n",
      "        view_default_390: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_145, [sym_size, 4096]);  mul_tensor_145 = None\n",
      "        mm_default_114: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_390, t_default_114);  view_default_390 = t_default_114 = None\n",
      "        view_default_391: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_114, [1, sym_size, 4096]);  mm_default_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_392: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_387, [1, sym_size, 32, 128])\n",
      "        transpose_int_80: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_392, 1, 2);  view_default_392 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_393: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_389, [1, sym_size, 32, 128])\n",
      "        transpose_int_81: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_393, 1, 2);  view_default_393 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_394: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_391, [1, sym_size, 32, 128])\n",
      "        transpose_int_82: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_394, 1, 2);  view_default_394 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant32 = self._tensor_constant32\n",
      "        slice_tensor_164: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant32, 0, 0, 9223372036854775807);  _tensor_constant32 = None\n",
      "        slice_tensor_165: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_164, 1, 0, 9223372036854775807);  slice_tensor_164 = None\n",
      "        sym_size_65: Sym(s0) = torch.ops.aten.sym_size(view_default_389, 1);  view_default_389 = None\n",
      "        slice_tensor_166: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_165, 2, 0, sym_size_65);  slice_tensor_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant33 = self._tensor_constant33\n",
      "        slice_tensor_167: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant33, 0, 0, 9223372036854775807);  _tensor_constant33 = None\n",
      "        slice_tensor_168: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_167, 1, 0, 9223372036854775807);  slice_tensor_167 = None\n",
      "        slice_tensor_169: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_168, 2, 0, sym_size_65);  slice_tensor_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_64: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_166, 1);  slice_tensor_166 = None\n",
      "        squeeze_dim_65: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_64, 0);  squeeze_dim_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_66: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_169, 1);  slice_tensor_169 = None\n",
      "        squeeze_dim_67: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_66, 0);  squeeze_dim_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_32: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_65, [view_default]);  squeeze_dim_65 = None\n",
      "        unsqueeze_default_37: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_32, 1);  index_tensor_32 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_33: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_67, [view_default]);  squeeze_dim_67 = None\n",
      "        unsqueeze_default_38: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_33, 1);  index_tensor_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_146: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_80, unsqueeze_default_37)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_170: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_171: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 64, 9223372036854775807);  transpose_int_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_32: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_171);  slice_tensor_171 = None\n",
      "        cat_default_32: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_32, slice_tensor_170], -1);  neg_default_32 = slice_tensor_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_147: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_32, unsqueeze_default_38);  cat_default_32 = None\n",
      "        add_tensor_115: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_146, mul_tensor_147);  mul_tensor_146 = mul_tensor_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_148: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_81, unsqueeze_default_37);  unsqueeze_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_172: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_173: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 64, 9223372036854775807);  transpose_int_81 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_33: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_173);  slice_tensor_173 = None\n",
      "        cat_default_33: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_33, slice_tensor_172], -1);  neg_default_33 = slice_tensor_172 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_149: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_33, unsqueeze_default_38);  cat_default_33 = unsqueeze_default_38 = None\n",
      "        add_tensor_116: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_148, mul_tensor_149);  mul_tensor_148 = mul_tensor_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_83: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_116, 2, 3)\n",
      "        sym_size_66: Sym(s0) = torch.ops.aten.sym_size(view_default_387, 1);  view_default_387 = None\n",
      "        expand_default_66: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_115, [1, 32, sym_size_66, 128]);  add_tensor_115 = None\n",
      "        view_default_395: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_66, [32, sym_size_66, 128]);  expand_default_66 = None\n",
      "        expand_default_67: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_83, [1, 32, 128, sym_size_65]);  transpose_int_83 = None\n",
      "        view_default_396: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_67, [32, 128, sym_size_65]);  expand_default_67 = None\n",
      "        bmm_default_32: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_395, view_default_396);  view_default_395 = view_default_396 = None\n",
      "        view_default_397: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_32, [1, 32, sym_size_66, sym_size_65]);  bmm_default_32 = None\n",
      "        div_tensor_16: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_397, 11.313708498984761);  view_default_397 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_117: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_16, add_tensor_1);  div_tensor_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_16: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_117, -1, False);  add_tensor_117 = None\n",
      "        detach_default_49: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_16)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_68: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_16, [1, 32, sym_size_66, sym_size_65]);  _softmax_default_16 = None\n",
      "        view_default_398: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_68, [32, sym_size_66, sym_size_65]);  expand_default_68 = sym_size_65 = None\n",
      "        sym_size_67: Sym(s0) = torch.ops.aten.sym_size(view_default_391, 1);  view_default_391 = None\n",
      "        expand_default_69: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_82, [1, 32, sym_size_67, 128])\n",
      "        view_default_399: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_69, [32, sym_size_67, 128]);  expand_default_69 = sym_size_67 = None\n",
      "        bmm_default_33: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_398, view_default_399);  view_default_398 = view_default_399 = None\n",
      "        view_default_400: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_33, [1, 32, sym_size_66, 128]);  bmm_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_84: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_400, 1, 2);  view_default_400 = None\n",
      "        clone_default_16: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_84, memory_format = torch.contiguous_format);  transpose_int_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_401: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_16, [1, sym_size, 4096]);  clone_default_16 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant149 = self._param_constant149\n",
      "        t_default_115: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant149);  _param_constant149 = None\n",
      "        view_default_402: f32[s0, 4096] = torch.ops.aten.view.default(view_default_401, [sym_size_66, 4096]);  view_default_401 = None\n",
      "        mm_default_115: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_402, t_default_115);  view_default_402 = t_default_115 = None\n",
      "        view_default_403: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_115, [1, sym_size_66, 4096]);  mm_default_115 = sym_size_66 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_118: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_113, view_default_403);  add_tensor_113 = view_default_403 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_33: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_118, 2)\n",
      "        mean_dim_33: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_33, [-1], True);  pow_tensor_scalar_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_119: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_33, 1e-06);  mean_dim_33 = None\n",
      "        rsqrt_default_33: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_119);  add_tensor_119 = None\n",
      "        detach_default_50: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_33)\n",
      "        mul_tensor_150: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_118, rsqrt_default_33);  rsqrt_default_33 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant150 = self._param_constant150\n",
      "        mul_tensor_151: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant150, mul_tensor_150);  _param_constant150 = mul_tensor_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant151 = self._param_constant151\n",
      "        t_default_116: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant151);  _param_constant151 = None\n",
      "        view_default_404: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_151, [sym_size, 4096])\n",
      "        mm_default_116: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_404, t_default_116);  view_default_404 = t_default_116 = None\n",
      "        view_default_405: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_116, [1, sym_size, 11008]);  mm_default_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_16: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_405)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant152 = self._param_constant152\n",
      "        t_default_117: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant152);  _param_constant152 = None\n",
      "        view_default_406: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_151, [sym_size, 4096]);  mul_tensor_151 = None\n",
      "        mm_default_117: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_406, t_default_117);  view_default_406 = t_default_117 = None\n",
      "        view_default_407: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_117, [1, sym_size, 11008]);  mm_default_117 = None\n",
      "        mul_tensor_152: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_16, view_default_407);  silu_default_16 = view_default_407 = None\n",
      "        _param_constant153 = self._param_constant153\n",
      "        t_default_118: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant153);  _param_constant153 = None\n",
      "        sym_size_68: Sym(s0) = torch.ops.aten.sym_size(view_default_405, 1);  view_default_405 = None\n",
      "        view_default_408: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_152, [sym_size_68, 11008]);  mul_tensor_152 = None\n",
      "        mm_default_118: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_408, t_default_118);  view_default_408 = t_default_118 = None\n",
      "        view_default_409: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_118, [1, sym_size_68, 4096]);  mm_default_118 = sym_size_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_120: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_118, view_default_409);  add_tensor_118 = view_default_409 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_34: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_120, 2)\n",
      "        mean_dim_34: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_34, [-1], True);  pow_tensor_scalar_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_121: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_34, 1e-06);  mean_dim_34 = None\n",
      "        rsqrt_default_34: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_121);  add_tensor_121 = None\n",
      "        detach_default_51: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_34)\n",
      "        mul_tensor_153: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_120, rsqrt_default_34);  rsqrt_default_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant154 = self._param_constant154\n",
      "        mul_tensor_154: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant154, mul_tensor_153);  _param_constant154 = mul_tensor_153 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant155 = self._param_constant155\n",
      "        t_default_119: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant155);  _param_constant155 = None\n",
      "        view_default_410: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_154, [sym_size, 4096])\n",
      "        mm_default_119: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_410, t_default_119);  view_default_410 = t_default_119 = None\n",
      "        view_default_411: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_119, [1, sym_size, 4096]);  mm_default_119 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant156 = self._param_constant156\n",
      "        t_default_120: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant156);  _param_constant156 = None\n",
      "        view_default_412: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_154, [sym_size, 4096])\n",
      "        mm_default_120: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_412, t_default_120);  view_default_412 = t_default_120 = None\n",
      "        view_default_413: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_120, [1, sym_size, 4096]);  mm_default_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant157 = self._param_constant157\n",
      "        t_default_121: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant157);  _param_constant157 = None\n",
      "        view_default_414: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_154, [sym_size, 4096]);  mul_tensor_154 = None\n",
      "        mm_default_121: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_414, t_default_121);  view_default_414 = t_default_121 = None\n",
      "        view_default_415: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_121, [1, sym_size, 4096]);  mm_default_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_416: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_411, [1, sym_size, 32, 128])\n",
      "        transpose_int_85: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_416, 1, 2);  view_default_416 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_417: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_413, [1, sym_size, 32, 128])\n",
      "        transpose_int_86: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_417, 1, 2);  view_default_417 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_418: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_415, [1, sym_size, 32, 128])\n",
      "        transpose_int_87: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_418, 1, 2);  view_default_418 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant34 = self._tensor_constant34\n",
      "        slice_tensor_174: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant34, 0, 0, 9223372036854775807);  _tensor_constant34 = None\n",
      "        slice_tensor_175: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_174, 1, 0, 9223372036854775807);  slice_tensor_174 = None\n",
      "        sym_size_69: Sym(s0) = torch.ops.aten.sym_size(view_default_413, 1);  view_default_413 = None\n",
      "        slice_tensor_176: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_175, 2, 0, sym_size_69);  slice_tensor_175 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant35 = self._tensor_constant35\n",
      "        slice_tensor_177: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant35, 0, 0, 9223372036854775807);  _tensor_constant35 = None\n",
      "        slice_tensor_178: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_177, 1, 0, 9223372036854775807);  slice_tensor_177 = None\n",
      "        slice_tensor_179: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_178, 2, 0, sym_size_69);  slice_tensor_178 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_68: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_176, 1);  slice_tensor_176 = None\n",
      "        squeeze_dim_69: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_68, 0);  squeeze_dim_68 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_70: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_179, 1);  slice_tensor_179 = None\n",
      "        squeeze_dim_71: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_70, 0);  squeeze_dim_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_34: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_69, [view_default]);  squeeze_dim_69 = None\n",
      "        unsqueeze_default_39: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_34, 1);  index_tensor_34 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_35: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_71, [view_default]);  squeeze_dim_71 = None\n",
      "        unsqueeze_default_40: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_35, 1);  index_tensor_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_155: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_85, unsqueeze_default_39)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_180: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_181: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 64, 9223372036854775807);  transpose_int_85 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_34: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_181);  slice_tensor_181 = None\n",
      "        cat_default_34: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_34, slice_tensor_180], -1);  neg_default_34 = slice_tensor_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_156: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_34, unsqueeze_default_40);  cat_default_34 = None\n",
      "        add_tensor_122: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_155, mul_tensor_156);  mul_tensor_155 = mul_tensor_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_157: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_86, unsqueeze_default_39);  unsqueeze_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_182: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_183: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 64, 9223372036854775807);  transpose_int_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_35: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_183);  slice_tensor_183 = None\n",
      "        cat_default_35: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_35, slice_tensor_182], -1);  neg_default_35 = slice_tensor_182 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_158: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_35, unsqueeze_default_40);  cat_default_35 = unsqueeze_default_40 = None\n",
      "        add_tensor_123: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_157, mul_tensor_158);  mul_tensor_157 = mul_tensor_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_88: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_123, 2, 3)\n",
      "        sym_size_70: Sym(s0) = torch.ops.aten.sym_size(view_default_411, 1);  view_default_411 = None\n",
      "        expand_default_70: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_122, [1, 32, sym_size_70, 128]);  add_tensor_122 = None\n",
      "        view_default_419: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_70, [32, sym_size_70, 128]);  expand_default_70 = None\n",
      "        expand_default_71: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_88, [1, 32, 128, sym_size_69]);  transpose_int_88 = None\n",
      "        view_default_420: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_71, [32, 128, sym_size_69]);  expand_default_71 = None\n",
      "        bmm_default_34: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_419, view_default_420);  view_default_419 = view_default_420 = None\n",
      "        view_default_421: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_34, [1, 32, sym_size_70, sym_size_69]);  bmm_default_34 = None\n",
      "        div_tensor_17: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_421, 11.313708498984761);  view_default_421 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_124: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_17, add_tensor_1);  div_tensor_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_17: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_124, -1, False);  add_tensor_124 = None\n",
      "        detach_default_52: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_17)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_72: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_17, [1, 32, sym_size_70, sym_size_69]);  _softmax_default_17 = None\n",
      "        view_default_422: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_72, [32, sym_size_70, sym_size_69]);  expand_default_72 = sym_size_69 = None\n",
      "        sym_size_71: Sym(s0) = torch.ops.aten.sym_size(view_default_415, 1);  view_default_415 = None\n",
      "        expand_default_73: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_87, [1, 32, sym_size_71, 128])\n",
      "        view_default_423: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_73, [32, sym_size_71, 128]);  expand_default_73 = sym_size_71 = None\n",
      "        bmm_default_35: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_422, view_default_423);  view_default_422 = view_default_423 = None\n",
      "        view_default_424: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_35, [1, 32, sym_size_70, 128]);  bmm_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_89: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_424, 1, 2);  view_default_424 = None\n",
      "        clone_default_17: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_89, memory_format = torch.contiguous_format);  transpose_int_89 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_425: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_17, [1, sym_size, 4096]);  clone_default_17 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant158 = self._param_constant158\n",
      "        t_default_122: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant158);  _param_constant158 = None\n",
      "        view_default_426: f32[s0, 4096] = torch.ops.aten.view.default(view_default_425, [sym_size_70, 4096]);  view_default_425 = None\n",
      "        mm_default_122: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_426, t_default_122);  view_default_426 = t_default_122 = None\n",
      "        view_default_427: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_122, [1, sym_size_70, 4096]);  mm_default_122 = sym_size_70 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_125: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_120, view_default_427);  add_tensor_120 = view_default_427 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_35: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_125, 2)\n",
      "        mean_dim_35: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_35, [-1], True);  pow_tensor_scalar_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_126: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_35, 1e-06);  mean_dim_35 = None\n",
      "        rsqrt_default_35: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_126);  add_tensor_126 = None\n",
      "        detach_default_53: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_35)\n",
      "        mul_tensor_159: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_125, rsqrt_default_35);  rsqrt_default_35 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant159 = self._param_constant159\n",
      "        mul_tensor_160: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant159, mul_tensor_159);  _param_constant159 = mul_tensor_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant160 = self._param_constant160\n",
      "        t_default_123: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant160);  _param_constant160 = None\n",
      "        view_default_428: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_160, [sym_size, 4096])\n",
      "        mm_default_123: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_428, t_default_123);  view_default_428 = t_default_123 = None\n",
      "        view_default_429: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_123, [1, sym_size, 11008]);  mm_default_123 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_17: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_429)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant161 = self._param_constant161\n",
      "        t_default_124: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant161);  _param_constant161 = None\n",
      "        view_default_430: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_160, [sym_size, 4096]);  mul_tensor_160 = None\n",
      "        mm_default_124: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_430, t_default_124);  view_default_430 = t_default_124 = None\n",
      "        view_default_431: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_124, [1, sym_size, 11008]);  mm_default_124 = None\n",
      "        mul_tensor_161: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_17, view_default_431);  silu_default_17 = view_default_431 = None\n",
      "        _param_constant162 = self._param_constant162\n",
      "        t_default_125: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant162);  _param_constant162 = None\n",
      "        sym_size_72: Sym(s0) = torch.ops.aten.sym_size(view_default_429, 1);  view_default_429 = None\n",
      "        view_default_432: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_161, [sym_size_72, 11008]);  mul_tensor_161 = None\n",
      "        mm_default_125: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_432, t_default_125);  view_default_432 = t_default_125 = None\n",
      "        view_default_433: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_125, [1, sym_size_72, 4096]);  mm_default_125 = sym_size_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_127: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_125, view_default_433);  add_tensor_125 = view_default_433 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_36: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_127, 2)\n",
      "        mean_dim_36: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_36, [-1], True);  pow_tensor_scalar_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_128: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_36, 1e-06);  mean_dim_36 = None\n",
      "        rsqrt_default_36: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_128);  add_tensor_128 = None\n",
      "        detach_default_54: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_36)\n",
      "        mul_tensor_162: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_127, rsqrt_default_36);  rsqrt_default_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant163 = self._param_constant163\n",
      "        mul_tensor_163: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant163, mul_tensor_162);  _param_constant163 = mul_tensor_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant164 = self._param_constant164\n",
      "        t_default_126: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant164);  _param_constant164 = None\n",
      "        view_default_434: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_163, [sym_size, 4096])\n",
      "        mm_default_126: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_434, t_default_126);  view_default_434 = t_default_126 = None\n",
      "        view_default_435: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_126, [1, sym_size, 4096]);  mm_default_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant165 = self._param_constant165\n",
      "        t_default_127: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant165);  _param_constant165 = None\n",
      "        view_default_436: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_163, [sym_size, 4096])\n",
      "        mm_default_127: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_436, t_default_127);  view_default_436 = t_default_127 = None\n",
      "        view_default_437: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_127, [1, sym_size, 4096]);  mm_default_127 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant166 = self._param_constant166\n",
      "        t_default_128: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant166);  _param_constant166 = None\n",
      "        view_default_438: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_163, [sym_size, 4096]);  mul_tensor_163 = None\n",
      "        mm_default_128: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_438, t_default_128);  view_default_438 = t_default_128 = None\n",
      "        view_default_439: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_128, [1, sym_size, 4096]);  mm_default_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_440: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_435, [1, sym_size, 32, 128])\n",
      "        transpose_int_90: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_440, 1, 2);  view_default_440 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_441: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_437, [1, sym_size, 32, 128])\n",
      "        transpose_int_91: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_441, 1, 2);  view_default_441 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_442: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_439, [1, sym_size, 32, 128])\n",
      "        transpose_int_92: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_442, 1, 2);  view_default_442 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant36 = self._tensor_constant36\n",
      "        slice_tensor_184: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant36, 0, 0, 9223372036854775807);  _tensor_constant36 = None\n",
      "        slice_tensor_185: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_184, 1, 0, 9223372036854775807);  slice_tensor_184 = None\n",
      "        sym_size_73: Sym(s0) = torch.ops.aten.sym_size(view_default_437, 1);  view_default_437 = None\n",
      "        slice_tensor_186: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_185, 2, 0, sym_size_73);  slice_tensor_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant37 = self._tensor_constant37\n",
      "        slice_tensor_187: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant37, 0, 0, 9223372036854775807);  _tensor_constant37 = None\n",
      "        slice_tensor_188: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_187, 1, 0, 9223372036854775807);  slice_tensor_187 = None\n",
      "        slice_tensor_189: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_188, 2, 0, sym_size_73);  slice_tensor_188 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_72: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_186, 1);  slice_tensor_186 = None\n",
      "        squeeze_dim_73: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_72, 0);  squeeze_dim_72 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_74: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_189, 1);  slice_tensor_189 = None\n",
      "        squeeze_dim_75: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_74, 0);  squeeze_dim_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_36: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_73, [view_default]);  squeeze_dim_73 = None\n",
      "        unsqueeze_default_41: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_36, 1);  index_tensor_36 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_37: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_75, [view_default]);  squeeze_dim_75 = None\n",
      "        unsqueeze_default_42: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_37, 1);  index_tensor_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_164: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_90, unsqueeze_default_41)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_190: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_191: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 64, 9223372036854775807);  transpose_int_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_36: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_191);  slice_tensor_191 = None\n",
      "        cat_default_36: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_36, slice_tensor_190], -1);  neg_default_36 = slice_tensor_190 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_165: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_36, unsqueeze_default_42);  cat_default_36 = None\n",
      "        add_tensor_129: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_164, mul_tensor_165);  mul_tensor_164 = mul_tensor_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_166: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_91, unsqueeze_default_41);  unsqueeze_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_192: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_193: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 64, 9223372036854775807);  transpose_int_91 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_37: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_193);  slice_tensor_193 = None\n",
      "        cat_default_37: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_37, slice_tensor_192], -1);  neg_default_37 = slice_tensor_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_167: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_37, unsqueeze_default_42);  cat_default_37 = unsqueeze_default_42 = None\n",
      "        add_tensor_130: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_166, mul_tensor_167);  mul_tensor_166 = mul_tensor_167 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_93: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_130, 2, 3)\n",
      "        sym_size_74: Sym(s0) = torch.ops.aten.sym_size(view_default_435, 1);  view_default_435 = None\n",
      "        expand_default_74: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_129, [1, 32, sym_size_74, 128]);  add_tensor_129 = None\n",
      "        view_default_443: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_74, [32, sym_size_74, 128]);  expand_default_74 = None\n",
      "        expand_default_75: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_93, [1, 32, 128, sym_size_73]);  transpose_int_93 = None\n",
      "        view_default_444: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_75, [32, 128, sym_size_73]);  expand_default_75 = None\n",
      "        bmm_default_36: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_443, view_default_444);  view_default_443 = view_default_444 = None\n",
      "        view_default_445: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_36, [1, 32, sym_size_74, sym_size_73]);  bmm_default_36 = None\n",
      "        div_tensor_18: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_445, 11.313708498984761);  view_default_445 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_131: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_18, add_tensor_1);  div_tensor_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_18: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_131, -1, False);  add_tensor_131 = None\n",
      "        detach_default_55: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_18)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_76: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_18, [1, 32, sym_size_74, sym_size_73]);  _softmax_default_18 = None\n",
      "        view_default_446: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_76, [32, sym_size_74, sym_size_73]);  expand_default_76 = sym_size_73 = None\n",
      "        sym_size_75: Sym(s0) = torch.ops.aten.sym_size(view_default_439, 1);  view_default_439 = None\n",
      "        expand_default_77: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_92, [1, 32, sym_size_75, 128])\n",
      "        view_default_447: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_77, [32, sym_size_75, 128]);  expand_default_77 = sym_size_75 = None\n",
      "        bmm_default_37: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_446, view_default_447);  view_default_446 = view_default_447 = None\n",
      "        view_default_448: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_37, [1, 32, sym_size_74, 128]);  bmm_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_94: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_448, 1, 2);  view_default_448 = None\n",
      "        clone_default_18: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_94, memory_format = torch.contiguous_format);  transpose_int_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_449: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_18, [1, sym_size, 4096]);  clone_default_18 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant167 = self._param_constant167\n",
      "        t_default_129: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant167);  _param_constant167 = None\n",
      "        view_default_450: f32[s0, 4096] = torch.ops.aten.view.default(view_default_449, [sym_size_74, 4096]);  view_default_449 = None\n",
      "        mm_default_129: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_450, t_default_129);  view_default_450 = t_default_129 = None\n",
      "        view_default_451: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_129, [1, sym_size_74, 4096]);  mm_default_129 = sym_size_74 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_132: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_127, view_default_451);  add_tensor_127 = view_default_451 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_37: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_132, 2)\n",
      "        mean_dim_37: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_37, [-1], True);  pow_tensor_scalar_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_133: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_37, 1e-06);  mean_dim_37 = None\n",
      "        rsqrt_default_37: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_133);  add_tensor_133 = None\n",
      "        detach_default_56: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_37)\n",
      "        mul_tensor_168: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_132, rsqrt_default_37);  rsqrt_default_37 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant168 = self._param_constant168\n",
      "        mul_tensor_169: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant168, mul_tensor_168);  _param_constant168 = mul_tensor_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant169 = self._param_constant169\n",
      "        t_default_130: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant169);  _param_constant169 = None\n",
      "        view_default_452: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_169, [sym_size, 4096])\n",
      "        mm_default_130: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_452, t_default_130);  view_default_452 = t_default_130 = None\n",
      "        view_default_453: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_130, [1, sym_size, 11008]);  mm_default_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_18: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_453)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant170 = self._param_constant170\n",
      "        t_default_131: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant170);  _param_constant170 = None\n",
      "        view_default_454: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_169, [sym_size, 4096]);  mul_tensor_169 = None\n",
      "        mm_default_131: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_454, t_default_131);  view_default_454 = t_default_131 = None\n",
      "        view_default_455: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_131, [1, sym_size, 11008]);  mm_default_131 = None\n",
      "        mul_tensor_170: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_18, view_default_455);  silu_default_18 = view_default_455 = None\n",
      "        _param_constant171 = self._param_constant171\n",
      "        t_default_132: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant171);  _param_constant171 = None\n",
      "        sym_size_76: Sym(s0) = torch.ops.aten.sym_size(view_default_453, 1);  view_default_453 = None\n",
      "        view_default_456: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_170, [sym_size_76, 11008]);  mul_tensor_170 = None\n",
      "        mm_default_132: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_456, t_default_132);  view_default_456 = t_default_132 = None\n",
      "        view_default_457: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_132, [1, sym_size_76, 4096]);  mm_default_132 = sym_size_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_134: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_132, view_default_457);  add_tensor_132 = view_default_457 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_38: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_134, 2)\n",
      "        mean_dim_38: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_38, [-1], True);  pow_tensor_scalar_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_135: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_38, 1e-06);  mean_dim_38 = None\n",
      "        rsqrt_default_38: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_135);  add_tensor_135 = None\n",
      "        detach_default_57: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_38)\n",
      "        mul_tensor_171: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_134, rsqrt_default_38);  rsqrt_default_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant172 = self._param_constant172\n",
      "        mul_tensor_172: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant172, mul_tensor_171);  _param_constant172 = mul_tensor_171 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant173 = self._param_constant173\n",
      "        t_default_133: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant173);  _param_constant173 = None\n",
      "        view_default_458: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_172, [sym_size, 4096])\n",
      "        mm_default_133: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_458, t_default_133);  view_default_458 = t_default_133 = None\n",
      "        view_default_459: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_133, [1, sym_size, 4096]);  mm_default_133 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant174 = self._param_constant174\n",
      "        t_default_134: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant174);  _param_constant174 = None\n",
      "        view_default_460: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_172, [sym_size, 4096])\n",
      "        mm_default_134: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_460, t_default_134);  view_default_460 = t_default_134 = None\n",
      "        view_default_461: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_134, [1, sym_size, 4096]);  mm_default_134 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant175 = self._param_constant175\n",
      "        t_default_135: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant175);  _param_constant175 = None\n",
      "        view_default_462: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_172, [sym_size, 4096]);  mul_tensor_172 = None\n",
      "        mm_default_135: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_462, t_default_135);  view_default_462 = t_default_135 = None\n",
      "        view_default_463: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_135, [1, sym_size, 4096]);  mm_default_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_464: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_459, [1, sym_size, 32, 128])\n",
      "        transpose_int_95: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_464, 1, 2);  view_default_464 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_465: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_461, [1, sym_size, 32, 128])\n",
      "        transpose_int_96: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_465, 1, 2);  view_default_465 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_466: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_463, [1, sym_size, 32, 128])\n",
      "        transpose_int_97: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_466, 1, 2);  view_default_466 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant38 = self._tensor_constant38\n",
      "        slice_tensor_194: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant38, 0, 0, 9223372036854775807);  _tensor_constant38 = None\n",
      "        slice_tensor_195: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_194, 1, 0, 9223372036854775807);  slice_tensor_194 = None\n",
      "        sym_size_77: Sym(s0) = torch.ops.aten.sym_size(view_default_461, 1);  view_default_461 = None\n",
      "        slice_tensor_196: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_195, 2, 0, sym_size_77);  slice_tensor_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant39 = self._tensor_constant39\n",
      "        slice_tensor_197: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant39, 0, 0, 9223372036854775807);  _tensor_constant39 = None\n",
      "        slice_tensor_198: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_197, 1, 0, 9223372036854775807);  slice_tensor_197 = None\n",
      "        slice_tensor_199: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_198, 2, 0, sym_size_77);  slice_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_76: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_196, 1);  slice_tensor_196 = None\n",
      "        squeeze_dim_77: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_76, 0);  squeeze_dim_76 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_78: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_199, 1);  slice_tensor_199 = None\n",
      "        squeeze_dim_79: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_78, 0);  squeeze_dim_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_38: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_77, [view_default]);  squeeze_dim_77 = None\n",
      "        unsqueeze_default_43: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_38, 1);  index_tensor_38 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_39: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_79, [view_default]);  squeeze_dim_79 = None\n",
      "        unsqueeze_default_44: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_39, 1);  index_tensor_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_173: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_95, unsqueeze_default_43)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_200: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_201: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 64, 9223372036854775807);  transpose_int_95 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_38: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_201);  slice_tensor_201 = None\n",
      "        cat_default_38: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_38, slice_tensor_200], -1);  neg_default_38 = slice_tensor_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_174: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_38, unsqueeze_default_44);  cat_default_38 = None\n",
      "        add_tensor_136: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_173, mul_tensor_174);  mul_tensor_173 = mul_tensor_174 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_175: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_96, unsqueeze_default_43);  unsqueeze_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_202: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_203: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 64, 9223372036854775807);  transpose_int_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_39: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_203);  slice_tensor_203 = None\n",
      "        cat_default_39: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_39, slice_tensor_202], -1);  neg_default_39 = slice_tensor_202 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_176: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_39, unsqueeze_default_44);  cat_default_39 = unsqueeze_default_44 = None\n",
      "        add_tensor_137: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_175, mul_tensor_176);  mul_tensor_175 = mul_tensor_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_98: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_137, 2, 3)\n",
      "        sym_size_78: Sym(s0) = torch.ops.aten.sym_size(view_default_459, 1);  view_default_459 = None\n",
      "        expand_default_78: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_136, [1, 32, sym_size_78, 128]);  add_tensor_136 = None\n",
      "        view_default_467: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_78, [32, sym_size_78, 128]);  expand_default_78 = None\n",
      "        expand_default_79: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_98, [1, 32, 128, sym_size_77]);  transpose_int_98 = None\n",
      "        view_default_468: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_79, [32, 128, sym_size_77]);  expand_default_79 = None\n",
      "        bmm_default_38: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_467, view_default_468);  view_default_467 = view_default_468 = None\n",
      "        view_default_469: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_38, [1, 32, sym_size_78, sym_size_77]);  bmm_default_38 = None\n",
      "        div_tensor_19: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_469, 11.313708498984761);  view_default_469 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_138: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_19, add_tensor_1);  div_tensor_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_19: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_138, -1, False);  add_tensor_138 = None\n",
      "        detach_default_58: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_19)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_80: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_19, [1, 32, sym_size_78, sym_size_77]);  _softmax_default_19 = None\n",
      "        view_default_470: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_80, [32, sym_size_78, sym_size_77]);  expand_default_80 = sym_size_77 = None\n",
      "        sym_size_79: Sym(s0) = torch.ops.aten.sym_size(view_default_463, 1);  view_default_463 = None\n",
      "        expand_default_81: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_97, [1, 32, sym_size_79, 128])\n",
      "        view_default_471: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_81, [32, sym_size_79, 128]);  expand_default_81 = sym_size_79 = None\n",
      "        bmm_default_39: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_470, view_default_471);  view_default_470 = view_default_471 = None\n",
      "        view_default_472: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_39, [1, 32, sym_size_78, 128]);  bmm_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_99: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_472, 1, 2);  view_default_472 = None\n",
      "        clone_default_19: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_99, memory_format = torch.contiguous_format);  transpose_int_99 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_473: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_19, [1, sym_size, 4096]);  clone_default_19 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant176 = self._param_constant176\n",
      "        t_default_136: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant176);  _param_constant176 = None\n",
      "        view_default_474: f32[s0, 4096] = torch.ops.aten.view.default(view_default_473, [sym_size_78, 4096]);  view_default_473 = None\n",
      "        mm_default_136: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_474, t_default_136);  view_default_474 = t_default_136 = None\n",
      "        view_default_475: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_136, [1, sym_size_78, 4096]);  mm_default_136 = sym_size_78 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_139: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_134, view_default_475);  add_tensor_134 = view_default_475 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_39: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_139, 2)\n",
      "        mean_dim_39: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_39, [-1], True);  pow_tensor_scalar_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_140: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_39, 1e-06);  mean_dim_39 = None\n",
      "        rsqrt_default_39: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_140);  add_tensor_140 = None\n",
      "        detach_default_59: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_39)\n",
      "        mul_tensor_177: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_139, rsqrt_default_39);  rsqrt_default_39 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant177 = self._param_constant177\n",
      "        mul_tensor_178: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant177, mul_tensor_177);  _param_constant177 = mul_tensor_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant178 = self._param_constant178\n",
      "        t_default_137: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant178);  _param_constant178 = None\n",
      "        view_default_476: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_178, [sym_size, 4096])\n",
      "        mm_default_137: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_476, t_default_137);  view_default_476 = t_default_137 = None\n",
      "        view_default_477: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_137, [1, sym_size, 11008]);  mm_default_137 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_19: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_477)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant179 = self._param_constant179\n",
      "        t_default_138: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant179);  _param_constant179 = None\n",
      "        view_default_478: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_178, [sym_size, 4096]);  mul_tensor_178 = None\n",
      "        mm_default_138: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_478, t_default_138);  view_default_478 = t_default_138 = None\n",
      "        view_default_479: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_138, [1, sym_size, 11008]);  mm_default_138 = None\n",
      "        mul_tensor_179: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_19, view_default_479);  silu_default_19 = view_default_479 = None\n",
      "        _param_constant180 = self._param_constant180\n",
      "        t_default_139: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant180);  _param_constant180 = None\n",
      "        sym_size_80: Sym(s0) = torch.ops.aten.sym_size(view_default_477, 1);  view_default_477 = None\n",
      "        view_default_480: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_179, [sym_size_80, 11008]);  mul_tensor_179 = None\n",
      "        mm_default_139: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_480, t_default_139);  view_default_480 = t_default_139 = None\n",
      "        view_default_481: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_139, [1, sym_size_80, 4096]);  mm_default_139 = sym_size_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_141: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_139, view_default_481);  add_tensor_139 = view_default_481 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_40: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_141, 2)\n",
      "        mean_dim_40: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_40, [-1], True);  pow_tensor_scalar_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_142: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_40, 1e-06);  mean_dim_40 = None\n",
      "        rsqrt_default_40: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_142);  add_tensor_142 = None\n",
      "        detach_default_60: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_40)\n",
      "        mul_tensor_180: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_141, rsqrt_default_40);  rsqrt_default_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant181 = self._param_constant181\n",
      "        mul_tensor_181: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant181, mul_tensor_180);  _param_constant181 = mul_tensor_180 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant182 = self._param_constant182\n",
      "        t_default_140: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant182);  _param_constant182 = None\n",
      "        view_default_482: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_181, [sym_size, 4096])\n",
      "        mm_default_140: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_482, t_default_140);  view_default_482 = t_default_140 = None\n",
      "        view_default_483: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_140, [1, sym_size, 4096]);  mm_default_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant183 = self._param_constant183\n",
      "        t_default_141: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant183);  _param_constant183 = None\n",
      "        view_default_484: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_181, [sym_size, 4096])\n",
      "        mm_default_141: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_484, t_default_141);  view_default_484 = t_default_141 = None\n",
      "        view_default_485: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_141, [1, sym_size, 4096]);  mm_default_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant184 = self._param_constant184\n",
      "        t_default_142: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant184);  _param_constant184 = None\n",
      "        view_default_486: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_181, [sym_size, 4096]);  mul_tensor_181 = None\n",
      "        mm_default_142: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_486, t_default_142);  view_default_486 = t_default_142 = None\n",
      "        view_default_487: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_142, [1, sym_size, 4096]);  mm_default_142 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_488: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_483, [1, sym_size, 32, 128])\n",
      "        transpose_int_100: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_488, 1, 2);  view_default_488 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_489: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_485, [1, sym_size, 32, 128])\n",
      "        transpose_int_101: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_489, 1, 2);  view_default_489 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_490: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_487, [1, sym_size, 32, 128])\n",
      "        transpose_int_102: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_490, 1, 2);  view_default_490 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant40 = self._tensor_constant40\n",
      "        slice_tensor_204: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant40, 0, 0, 9223372036854775807);  _tensor_constant40 = None\n",
      "        slice_tensor_205: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_204, 1, 0, 9223372036854775807);  slice_tensor_204 = None\n",
      "        sym_size_81: Sym(s0) = torch.ops.aten.sym_size(view_default_485, 1);  view_default_485 = None\n",
      "        slice_tensor_206: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_205, 2, 0, sym_size_81);  slice_tensor_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant41 = self._tensor_constant41\n",
      "        slice_tensor_207: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant41, 0, 0, 9223372036854775807);  _tensor_constant41 = None\n",
      "        slice_tensor_208: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_207, 1, 0, 9223372036854775807);  slice_tensor_207 = None\n",
      "        slice_tensor_209: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_208, 2, 0, sym_size_81);  slice_tensor_208 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_80: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_206, 1);  slice_tensor_206 = None\n",
      "        squeeze_dim_81: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_80, 0);  squeeze_dim_80 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_82: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_209, 1);  slice_tensor_209 = None\n",
      "        squeeze_dim_83: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_82, 0);  squeeze_dim_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_40: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_81, [view_default]);  squeeze_dim_81 = None\n",
      "        unsqueeze_default_45: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_40, 1);  index_tensor_40 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_41: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_83, [view_default]);  squeeze_dim_83 = None\n",
      "        unsqueeze_default_46: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_41, 1);  index_tensor_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_182: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_100, unsqueeze_default_45)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_210: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_211: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 64, 9223372036854775807);  transpose_int_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_40: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_211);  slice_tensor_211 = None\n",
      "        cat_default_40: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_40, slice_tensor_210], -1);  neg_default_40 = slice_tensor_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_183: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_40, unsqueeze_default_46);  cat_default_40 = None\n",
      "        add_tensor_143: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_182, mul_tensor_183);  mul_tensor_182 = mul_tensor_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_184: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_101, unsqueeze_default_45);  unsqueeze_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_212: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_213: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 64, 9223372036854775807);  transpose_int_101 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_41: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_213);  slice_tensor_213 = None\n",
      "        cat_default_41: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_41, slice_tensor_212], -1);  neg_default_41 = slice_tensor_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_185: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_41, unsqueeze_default_46);  cat_default_41 = unsqueeze_default_46 = None\n",
      "        add_tensor_144: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_184, mul_tensor_185);  mul_tensor_184 = mul_tensor_185 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_103: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_144, 2, 3)\n",
      "        sym_size_82: Sym(s0) = torch.ops.aten.sym_size(view_default_483, 1);  view_default_483 = None\n",
      "        expand_default_82: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_143, [1, 32, sym_size_82, 128]);  add_tensor_143 = None\n",
      "        view_default_491: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_82, [32, sym_size_82, 128]);  expand_default_82 = None\n",
      "        expand_default_83: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_103, [1, 32, 128, sym_size_81]);  transpose_int_103 = None\n",
      "        view_default_492: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_83, [32, 128, sym_size_81]);  expand_default_83 = None\n",
      "        bmm_default_40: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_491, view_default_492);  view_default_491 = view_default_492 = None\n",
      "        view_default_493: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_40, [1, 32, sym_size_82, sym_size_81]);  bmm_default_40 = None\n",
      "        div_tensor_20: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_493, 11.313708498984761);  view_default_493 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_145: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_20, add_tensor_1);  div_tensor_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_20: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_145, -1, False);  add_tensor_145 = None\n",
      "        detach_default_61: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_20)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_84: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_20, [1, 32, sym_size_82, sym_size_81]);  _softmax_default_20 = None\n",
      "        view_default_494: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_84, [32, sym_size_82, sym_size_81]);  expand_default_84 = sym_size_81 = None\n",
      "        sym_size_83: Sym(s0) = torch.ops.aten.sym_size(view_default_487, 1);  view_default_487 = None\n",
      "        expand_default_85: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_102, [1, 32, sym_size_83, 128])\n",
      "        view_default_495: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_85, [32, sym_size_83, 128]);  expand_default_85 = sym_size_83 = None\n",
      "        bmm_default_41: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_494, view_default_495);  view_default_494 = view_default_495 = None\n",
      "        view_default_496: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_41, [1, 32, sym_size_82, 128]);  bmm_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_104: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_496, 1, 2);  view_default_496 = None\n",
      "        clone_default_20: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_104, memory_format = torch.contiguous_format);  transpose_int_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_497: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_20, [1, sym_size, 4096]);  clone_default_20 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant185 = self._param_constant185\n",
      "        t_default_143: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant185);  _param_constant185 = None\n",
      "        view_default_498: f32[s0, 4096] = torch.ops.aten.view.default(view_default_497, [sym_size_82, 4096]);  view_default_497 = None\n",
      "        mm_default_143: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_498, t_default_143);  view_default_498 = t_default_143 = None\n",
      "        view_default_499: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_143, [1, sym_size_82, 4096]);  mm_default_143 = sym_size_82 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_146: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_141, view_default_499);  add_tensor_141 = view_default_499 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_41: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_146, 2)\n",
      "        mean_dim_41: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_41, [-1], True);  pow_tensor_scalar_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_147: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_41, 1e-06);  mean_dim_41 = None\n",
      "        rsqrt_default_41: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_147);  add_tensor_147 = None\n",
      "        detach_default_62: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_41)\n",
      "        mul_tensor_186: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_146, rsqrt_default_41);  rsqrt_default_41 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant186 = self._param_constant186\n",
      "        mul_tensor_187: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant186, mul_tensor_186);  _param_constant186 = mul_tensor_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant187 = self._param_constant187\n",
      "        t_default_144: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant187);  _param_constant187 = None\n",
      "        view_default_500: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_187, [sym_size, 4096])\n",
      "        mm_default_144: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_500, t_default_144);  view_default_500 = t_default_144 = None\n",
      "        view_default_501: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_144, [1, sym_size, 11008]);  mm_default_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_20: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_501)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant188 = self._param_constant188\n",
      "        t_default_145: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant188);  _param_constant188 = None\n",
      "        view_default_502: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_187, [sym_size, 4096]);  mul_tensor_187 = None\n",
      "        mm_default_145: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_502, t_default_145);  view_default_502 = t_default_145 = None\n",
      "        view_default_503: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_145, [1, sym_size, 11008]);  mm_default_145 = None\n",
      "        mul_tensor_188: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_20, view_default_503);  silu_default_20 = view_default_503 = None\n",
      "        _param_constant189 = self._param_constant189\n",
      "        t_default_146: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant189);  _param_constant189 = None\n",
      "        sym_size_84: Sym(s0) = torch.ops.aten.sym_size(view_default_501, 1);  view_default_501 = None\n",
      "        view_default_504: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_188, [sym_size_84, 11008]);  mul_tensor_188 = None\n",
      "        mm_default_146: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_504, t_default_146);  view_default_504 = t_default_146 = None\n",
      "        view_default_505: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_146, [1, sym_size_84, 4096]);  mm_default_146 = sym_size_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_148: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_146, view_default_505);  add_tensor_146 = view_default_505 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_42: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_148, 2)\n",
      "        mean_dim_42: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_42, [-1], True);  pow_tensor_scalar_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_149: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_42, 1e-06);  mean_dim_42 = None\n",
      "        rsqrt_default_42: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_149);  add_tensor_149 = None\n",
      "        detach_default_63: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_42)\n",
      "        mul_tensor_189: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_148, rsqrt_default_42);  rsqrt_default_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant190 = self._param_constant190\n",
      "        mul_tensor_190: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant190, mul_tensor_189);  _param_constant190 = mul_tensor_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant191 = self._param_constant191\n",
      "        t_default_147: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant191);  _param_constant191 = None\n",
      "        view_default_506: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_190, [sym_size, 4096])\n",
      "        mm_default_147: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_506, t_default_147);  view_default_506 = t_default_147 = None\n",
      "        view_default_507: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_147, [1, sym_size, 4096]);  mm_default_147 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant192 = self._param_constant192\n",
      "        t_default_148: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant192);  _param_constant192 = None\n",
      "        view_default_508: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_190, [sym_size, 4096])\n",
      "        mm_default_148: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_508, t_default_148);  view_default_508 = t_default_148 = None\n",
      "        view_default_509: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_148, [1, sym_size, 4096]);  mm_default_148 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant193 = self._param_constant193\n",
      "        t_default_149: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant193);  _param_constant193 = None\n",
      "        view_default_510: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_190, [sym_size, 4096]);  mul_tensor_190 = None\n",
      "        mm_default_149: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_510, t_default_149);  view_default_510 = t_default_149 = None\n",
      "        view_default_511: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_149, [1, sym_size, 4096]);  mm_default_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_512: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_507, [1, sym_size, 32, 128])\n",
      "        transpose_int_105: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_512, 1, 2);  view_default_512 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_513: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_509, [1, sym_size, 32, 128])\n",
      "        transpose_int_106: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_513, 1, 2);  view_default_513 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_514: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_511, [1, sym_size, 32, 128])\n",
      "        transpose_int_107: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_514, 1, 2);  view_default_514 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant42 = self._tensor_constant42\n",
      "        slice_tensor_214: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant42, 0, 0, 9223372036854775807);  _tensor_constant42 = None\n",
      "        slice_tensor_215: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_214, 1, 0, 9223372036854775807);  slice_tensor_214 = None\n",
      "        sym_size_85: Sym(s0) = torch.ops.aten.sym_size(view_default_509, 1);  view_default_509 = None\n",
      "        slice_tensor_216: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_215, 2, 0, sym_size_85);  slice_tensor_215 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant43 = self._tensor_constant43\n",
      "        slice_tensor_217: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant43, 0, 0, 9223372036854775807);  _tensor_constant43 = None\n",
      "        slice_tensor_218: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_217, 1, 0, 9223372036854775807);  slice_tensor_217 = None\n",
      "        slice_tensor_219: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_218, 2, 0, sym_size_85);  slice_tensor_218 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_84: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_216, 1);  slice_tensor_216 = None\n",
      "        squeeze_dim_85: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_84, 0);  squeeze_dim_84 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_86: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_219, 1);  slice_tensor_219 = None\n",
      "        squeeze_dim_87: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_86, 0);  squeeze_dim_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_42: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_85, [view_default]);  squeeze_dim_85 = None\n",
      "        unsqueeze_default_47: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_42, 1);  index_tensor_42 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_43: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_87, [view_default]);  squeeze_dim_87 = None\n",
      "        unsqueeze_default_48: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_43, 1);  index_tensor_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_191: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_105, unsqueeze_default_47)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_220: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_221: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 64, 9223372036854775807);  transpose_int_105 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_42: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_221);  slice_tensor_221 = None\n",
      "        cat_default_42: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_42, slice_tensor_220], -1);  neg_default_42 = slice_tensor_220 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_192: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_42, unsqueeze_default_48);  cat_default_42 = None\n",
      "        add_tensor_150: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_191, mul_tensor_192);  mul_tensor_191 = mul_tensor_192 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_193: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_106, unsqueeze_default_47);  unsqueeze_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_222: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_223: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 64, 9223372036854775807);  transpose_int_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_43: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_223);  slice_tensor_223 = None\n",
      "        cat_default_43: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_43, slice_tensor_222], -1);  neg_default_43 = slice_tensor_222 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_194: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_43, unsqueeze_default_48);  cat_default_43 = unsqueeze_default_48 = None\n",
      "        add_tensor_151: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_193, mul_tensor_194);  mul_tensor_193 = mul_tensor_194 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_108: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_151, 2, 3)\n",
      "        sym_size_86: Sym(s0) = torch.ops.aten.sym_size(view_default_507, 1);  view_default_507 = None\n",
      "        expand_default_86: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_150, [1, 32, sym_size_86, 128]);  add_tensor_150 = None\n",
      "        view_default_515: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_86, [32, sym_size_86, 128]);  expand_default_86 = None\n",
      "        expand_default_87: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_108, [1, 32, 128, sym_size_85]);  transpose_int_108 = None\n",
      "        view_default_516: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_87, [32, 128, sym_size_85]);  expand_default_87 = None\n",
      "        bmm_default_42: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_515, view_default_516);  view_default_515 = view_default_516 = None\n",
      "        view_default_517: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_42, [1, 32, sym_size_86, sym_size_85]);  bmm_default_42 = None\n",
      "        div_tensor_21: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_517, 11.313708498984761);  view_default_517 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_152: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_21, add_tensor_1);  div_tensor_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_21: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_152, -1, False);  add_tensor_152 = None\n",
      "        detach_default_64: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_21)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_88: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_21, [1, 32, sym_size_86, sym_size_85]);  _softmax_default_21 = None\n",
      "        view_default_518: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_88, [32, sym_size_86, sym_size_85]);  expand_default_88 = sym_size_85 = None\n",
      "        sym_size_87: Sym(s0) = torch.ops.aten.sym_size(view_default_511, 1);  view_default_511 = None\n",
      "        expand_default_89: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_107, [1, 32, sym_size_87, 128])\n",
      "        view_default_519: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_89, [32, sym_size_87, 128]);  expand_default_89 = sym_size_87 = None\n",
      "        bmm_default_43: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_518, view_default_519);  view_default_518 = view_default_519 = None\n",
      "        view_default_520: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_43, [1, 32, sym_size_86, 128]);  bmm_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_109: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_520, 1, 2);  view_default_520 = None\n",
      "        clone_default_21: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_109, memory_format = torch.contiguous_format);  transpose_int_109 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_521: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_21, [1, sym_size, 4096]);  clone_default_21 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant194 = self._param_constant194\n",
      "        t_default_150: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant194);  _param_constant194 = None\n",
      "        view_default_522: f32[s0, 4096] = torch.ops.aten.view.default(view_default_521, [sym_size_86, 4096]);  view_default_521 = None\n",
      "        mm_default_150: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_522, t_default_150);  view_default_522 = t_default_150 = None\n",
      "        view_default_523: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_150, [1, sym_size_86, 4096]);  mm_default_150 = sym_size_86 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_153: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_148, view_default_523);  add_tensor_148 = view_default_523 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_43: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_153, 2)\n",
      "        mean_dim_43: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_43, [-1], True);  pow_tensor_scalar_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_154: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_43, 1e-06);  mean_dim_43 = None\n",
      "        rsqrt_default_43: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_154);  add_tensor_154 = None\n",
      "        detach_default_65: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_43)\n",
      "        mul_tensor_195: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_153, rsqrt_default_43);  rsqrt_default_43 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant195 = self._param_constant195\n",
      "        mul_tensor_196: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant195, mul_tensor_195);  _param_constant195 = mul_tensor_195 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant196 = self._param_constant196\n",
      "        t_default_151: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant196);  _param_constant196 = None\n",
      "        view_default_524: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_196, [sym_size, 4096])\n",
      "        mm_default_151: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_524, t_default_151);  view_default_524 = t_default_151 = None\n",
      "        view_default_525: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_151, [1, sym_size, 11008]);  mm_default_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_21: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_525)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant197 = self._param_constant197\n",
      "        t_default_152: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant197);  _param_constant197 = None\n",
      "        view_default_526: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_196, [sym_size, 4096]);  mul_tensor_196 = None\n",
      "        mm_default_152: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_526, t_default_152);  view_default_526 = t_default_152 = None\n",
      "        view_default_527: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_152, [1, sym_size, 11008]);  mm_default_152 = None\n",
      "        mul_tensor_197: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_21, view_default_527);  silu_default_21 = view_default_527 = None\n",
      "        _param_constant198 = self._param_constant198\n",
      "        t_default_153: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant198);  _param_constant198 = None\n",
      "        sym_size_88: Sym(s0) = torch.ops.aten.sym_size(view_default_525, 1);  view_default_525 = None\n",
      "        view_default_528: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_197, [sym_size_88, 11008]);  mul_tensor_197 = None\n",
      "        mm_default_153: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_528, t_default_153);  view_default_528 = t_default_153 = None\n",
      "        view_default_529: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_153, [1, sym_size_88, 4096]);  mm_default_153 = sym_size_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_155: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_153, view_default_529);  add_tensor_153 = view_default_529 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_44: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_155, 2)\n",
      "        mean_dim_44: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_44, [-1], True);  pow_tensor_scalar_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_156: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_44, 1e-06);  mean_dim_44 = None\n",
      "        rsqrt_default_44: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_156);  add_tensor_156 = None\n",
      "        detach_default_66: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_44)\n",
      "        mul_tensor_198: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_155, rsqrt_default_44);  rsqrt_default_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant199 = self._param_constant199\n",
      "        mul_tensor_199: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant199, mul_tensor_198);  _param_constant199 = mul_tensor_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant200 = self._param_constant200\n",
      "        t_default_154: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant200);  _param_constant200 = None\n",
      "        view_default_530: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_199, [sym_size, 4096])\n",
      "        mm_default_154: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_530, t_default_154);  view_default_530 = t_default_154 = None\n",
      "        view_default_531: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_154, [1, sym_size, 4096]);  mm_default_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant201 = self._param_constant201\n",
      "        t_default_155: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant201);  _param_constant201 = None\n",
      "        view_default_532: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_199, [sym_size, 4096])\n",
      "        mm_default_155: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_532, t_default_155);  view_default_532 = t_default_155 = None\n",
      "        view_default_533: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_155, [1, sym_size, 4096]);  mm_default_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant202 = self._param_constant202\n",
      "        t_default_156: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant202);  _param_constant202 = None\n",
      "        view_default_534: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_199, [sym_size, 4096]);  mul_tensor_199 = None\n",
      "        mm_default_156: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_534, t_default_156);  view_default_534 = t_default_156 = None\n",
      "        view_default_535: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_156, [1, sym_size, 4096]);  mm_default_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_536: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_531, [1, sym_size, 32, 128])\n",
      "        transpose_int_110: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_536, 1, 2);  view_default_536 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_537: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_533, [1, sym_size, 32, 128])\n",
      "        transpose_int_111: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_537, 1, 2);  view_default_537 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_538: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_535, [1, sym_size, 32, 128])\n",
      "        transpose_int_112: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_538, 1, 2);  view_default_538 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant44 = self._tensor_constant44\n",
      "        slice_tensor_224: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant44, 0, 0, 9223372036854775807);  _tensor_constant44 = None\n",
      "        slice_tensor_225: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_224, 1, 0, 9223372036854775807);  slice_tensor_224 = None\n",
      "        sym_size_89: Sym(s0) = torch.ops.aten.sym_size(view_default_533, 1);  view_default_533 = None\n",
      "        slice_tensor_226: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_225, 2, 0, sym_size_89);  slice_tensor_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant45 = self._tensor_constant45\n",
      "        slice_tensor_227: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant45, 0, 0, 9223372036854775807);  _tensor_constant45 = None\n",
      "        slice_tensor_228: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_227, 1, 0, 9223372036854775807);  slice_tensor_227 = None\n",
      "        slice_tensor_229: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_228, 2, 0, sym_size_89);  slice_tensor_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_88: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_226, 1);  slice_tensor_226 = None\n",
      "        squeeze_dim_89: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_88, 0);  squeeze_dim_88 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_90: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_229, 1);  slice_tensor_229 = None\n",
      "        squeeze_dim_91: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_90, 0);  squeeze_dim_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_44: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_89, [view_default]);  squeeze_dim_89 = None\n",
      "        unsqueeze_default_49: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_44, 1);  index_tensor_44 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_45: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_91, [view_default]);  squeeze_dim_91 = None\n",
      "        unsqueeze_default_50: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_45, 1);  index_tensor_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_200: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_110, unsqueeze_default_49)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_230: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_231: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 64, 9223372036854775807);  transpose_int_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_44: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_231);  slice_tensor_231 = None\n",
      "        cat_default_44: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_44, slice_tensor_230], -1);  neg_default_44 = slice_tensor_230 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_201: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_44, unsqueeze_default_50);  cat_default_44 = None\n",
      "        add_tensor_157: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_200, mul_tensor_201);  mul_tensor_200 = mul_tensor_201 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_202: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_111, unsqueeze_default_49);  unsqueeze_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_232: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_233: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 64, 9223372036854775807);  transpose_int_111 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_45: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_233);  slice_tensor_233 = None\n",
      "        cat_default_45: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_45, slice_tensor_232], -1);  neg_default_45 = slice_tensor_232 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_203: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_45, unsqueeze_default_50);  cat_default_45 = unsqueeze_default_50 = None\n",
      "        add_tensor_158: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_202, mul_tensor_203);  mul_tensor_202 = mul_tensor_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_113: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_158, 2, 3)\n",
      "        sym_size_90: Sym(s0) = torch.ops.aten.sym_size(view_default_531, 1);  view_default_531 = None\n",
      "        expand_default_90: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_157, [1, 32, sym_size_90, 128]);  add_tensor_157 = None\n",
      "        view_default_539: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_90, [32, sym_size_90, 128]);  expand_default_90 = None\n",
      "        expand_default_91: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_113, [1, 32, 128, sym_size_89]);  transpose_int_113 = None\n",
      "        view_default_540: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_91, [32, 128, sym_size_89]);  expand_default_91 = None\n",
      "        bmm_default_44: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_539, view_default_540);  view_default_539 = view_default_540 = None\n",
      "        view_default_541: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_44, [1, 32, sym_size_90, sym_size_89]);  bmm_default_44 = None\n",
      "        div_tensor_22: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_541, 11.313708498984761);  view_default_541 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_159: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_22, add_tensor_1);  div_tensor_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_22: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_159, -1, False);  add_tensor_159 = None\n",
      "        detach_default_67: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_22)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_92: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_22, [1, 32, sym_size_90, sym_size_89]);  _softmax_default_22 = None\n",
      "        view_default_542: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_92, [32, sym_size_90, sym_size_89]);  expand_default_92 = sym_size_89 = None\n",
      "        sym_size_91: Sym(s0) = torch.ops.aten.sym_size(view_default_535, 1);  view_default_535 = None\n",
      "        expand_default_93: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_112, [1, 32, sym_size_91, 128])\n",
      "        view_default_543: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_93, [32, sym_size_91, 128]);  expand_default_93 = sym_size_91 = None\n",
      "        bmm_default_45: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_542, view_default_543);  view_default_542 = view_default_543 = None\n",
      "        view_default_544: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_45, [1, 32, sym_size_90, 128]);  bmm_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_114: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_544, 1, 2);  view_default_544 = None\n",
      "        clone_default_22: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_114, memory_format = torch.contiguous_format);  transpose_int_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_545: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_22, [1, sym_size, 4096]);  clone_default_22 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant203 = self._param_constant203\n",
      "        t_default_157: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant203);  _param_constant203 = None\n",
      "        view_default_546: f32[s0, 4096] = torch.ops.aten.view.default(view_default_545, [sym_size_90, 4096]);  view_default_545 = None\n",
      "        mm_default_157: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_546, t_default_157);  view_default_546 = t_default_157 = None\n",
      "        view_default_547: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_157, [1, sym_size_90, 4096]);  mm_default_157 = sym_size_90 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_160: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_155, view_default_547);  add_tensor_155 = view_default_547 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_45: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_160, 2)\n",
      "        mean_dim_45: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_45, [-1], True);  pow_tensor_scalar_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_161: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_45, 1e-06);  mean_dim_45 = None\n",
      "        rsqrt_default_45: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_161);  add_tensor_161 = None\n",
      "        detach_default_68: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_45)\n",
      "        mul_tensor_204: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_160, rsqrt_default_45);  rsqrt_default_45 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant204 = self._param_constant204\n",
      "        mul_tensor_205: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant204, mul_tensor_204);  _param_constant204 = mul_tensor_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant205 = self._param_constant205\n",
      "        t_default_158: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant205);  _param_constant205 = None\n",
      "        view_default_548: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_205, [sym_size, 4096])\n",
      "        mm_default_158: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_548, t_default_158);  view_default_548 = t_default_158 = None\n",
      "        view_default_549: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_158, [1, sym_size, 11008]);  mm_default_158 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_22: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_549)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant206 = self._param_constant206\n",
      "        t_default_159: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant206);  _param_constant206 = None\n",
      "        view_default_550: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_205, [sym_size, 4096]);  mul_tensor_205 = None\n",
      "        mm_default_159: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_550, t_default_159);  view_default_550 = t_default_159 = None\n",
      "        view_default_551: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_159, [1, sym_size, 11008]);  mm_default_159 = None\n",
      "        mul_tensor_206: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_22, view_default_551);  silu_default_22 = view_default_551 = None\n",
      "        _param_constant207 = self._param_constant207\n",
      "        t_default_160: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant207);  _param_constant207 = None\n",
      "        sym_size_92: Sym(s0) = torch.ops.aten.sym_size(view_default_549, 1);  view_default_549 = None\n",
      "        view_default_552: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_206, [sym_size_92, 11008]);  mul_tensor_206 = None\n",
      "        mm_default_160: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_552, t_default_160);  view_default_552 = t_default_160 = None\n",
      "        view_default_553: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_160, [1, sym_size_92, 4096]);  mm_default_160 = sym_size_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_162: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_160, view_default_553);  add_tensor_160 = view_default_553 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_46: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_162, 2)\n",
      "        mean_dim_46: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_46, [-1], True);  pow_tensor_scalar_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_163: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_46, 1e-06);  mean_dim_46 = None\n",
      "        rsqrt_default_46: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_163);  add_tensor_163 = None\n",
      "        detach_default_69: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_46)\n",
      "        mul_tensor_207: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_162, rsqrt_default_46);  rsqrt_default_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant208 = self._param_constant208\n",
      "        mul_tensor_208: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant208, mul_tensor_207);  _param_constant208 = mul_tensor_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant209 = self._param_constant209\n",
      "        t_default_161: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant209);  _param_constant209 = None\n",
      "        view_default_554: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_208, [sym_size, 4096])\n",
      "        mm_default_161: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_554, t_default_161);  view_default_554 = t_default_161 = None\n",
      "        view_default_555: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_161, [1, sym_size, 4096]);  mm_default_161 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant210 = self._param_constant210\n",
      "        t_default_162: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant210);  _param_constant210 = None\n",
      "        view_default_556: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_208, [sym_size, 4096])\n",
      "        mm_default_162: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_556, t_default_162);  view_default_556 = t_default_162 = None\n",
      "        view_default_557: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_162, [1, sym_size, 4096]);  mm_default_162 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant211 = self._param_constant211\n",
      "        t_default_163: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant211);  _param_constant211 = None\n",
      "        view_default_558: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_208, [sym_size, 4096]);  mul_tensor_208 = None\n",
      "        mm_default_163: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_558, t_default_163);  view_default_558 = t_default_163 = None\n",
      "        view_default_559: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_163, [1, sym_size, 4096]);  mm_default_163 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_560: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_555, [1, sym_size, 32, 128])\n",
      "        transpose_int_115: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_560, 1, 2);  view_default_560 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_561: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_557, [1, sym_size, 32, 128])\n",
      "        transpose_int_116: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_561, 1, 2);  view_default_561 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_562: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_559, [1, sym_size, 32, 128])\n",
      "        transpose_int_117: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_562, 1, 2);  view_default_562 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant46 = self._tensor_constant46\n",
      "        slice_tensor_234: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant46, 0, 0, 9223372036854775807);  _tensor_constant46 = None\n",
      "        slice_tensor_235: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_234, 1, 0, 9223372036854775807);  slice_tensor_234 = None\n",
      "        sym_size_93: Sym(s0) = torch.ops.aten.sym_size(view_default_557, 1);  view_default_557 = None\n",
      "        slice_tensor_236: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_235, 2, 0, sym_size_93);  slice_tensor_235 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant47 = self._tensor_constant47\n",
      "        slice_tensor_237: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant47, 0, 0, 9223372036854775807);  _tensor_constant47 = None\n",
      "        slice_tensor_238: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_237, 1, 0, 9223372036854775807);  slice_tensor_237 = None\n",
      "        slice_tensor_239: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_238, 2, 0, sym_size_93);  slice_tensor_238 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_92: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_236, 1);  slice_tensor_236 = None\n",
      "        squeeze_dim_93: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_92, 0);  squeeze_dim_92 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_94: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_239, 1);  slice_tensor_239 = None\n",
      "        squeeze_dim_95: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_94, 0);  squeeze_dim_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_46: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_93, [view_default]);  squeeze_dim_93 = None\n",
      "        unsqueeze_default_51: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_46, 1);  index_tensor_46 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_47: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_95, [view_default]);  squeeze_dim_95 = None\n",
      "        unsqueeze_default_52: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_47, 1);  index_tensor_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_209: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_115, unsqueeze_default_51)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_240: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_241: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 64, 9223372036854775807);  transpose_int_115 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_46: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_241);  slice_tensor_241 = None\n",
      "        cat_default_46: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_46, slice_tensor_240], -1);  neg_default_46 = slice_tensor_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_210: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_46, unsqueeze_default_52);  cat_default_46 = None\n",
      "        add_tensor_164: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_209, mul_tensor_210);  mul_tensor_209 = mul_tensor_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_211: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_116, unsqueeze_default_51);  unsqueeze_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_242: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_243: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 64, 9223372036854775807);  transpose_int_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_47: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_243);  slice_tensor_243 = None\n",
      "        cat_default_47: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_47, slice_tensor_242], -1);  neg_default_47 = slice_tensor_242 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_212: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_47, unsqueeze_default_52);  cat_default_47 = unsqueeze_default_52 = None\n",
      "        add_tensor_165: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_211, mul_tensor_212);  mul_tensor_211 = mul_tensor_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_118: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_165, 2, 3)\n",
      "        sym_size_94: Sym(s0) = torch.ops.aten.sym_size(view_default_555, 1);  view_default_555 = None\n",
      "        expand_default_94: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_164, [1, 32, sym_size_94, 128]);  add_tensor_164 = None\n",
      "        view_default_563: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_94, [32, sym_size_94, 128]);  expand_default_94 = None\n",
      "        expand_default_95: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_118, [1, 32, 128, sym_size_93]);  transpose_int_118 = None\n",
      "        view_default_564: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_95, [32, 128, sym_size_93]);  expand_default_95 = None\n",
      "        bmm_default_46: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_563, view_default_564);  view_default_563 = view_default_564 = None\n",
      "        view_default_565: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_46, [1, 32, sym_size_94, sym_size_93]);  bmm_default_46 = None\n",
      "        div_tensor_23: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_565, 11.313708498984761);  view_default_565 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_166: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_23, add_tensor_1);  div_tensor_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_23: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_166, -1, False);  add_tensor_166 = None\n",
      "        detach_default_70: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_23)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_96: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_23, [1, 32, sym_size_94, sym_size_93]);  _softmax_default_23 = None\n",
      "        view_default_566: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_96, [32, sym_size_94, sym_size_93]);  expand_default_96 = sym_size_93 = None\n",
      "        sym_size_95: Sym(s0) = torch.ops.aten.sym_size(view_default_559, 1);  view_default_559 = None\n",
      "        expand_default_97: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_117, [1, 32, sym_size_95, 128])\n",
      "        view_default_567: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_97, [32, sym_size_95, 128]);  expand_default_97 = sym_size_95 = None\n",
      "        bmm_default_47: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_566, view_default_567);  view_default_566 = view_default_567 = None\n",
      "        view_default_568: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_47, [1, 32, sym_size_94, 128]);  bmm_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_119: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_568, 1, 2);  view_default_568 = None\n",
      "        clone_default_23: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_119, memory_format = torch.contiguous_format);  transpose_int_119 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_569: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_23, [1, sym_size, 4096]);  clone_default_23 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant212 = self._param_constant212\n",
      "        t_default_164: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant212);  _param_constant212 = None\n",
      "        view_default_570: f32[s0, 4096] = torch.ops.aten.view.default(view_default_569, [sym_size_94, 4096]);  view_default_569 = None\n",
      "        mm_default_164: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_570, t_default_164);  view_default_570 = t_default_164 = None\n",
      "        view_default_571: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_164, [1, sym_size_94, 4096]);  mm_default_164 = sym_size_94 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_167: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_162, view_default_571);  add_tensor_162 = view_default_571 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_47: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_167, 2)\n",
      "        mean_dim_47: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_47, [-1], True);  pow_tensor_scalar_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_168: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_47, 1e-06);  mean_dim_47 = None\n",
      "        rsqrt_default_47: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_168);  add_tensor_168 = None\n",
      "        detach_default_71: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_47)\n",
      "        mul_tensor_213: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_167, rsqrt_default_47);  rsqrt_default_47 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant213 = self._param_constant213\n",
      "        mul_tensor_214: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant213, mul_tensor_213);  _param_constant213 = mul_tensor_213 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant214 = self._param_constant214\n",
      "        t_default_165: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant214);  _param_constant214 = None\n",
      "        view_default_572: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_214, [sym_size, 4096])\n",
      "        mm_default_165: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_572, t_default_165);  view_default_572 = t_default_165 = None\n",
      "        view_default_573: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_165, [1, sym_size, 11008]);  mm_default_165 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_23: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_573)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant215 = self._param_constant215\n",
      "        t_default_166: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant215);  _param_constant215 = None\n",
      "        view_default_574: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_214, [sym_size, 4096]);  mul_tensor_214 = None\n",
      "        mm_default_166: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_574, t_default_166);  view_default_574 = t_default_166 = None\n",
      "        view_default_575: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_166, [1, sym_size, 11008]);  mm_default_166 = None\n",
      "        mul_tensor_215: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_23, view_default_575);  silu_default_23 = view_default_575 = None\n",
      "        _param_constant216 = self._param_constant216\n",
      "        t_default_167: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant216);  _param_constant216 = None\n",
      "        sym_size_96: Sym(s0) = torch.ops.aten.sym_size(view_default_573, 1);  view_default_573 = None\n",
      "        view_default_576: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_215, [sym_size_96, 11008]);  mul_tensor_215 = None\n",
      "        mm_default_167: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_576, t_default_167);  view_default_576 = t_default_167 = None\n",
      "        view_default_577: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_167, [1, sym_size_96, 4096]);  mm_default_167 = sym_size_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_169: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_167, view_default_577);  add_tensor_167 = view_default_577 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_48: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_169, 2)\n",
      "        mean_dim_48: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_48, [-1], True);  pow_tensor_scalar_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_170: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_48, 1e-06);  mean_dim_48 = None\n",
      "        rsqrt_default_48: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_170);  add_tensor_170 = None\n",
      "        detach_default_72: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_48)\n",
      "        mul_tensor_216: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_169, rsqrt_default_48);  rsqrt_default_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant217 = self._param_constant217\n",
      "        mul_tensor_217: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant217, mul_tensor_216);  _param_constant217 = mul_tensor_216 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant218 = self._param_constant218\n",
      "        t_default_168: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant218);  _param_constant218 = None\n",
      "        view_default_578: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_217, [sym_size, 4096])\n",
      "        mm_default_168: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_578, t_default_168);  view_default_578 = t_default_168 = None\n",
      "        view_default_579: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_168, [1, sym_size, 4096]);  mm_default_168 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant219 = self._param_constant219\n",
      "        t_default_169: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant219);  _param_constant219 = None\n",
      "        view_default_580: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_217, [sym_size, 4096])\n",
      "        mm_default_169: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_580, t_default_169);  view_default_580 = t_default_169 = None\n",
      "        view_default_581: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_169, [1, sym_size, 4096]);  mm_default_169 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant220 = self._param_constant220\n",
      "        t_default_170: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant220);  _param_constant220 = None\n",
      "        view_default_582: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_217, [sym_size, 4096]);  mul_tensor_217 = None\n",
      "        mm_default_170: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_582, t_default_170);  view_default_582 = t_default_170 = None\n",
      "        view_default_583: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_170, [1, sym_size, 4096]);  mm_default_170 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_584: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_579, [1, sym_size, 32, 128])\n",
      "        transpose_int_120: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_584, 1, 2);  view_default_584 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_585: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_581, [1, sym_size, 32, 128])\n",
      "        transpose_int_121: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_585, 1, 2);  view_default_585 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_586: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_583, [1, sym_size, 32, 128])\n",
      "        transpose_int_122: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_586, 1, 2);  view_default_586 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant48 = self._tensor_constant48\n",
      "        slice_tensor_244: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant48, 0, 0, 9223372036854775807);  _tensor_constant48 = None\n",
      "        slice_tensor_245: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_244, 1, 0, 9223372036854775807);  slice_tensor_244 = None\n",
      "        sym_size_97: Sym(s0) = torch.ops.aten.sym_size(view_default_581, 1);  view_default_581 = None\n",
      "        slice_tensor_246: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_245, 2, 0, sym_size_97);  slice_tensor_245 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant49 = self._tensor_constant49\n",
      "        slice_tensor_247: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant49, 0, 0, 9223372036854775807);  _tensor_constant49 = None\n",
      "        slice_tensor_248: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_247, 1, 0, 9223372036854775807);  slice_tensor_247 = None\n",
      "        slice_tensor_249: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_248, 2, 0, sym_size_97);  slice_tensor_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_96: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_246, 1);  slice_tensor_246 = None\n",
      "        squeeze_dim_97: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_96, 0);  squeeze_dim_96 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_98: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_249, 1);  slice_tensor_249 = None\n",
      "        squeeze_dim_99: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_98, 0);  squeeze_dim_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_48: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_97, [view_default]);  squeeze_dim_97 = None\n",
      "        unsqueeze_default_53: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_48, 1);  index_tensor_48 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_49: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_99, [view_default]);  squeeze_dim_99 = None\n",
      "        unsqueeze_default_54: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_49, 1);  index_tensor_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_218: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_120, unsqueeze_default_53)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_250: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_251: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 64, 9223372036854775807);  transpose_int_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_48: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_251);  slice_tensor_251 = None\n",
      "        cat_default_48: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_48, slice_tensor_250], -1);  neg_default_48 = slice_tensor_250 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_219: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_48, unsqueeze_default_54);  cat_default_48 = None\n",
      "        add_tensor_171: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_218, mul_tensor_219);  mul_tensor_218 = mul_tensor_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_220: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_121, unsqueeze_default_53);  unsqueeze_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_252: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_253: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 64, 9223372036854775807);  transpose_int_121 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_49: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_253);  slice_tensor_253 = None\n",
      "        cat_default_49: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_49, slice_tensor_252], -1);  neg_default_49 = slice_tensor_252 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_221: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_49, unsqueeze_default_54);  cat_default_49 = unsqueeze_default_54 = None\n",
      "        add_tensor_172: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_220, mul_tensor_221);  mul_tensor_220 = mul_tensor_221 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_123: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_172, 2, 3)\n",
      "        sym_size_98: Sym(s0) = torch.ops.aten.sym_size(view_default_579, 1);  view_default_579 = None\n",
      "        expand_default_98: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_171, [1, 32, sym_size_98, 128]);  add_tensor_171 = None\n",
      "        view_default_587: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_98, [32, sym_size_98, 128]);  expand_default_98 = None\n",
      "        expand_default_99: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_123, [1, 32, 128, sym_size_97]);  transpose_int_123 = None\n",
      "        view_default_588: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_99, [32, 128, sym_size_97]);  expand_default_99 = None\n",
      "        bmm_default_48: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_587, view_default_588);  view_default_587 = view_default_588 = None\n",
      "        view_default_589: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_48, [1, 32, sym_size_98, sym_size_97]);  bmm_default_48 = None\n",
      "        div_tensor_24: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_589, 11.313708498984761);  view_default_589 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_173: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_24, add_tensor_1);  div_tensor_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_24: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_173, -1, False);  add_tensor_173 = None\n",
      "        detach_default_73: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_24)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_100: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_24, [1, 32, sym_size_98, sym_size_97]);  _softmax_default_24 = None\n",
      "        view_default_590: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_100, [32, sym_size_98, sym_size_97]);  expand_default_100 = sym_size_97 = None\n",
      "        sym_size_99: Sym(s0) = torch.ops.aten.sym_size(view_default_583, 1);  view_default_583 = None\n",
      "        expand_default_101: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_122, [1, 32, sym_size_99, 128])\n",
      "        view_default_591: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_101, [32, sym_size_99, 128]);  expand_default_101 = sym_size_99 = None\n",
      "        bmm_default_49: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_590, view_default_591);  view_default_590 = view_default_591 = None\n",
      "        view_default_592: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_49, [1, 32, sym_size_98, 128]);  bmm_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_124: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_592, 1, 2);  view_default_592 = None\n",
      "        clone_default_24: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_124, memory_format = torch.contiguous_format);  transpose_int_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_593: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_24, [1, sym_size, 4096]);  clone_default_24 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant221 = self._param_constant221\n",
      "        t_default_171: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant221);  _param_constant221 = None\n",
      "        view_default_594: f32[s0, 4096] = torch.ops.aten.view.default(view_default_593, [sym_size_98, 4096]);  view_default_593 = None\n",
      "        mm_default_171: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_594, t_default_171);  view_default_594 = t_default_171 = None\n",
      "        view_default_595: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_171, [1, sym_size_98, 4096]);  mm_default_171 = sym_size_98 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_174: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_169, view_default_595);  add_tensor_169 = view_default_595 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_49: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_174, 2)\n",
      "        mean_dim_49: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_49, [-1], True);  pow_tensor_scalar_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_175: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_49, 1e-06);  mean_dim_49 = None\n",
      "        rsqrt_default_49: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_175);  add_tensor_175 = None\n",
      "        detach_default_74: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_49)\n",
      "        mul_tensor_222: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_174, rsqrt_default_49);  rsqrt_default_49 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant222 = self._param_constant222\n",
      "        mul_tensor_223: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant222, mul_tensor_222);  _param_constant222 = mul_tensor_222 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant223 = self._param_constant223\n",
      "        t_default_172: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant223);  _param_constant223 = None\n",
      "        view_default_596: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_223, [sym_size, 4096])\n",
      "        mm_default_172: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_596, t_default_172);  view_default_596 = t_default_172 = None\n",
      "        view_default_597: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_172, [1, sym_size, 11008]);  mm_default_172 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_24: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_597)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant224 = self._param_constant224\n",
      "        t_default_173: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant224);  _param_constant224 = None\n",
      "        view_default_598: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_223, [sym_size, 4096]);  mul_tensor_223 = None\n",
      "        mm_default_173: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_598, t_default_173);  view_default_598 = t_default_173 = None\n",
      "        view_default_599: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_173, [1, sym_size, 11008]);  mm_default_173 = None\n",
      "        mul_tensor_224: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_24, view_default_599);  silu_default_24 = view_default_599 = None\n",
      "        _param_constant225 = self._param_constant225\n",
      "        t_default_174: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant225);  _param_constant225 = None\n",
      "        sym_size_100: Sym(s0) = torch.ops.aten.sym_size(view_default_597, 1);  view_default_597 = None\n",
      "        view_default_600: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_224, [sym_size_100, 11008]);  mul_tensor_224 = None\n",
      "        mm_default_174: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_600, t_default_174);  view_default_600 = t_default_174 = None\n",
      "        view_default_601: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_174, [1, sym_size_100, 4096]);  mm_default_174 = sym_size_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_176: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_174, view_default_601);  add_tensor_174 = view_default_601 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_50: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_176, 2)\n",
      "        mean_dim_50: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_50, [-1], True);  pow_tensor_scalar_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_177: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_50, 1e-06);  mean_dim_50 = None\n",
      "        rsqrt_default_50: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_177);  add_tensor_177 = None\n",
      "        detach_default_75: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_50)\n",
      "        mul_tensor_225: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_176, rsqrt_default_50);  rsqrt_default_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant226 = self._param_constant226\n",
      "        mul_tensor_226: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant226, mul_tensor_225);  _param_constant226 = mul_tensor_225 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant227 = self._param_constant227\n",
      "        t_default_175: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant227);  _param_constant227 = None\n",
      "        view_default_602: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_226, [sym_size, 4096])\n",
      "        mm_default_175: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_602, t_default_175);  view_default_602 = t_default_175 = None\n",
      "        view_default_603: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_175, [1, sym_size, 4096]);  mm_default_175 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant228 = self._param_constant228\n",
      "        t_default_176: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant228);  _param_constant228 = None\n",
      "        view_default_604: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_226, [sym_size, 4096])\n",
      "        mm_default_176: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_604, t_default_176);  view_default_604 = t_default_176 = None\n",
      "        view_default_605: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_176, [1, sym_size, 4096]);  mm_default_176 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant229 = self._param_constant229\n",
      "        t_default_177: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant229);  _param_constant229 = None\n",
      "        view_default_606: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_226, [sym_size, 4096]);  mul_tensor_226 = None\n",
      "        mm_default_177: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_606, t_default_177);  view_default_606 = t_default_177 = None\n",
      "        view_default_607: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_177, [1, sym_size, 4096]);  mm_default_177 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_608: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_603, [1, sym_size, 32, 128])\n",
      "        transpose_int_125: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_608, 1, 2);  view_default_608 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_609: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_605, [1, sym_size, 32, 128])\n",
      "        transpose_int_126: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_609, 1, 2);  view_default_609 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_610: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_607, [1, sym_size, 32, 128])\n",
      "        transpose_int_127: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_610, 1, 2);  view_default_610 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant50 = self._tensor_constant50\n",
      "        slice_tensor_254: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant50, 0, 0, 9223372036854775807);  _tensor_constant50 = None\n",
      "        slice_tensor_255: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_254, 1, 0, 9223372036854775807);  slice_tensor_254 = None\n",
      "        sym_size_101: Sym(s0) = torch.ops.aten.sym_size(view_default_605, 1);  view_default_605 = None\n",
      "        slice_tensor_256: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_255, 2, 0, sym_size_101);  slice_tensor_255 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant51 = self._tensor_constant51\n",
      "        slice_tensor_257: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant51, 0, 0, 9223372036854775807);  _tensor_constant51 = None\n",
      "        slice_tensor_258: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_257, 1, 0, 9223372036854775807);  slice_tensor_257 = None\n",
      "        slice_tensor_259: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_258, 2, 0, sym_size_101);  slice_tensor_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_100: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_256, 1);  slice_tensor_256 = None\n",
      "        squeeze_dim_101: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_100, 0);  squeeze_dim_100 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_102: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_259, 1);  slice_tensor_259 = None\n",
      "        squeeze_dim_103: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_102, 0);  squeeze_dim_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_50: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_101, [view_default]);  squeeze_dim_101 = None\n",
      "        unsqueeze_default_55: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_50, 1);  index_tensor_50 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_51: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_103, [view_default]);  squeeze_dim_103 = None\n",
      "        unsqueeze_default_56: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_51, 1);  index_tensor_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_227: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_125, unsqueeze_default_55)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_260: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_261: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 64, 9223372036854775807);  transpose_int_125 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_50: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_261);  slice_tensor_261 = None\n",
      "        cat_default_50: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_50, slice_tensor_260], -1);  neg_default_50 = slice_tensor_260 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_228: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_50, unsqueeze_default_56);  cat_default_50 = None\n",
      "        add_tensor_178: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_227, mul_tensor_228);  mul_tensor_227 = mul_tensor_228 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_229: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_126, unsqueeze_default_55);  unsqueeze_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_262: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_263: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 64, 9223372036854775807);  transpose_int_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_51: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_263);  slice_tensor_263 = None\n",
      "        cat_default_51: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_51, slice_tensor_262], -1);  neg_default_51 = slice_tensor_262 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_230: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_51, unsqueeze_default_56);  cat_default_51 = unsqueeze_default_56 = None\n",
      "        add_tensor_179: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_229, mul_tensor_230);  mul_tensor_229 = mul_tensor_230 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_128: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_179, 2, 3)\n",
      "        sym_size_102: Sym(s0) = torch.ops.aten.sym_size(view_default_603, 1);  view_default_603 = None\n",
      "        expand_default_102: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_178, [1, 32, sym_size_102, 128]);  add_tensor_178 = None\n",
      "        view_default_611: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_102, [32, sym_size_102, 128]);  expand_default_102 = None\n",
      "        expand_default_103: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_128, [1, 32, 128, sym_size_101]);  transpose_int_128 = None\n",
      "        view_default_612: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_103, [32, 128, sym_size_101]);  expand_default_103 = None\n",
      "        bmm_default_50: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_611, view_default_612);  view_default_611 = view_default_612 = None\n",
      "        view_default_613: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_50, [1, 32, sym_size_102, sym_size_101]);  bmm_default_50 = None\n",
      "        div_tensor_25: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_613, 11.313708498984761);  view_default_613 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_180: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_25, add_tensor_1);  div_tensor_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_25: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_180, -1, False);  add_tensor_180 = None\n",
      "        detach_default_76: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_25)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_104: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_25, [1, 32, sym_size_102, sym_size_101]);  _softmax_default_25 = None\n",
      "        view_default_614: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_104, [32, sym_size_102, sym_size_101]);  expand_default_104 = sym_size_101 = None\n",
      "        sym_size_103: Sym(s0) = torch.ops.aten.sym_size(view_default_607, 1);  view_default_607 = None\n",
      "        expand_default_105: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_127, [1, 32, sym_size_103, 128])\n",
      "        view_default_615: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_105, [32, sym_size_103, 128]);  expand_default_105 = sym_size_103 = None\n",
      "        bmm_default_51: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_614, view_default_615);  view_default_614 = view_default_615 = None\n",
      "        view_default_616: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_51, [1, 32, sym_size_102, 128]);  bmm_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_129: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_616, 1, 2);  view_default_616 = None\n",
      "        clone_default_25: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_129, memory_format = torch.contiguous_format);  transpose_int_129 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_617: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_25, [1, sym_size, 4096]);  clone_default_25 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant230 = self._param_constant230\n",
      "        t_default_178: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant230);  _param_constant230 = None\n",
      "        view_default_618: f32[s0, 4096] = torch.ops.aten.view.default(view_default_617, [sym_size_102, 4096]);  view_default_617 = None\n",
      "        mm_default_178: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_618, t_default_178);  view_default_618 = t_default_178 = None\n",
      "        view_default_619: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_178, [1, sym_size_102, 4096]);  mm_default_178 = sym_size_102 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_181: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_176, view_default_619);  add_tensor_176 = view_default_619 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_51: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_181, 2)\n",
      "        mean_dim_51: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_51, [-1], True);  pow_tensor_scalar_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_182: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_51, 1e-06);  mean_dim_51 = None\n",
      "        rsqrt_default_51: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_182);  add_tensor_182 = None\n",
      "        detach_default_77: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_51)\n",
      "        mul_tensor_231: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_181, rsqrt_default_51);  rsqrt_default_51 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant231 = self._param_constant231\n",
      "        mul_tensor_232: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant231, mul_tensor_231);  _param_constant231 = mul_tensor_231 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant232 = self._param_constant232\n",
      "        t_default_179: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant232);  _param_constant232 = None\n",
      "        view_default_620: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_232, [sym_size, 4096])\n",
      "        mm_default_179: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_620, t_default_179);  view_default_620 = t_default_179 = None\n",
      "        view_default_621: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_179, [1, sym_size, 11008]);  mm_default_179 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_25: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_621)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant233 = self._param_constant233\n",
      "        t_default_180: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant233);  _param_constant233 = None\n",
      "        view_default_622: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_232, [sym_size, 4096]);  mul_tensor_232 = None\n",
      "        mm_default_180: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_622, t_default_180);  view_default_622 = t_default_180 = None\n",
      "        view_default_623: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_180, [1, sym_size, 11008]);  mm_default_180 = None\n",
      "        mul_tensor_233: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_25, view_default_623);  silu_default_25 = view_default_623 = None\n",
      "        _param_constant234 = self._param_constant234\n",
      "        t_default_181: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant234);  _param_constant234 = None\n",
      "        sym_size_104: Sym(s0) = torch.ops.aten.sym_size(view_default_621, 1);  view_default_621 = None\n",
      "        view_default_624: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_233, [sym_size_104, 11008]);  mul_tensor_233 = None\n",
      "        mm_default_181: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_624, t_default_181);  view_default_624 = t_default_181 = None\n",
      "        view_default_625: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_181, [1, sym_size_104, 4096]);  mm_default_181 = sym_size_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_183: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_181, view_default_625);  add_tensor_181 = view_default_625 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_52: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_183, 2)\n",
      "        mean_dim_52: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_52, [-1], True);  pow_tensor_scalar_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_184: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_52, 1e-06);  mean_dim_52 = None\n",
      "        rsqrt_default_52: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_184);  add_tensor_184 = None\n",
      "        detach_default_78: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_52)\n",
      "        mul_tensor_234: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_183, rsqrt_default_52);  rsqrt_default_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant235 = self._param_constant235\n",
      "        mul_tensor_235: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant235, mul_tensor_234);  _param_constant235 = mul_tensor_234 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant236 = self._param_constant236\n",
      "        t_default_182: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant236);  _param_constant236 = None\n",
      "        view_default_626: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_235, [sym_size, 4096])\n",
      "        mm_default_182: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_626, t_default_182);  view_default_626 = t_default_182 = None\n",
      "        view_default_627: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_182, [1, sym_size, 4096]);  mm_default_182 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant237 = self._param_constant237\n",
      "        t_default_183: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant237);  _param_constant237 = None\n",
      "        view_default_628: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_235, [sym_size, 4096])\n",
      "        mm_default_183: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_628, t_default_183);  view_default_628 = t_default_183 = None\n",
      "        view_default_629: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_183, [1, sym_size, 4096]);  mm_default_183 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant238 = self._param_constant238\n",
      "        t_default_184: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant238);  _param_constant238 = None\n",
      "        view_default_630: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_235, [sym_size, 4096]);  mul_tensor_235 = None\n",
      "        mm_default_184: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_630, t_default_184);  view_default_630 = t_default_184 = None\n",
      "        view_default_631: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_184, [1, sym_size, 4096]);  mm_default_184 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_632: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_627, [1, sym_size, 32, 128])\n",
      "        transpose_int_130: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_632, 1, 2);  view_default_632 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_633: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_629, [1, sym_size, 32, 128])\n",
      "        transpose_int_131: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_633, 1, 2);  view_default_633 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_634: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_631, [1, sym_size, 32, 128])\n",
      "        transpose_int_132: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_634, 1, 2);  view_default_634 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant52 = self._tensor_constant52\n",
      "        slice_tensor_264: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant52, 0, 0, 9223372036854775807);  _tensor_constant52 = None\n",
      "        slice_tensor_265: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_264, 1, 0, 9223372036854775807);  slice_tensor_264 = None\n",
      "        sym_size_105: Sym(s0) = torch.ops.aten.sym_size(view_default_629, 1);  view_default_629 = None\n",
      "        slice_tensor_266: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_265, 2, 0, sym_size_105);  slice_tensor_265 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant53 = self._tensor_constant53\n",
      "        slice_tensor_267: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant53, 0, 0, 9223372036854775807);  _tensor_constant53 = None\n",
      "        slice_tensor_268: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_267, 1, 0, 9223372036854775807);  slice_tensor_267 = None\n",
      "        slice_tensor_269: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_268, 2, 0, sym_size_105);  slice_tensor_268 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_104: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_266, 1);  slice_tensor_266 = None\n",
      "        squeeze_dim_105: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_104, 0);  squeeze_dim_104 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_106: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_269, 1);  slice_tensor_269 = None\n",
      "        squeeze_dim_107: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_106, 0);  squeeze_dim_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_52: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_105, [view_default]);  squeeze_dim_105 = None\n",
      "        unsqueeze_default_57: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_52, 1);  index_tensor_52 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_53: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_107, [view_default]);  squeeze_dim_107 = None\n",
      "        unsqueeze_default_58: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_53, 1);  index_tensor_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_236: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_130, unsqueeze_default_57)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_270: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_271: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 64, 9223372036854775807);  transpose_int_130 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_52: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_271);  slice_tensor_271 = None\n",
      "        cat_default_52: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_52, slice_tensor_270], -1);  neg_default_52 = slice_tensor_270 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_237: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_52, unsqueeze_default_58);  cat_default_52 = None\n",
      "        add_tensor_185: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_236, mul_tensor_237);  mul_tensor_236 = mul_tensor_237 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_238: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_131, unsqueeze_default_57);  unsqueeze_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_272: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_273: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 64, 9223372036854775807);  transpose_int_131 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_53: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_273);  slice_tensor_273 = None\n",
      "        cat_default_53: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_53, slice_tensor_272], -1);  neg_default_53 = slice_tensor_272 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_239: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_53, unsqueeze_default_58);  cat_default_53 = unsqueeze_default_58 = None\n",
      "        add_tensor_186: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_238, mul_tensor_239);  mul_tensor_238 = mul_tensor_239 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_133: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_186, 2, 3)\n",
      "        sym_size_106: Sym(s0) = torch.ops.aten.sym_size(view_default_627, 1);  view_default_627 = None\n",
      "        expand_default_106: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_185, [1, 32, sym_size_106, 128]);  add_tensor_185 = None\n",
      "        view_default_635: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_106, [32, sym_size_106, 128]);  expand_default_106 = None\n",
      "        expand_default_107: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_133, [1, 32, 128, sym_size_105]);  transpose_int_133 = None\n",
      "        view_default_636: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_107, [32, 128, sym_size_105]);  expand_default_107 = None\n",
      "        bmm_default_52: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_635, view_default_636);  view_default_635 = view_default_636 = None\n",
      "        view_default_637: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_52, [1, 32, sym_size_106, sym_size_105]);  bmm_default_52 = None\n",
      "        div_tensor_26: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_637, 11.313708498984761);  view_default_637 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_187: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_26, add_tensor_1);  div_tensor_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_26: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_187, -1, False);  add_tensor_187 = None\n",
      "        detach_default_79: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_26)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_108: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_26, [1, 32, sym_size_106, sym_size_105]);  _softmax_default_26 = None\n",
      "        view_default_638: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_108, [32, sym_size_106, sym_size_105]);  expand_default_108 = sym_size_105 = None\n",
      "        sym_size_107: Sym(s0) = torch.ops.aten.sym_size(view_default_631, 1);  view_default_631 = None\n",
      "        expand_default_109: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_132, [1, 32, sym_size_107, 128])\n",
      "        view_default_639: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_109, [32, sym_size_107, 128]);  expand_default_109 = sym_size_107 = None\n",
      "        bmm_default_53: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_638, view_default_639);  view_default_638 = view_default_639 = None\n",
      "        view_default_640: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_53, [1, 32, sym_size_106, 128]);  bmm_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_134: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_640, 1, 2);  view_default_640 = None\n",
      "        clone_default_26: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_134, memory_format = torch.contiguous_format);  transpose_int_134 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_641: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_26, [1, sym_size, 4096]);  clone_default_26 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant239 = self._param_constant239\n",
      "        t_default_185: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant239);  _param_constant239 = None\n",
      "        view_default_642: f32[s0, 4096] = torch.ops.aten.view.default(view_default_641, [sym_size_106, 4096]);  view_default_641 = None\n",
      "        mm_default_185: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_642, t_default_185);  view_default_642 = t_default_185 = None\n",
      "        view_default_643: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_185, [1, sym_size_106, 4096]);  mm_default_185 = sym_size_106 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_188: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_183, view_default_643);  add_tensor_183 = view_default_643 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_53: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_188, 2)\n",
      "        mean_dim_53: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_53, [-1], True);  pow_tensor_scalar_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_189: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_53, 1e-06);  mean_dim_53 = None\n",
      "        rsqrt_default_53: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_189);  add_tensor_189 = None\n",
      "        detach_default_80: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_53)\n",
      "        mul_tensor_240: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_188, rsqrt_default_53);  rsqrt_default_53 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant240 = self._param_constant240\n",
      "        mul_tensor_241: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant240, mul_tensor_240);  _param_constant240 = mul_tensor_240 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant241 = self._param_constant241\n",
      "        t_default_186: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant241);  _param_constant241 = None\n",
      "        view_default_644: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_241, [sym_size, 4096])\n",
      "        mm_default_186: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_644, t_default_186);  view_default_644 = t_default_186 = None\n",
      "        view_default_645: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_186, [1, sym_size, 11008]);  mm_default_186 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_26: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_645)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant242 = self._param_constant242\n",
      "        t_default_187: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant242);  _param_constant242 = None\n",
      "        view_default_646: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_241, [sym_size, 4096]);  mul_tensor_241 = None\n",
      "        mm_default_187: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_646, t_default_187);  view_default_646 = t_default_187 = None\n",
      "        view_default_647: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_187, [1, sym_size, 11008]);  mm_default_187 = None\n",
      "        mul_tensor_242: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_26, view_default_647);  silu_default_26 = view_default_647 = None\n",
      "        _param_constant243 = self._param_constant243\n",
      "        t_default_188: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant243);  _param_constant243 = None\n",
      "        sym_size_108: Sym(s0) = torch.ops.aten.sym_size(view_default_645, 1);  view_default_645 = None\n",
      "        view_default_648: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_242, [sym_size_108, 11008]);  mul_tensor_242 = None\n",
      "        mm_default_188: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_648, t_default_188);  view_default_648 = t_default_188 = None\n",
      "        view_default_649: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_188, [1, sym_size_108, 4096]);  mm_default_188 = sym_size_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_190: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_188, view_default_649);  add_tensor_188 = view_default_649 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_54: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_190, 2)\n",
      "        mean_dim_54: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_54, [-1], True);  pow_tensor_scalar_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_191: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_54, 1e-06);  mean_dim_54 = None\n",
      "        rsqrt_default_54: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_191);  add_tensor_191 = None\n",
      "        detach_default_81: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_54)\n",
      "        mul_tensor_243: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_190, rsqrt_default_54);  rsqrt_default_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant244 = self._param_constant244\n",
      "        mul_tensor_244: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant244, mul_tensor_243);  _param_constant244 = mul_tensor_243 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant245 = self._param_constant245\n",
      "        t_default_189: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant245);  _param_constant245 = None\n",
      "        view_default_650: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_244, [sym_size, 4096])\n",
      "        mm_default_189: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_650, t_default_189);  view_default_650 = t_default_189 = None\n",
      "        view_default_651: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_189, [1, sym_size, 4096]);  mm_default_189 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant246 = self._param_constant246\n",
      "        t_default_190: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant246);  _param_constant246 = None\n",
      "        view_default_652: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_244, [sym_size, 4096])\n",
      "        mm_default_190: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_652, t_default_190);  view_default_652 = t_default_190 = None\n",
      "        view_default_653: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_190, [1, sym_size, 4096]);  mm_default_190 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant247 = self._param_constant247\n",
      "        t_default_191: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant247);  _param_constant247 = None\n",
      "        view_default_654: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_244, [sym_size, 4096]);  mul_tensor_244 = None\n",
      "        mm_default_191: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_654, t_default_191);  view_default_654 = t_default_191 = None\n",
      "        view_default_655: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_191, [1, sym_size, 4096]);  mm_default_191 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_656: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_651, [1, sym_size, 32, 128])\n",
      "        transpose_int_135: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_656, 1, 2);  view_default_656 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_657: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_653, [1, sym_size, 32, 128])\n",
      "        transpose_int_136: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_657, 1, 2);  view_default_657 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_658: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_655, [1, sym_size, 32, 128])\n",
      "        transpose_int_137: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_658, 1, 2);  view_default_658 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant54 = self._tensor_constant54\n",
      "        slice_tensor_274: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant54, 0, 0, 9223372036854775807);  _tensor_constant54 = None\n",
      "        slice_tensor_275: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_274, 1, 0, 9223372036854775807);  slice_tensor_274 = None\n",
      "        sym_size_109: Sym(s0) = torch.ops.aten.sym_size(view_default_653, 1);  view_default_653 = None\n",
      "        slice_tensor_276: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_275, 2, 0, sym_size_109);  slice_tensor_275 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant55 = self._tensor_constant55\n",
      "        slice_tensor_277: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant55, 0, 0, 9223372036854775807);  _tensor_constant55 = None\n",
      "        slice_tensor_278: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_277, 1, 0, 9223372036854775807);  slice_tensor_277 = None\n",
      "        slice_tensor_279: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_278, 2, 0, sym_size_109);  slice_tensor_278 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_108: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_276, 1);  slice_tensor_276 = None\n",
      "        squeeze_dim_109: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_108, 0);  squeeze_dim_108 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_110: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_279, 1);  slice_tensor_279 = None\n",
      "        squeeze_dim_111: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_110, 0);  squeeze_dim_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_54: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_109, [view_default]);  squeeze_dim_109 = None\n",
      "        unsqueeze_default_59: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_54, 1);  index_tensor_54 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_55: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_111, [view_default]);  squeeze_dim_111 = None\n",
      "        unsqueeze_default_60: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_55, 1);  index_tensor_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_245: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_135, unsqueeze_default_59)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_280: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_281: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 64, 9223372036854775807);  transpose_int_135 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_54: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_281);  slice_tensor_281 = None\n",
      "        cat_default_54: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_54, slice_tensor_280], -1);  neg_default_54 = slice_tensor_280 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_246: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_54, unsqueeze_default_60);  cat_default_54 = None\n",
      "        add_tensor_192: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_245, mul_tensor_246);  mul_tensor_245 = mul_tensor_246 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_247: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_136, unsqueeze_default_59);  unsqueeze_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_282: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_283: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 64, 9223372036854775807);  transpose_int_136 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_55: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_283);  slice_tensor_283 = None\n",
      "        cat_default_55: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_55, slice_tensor_282], -1);  neg_default_55 = slice_tensor_282 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_248: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_55, unsqueeze_default_60);  cat_default_55 = unsqueeze_default_60 = None\n",
      "        add_tensor_193: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_247, mul_tensor_248);  mul_tensor_247 = mul_tensor_248 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_138: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_193, 2, 3)\n",
      "        sym_size_110: Sym(s0) = torch.ops.aten.sym_size(view_default_651, 1);  view_default_651 = None\n",
      "        expand_default_110: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_192, [1, 32, sym_size_110, 128]);  add_tensor_192 = None\n",
      "        view_default_659: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_110, [32, sym_size_110, 128]);  expand_default_110 = None\n",
      "        expand_default_111: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_138, [1, 32, 128, sym_size_109]);  transpose_int_138 = None\n",
      "        view_default_660: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_111, [32, 128, sym_size_109]);  expand_default_111 = None\n",
      "        bmm_default_54: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_659, view_default_660);  view_default_659 = view_default_660 = None\n",
      "        view_default_661: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_54, [1, 32, sym_size_110, sym_size_109]);  bmm_default_54 = None\n",
      "        div_tensor_27: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_661, 11.313708498984761);  view_default_661 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_194: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_27, add_tensor_1);  div_tensor_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_27: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_194, -1, False);  add_tensor_194 = None\n",
      "        detach_default_82: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_27)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_112: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_27, [1, 32, sym_size_110, sym_size_109]);  _softmax_default_27 = None\n",
      "        view_default_662: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_112, [32, sym_size_110, sym_size_109]);  expand_default_112 = sym_size_109 = None\n",
      "        sym_size_111: Sym(s0) = torch.ops.aten.sym_size(view_default_655, 1);  view_default_655 = None\n",
      "        expand_default_113: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_137, [1, 32, sym_size_111, 128])\n",
      "        view_default_663: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_113, [32, sym_size_111, 128]);  expand_default_113 = sym_size_111 = None\n",
      "        bmm_default_55: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_662, view_default_663);  view_default_662 = view_default_663 = None\n",
      "        view_default_664: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_55, [1, 32, sym_size_110, 128]);  bmm_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_139: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_664, 1, 2);  view_default_664 = None\n",
      "        clone_default_27: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_139, memory_format = torch.contiguous_format);  transpose_int_139 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_665: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_27, [1, sym_size, 4096]);  clone_default_27 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant248 = self._param_constant248\n",
      "        t_default_192: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant248);  _param_constant248 = None\n",
      "        view_default_666: f32[s0, 4096] = torch.ops.aten.view.default(view_default_665, [sym_size_110, 4096]);  view_default_665 = None\n",
      "        mm_default_192: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_666, t_default_192);  view_default_666 = t_default_192 = None\n",
      "        view_default_667: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_192, [1, sym_size_110, 4096]);  mm_default_192 = sym_size_110 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_195: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_190, view_default_667);  add_tensor_190 = view_default_667 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_55: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_195, 2)\n",
      "        mean_dim_55: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_55, [-1], True);  pow_tensor_scalar_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_196: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_55, 1e-06);  mean_dim_55 = None\n",
      "        rsqrt_default_55: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_196);  add_tensor_196 = None\n",
      "        detach_default_83: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_55)\n",
      "        mul_tensor_249: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_195, rsqrt_default_55);  rsqrt_default_55 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant249 = self._param_constant249\n",
      "        mul_tensor_250: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant249, mul_tensor_249);  _param_constant249 = mul_tensor_249 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant250 = self._param_constant250\n",
      "        t_default_193: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant250);  _param_constant250 = None\n",
      "        view_default_668: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_250, [sym_size, 4096])\n",
      "        mm_default_193: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_668, t_default_193);  view_default_668 = t_default_193 = None\n",
      "        view_default_669: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_193, [1, sym_size, 11008]);  mm_default_193 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_27: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_669)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant251 = self._param_constant251\n",
      "        t_default_194: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant251);  _param_constant251 = None\n",
      "        view_default_670: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_250, [sym_size, 4096]);  mul_tensor_250 = None\n",
      "        mm_default_194: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_670, t_default_194);  view_default_670 = t_default_194 = None\n",
      "        view_default_671: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_194, [1, sym_size, 11008]);  mm_default_194 = None\n",
      "        mul_tensor_251: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_27, view_default_671);  silu_default_27 = view_default_671 = None\n",
      "        _param_constant252 = self._param_constant252\n",
      "        t_default_195: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant252);  _param_constant252 = None\n",
      "        sym_size_112: Sym(s0) = torch.ops.aten.sym_size(view_default_669, 1);  view_default_669 = None\n",
      "        view_default_672: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_251, [sym_size_112, 11008]);  mul_tensor_251 = None\n",
      "        mm_default_195: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_672, t_default_195);  view_default_672 = t_default_195 = None\n",
      "        view_default_673: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_195, [1, sym_size_112, 4096]);  mm_default_195 = sym_size_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_197: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_195, view_default_673);  add_tensor_195 = view_default_673 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_56: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_197, 2)\n",
      "        mean_dim_56: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_56, [-1], True);  pow_tensor_scalar_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_198: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_56, 1e-06);  mean_dim_56 = None\n",
      "        rsqrt_default_56: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_198);  add_tensor_198 = None\n",
      "        detach_default_84: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_56)\n",
      "        mul_tensor_252: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_197, rsqrt_default_56);  rsqrt_default_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant253 = self._param_constant253\n",
      "        mul_tensor_253: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant253, mul_tensor_252);  _param_constant253 = mul_tensor_252 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant254 = self._param_constant254\n",
      "        t_default_196: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant254);  _param_constant254 = None\n",
      "        view_default_674: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_253, [sym_size, 4096])\n",
      "        mm_default_196: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_674, t_default_196);  view_default_674 = t_default_196 = None\n",
      "        view_default_675: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_196, [1, sym_size, 4096]);  mm_default_196 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant255 = self._param_constant255\n",
      "        t_default_197: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant255);  _param_constant255 = None\n",
      "        view_default_676: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_253, [sym_size, 4096])\n",
      "        mm_default_197: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_676, t_default_197);  view_default_676 = t_default_197 = None\n",
      "        view_default_677: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_197, [1, sym_size, 4096]);  mm_default_197 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant256 = self._param_constant256\n",
      "        t_default_198: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant256);  _param_constant256 = None\n",
      "        view_default_678: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_253, [sym_size, 4096]);  mul_tensor_253 = None\n",
      "        mm_default_198: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_678, t_default_198);  view_default_678 = t_default_198 = None\n",
      "        view_default_679: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_198, [1, sym_size, 4096]);  mm_default_198 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_680: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_675, [1, sym_size, 32, 128])\n",
      "        transpose_int_140: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_680, 1, 2);  view_default_680 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_681: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_677, [1, sym_size, 32, 128])\n",
      "        transpose_int_141: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_681, 1, 2);  view_default_681 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_682: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_679, [1, sym_size, 32, 128])\n",
      "        transpose_int_142: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_682, 1, 2);  view_default_682 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant56 = self._tensor_constant56\n",
      "        slice_tensor_284: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant56, 0, 0, 9223372036854775807);  _tensor_constant56 = None\n",
      "        slice_tensor_285: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_284, 1, 0, 9223372036854775807);  slice_tensor_284 = None\n",
      "        sym_size_113: Sym(s0) = torch.ops.aten.sym_size(view_default_677, 1);  view_default_677 = None\n",
      "        slice_tensor_286: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_285, 2, 0, sym_size_113);  slice_tensor_285 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant57 = self._tensor_constant57\n",
      "        slice_tensor_287: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant57, 0, 0, 9223372036854775807);  _tensor_constant57 = None\n",
      "        slice_tensor_288: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_287, 1, 0, 9223372036854775807);  slice_tensor_287 = None\n",
      "        slice_tensor_289: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_288, 2, 0, sym_size_113);  slice_tensor_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_112: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_286, 1);  slice_tensor_286 = None\n",
      "        squeeze_dim_113: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_112, 0);  squeeze_dim_112 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_114: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_289, 1);  slice_tensor_289 = None\n",
      "        squeeze_dim_115: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_114, 0);  squeeze_dim_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_56: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_113, [view_default]);  squeeze_dim_113 = None\n",
      "        unsqueeze_default_61: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_56, 1);  index_tensor_56 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_57: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_115, [view_default]);  squeeze_dim_115 = None\n",
      "        unsqueeze_default_62: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_57, 1);  index_tensor_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_254: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_140, unsqueeze_default_61)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_290: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_291: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 64, 9223372036854775807);  transpose_int_140 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_56: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_291);  slice_tensor_291 = None\n",
      "        cat_default_56: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_56, slice_tensor_290], -1);  neg_default_56 = slice_tensor_290 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_255: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_56, unsqueeze_default_62);  cat_default_56 = None\n",
      "        add_tensor_199: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_254, mul_tensor_255);  mul_tensor_254 = mul_tensor_255 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_256: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_141, unsqueeze_default_61);  unsqueeze_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_292: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_293: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 64, 9223372036854775807);  transpose_int_141 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_57: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_293);  slice_tensor_293 = None\n",
      "        cat_default_57: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_57, slice_tensor_292], -1);  neg_default_57 = slice_tensor_292 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_257: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_57, unsqueeze_default_62);  cat_default_57 = unsqueeze_default_62 = None\n",
      "        add_tensor_200: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_256, mul_tensor_257);  mul_tensor_256 = mul_tensor_257 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_143: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_200, 2, 3)\n",
      "        sym_size_114: Sym(s0) = torch.ops.aten.sym_size(view_default_675, 1);  view_default_675 = None\n",
      "        expand_default_114: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_199, [1, 32, sym_size_114, 128]);  add_tensor_199 = None\n",
      "        view_default_683: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_114, [32, sym_size_114, 128]);  expand_default_114 = None\n",
      "        expand_default_115: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_143, [1, 32, 128, sym_size_113]);  transpose_int_143 = None\n",
      "        view_default_684: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_115, [32, 128, sym_size_113]);  expand_default_115 = None\n",
      "        bmm_default_56: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_683, view_default_684);  view_default_683 = view_default_684 = None\n",
      "        view_default_685: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_56, [1, 32, sym_size_114, sym_size_113]);  bmm_default_56 = None\n",
      "        div_tensor_28: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_685, 11.313708498984761);  view_default_685 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_201: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_28, add_tensor_1);  div_tensor_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_28: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_201, -1, False);  add_tensor_201 = None\n",
      "        detach_default_85: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_28)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_116: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_28, [1, 32, sym_size_114, sym_size_113]);  _softmax_default_28 = None\n",
      "        view_default_686: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_116, [32, sym_size_114, sym_size_113]);  expand_default_116 = sym_size_113 = None\n",
      "        sym_size_115: Sym(s0) = torch.ops.aten.sym_size(view_default_679, 1);  view_default_679 = None\n",
      "        expand_default_117: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_142, [1, 32, sym_size_115, 128])\n",
      "        view_default_687: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_117, [32, sym_size_115, 128]);  expand_default_117 = sym_size_115 = None\n",
      "        bmm_default_57: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_686, view_default_687);  view_default_686 = view_default_687 = None\n",
      "        view_default_688: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_57, [1, 32, sym_size_114, 128]);  bmm_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_144: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_688, 1, 2);  view_default_688 = None\n",
      "        clone_default_28: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_144, memory_format = torch.contiguous_format);  transpose_int_144 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_689: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_28, [1, sym_size, 4096]);  clone_default_28 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant257 = self._param_constant257\n",
      "        t_default_199: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant257);  _param_constant257 = None\n",
      "        view_default_690: f32[s0, 4096] = torch.ops.aten.view.default(view_default_689, [sym_size_114, 4096]);  view_default_689 = None\n",
      "        mm_default_199: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_690, t_default_199);  view_default_690 = t_default_199 = None\n",
      "        view_default_691: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_199, [1, sym_size_114, 4096]);  mm_default_199 = sym_size_114 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_202: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_197, view_default_691);  add_tensor_197 = view_default_691 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_57: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_202, 2)\n",
      "        mean_dim_57: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_57, [-1], True);  pow_tensor_scalar_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_203: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_57, 1e-06);  mean_dim_57 = None\n",
      "        rsqrt_default_57: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_203);  add_tensor_203 = None\n",
      "        detach_default_86: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_57)\n",
      "        mul_tensor_258: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_202, rsqrt_default_57);  rsqrt_default_57 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant258 = self._param_constant258\n",
      "        mul_tensor_259: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant258, mul_tensor_258);  _param_constant258 = mul_tensor_258 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant259 = self._param_constant259\n",
      "        t_default_200: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant259);  _param_constant259 = None\n",
      "        view_default_692: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_259, [sym_size, 4096])\n",
      "        mm_default_200: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_692, t_default_200);  view_default_692 = t_default_200 = None\n",
      "        view_default_693: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_200, [1, sym_size, 11008]);  mm_default_200 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_28: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_693)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant260 = self._param_constant260\n",
      "        t_default_201: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant260);  _param_constant260 = None\n",
      "        view_default_694: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_259, [sym_size, 4096]);  mul_tensor_259 = None\n",
      "        mm_default_201: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_694, t_default_201);  view_default_694 = t_default_201 = None\n",
      "        view_default_695: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_201, [1, sym_size, 11008]);  mm_default_201 = None\n",
      "        mul_tensor_260: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_28, view_default_695);  silu_default_28 = view_default_695 = None\n",
      "        _param_constant261 = self._param_constant261\n",
      "        t_default_202: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant261);  _param_constant261 = None\n",
      "        sym_size_116: Sym(s0) = torch.ops.aten.sym_size(view_default_693, 1);  view_default_693 = None\n",
      "        view_default_696: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_260, [sym_size_116, 11008]);  mul_tensor_260 = None\n",
      "        mm_default_202: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_696, t_default_202);  view_default_696 = t_default_202 = None\n",
      "        view_default_697: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_202, [1, sym_size_116, 4096]);  mm_default_202 = sym_size_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_204: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_202, view_default_697);  add_tensor_202 = view_default_697 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_58: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_204, 2)\n",
      "        mean_dim_58: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_58, [-1], True);  pow_tensor_scalar_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_205: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_58, 1e-06);  mean_dim_58 = None\n",
      "        rsqrt_default_58: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_205);  add_tensor_205 = None\n",
      "        detach_default_87: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_58)\n",
      "        mul_tensor_261: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_204, rsqrt_default_58);  rsqrt_default_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant262 = self._param_constant262\n",
      "        mul_tensor_262: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant262, mul_tensor_261);  _param_constant262 = mul_tensor_261 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant263 = self._param_constant263\n",
      "        t_default_203: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant263);  _param_constant263 = None\n",
      "        view_default_698: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_262, [sym_size, 4096])\n",
      "        mm_default_203: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_698, t_default_203);  view_default_698 = t_default_203 = None\n",
      "        view_default_699: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_203, [1, sym_size, 4096]);  mm_default_203 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant264 = self._param_constant264\n",
      "        t_default_204: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant264);  _param_constant264 = None\n",
      "        view_default_700: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_262, [sym_size, 4096])\n",
      "        mm_default_204: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_700, t_default_204);  view_default_700 = t_default_204 = None\n",
      "        view_default_701: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_204, [1, sym_size, 4096]);  mm_default_204 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant265 = self._param_constant265\n",
      "        t_default_205: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant265);  _param_constant265 = None\n",
      "        view_default_702: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_262, [sym_size, 4096]);  mul_tensor_262 = None\n",
      "        mm_default_205: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_702, t_default_205);  view_default_702 = t_default_205 = None\n",
      "        view_default_703: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_205, [1, sym_size, 4096]);  mm_default_205 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_704: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_699, [1, sym_size, 32, 128])\n",
      "        transpose_int_145: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_704, 1, 2);  view_default_704 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_705: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_701, [1, sym_size, 32, 128])\n",
      "        transpose_int_146: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_705, 1, 2);  view_default_705 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_706: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_703, [1, sym_size, 32, 128])\n",
      "        transpose_int_147: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_706, 1, 2);  view_default_706 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant58 = self._tensor_constant58\n",
      "        slice_tensor_294: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant58, 0, 0, 9223372036854775807);  _tensor_constant58 = None\n",
      "        slice_tensor_295: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_294, 1, 0, 9223372036854775807);  slice_tensor_294 = None\n",
      "        sym_size_117: Sym(s0) = torch.ops.aten.sym_size(view_default_701, 1);  view_default_701 = None\n",
      "        slice_tensor_296: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_295, 2, 0, sym_size_117);  slice_tensor_295 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant59 = self._tensor_constant59\n",
      "        slice_tensor_297: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant59, 0, 0, 9223372036854775807);  _tensor_constant59 = None\n",
      "        slice_tensor_298: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_297, 1, 0, 9223372036854775807);  slice_tensor_297 = None\n",
      "        slice_tensor_299: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_298, 2, 0, sym_size_117);  slice_tensor_298 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_116: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_296, 1);  slice_tensor_296 = None\n",
      "        squeeze_dim_117: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_116, 0);  squeeze_dim_116 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_118: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_299, 1);  slice_tensor_299 = None\n",
      "        squeeze_dim_119: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_118, 0);  squeeze_dim_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_58: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_117, [view_default]);  squeeze_dim_117 = None\n",
      "        unsqueeze_default_63: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_58, 1);  index_tensor_58 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_59: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_119, [view_default]);  squeeze_dim_119 = None\n",
      "        unsqueeze_default_64: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_59, 1);  index_tensor_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_263: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_145, unsqueeze_default_63)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_300: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_301: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 64, 9223372036854775807);  transpose_int_145 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_58: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_301);  slice_tensor_301 = None\n",
      "        cat_default_58: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_58, slice_tensor_300], -1);  neg_default_58 = slice_tensor_300 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_264: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_58, unsqueeze_default_64);  cat_default_58 = None\n",
      "        add_tensor_206: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_263, mul_tensor_264);  mul_tensor_263 = mul_tensor_264 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_265: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_146, unsqueeze_default_63);  unsqueeze_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_302: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_303: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 64, 9223372036854775807);  transpose_int_146 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_59: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_303);  slice_tensor_303 = None\n",
      "        cat_default_59: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_59, slice_tensor_302], -1);  neg_default_59 = slice_tensor_302 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_266: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_59, unsqueeze_default_64);  cat_default_59 = unsqueeze_default_64 = None\n",
      "        add_tensor_207: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_265, mul_tensor_266);  mul_tensor_265 = mul_tensor_266 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_148: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_207, 2, 3)\n",
      "        sym_size_118: Sym(s0) = torch.ops.aten.sym_size(view_default_699, 1);  view_default_699 = None\n",
      "        expand_default_118: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_206, [1, 32, sym_size_118, 128]);  add_tensor_206 = None\n",
      "        view_default_707: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_118, [32, sym_size_118, 128]);  expand_default_118 = None\n",
      "        expand_default_119: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_148, [1, 32, 128, sym_size_117]);  transpose_int_148 = None\n",
      "        view_default_708: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_119, [32, 128, sym_size_117]);  expand_default_119 = None\n",
      "        bmm_default_58: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_707, view_default_708);  view_default_707 = view_default_708 = None\n",
      "        view_default_709: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_58, [1, 32, sym_size_118, sym_size_117]);  bmm_default_58 = None\n",
      "        div_tensor_29: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_709, 11.313708498984761);  view_default_709 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_208: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_29, add_tensor_1);  div_tensor_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_29: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_208, -1, False);  add_tensor_208 = None\n",
      "        detach_default_88: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_29)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_120: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_29, [1, 32, sym_size_118, sym_size_117]);  _softmax_default_29 = None\n",
      "        view_default_710: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_120, [32, sym_size_118, sym_size_117]);  expand_default_120 = sym_size_117 = None\n",
      "        sym_size_119: Sym(s0) = torch.ops.aten.sym_size(view_default_703, 1);  view_default_703 = None\n",
      "        expand_default_121: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_147, [1, 32, sym_size_119, 128])\n",
      "        view_default_711: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_121, [32, sym_size_119, 128]);  expand_default_121 = sym_size_119 = None\n",
      "        bmm_default_59: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_710, view_default_711);  view_default_710 = view_default_711 = None\n",
      "        view_default_712: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_59, [1, 32, sym_size_118, 128]);  bmm_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_149: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_712, 1, 2);  view_default_712 = None\n",
      "        clone_default_29: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_149, memory_format = torch.contiguous_format);  transpose_int_149 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_713: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_29, [1, sym_size, 4096]);  clone_default_29 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant266 = self._param_constant266\n",
      "        t_default_206: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant266);  _param_constant266 = None\n",
      "        view_default_714: f32[s0, 4096] = torch.ops.aten.view.default(view_default_713, [sym_size_118, 4096]);  view_default_713 = None\n",
      "        mm_default_206: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_714, t_default_206);  view_default_714 = t_default_206 = None\n",
      "        view_default_715: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_206, [1, sym_size_118, 4096]);  mm_default_206 = sym_size_118 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_209: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_204, view_default_715);  add_tensor_204 = view_default_715 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_59: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_209, 2)\n",
      "        mean_dim_59: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_59, [-1], True);  pow_tensor_scalar_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_210: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_59, 1e-06);  mean_dim_59 = None\n",
      "        rsqrt_default_59: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_210);  add_tensor_210 = None\n",
      "        detach_default_89: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_59)\n",
      "        mul_tensor_267: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_209, rsqrt_default_59);  rsqrt_default_59 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant267 = self._param_constant267\n",
      "        mul_tensor_268: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant267, mul_tensor_267);  _param_constant267 = mul_tensor_267 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant268 = self._param_constant268\n",
      "        t_default_207: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant268);  _param_constant268 = None\n",
      "        view_default_716: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_268, [sym_size, 4096])\n",
      "        mm_default_207: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_716, t_default_207);  view_default_716 = t_default_207 = None\n",
      "        view_default_717: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_207, [1, sym_size, 11008]);  mm_default_207 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_29: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_717)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant269 = self._param_constant269\n",
      "        t_default_208: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant269);  _param_constant269 = None\n",
      "        view_default_718: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_268, [sym_size, 4096]);  mul_tensor_268 = None\n",
      "        mm_default_208: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_718, t_default_208);  view_default_718 = t_default_208 = None\n",
      "        view_default_719: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_208, [1, sym_size, 11008]);  mm_default_208 = None\n",
      "        mul_tensor_269: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_29, view_default_719);  silu_default_29 = view_default_719 = None\n",
      "        _param_constant270 = self._param_constant270\n",
      "        t_default_209: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant270);  _param_constant270 = None\n",
      "        sym_size_120: Sym(s0) = torch.ops.aten.sym_size(view_default_717, 1);  view_default_717 = None\n",
      "        view_default_720: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_269, [sym_size_120, 11008]);  mul_tensor_269 = None\n",
      "        mm_default_209: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_720, t_default_209);  view_default_720 = t_default_209 = None\n",
      "        view_default_721: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_209, [1, sym_size_120, 4096]);  mm_default_209 = sym_size_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_211: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_209, view_default_721);  add_tensor_209 = view_default_721 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_60: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_211, 2)\n",
      "        mean_dim_60: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_60, [-1], True);  pow_tensor_scalar_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_212: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_60, 1e-06);  mean_dim_60 = None\n",
      "        rsqrt_default_60: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_212);  add_tensor_212 = None\n",
      "        detach_default_90: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_60)\n",
      "        mul_tensor_270: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_211, rsqrt_default_60);  rsqrt_default_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant271 = self._param_constant271\n",
      "        mul_tensor_271: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant271, mul_tensor_270);  _param_constant271 = mul_tensor_270 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant272 = self._param_constant272\n",
      "        t_default_210: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant272);  _param_constant272 = None\n",
      "        view_default_722: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_271, [sym_size, 4096])\n",
      "        mm_default_210: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_722, t_default_210);  view_default_722 = t_default_210 = None\n",
      "        view_default_723: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_210, [1, sym_size, 4096]);  mm_default_210 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant273 = self._param_constant273\n",
      "        t_default_211: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant273);  _param_constant273 = None\n",
      "        view_default_724: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_271, [sym_size, 4096])\n",
      "        mm_default_211: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_724, t_default_211);  view_default_724 = t_default_211 = None\n",
      "        view_default_725: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_211, [1, sym_size, 4096]);  mm_default_211 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant274 = self._param_constant274\n",
      "        t_default_212: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant274);  _param_constant274 = None\n",
      "        view_default_726: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_271, [sym_size, 4096]);  mul_tensor_271 = None\n",
      "        mm_default_212: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_726, t_default_212);  view_default_726 = t_default_212 = None\n",
      "        view_default_727: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_212, [1, sym_size, 4096]);  mm_default_212 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_728: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_723, [1, sym_size, 32, 128])\n",
      "        transpose_int_150: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_728, 1, 2);  view_default_728 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_729: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_725, [1, sym_size, 32, 128])\n",
      "        transpose_int_151: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_729, 1, 2);  view_default_729 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_730: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_727, [1, sym_size, 32, 128])\n",
      "        transpose_int_152: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_730, 1, 2);  view_default_730 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant60 = self._tensor_constant60\n",
      "        slice_tensor_304: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant60, 0, 0, 9223372036854775807);  _tensor_constant60 = None\n",
      "        slice_tensor_305: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_304, 1, 0, 9223372036854775807);  slice_tensor_304 = None\n",
      "        sym_size_121: Sym(s0) = torch.ops.aten.sym_size(view_default_725, 1);  view_default_725 = None\n",
      "        slice_tensor_306: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_305, 2, 0, sym_size_121);  slice_tensor_305 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant61 = self._tensor_constant61\n",
      "        slice_tensor_307: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant61, 0, 0, 9223372036854775807);  _tensor_constant61 = None\n",
      "        slice_tensor_308: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_307, 1, 0, 9223372036854775807);  slice_tensor_307 = None\n",
      "        slice_tensor_309: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_308, 2, 0, sym_size_121);  slice_tensor_308 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_120: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_306, 1);  slice_tensor_306 = None\n",
      "        squeeze_dim_121: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_120, 0);  squeeze_dim_120 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_122: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_309, 1);  slice_tensor_309 = None\n",
      "        squeeze_dim_123: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_122, 0);  squeeze_dim_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_60: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_121, [view_default]);  squeeze_dim_121 = None\n",
      "        unsqueeze_default_65: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_60, 1);  index_tensor_60 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_61: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_123, [view_default]);  squeeze_dim_123 = None\n",
      "        unsqueeze_default_66: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_61, 1);  index_tensor_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_272: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_150, unsqueeze_default_65)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_310: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_311: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 64, 9223372036854775807);  transpose_int_150 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_60: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_311);  slice_tensor_311 = None\n",
      "        cat_default_60: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_60, slice_tensor_310], -1);  neg_default_60 = slice_tensor_310 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_273: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_60, unsqueeze_default_66);  cat_default_60 = None\n",
      "        add_tensor_213: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_272, mul_tensor_273);  mul_tensor_272 = mul_tensor_273 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_274: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_151, unsqueeze_default_65);  unsqueeze_default_65 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_312: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_313: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 64, 9223372036854775807);  transpose_int_151 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_61: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_313);  slice_tensor_313 = None\n",
      "        cat_default_61: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_61, slice_tensor_312], -1);  neg_default_61 = slice_tensor_312 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_275: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_61, unsqueeze_default_66);  cat_default_61 = unsqueeze_default_66 = None\n",
      "        add_tensor_214: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_274, mul_tensor_275);  mul_tensor_274 = mul_tensor_275 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_153: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_214, 2, 3)\n",
      "        sym_size_122: Sym(s0) = torch.ops.aten.sym_size(view_default_723, 1);  view_default_723 = None\n",
      "        expand_default_122: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_213, [1, 32, sym_size_122, 128]);  add_tensor_213 = None\n",
      "        view_default_731: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_122, [32, sym_size_122, 128]);  expand_default_122 = None\n",
      "        expand_default_123: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_153, [1, 32, 128, sym_size_121]);  transpose_int_153 = None\n",
      "        view_default_732: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_123, [32, 128, sym_size_121]);  expand_default_123 = None\n",
      "        bmm_default_60: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_731, view_default_732);  view_default_731 = view_default_732 = None\n",
      "        view_default_733: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_60, [1, 32, sym_size_122, sym_size_121]);  bmm_default_60 = None\n",
      "        div_tensor_30: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_733, 11.313708498984761);  view_default_733 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_215: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_30, add_tensor_1);  div_tensor_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_30: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_215, -1, False);  add_tensor_215 = None\n",
      "        detach_default_91: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_30)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_124: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_30, [1, 32, sym_size_122, sym_size_121]);  _softmax_default_30 = None\n",
      "        view_default_734: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_124, [32, sym_size_122, sym_size_121]);  expand_default_124 = sym_size_121 = None\n",
      "        sym_size_123: Sym(s0) = torch.ops.aten.sym_size(view_default_727, 1);  view_default_727 = None\n",
      "        expand_default_125: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_152, [1, 32, sym_size_123, 128])\n",
      "        view_default_735: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_125, [32, sym_size_123, 128]);  expand_default_125 = sym_size_123 = None\n",
      "        bmm_default_61: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_734, view_default_735);  view_default_734 = view_default_735 = None\n",
      "        view_default_736: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_61, [1, 32, sym_size_122, 128]);  bmm_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_154: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_736, 1, 2);  view_default_736 = None\n",
      "        clone_default_30: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_154, memory_format = torch.contiguous_format);  transpose_int_154 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_737: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_30, [1, sym_size, 4096]);  clone_default_30 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant275 = self._param_constant275\n",
      "        t_default_213: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant275);  _param_constant275 = None\n",
      "        view_default_738: f32[s0, 4096] = torch.ops.aten.view.default(view_default_737, [sym_size_122, 4096]);  view_default_737 = None\n",
      "        mm_default_213: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_738, t_default_213);  view_default_738 = t_default_213 = None\n",
      "        view_default_739: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_213, [1, sym_size_122, 4096]);  mm_default_213 = sym_size_122 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_216: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_211, view_default_739);  add_tensor_211 = view_default_739 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_61: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_216, 2)\n",
      "        mean_dim_61: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_61, [-1], True);  pow_tensor_scalar_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_217: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_61, 1e-06);  mean_dim_61 = None\n",
      "        rsqrt_default_61: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_217);  add_tensor_217 = None\n",
      "        detach_default_92: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_61)\n",
      "        mul_tensor_276: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_216, rsqrt_default_61);  rsqrt_default_61 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant276 = self._param_constant276\n",
      "        mul_tensor_277: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant276, mul_tensor_276);  _param_constant276 = mul_tensor_276 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant277 = self._param_constant277\n",
      "        t_default_214: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant277);  _param_constant277 = None\n",
      "        view_default_740: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_277, [sym_size, 4096])\n",
      "        mm_default_214: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_740, t_default_214);  view_default_740 = t_default_214 = None\n",
      "        view_default_741: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_214, [1, sym_size, 11008]);  mm_default_214 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_30: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_741)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant278 = self._param_constant278\n",
      "        t_default_215: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant278);  _param_constant278 = None\n",
      "        view_default_742: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_277, [sym_size, 4096]);  mul_tensor_277 = None\n",
      "        mm_default_215: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_742, t_default_215);  view_default_742 = t_default_215 = None\n",
      "        view_default_743: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_215, [1, sym_size, 11008]);  mm_default_215 = None\n",
      "        mul_tensor_278: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_30, view_default_743);  silu_default_30 = view_default_743 = None\n",
      "        _param_constant279 = self._param_constant279\n",
      "        t_default_216: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant279);  _param_constant279 = None\n",
      "        sym_size_124: Sym(s0) = torch.ops.aten.sym_size(view_default_741, 1);  view_default_741 = None\n",
      "        view_default_744: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_278, [sym_size_124, 11008]);  mul_tensor_278 = None\n",
      "        mm_default_216: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_744, t_default_216);  view_default_744 = t_default_216 = None\n",
      "        view_default_745: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_216, [1, sym_size_124, 4096]);  mm_default_216 = sym_size_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_218: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_216, view_default_745);  add_tensor_216 = view_default_745 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_62: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_218, 2)\n",
      "        mean_dim_62: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_62, [-1], True);  pow_tensor_scalar_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_219: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_62, 1e-06);  mean_dim_62 = None\n",
      "        rsqrt_default_62: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_219);  add_tensor_219 = None\n",
      "        detach_default_93: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_62)\n",
      "        mul_tensor_279: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_218, rsqrt_default_62);  rsqrt_default_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant280 = self._param_constant280\n",
      "        mul_tensor_280: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant280, mul_tensor_279);  _param_constant280 = mul_tensor_279 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\n",
      "        _param_constant281 = self._param_constant281\n",
      "        t_default_217: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant281);  _param_constant281 = None\n",
      "        view_default_746: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_280, [sym_size, 4096])\n",
      "        mm_default_217: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_746, t_default_217);  view_default_746 = t_default_217 = None\n",
      "        view_default_747: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_217, [1, sym_size, 4096]);  mm_default_217 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\n",
      "        _param_constant282 = self._param_constant282\n",
      "        t_default_218: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant282);  _param_constant282 = None\n",
      "        view_default_748: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_280, [sym_size, 4096])\n",
      "        mm_default_218: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_748, t_default_218);  view_default_748 = t_default_218 = None\n",
      "        view_default_749: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_218, [1, sym_size, 4096]);  mm_default_218 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\n",
      "        _param_constant283 = self._param_constant283\n",
      "        t_default_219: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant283);  _param_constant283 = None\n",
      "        view_default_750: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_280, [sym_size, 4096]);  mul_tensor_280 = None\n",
      "        mm_default_219: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_750, t_default_219);  view_default_750 = t_default_219 = None\n",
      "        view_default_751: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_219, [1, sym_size, 4096]);  mm_default_219 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_752: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_747, [1, sym_size, 32, 128])\n",
      "        transpose_int_155: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_752, 1, 2);  view_default_752 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_753: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_749, [1, sym_size, 32, 128])\n",
      "        transpose_int_156: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_753, 1, 2);  view_default_753 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        view_default_754: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_751, [1, sym_size, 32, 128])\n",
      "        transpose_int_157: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_754, 1, 2);  view_default_754 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant62 = self._tensor_constant62\n",
      "        slice_tensor_314: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant62, 0, 0, 9223372036854775807);  _tensor_constant62 = None\n",
      "        slice_tensor_315: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_314, 1, 0, 9223372036854775807);  slice_tensor_314 = None\n",
      "        sym_size_125: Sym(s0) = torch.ops.aten.sym_size(view_default_749, 1);  view_default_749 = None\n",
      "        slice_tensor_316: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_315, 2, 0, sym_size_125);  slice_tensor_315 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        _tensor_constant63 = self._tensor_constant63\n",
      "        slice_tensor_317: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant63, 0, 0, 9223372036854775807);  _tensor_constant63 = None\n",
      "        slice_tensor_318: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_317, 1, 0, 9223372036854775807);  slice_tensor_317 = None\n",
      "        slice_tensor_319: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_318, 2, 0, sym_size_125);  slice_tensor_318 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_124: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_316, 1);  slice_tensor_316 = None\n",
      "        squeeze_dim_125: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_124, 0);  squeeze_dim_124 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
      "        squeeze_dim_126: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_319, 1);  slice_tensor_319 = None\n",
      "        squeeze_dim_127: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_126, 0);  squeeze_dim_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_62: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_125, [view_default]);  squeeze_dim_125 = None\n",
      "        unsqueeze_default_67: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_62, 1);  index_tensor_62 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
      "        index_tensor_63: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_127, [view_default]);  squeeze_dim_127 = view_default = None\n",
      "        unsqueeze_default_68: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_63, 1);  index_tensor_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_281: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_155, unsqueeze_default_67)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_320: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_321: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 64, 9223372036854775807);  transpose_int_155 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_62: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_321);  slice_tensor_321 = None\n",
      "        cat_default_62: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_62, slice_tensor_320], -1);  neg_default_62 = slice_tensor_320 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n",
      "        mul_tensor_282: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_62, unsqueeze_default_68);  cat_default_62 = None\n",
      "        add_tensor_220: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_281, mul_tensor_282);  mul_tensor_281 = mul_tensor_282 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_283: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_156, unsqueeze_default_67);  unsqueeze_default_67 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\n",
      "        slice_tensor_322: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 0, 64)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\n",
      "        slice_tensor_323: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 64, 9223372036854775807);  transpose_int_156 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\n",
      "        neg_default_63: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_323);  slice_tensor_323 = None\n",
      "        cat_default_63: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_63, slice_tensor_322], -1);  neg_default_63 = slice_tensor_322 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n",
      "        mul_tensor_284: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_63, unsqueeze_default_68);  cat_default_63 = unsqueeze_default_68 = None\n",
      "        add_tensor_221: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_283, mul_tensor_284);  mul_tensor_283 = mul_tensor_284 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "        transpose_int_158: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_221, 2, 3)\n",
      "        sym_size_126: Sym(s0) = torch.ops.aten.sym_size(view_default_747, 1);  view_default_747 = None\n",
      "        expand_default_126: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_220, [1, 32, sym_size_126, 128]);  add_tensor_220 = None\n",
      "        view_default_755: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_126, [32, sym_size_126, 128]);  expand_default_126 = None\n",
      "        expand_default_127: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_158, [1, 32, 128, sym_size_125]);  transpose_int_158 = None\n",
      "        view_default_756: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_127, [32, 128, sym_size_125]);  expand_default_127 = None\n",
      "        bmm_default_62: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_755, view_default_756);  view_default_755 = view_default_756 = None\n",
      "        view_default_757: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_62, [1, 32, sym_size_126, sym_size_125]);  bmm_default_62 = None\n",
      "        div_tensor_31: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_757, 11.313708498984761);  view_default_757 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\n",
      "        add_tensor_222: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_31, add_tensor_1);  div_tensor_31 = add_tensor_1 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        _softmax_default_31: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_222, -1, False);  add_tensor_222 = None\n",
      "        detach_default_94: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_31)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "        expand_default_128: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_31, [1, 32, sym_size_126, sym_size_125]);  _softmax_default_31 = None\n",
      "        view_default_758: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_128, [32, sym_size_126, sym_size_125]);  expand_default_128 = sym_size_125 = None\n",
      "        sym_size_127: Sym(s0) = torch.ops.aten.sym_size(view_default_751, 1);  view_default_751 = None\n",
      "        expand_default_129: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_157, [1, 32, sym_size_127, 128])\n",
      "        view_default_759: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_129, [32, sym_size_127, 128]);  expand_default_129 = sym_size_127 = None\n",
      "        bmm_default_63: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_758, view_default_759);  view_default_758 = view_default_759 = None\n",
      "        view_default_760: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_63, [1, 32, sym_size_126, 128]);  bmm_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        transpose_int_159: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_760, 1, 2);  view_default_760 = None\n",
      "        clone_default_31: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_159, memory_format = torch.contiguous_format);  transpose_int_159 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "        view_default_761: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_31, [1, sym_size, 4096]);  clone_default_31 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\n",
      "        _param_constant284 = self._param_constant284\n",
      "        t_default_220: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant284);  _param_constant284 = None\n",
      "        view_default_762: f32[s0, 4096] = torch.ops.aten.view.default(view_default_761, [sym_size_126, 4096]);  view_default_761 = None\n",
      "        mm_default_220: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_762, t_default_220);  view_default_762 = t_default_220 = None\n",
      "        view_default_763: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_220, [1, sym_size_126, 4096]);  mm_default_220 = sym_size_126 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_223: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_218, view_default_763);  add_tensor_218 = view_default_763 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_63: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_223, 2)\n",
      "        mean_dim_63: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_63, [-1], True);  pow_tensor_scalar_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_224: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_63, 1e-06);  mean_dim_63 = None\n",
      "        rsqrt_default_63: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_224);  add_tensor_224 = None\n",
      "        detach_default_95: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_63)\n",
      "        mul_tensor_285: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_223, rsqrt_default_63);  rsqrt_default_63 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant285 = self._param_constant285\n",
      "        mul_tensor_286: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant285, mul_tensor_285);  _param_constant285 = mul_tensor_285 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant286 = self._param_constant286\n",
      "        t_default_221: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant286);  _param_constant286 = None\n",
      "        view_default_764: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_286, [sym_size, 4096])\n",
      "        mm_default_221: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_764, t_default_221);  view_default_764 = t_default_221 = None\n",
      "        view_default_765: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_221, [1, sym_size, 11008]);  mm_default_221 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\n",
      "        silu_default_31: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_765)\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        _param_constant287 = self._param_constant287\n",
      "        t_default_222: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant287);  _param_constant287 = None\n",
      "        view_default_766: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_286, [sym_size, 4096]);  mul_tensor_286 = None\n",
      "        mm_default_222: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_766, t_default_222);  view_default_766 = t_default_222 = None\n",
      "        view_default_767: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_222, [1, sym_size, 11008]);  mm_default_222 = None\n",
      "        mul_tensor_287: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_31, view_default_767);  silu_default_31 = view_default_767 = None\n",
      "        _param_constant288 = self._param_constant288\n",
      "        t_default_223: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant288);  _param_constant288 = None\n",
      "        sym_size_128: Sym(s0) = torch.ops.aten.sym_size(view_default_765, 1);  view_default_765 = None\n",
      "        view_default_768: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_287, [sym_size_128, 11008]);  mul_tensor_287 = None\n",
      "        mm_default_223: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_768, t_default_223);  view_default_768 = t_default_223 = None\n",
      "        view_default_769: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_223, [1, sym_size_128, 4096]);  mm_default_223 = sym_size_128 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\n",
      "        add_tensor_225: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_223, view_default_769);  add_tensor_223 = view_default_769 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        pow_tensor_scalar_64: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_225, 2)\n",
      "        mean_dim_64: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_64, [-1], True);  pow_tensor_scalar_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        add_tensor_226: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_64, 1e-06);  mean_dim_64 = None\n",
      "        rsqrt_default_64: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_226);  add_tensor_226 = None\n",
      "        detach_default_96: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_64)\n",
      "        mul_tensor_288: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_225, rsqrt_default_64);  add_tensor_225 = rsqrt_default_64 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\n",
      "        _param_constant289 = self._param_constant289\n",
      "        mul_tensor_289: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant289, mul_tensor_288);  _param_constant289 = mul_tensor_288 = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:827, code: logits = self.lm_head(hidden_states)\n",
      "        _param_constant290 = self._param_constant290\n",
      "        t_default_224: f32[4096, 32000] = torch.ops.aten.t.default(_param_constant290);  _param_constant290 = None\n",
      "        view_default_770: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_289, [sym_size, 4096]);  mul_tensor_289 = None\n",
      "        mm_default_224: f32[s0, 32000] = torch.ops.aten.mm.default(view_default_770, t_default_224);  view_default_770 = t_default_224 = None\n",
      "        view_default_771: f32[1, s0, 32000] = torch.ops.aten.view.default(mm_default_224, [1, sym_size, 32000]);  mm_default_224 = sym_size = None\n",
      "        \n",
      "        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/torch/_export/constraints.py:13, code: torch.sym_constrain_range(symbol, min=min, max=max)\n",
      "        sym_constrain_range_default_1 = torch.ops.aten.sym_constrain_range.default(sym_size_1)\n",
      "        \n",
      "        # File: /tmp/ipykernel_881114/4291870228.py:46, code: state[:, :, 0:seq_length, :] = update[:, :, :, :]\n",
      "        slice_tensor_324: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_4, 0, 0, 9223372036854775807);  add_tensor_4 = None\n",
      "        slice_tensor_325: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_324, 1, 0, 9223372036854775807);  slice_tensor_324 = None\n",
      "        slice_tensor_326: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_325, 2, 0, 9223372036854775807);  slice_tensor_325 = None\n",
      "        slice_tensor_327: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_326, 3, 0, 9223372036854775807);  slice_tensor_326 = None\n",
      "        _tensor_constant64 = self._tensor_constant64\n",
      "        slice_tensor_328: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant64, 0, 0, 9223372036854775807);  _tensor_constant64 = None\n",
      "        slice_tensor_329: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_328, 1, 0, 9223372036854775807);  slice_tensor_328 = None\n",
      "        slice_tensor_330: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_329, 2, 0, sym_size_1);  slice_tensor_329 = None\n",
      "        slice_tensor_331: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_330, 3, 0, 9223372036854775807);  slice_tensor_330 = None\n",
      "        copy__default: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_331, slice_tensor_327);  slice_tensor_331 = slice_tensor_327 = None\n",
      "        slice_tensor_332: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_2, 0, 0, 9223372036854775807);  transpose_int_2 = None\n",
      "        slice_tensor_333: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_332, 1, 0, 9223372036854775807);  slice_tensor_332 = None\n",
      "        slice_tensor_334: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_333, 2, 0, 9223372036854775807);  slice_tensor_333 = None\n",
      "        slice_tensor_335: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_334, 3, 0, 9223372036854775807);  slice_tensor_334 = None\n",
      "        _tensor_constant65 = self._tensor_constant65\n",
      "        slice_tensor_336: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant65, 0, 0, 9223372036854775807);  _tensor_constant65 = None\n",
      "        slice_tensor_337: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_336, 1, 0, 9223372036854775807);  slice_tensor_336 = None\n",
      "        slice_tensor_338: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_337, 2, 0, sym_size_1);  slice_tensor_337 = None\n",
      "        slice_tensor_339: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_338, 3, 0, 9223372036854775807);  slice_tensor_338 = None\n",
      "        copy__default_1: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_339, slice_tensor_335);  slice_tensor_339 = slice_tensor_335 = None\n",
      "        slice_tensor_340: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_11, 0, 0, 9223372036854775807);  add_tensor_11 = None\n",
      "        slice_tensor_341: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_340, 1, 0, 9223372036854775807);  slice_tensor_340 = None\n",
      "        slice_tensor_342: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_341, 2, 0, 9223372036854775807);  slice_tensor_341 = None\n",
      "        slice_tensor_343: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_342, 3, 0, 9223372036854775807);  slice_tensor_342 = None\n",
      "        _tensor_constant66 = self._tensor_constant66\n",
      "        slice_tensor_344: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant66, 0, 0, 9223372036854775807);  _tensor_constant66 = None\n",
      "        slice_tensor_345: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_344, 1, 0, 9223372036854775807);  slice_tensor_344 = None\n",
      "        slice_tensor_346: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_345, 2, 0, sym_size_1);  slice_tensor_345 = None\n",
      "        slice_tensor_347: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_346, 3, 0, 9223372036854775807);  slice_tensor_346 = None\n",
      "        copy__default_2: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_347, slice_tensor_343);  slice_tensor_347 = slice_tensor_343 = None\n",
      "        slice_tensor_348: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_7, 0, 0, 9223372036854775807);  transpose_int_7 = None\n",
      "        slice_tensor_349: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_348, 1, 0, 9223372036854775807);  slice_tensor_348 = None\n",
      "        slice_tensor_350: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_349, 2, 0, 9223372036854775807);  slice_tensor_349 = None\n",
      "        slice_tensor_351: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_350, 3, 0, 9223372036854775807);  slice_tensor_350 = None\n",
      "        _tensor_constant67 = self._tensor_constant67\n",
      "        slice_tensor_352: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant67, 0, 0, 9223372036854775807);  _tensor_constant67 = None\n",
      "        slice_tensor_353: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_352, 1, 0, 9223372036854775807);  slice_tensor_352 = None\n",
      "        slice_tensor_354: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_353, 2, 0, sym_size_1);  slice_tensor_353 = None\n",
      "        slice_tensor_355: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_354, 3, 0, 9223372036854775807);  slice_tensor_354 = None\n",
      "        copy__default_3: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_355, slice_tensor_351);  slice_tensor_355 = slice_tensor_351 = None\n",
      "        slice_tensor_356: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_18, 0, 0, 9223372036854775807);  add_tensor_18 = None\n",
      "        slice_tensor_357: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_356, 1, 0, 9223372036854775807);  slice_tensor_356 = None\n",
      "        slice_tensor_358: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_357, 2, 0, 9223372036854775807);  slice_tensor_357 = None\n",
      "        slice_tensor_359: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_358, 3, 0, 9223372036854775807);  slice_tensor_358 = None\n",
      "        _tensor_constant68 = self._tensor_constant68\n",
      "        slice_tensor_360: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant68, 0, 0, 9223372036854775807);  _tensor_constant68 = None\n",
      "        slice_tensor_361: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_360, 1, 0, 9223372036854775807);  slice_tensor_360 = None\n",
      "        slice_tensor_362: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_361, 2, 0, sym_size_1);  slice_tensor_361 = None\n",
      "        slice_tensor_363: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_362, 3, 0, 9223372036854775807);  slice_tensor_362 = None\n",
      "        copy__default_4: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_363, slice_tensor_359);  slice_tensor_363 = slice_tensor_359 = None\n",
      "        slice_tensor_364: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_12, 0, 0, 9223372036854775807);  transpose_int_12 = None\n",
      "        slice_tensor_365: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_364, 1, 0, 9223372036854775807);  slice_tensor_364 = None\n",
      "        slice_tensor_366: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_365, 2, 0, 9223372036854775807);  slice_tensor_365 = None\n",
      "        slice_tensor_367: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_366, 3, 0, 9223372036854775807);  slice_tensor_366 = None\n",
      "        _tensor_constant69 = self._tensor_constant69\n",
      "        slice_tensor_368: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant69, 0, 0, 9223372036854775807);  _tensor_constant69 = None\n",
      "        slice_tensor_369: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_368, 1, 0, 9223372036854775807);  slice_tensor_368 = None\n",
      "        slice_tensor_370: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_369, 2, 0, sym_size_1);  slice_tensor_369 = None\n",
      "        slice_tensor_371: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_370, 3, 0, 9223372036854775807);  slice_tensor_370 = None\n",
      "        copy__default_5: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_371, slice_tensor_367);  slice_tensor_371 = slice_tensor_367 = None\n",
      "        slice_tensor_372: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_25, 0, 0, 9223372036854775807);  add_tensor_25 = None\n",
      "        slice_tensor_373: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_372, 1, 0, 9223372036854775807);  slice_tensor_372 = None\n",
      "        slice_tensor_374: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_373, 2, 0, 9223372036854775807);  slice_tensor_373 = None\n",
      "        slice_tensor_375: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_374, 3, 0, 9223372036854775807);  slice_tensor_374 = None\n",
      "        _tensor_constant70 = self._tensor_constant70\n",
      "        slice_tensor_376: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant70, 0, 0, 9223372036854775807);  _tensor_constant70 = None\n",
      "        slice_tensor_377: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_376, 1, 0, 9223372036854775807);  slice_tensor_376 = None\n",
      "        slice_tensor_378: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_377, 2, 0, sym_size_1);  slice_tensor_377 = None\n",
      "        slice_tensor_379: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_378, 3, 0, 9223372036854775807);  slice_tensor_378 = None\n",
      "        copy__default_6: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_379, slice_tensor_375);  slice_tensor_379 = slice_tensor_375 = None\n",
      "        slice_tensor_380: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_17, 0, 0, 9223372036854775807);  transpose_int_17 = None\n",
      "        slice_tensor_381: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_380, 1, 0, 9223372036854775807);  slice_tensor_380 = None\n",
      "        slice_tensor_382: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_381, 2, 0, 9223372036854775807);  slice_tensor_381 = None\n",
      "        slice_tensor_383: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_382, 3, 0, 9223372036854775807);  slice_tensor_382 = None\n",
      "        _tensor_constant71 = self._tensor_constant71\n",
      "        slice_tensor_384: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant71, 0, 0, 9223372036854775807);  _tensor_constant71 = None\n",
      "        slice_tensor_385: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_384, 1, 0, 9223372036854775807);  slice_tensor_384 = None\n",
      "        slice_tensor_386: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_385, 2, 0, sym_size_1);  slice_tensor_385 = None\n",
      "        slice_tensor_387: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_386, 3, 0, 9223372036854775807);  slice_tensor_386 = None\n",
      "        copy__default_7: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_387, slice_tensor_383);  slice_tensor_387 = slice_tensor_383 = None\n",
      "        slice_tensor_388: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_32, 0, 0, 9223372036854775807);  add_tensor_32 = None\n",
      "        slice_tensor_389: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_388, 1, 0, 9223372036854775807);  slice_tensor_388 = None\n",
      "        slice_tensor_390: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_389, 2, 0, 9223372036854775807);  slice_tensor_389 = None\n",
      "        slice_tensor_391: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_390, 3, 0, 9223372036854775807);  slice_tensor_390 = None\n",
      "        _tensor_constant72 = self._tensor_constant72\n",
      "        slice_tensor_392: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant72, 0, 0, 9223372036854775807);  _tensor_constant72 = None\n",
      "        slice_tensor_393: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_392, 1, 0, 9223372036854775807);  slice_tensor_392 = None\n",
      "        slice_tensor_394: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_393, 2, 0, sym_size_1);  slice_tensor_393 = None\n",
      "        slice_tensor_395: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_394, 3, 0, 9223372036854775807);  slice_tensor_394 = None\n",
      "        copy__default_8: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_395, slice_tensor_391);  slice_tensor_395 = slice_tensor_391 = None\n",
      "        slice_tensor_396: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_22, 0, 0, 9223372036854775807);  transpose_int_22 = None\n",
      "        slice_tensor_397: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_396, 1, 0, 9223372036854775807);  slice_tensor_396 = None\n",
      "        slice_tensor_398: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_397, 2, 0, 9223372036854775807);  slice_tensor_397 = None\n",
      "        slice_tensor_399: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_398, 3, 0, 9223372036854775807);  slice_tensor_398 = None\n",
      "        _tensor_constant73 = self._tensor_constant73\n",
      "        slice_tensor_400: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant73, 0, 0, 9223372036854775807);  _tensor_constant73 = None\n",
      "        slice_tensor_401: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_400, 1, 0, 9223372036854775807);  slice_tensor_400 = None\n",
      "        slice_tensor_402: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_401, 2, 0, sym_size_1);  slice_tensor_401 = None\n",
      "        slice_tensor_403: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_402, 3, 0, 9223372036854775807);  slice_tensor_402 = None\n",
      "        copy__default_9: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_403, slice_tensor_399);  slice_tensor_403 = slice_tensor_399 = None\n",
      "        slice_tensor_404: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_39, 0, 0, 9223372036854775807);  add_tensor_39 = None\n",
      "        slice_tensor_405: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_404, 1, 0, 9223372036854775807);  slice_tensor_404 = None\n",
      "        slice_tensor_406: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_405, 2, 0, 9223372036854775807);  slice_tensor_405 = None\n",
      "        slice_tensor_407: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_406, 3, 0, 9223372036854775807);  slice_tensor_406 = None\n",
      "        _tensor_constant74 = self._tensor_constant74\n",
      "        slice_tensor_408: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant74, 0, 0, 9223372036854775807);  _tensor_constant74 = None\n",
      "        slice_tensor_409: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_408, 1, 0, 9223372036854775807);  slice_tensor_408 = None\n",
      "        slice_tensor_410: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_409, 2, 0, sym_size_1);  slice_tensor_409 = None\n",
      "        slice_tensor_411: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_410, 3, 0, 9223372036854775807);  slice_tensor_410 = None\n",
      "        copy__default_10: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_411, slice_tensor_407);  slice_tensor_411 = slice_tensor_407 = None\n",
      "        slice_tensor_412: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_27, 0, 0, 9223372036854775807);  transpose_int_27 = None\n",
      "        slice_tensor_413: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_412, 1, 0, 9223372036854775807);  slice_tensor_412 = None\n",
      "        slice_tensor_414: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_413, 2, 0, 9223372036854775807);  slice_tensor_413 = None\n",
      "        slice_tensor_415: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_414, 3, 0, 9223372036854775807);  slice_tensor_414 = None\n",
      "        _tensor_constant75 = self._tensor_constant75\n",
      "        slice_tensor_416: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant75, 0, 0, 9223372036854775807);  _tensor_constant75 = None\n",
      "        slice_tensor_417: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_416, 1, 0, 9223372036854775807);  slice_tensor_416 = None\n",
      "        slice_tensor_418: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_417, 2, 0, sym_size_1);  slice_tensor_417 = None\n",
      "        slice_tensor_419: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_418, 3, 0, 9223372036854775807);  slice_tensor_418 = None\n",
      "        copy__default_11: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_419, slice_tensor_415);  slice_tensor_419 = slice_tensor_415 = None\n",
      "        slice_tensor_420: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_46, 0, 0, 9223372036854775807);  add_tensor_46 = None\n",
      "        slice_tensor_421: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_420, 1, 0, 9223372036854775807);  slice_tensor_420 = None\n",
      "        slice_tensor_422: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_421, 2, 0, 9223372036854775807);  slice_tensor_421 = None\n",
      "        slice_tensor_423: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_422, 3, 0, 9223372036854775807);  slice_tensor_422 = None\n",
      "        _tensor_constant76 = self._tensor_constant76\n",
      "        slice_tensor_424: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant76, 0, 0, 9223372036854775807);  _tensor_constant76 = None\n",
      "        slice_tensor_425: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_424, 1, 0, 9223372036854775807);  slice_tensor_424 = None\n",
      "        slice_tensor_426: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_425, 2, 0, sym_size_1);  slice_tensor_425 = None\n",
      "        slice_tensor_427: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_426, 3, 0, 9223372036854775807);  slice_tensor_426 = None\n",
      "        copy__default_12: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_427, slice_tensor_423);  slice_tensor_427 = slice_tensor_423 = None\n",
      "        slice_tensor_428: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_32, 0, 0, 9223372036854775807);  transpose_int_32 = None\n",
      "        slice_tensor_429: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_428, 1, 0, 9223372036854775807);  slice_tensor_428 = None\n",
      "        slice_tensor_430: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_429, 2, 0, 9223372036854775807);  slice_tensor_429 = None\n",
      "        slice_tensor_431: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_430, 3, 0, 9223372036854775807);  slice_tensor_430 = None\n",
      "        _tensor_constant77 = self._tensor_constant77\n",
      "        slice_tensor_432: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant77, 0, 0, 9223372036854775807);  _tensor_constant77 = None\n",
      "        slice_tensor_433: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_432, 1, 0, 9223372036854775807);  slice_tensor_432 = None\n",
      "        slice_tensor_434: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_433, 2, 0, sym_size_1);  slice_tensor_433 = None\n",
      "        slice_tensor_435: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_434, 3, 0, 9223372036854775807);  slice_tensor_434 = None\n",
      "        copy__default_13: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_435, slice_tensor_431);  slice_tensor_435 = slice_tensor_431 = None\n",
      "        slice_tensor_436: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_53, 0, 0, 9223372036854775807);  add_tensor_53 = None\n",
      "        slice_tensor_437: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_436, 1, 0, 9223372036854775807);  slice_tensor_436 = None\n",
      "        slice_tensor_438: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_437, 2, 0, 9223372036854775807);  slice_tensor_437 = None\n",
      "        slice_tensor_439: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_438, 3, 0, 9223372036854775807);  slice_tensor_438 = None\n",
      "        _tensor_constant78 = self._tensor_constant78\n",
      "        slice_tensor_440: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant78, 0, 0, 9223372036854775807);  _tensor_constant78 = None\n",
      "        slice_tensor_441: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_440, 1, 0, 9223372036854775807);  slice_tensor_440 = None\n",
      "        slice_tensor_442: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_441, 2, 0, sym_size_1);  slice_tensor_441 = None\n",
      "        slice_tensor_443: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_442, 3, 0, 9223372036854775807);  slice_tensor_442 = None\n",
      "        copy__default_14: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_443, slice_tensor_439);  slice_tensor_443 = slice_tensor_439 = None\n",
      "        slice_tensor_444: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_37, 0, 0, 9223372036854775807);  transpose_int_37 = None\n",
      "        slice_tensor_445: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_444, 1, 0, 9223372036854775807);  slice_tensor_444 = None\n",
      "        slice_tensor_446: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_445, 2, 0, 9223372036854775807);  slice_tensor_445 = None\n",
      "        slice_tensor_447: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_446, 3, 0, 9223372036854775807);  slice_tensor_446 = None\n",
      "        _tensor_constant79 = self._tensor_constant79\n",
      "        slice_tensor_448: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant79, 0, 0, 9223372036854775807);  _tensor_constant79 = None\n",
      "        slice_tensor_449: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_448, 1, 0, 9223372036854775807);  slice_tensor_448 = None\n",
      "        slice_tensor_450: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_449, 2, 0, sym_size_1);  slice_tensor_449 = None\n",
      "        slice_tensor_451: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_450, 3, 0, 9223372036854775807);  slice_tensor_450 = None\n",
      "        copy__default_15: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_451, slice_tensor_447);  slice_tensor_451 = slice_tensor_447 = None\n",
      "        slice_tensor_452: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_60, 0, 0, 9223372036854775807);  add_tensor_60 = None\n",
      "        slice_tensor_453: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_452, 1, 0, 9223372036854775807);  slice_tensor_452 = None\n",
      "        slice_tensor_454: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_453, 2, 0, 9223372036854775807);  slice_tensor_453 = None\n",
      "        slice_tensor_455: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_454, 3, 0, 9223372036854775807);  slice_tensor_454 = None\n",
      "        _tensor_constant80 = self._tensor_constant80\n",
      "        slice_tensor_456: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant80, 0, 0, 9223372036854775807);  _tensor_constant80 = None\n",
      "        slice_tensor_457: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_456, 1, 0, 9223372036854775807);  slice_tensor_456 = None\n",
      "        slice_tensor_458: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_457, 2, 0, sym_size_1);  slice_tensor_457 = None\n",
      "        slice_tensor_459: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_458, 3, 0, 9223372036854775807);  slice_tensor_458 = None\n",
      "        copy__default_16: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_459, slice_tensor_455);  slice_tensor_459 = slice_tensor_455 = None\n",
      "        slice_tensor_460: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_42, 0, 0, 9223372036854775807);  transpose_int_42 = None\n",
      "        slice_tensor_461: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_460, 1, 0, 9223372036854775807);  slice_tensor_460 = None\n",
      "        slice_tensor_462: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_461, 2, 0, 9223372036854775807);  slice_tensor_461 = None\n",
      "        slice_tensor_463: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_462, 3, 0, 9223372036854775807);  slice_tensor_462 = None\n",
      "        _tensor_constant81 = self._tensor_constant81\n",
      "        slice_tensor_464: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant81, 0, 0, 9223372036854775807);  _tensor_constant81 = None\n",
      "        slice_tensor_465: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_464, 1, 0, 9223372036854775807);  slice_tensor_464 = None\n",
      "        slice_tensor_466: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_465, 2, 0, sym_size_1);  slice_tensor_465 = None\n",
      "        slice_tensor_467: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_466, 3, 0, 9223372036854775807);  slice_tensor_466 = None\n",
      "        copy__default_17: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_467, slice_tensor_463);  slice_tensor_467 = slice_tensor_463 = None\n",
      "        slice_tensor_468: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_67, 0, 0, 9223372036854775807);  add_tensor_67 = None\n",
      "        slice_tensor_469: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_468, 1, 0, 9223372036854775807);  slice_tensor_468 = None\n",
      "        slice_tensor_470: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_469, 2, 0, 9223372036854775807);  slice_tensor_469 = None\n",
      "        slice_tensor_471: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_470, 3, 0, 9223372036854775807);  slice_tensor_470 = None\n",
      "        _tensor_constant82 = self._tensor_constant82\n",
      "        slice_tensor_472: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant82, 0, 0, 9223372036854775807);  _tensor_constant82 = None\n",
      "        slice_tensor_473: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_472, 1, 0, 9223372036854775807);  slice_tensor_472 = None\n",
      "        slice_tensor_474: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_473, 2, 0, sym_size_1);  slice_tensor_473 = None\n",
      "        slice_tensor_475: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_474, 3, 0, 9223372036854775807);  slice_tensor_474 = None\n",
      "        copy__default_18: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_475, slice_tensor_471);  slice_tensor_475 = slice_tensor_471 = None\n",
      "        slice_tensor_476: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_47, 0, 0, 9223372036854775807);  transpose_int_47 = None\n",
      "        slice_tensor_477: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_476, 1, 0, 9223372036854775807);  slice_tensor_476 = None\n",
      "        slice_tensor_478: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_477, 2, 0, 9223372036854775807);  slice_tensor_477 = None\n",
      "        slice_tensor_479: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_478, 3, 0, 9223372036854775807);  slice_tensor_478 = None\n",
      "        _tensor_constant83 = self._tensor_constant83\n",
      "        slice_tensor_480: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant83, 0, 0, 9223372036854775807);  _tensor_constant83 = None\n",
      "        slice_tensor_481: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_480, 1, 0, 9223372036854775807);  slice_tensor_480 = None\n",
      "        slice_tensor_482: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_481, 2, 0, sym_size_1);  slice_tensor_481 = None\n",
      "        slice_tensor_483: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_482, 3, 0, 9223372036854775807);  slice_tensor_482 = None\n",
      "        copy__default_19: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_483, slice_tensor_479);  slice_tensor_483 = slice_tensor_479 = None\n",
      "        slice_tensor_484: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_74, 0, 0, 9223372036854775807);  add_tensor_74 = None\n",
      "        slice_tensor_485: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_484, 1, 0, 9223372036854775807);  slice_tensor_484 = None\n",
      "        slice_tensor_486: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_485, 2, 0, 9223372036854775807);  slice_tensor_485 = None\n",
      "        slice_tensor_487: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_486, 3, 0, 9223372036854775807);  slice_tensor_486 = None\n",
      "        _tensor_constant84 = self._tensor_constant84\n",
      "        slice_tensor_488: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant84, 0, 0, 9223372036854775807);  _tensor_constant84 = None\n",
      "        slice_tensor_489: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_488, 1, 0, 9223372036854775807);  slice_tensor_488 = None\n",
      "        slice_tensor_490: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_489, 2, 0, sym_size_1);  slice_tensor_489 = None\n",
      "        slice_tensor_491: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_490, 3, 0, 9223372036854775807);  slice_tensor_490 = None\n",
      "        copy__default_20: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_491, slice_tensor_487);  slice_tensor_491 = slice_tensor_487 = None\n",
      "        slice_tensor_492: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_52, 0, 0, 9223372036854775807);  transpose_int_52 = None\n",
      "        slice_tensor_493: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_492, 1, 0, 9223372036854775807);  slice_tensor_492 = None\n",
      "        slice_tensor_494: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_493, 2, 0, 9223372036854775807);  slice_tensor_493 = None\n",
      "        slice_tensor_495: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_494, 3, 0, 9223372036854775807);  slice_tensor_494 = None\n",
      "        _tensor_constant85 = self._tensor_constant85\n",
      "        slice_tensor_496: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant85, 0, 0, 9223372036854775807);  _tensor_constant85 = None\n",
      "        slice_tensor_497: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_496, 1, 0, 9223372036854775807);  slice_tensor_496 = None\n",
      "        slice_tensor_498: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_497, 2, 0, sym_size_1);  slice_tensor_497 = None\n",
      "        slice_tensor_499: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_498, 3, 0, 9223372036854775807);  slice_tensor_498 = None\n",
      "        copy__default_21: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_499, slice_tensor_495);  slice_tensor_499 = slice_tensor_495 = None\n",
      "        slice_tensor_500: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_81, 0, 0, 9223372036854775807);  add_tensor_81 = None\n",
      "        slice_tensor_501: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_500, 1, 0, 9223372036854775807);  slice_tensor_500 = None\n",
      "        slice_tensor_502: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_501, 2, 0, 9223372036854775807);  slice_tensor_501 = None\n",
      "        slice_tensor_503: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_502, 3, 0, 9223372036854775807);  slice_tensor_502 = None\n",
      "        _tensor_constant86 = self._tensor_constant86\n",
      "        slice_tensor_504: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant86, 0, 0, 9223372036854775807);  _tensor_constant86 = None\n",
      "        slice_tensor_505: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_504, 1, 0, 9223372036854775807);  slice_tensor_504 = None\n",
      "        slice_tensor_506: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_505, 2, 0, sym_size_1);  slice_tensor_505 = None\n",
      "        slice_tensor_507: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_506, 3, 0, 9223372036854775807);  slice_tensor_506 = None\n",
      "        copy__default_22: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_507, slice_tensor_503);  slice_tensor_507 = slice_tensor_503 = None\n",
      "        slice_tensor_508: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_57, 0, 0, 9223372036854775807);  transpose_int_57 = None\n",
      "        slice_tensor_509: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_508, 1, 0, 9223372036854775807);  slice_tensor_508 = None\n",
      "        slice_tensor_510: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_509, 2, 0, 9223372036854775807);  slice_tensor_509 = None\n",
      "        slice_tensor_511: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_510, 3, 0, 9223372036854775807);  slice_tensor_510 = None\n",
      "        _tensor_constant87 = self._tensor_constant87\n",
      "        slice_tensor_512: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant87, 0, 0, 9223372036854775807);  _tensor_constant87 = None\n",
      "        slice_tensor_513: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_512, 1, 0, 9223372036854775807);  slice_tensor_512 = None\n",
      "        slice_tensor_514: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_513, 2, 0, sym_size_1);  slice_tensor_513 = None\n",
      "        slice_tensor_515: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_514, 3, 0, 9223372036854775807);  slice_tensor_514 = None\n",
      "        copy__default_23: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_515, slice_tensor_511);  slice_tensor_515 = slice_tensor_511 = None\n",
      "        slice_tensor_516: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_88, 0, 0, 9223372036854775807);  add_tensor_88 = None\n",
      "        slice_tensor_517: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_516, 1, 0, 9223372036854775807);  slice_tensor_516 = None\n",
      "        slice_tensor_518: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_517, 2, 0, 9223372036854775807);  slice_tensor_517 = None\n",
      "        slice_tensor_519: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_518, 3, 0, 9223372036854775807);  slice_tensor_518 = None\n",
      "        _tensor_constant88 = self._tensor_constant88\n",
      "        slice_tensor_520: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant88, 0, 0, 9223372036854775807);  _tensor_constant88 = None\n",
      "        slice_tensor_521: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_520, 1, 0, 9223372036854775807);  slice_tensor_520 = None\n",
      "        slice_tensor_522: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_521, 2, 0, sym_size_1);  slice_tensor_521 = None\n",
      "        slice_tensor_523: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_522, 3, 0, 9223372036854775807);  slice_tensor_522 = None\n",
      "        copy__default_24: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_523, slice_tensor_519);  slice_tensor_523 = slice_tensor_519 = None\n",
      "        slice_tensor_524: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_62, 0, 0, 9223372036854775807);  transpose_int_62 = None\n",
      "        slice_tensor_525: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_524, 1, 0, 9223372036854775807);  slice_tensor_524 = None\n",
      "        slice_tensor_526: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_525, 2, 0, 9223372036854775807);  slice_tensor_525 = None\n",
      "        slice_tensor_527: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_526, 3, 0, 9223372036854775807);  slice_tensor_526 = None\n",
      "        _tensor_constant89 = self._tensor_constant89\n",
      "        slice_tensor_528: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant89, 0, 0, 9223372036854775807);  _tensor_constant89 = None\n",
      "        slice_tensor_529: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_528, 1, 0, 9223372036854775807);  slice_tensor_528 = None\n",
      "        slice_tensor_530: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_529, 2, 0, sym_size_1);  slice_tensor_529 = None\n",
      "        slice_tensor_531: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_530, 3, 0, 9223372036854775807);  slice_tensor_530 = None\n",
      "        copy__default_25: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_531, slice_tensor_527);  slice_tensor_531 = slice_tensor_527 = None\n",
      "        slice_tensor_532: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_95, 0, 0, 9223372036854775807);  add_tensor_95 = None\n",
      "        slice_tensor_533: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_532, 1, 0, 9223372036854775807);  slice_tensor_532 = None\n",
      "        slice_tensor_534: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_533, 2, 0, 9223372036854775807);  slice_tensor_533 = None\n",
      "        slice_tensor_535: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_534, 3, 0, 9223372036854775807);  slice_tensor_534 = None\n",
      "        _tensor_constant90 = self._tensor_constant90\n",
      "        slice_tensor_536: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant90, 0, 0, 9223372036854775807);  _tensor_constant90 = None\n",
      "        slice_tensor_537: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_536, 1, 0, 9223372036854775807);  slice_tensor_536 = None\n",
      "        slice_tensor_538: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_537, 2, 0, sym_size_1);  slice_tensor_537 = None\n",
      "        slice_tensor_539: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_538, 3, 0, 9223372036854775807);  slice_tensor_538 = None\n",
      "        copy__default_26: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_539, slice_tensor_535);  slice_tensor_539 = slice_tensor_535 = None\n",
      "        slice_tensor_540: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_67, 0, 0, 9223372036854775807);  transpose_int_67 = None\n",
      "        slice_tensor_541: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_540, 1, 0, 9223372036854775807);  slice_tensor_540 = None\n",
      "        slice_tensor_542: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_541, 2, 0, 9223372036854775807);  slice_tensor_541 = None\n",
      "        slice_tensor_543: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_542, 3, 0, 9223372036854775807);  slice_tensor_542 = None\n",
      "        _tensor_constant91 = self._tensor_constant91\n",
      "        slice_tensor_544: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant91, 0, 0, 9223372036854775807);  _tensor_constant91 = None\n",
      "        slice_tensor_545: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_544, 1, 0, 9223372036854775807);  slice_tensor_544 = None\n",
      "        slice_tensor_546: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_545, 2, 0, sym_size_1);  slice_tensor_545 = None\n",
      "        slice_tensor_547: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_546, 3, 0, 9223372036854775807);  slice_tensor_546 = None\n",
      "        copy__default_27: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_547, slice_tensor_543);  slice_tensor_547 = slice_tensor_543 = None\n",
      "        slice_tensor_548: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_102, 0, 0, 9223372036854775807);  add_tensor_102 = None\n",
      "        slice_tensor_549: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_548, 1, 0, 9223372036854775807);  slice_tensor_548 = None\n",
      "        slice_tensor_550: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_549, 2, 0, 9223372036854775807);  slice_tensor_549 = None\n",
      "        slice_tensor_551: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_550, 3, 0, 9223372036854775807);  slice_tensor_550 = None\n",
      "        _tensor_constant92 = self._tensor_constant92\n",
      "        slice_tensor_552: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant92, 0, 0, 9223372036854775807);  _tensor_constant92 = None\n",
      "        slice_tensor_553: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_552, 1, 0, 9223372036854775807);  slice_tensor_552 = None\n",
      "        slice_tensor_554: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_553, 2, 0, sym_size_1);  slice_tensor_553 = None\n",
      "        slice_tensor_555: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_554, 3, 0, 9223372036854775807);  slice_tensor_554 = None\n",
      "        copy__default_28: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_555, slice_tensor_551);  slice_tensor_555 = slice_tensor_551 = None\n",
      "        slice_tensor_556: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_72, 0, 0, 9223372036854775807);  transpose_int_72 = None\n",
      "        slice_tensor_557: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_556, 1, 0, 9223372036854775807);  slice_tensor_556 = None\n",
      "        slice_tensor_558: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_557, 2, 0, 9223372036854775807);  slice_tensor_557 = None\n",
      "        slice_tensor_559: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_558, 3, 0, 9223372036854775807);  slice_tensor_558 = None\n",
      "        _tensor_constant93 = self._tensor_constant93\n",
      "        slice_tensor_560: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant93, 0, 0, 9223372036854775807);  _tensor_constant93 = None\n",
      "        slice_tensor_561: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_560, 1, 0, 9223372036854775807);  slice_tensor_560 = None\n",
      "        slice_tensor_562: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_561, 2, 0, sym_size_1);  slice_tensor_561 = None\n",
      "        slice_tensor_563: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_562, 3, 0, 9223372036854775807);  slice_tensor_562 = None\n",
      "        copy__default_29: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_563, slice_tensor_559);  slice_tensor_563 = slice_tensor_559 = None\n",
      "        slice_tensor_564: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_109, 0, 0, 9223372036854775807);  add_tensor_109 = None\n",
      "        slice_tensor_565: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_564, 1, 0, 9223372036854775807);  slice_tensor_564 = None\n",
      "        slice_tensor_566: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_565, 2, 0, 9223372036854775807);  slice_tensor_565 = None\n",
      "        slice_tensor_567: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_566, 3, 0, 9223372036854775807);  slice_tensor_566 = None\n",
      "        _tensor_constant94 = self._tensor_constant94\n",
      "        slice_tensor_568: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant94, 0, 0, 9223372036854775807);  _tensor_constant94 = None\n",
      "        slice_tensor_569: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_568, 1, 0, 9223372036854775807);  slice_tensor_568 = None\n",
      "        slice_tensor_570: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_569, 2, 0, sym_size_1);  slice_tensor_569 = None\n",
      "        slice_tensor_571: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_570, 3, 0, 9223372036854775807);  slice_tensor_570 = None\n",
      "        copy__default_30: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_571, slice_tensor_567);  slice_tensor_571 = slice_tensor_567 = None\n",
      "        slice_tensor_572: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_77, 0, 0, 9223372036854775807);  transpose_int_77 = None\n",
      "        slice_tensor_573: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_572, 1, 0, 9223372036854775807);  slice_tensor_572 = None\n",
      "        slice_tensor_574: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_573, 2, 0, 9223372036854775807);  slice_tensor_573 = None\n",
      "        slice_tensor_575: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_574, 3, 0, 9223372036854775807);  slice_tensor_574 = None\n",
      "        _tensor_constant95 = self._tensor_constant95\n",
      "        slice_tensor_576: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant95, 0, 0, 9223372036854775807);  _tensor_constant95 = None\n",
      "        slice_tensor_577: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_576, 1, 0, 9223372036854775807);  slice_tensor_576 = None\n",
      "        slice_tensor_578: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_577, 2, 0, sym_size_1);  slice_tensor_577 = None\n",
      "        slice_tensor_579: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_578, 3, 0, 9223372036854775807);  slice_tensor_578 = None\n",
      "        copy__default_31: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_579, slice_tensor_575);  slice_tensor_579 = slice_tensor_575 = None\n",
      "        slice_tensor_580: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_116, 0, 0, 9223372036854775807);  add_tensor_116 = None\n",
      "        slice_tensor_581: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_580, 1, 0, 9223372036854775807);  slice_tensor_580 = None\n",
      "        slice_tensor_582: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_581, 2, 0, 9223372036854775807);  slice_tensor_581 = None\n",
      "        slice_tensor_583: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_582, 3, 0, 9223372036854775807);  slice_tensor_582 = None\n",
      "        _tensor_constant96 = self._tensor_constant96\n",
      "        slice_tensor_584: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant96, 0, 0, 9223372036854775807);  _tensor_constant96 = None\n",
      "        slice_tensor_585: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_584, 1, 0, 9223372036854775807);  slice_tensor_584 = None\n",
      "        slice_tensor_586: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_585, 2, 0, sym_size_1);  slice_tensor_585 = None\n",
      "        slice_tensor_587: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_586, 3, 0, 9223372036854775807);  slice_tensor_586 = None\n",
      "        copy__default_32: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_587, slice_tensor_583);  slice_tensor_587 = slice_tensor_583 = None\n",
      "        slice_tensor_588: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_82, 0, 0, 9223372036854775807);  transpose_int_82 = None\n",
      "        slice_tensor_589: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_588, 1, 0, 9223372036854775807);  slice_tensor_588 = None\n",
      "        slice_tensor_590: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_589, 2, 0, 9223372036854775807);  slice_tensor_589 = None\n",
      "        slice_tensor_591: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_590, 3, 0, 9223372036854775807);  slice_tensor_590 = None\n",
      "        _tensor_constant97 = self._tensor_constant97\n",
      "        slice_tensor_592: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant97, 0, 0, 9223372036854775807);  _tensor_constant97 = None\n",
      "        slice_tensor_593: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_592, 1, 0, 9223372036854775807);  slice_tensor_592 = None\n",
      "        slice_tensor_594: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_593, 2, 0, sym_size_1);  slice_tensor_593 = None\n",
      "        slice_tensor_595: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_594, 3, 0, 9223372036854775807);  slice_tensor_594 = None\n",
      "        copy__default_33: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_595, slice_tensor_591);  slice_tensor_595 = slice_tensor_591 = None\n",
      "        slice_tensor_596: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_123, 0, 0, 9223372036854775807);  add_tensor_123 = None\n",
      "        slice_tensor_597: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_596, 1, 0, 9223372036854775807);  slice_tensor_596 = None\n",
      "        slice_tensor_598: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_597, 2, 0, 9223372036854775807);  slice_tensor_597 = None\n",
      "        slice_tensor_599: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_598, 3, 0, 9223372036854775807);  slice_tensor_598 = None\n",
      "        _tensor_constant98 = self._tensor_constant98\n",
      "        slice_tensor_600: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant98, 0, 0, 9223372036854775807);  _tensor_constant98 = None\n",
      "        slice_tensor_601: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_600, 1, 0, 9223372036854775807);  slice_tensor_600 = None\n",
      "        slice_tensor_602: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_601, 2, 0, sym_size_1);  slice_tensor_601 = None\n",
      "        slice_tensor_603: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_602, 3, 0, 9223372036854775807);  slice_tensor_602 = None\n",
      "        copy__default_34: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_603, slice_tensor_599);  slice_tensor_603 = slice_tensor_599 = None\n",
      "        slice_tensor_604: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_87, 0, 0, 9223372036854775807);  transpose_int_87 = None\n",
      "        slice_tensor_605: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_604, 1, 0, 9223372036854775807);  slice_tensor_604 = None\n",
      "        slice_tensor_606: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_605, 2, 0, 9223372036854775807);  slice_tensor_605 = None\n",
      "        slice_tensor_607: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_606, 3, 0, 9223372036854775807);  slice_tensor_606 = None\n",
      "        _tensor_constant99 = self._tensor_constant99\n",
      "        slice_tensor_608: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant99, 0, 0, 9223372036854775807);  _tensor_constant99 = None\n",
      "        slice_tensor_609: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_608, 1, 0, 9223372036854775807);  slice_tensor_608 = None\n",
      "        slice_tensor_610: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_609, 2, 0, sym_size_1);  slice_tensor_609 = None\n",
      "        slice_tensor_611: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_610, 3, 0, 9223372036854775807);  slice_tensor_610 = None\n",
      "        copy__default_35: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_611, slice_tensor_607);  slice_tensor_611 = slice_tensor_607 = None\n",
      "        slice_tensor_612: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_130, 0, 0, 9223372036854775807);  add_tensor_130 = None\n",
      "        slice_tensor_613: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_612, 1, 0, 9223372036854775807);  slice_tensor_612 = None\n",
      "        slice_tensor_614: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_613, 2, 0, 9223372036854775807);  slice_tensor_613 = None\n",
      "        slice_tensor_615: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_614, 3, 0, 9223372036854775807);  slice_tensor_614 = None\n",
      "        _tensor_constant100 = self._tensor_constant100\n",
      "        slice_tensor_616: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant100, 0, 0, 9223372036854775807);  _tensor_constant100 = None\n",
      "        slice_tensor_617: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_616, 1, 0, 9223372036854775807);  slice_tensor_616 = None\n",
      "        slice_tensor_618: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_617, 2, 0, sym_size_1);  slice_tensor_617 = None\n",
      "        slice_tensor_619: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_618, 3, 0, 9223372036854775807);  slice_tensor_618 = None\n",
      "        copy__default_36: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_619, slice_tensor_615);  slice_tensor_619 = slice_tensor_615 = None\n",
      "        slice_tensor_620: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_92, 0, 0, 9223372036854775807);  transpose_int_92 = None\n",
      "        slice_tensor_621: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_620, 1, 0, 9223372036854775807);  slice_tensor_620 = None\n",
      "        slice_tensor_622: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_621, 2, 0, 9223372036854775807);  slice_tensor_621 = None\n",
      "        slice_tensor_623: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_622, 3, 0, 9223372036854775807);  slice_tensor_622 = None\n",
      "        _tensor_constant101 = self._tensor_constant101\n",
      "        slice_tensor_624: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant101, 0, 0, 9223372036854775807);  _tensor_constant101 = None\n",
      "        slice_tensor_625: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_624, 1, 0, 9223372036854775807);  slice_tensor_624 = None\n",
      "        slice_tensor_626: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_625, 2, 0, sym_size_1);  slice_tensor_625 = None\n",
      "        slice_tensor_627: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_626, 3, 0, 9223372036854775807);  slice_tensor_626 = None\n",
      "        copy__default_37: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_627, slice_tensor_623);  slice_tensor_627 = slice_tensor_623 = None\n",
      "        slice_tensor_628: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_137, 0, 0, 9223372036854775807);  add_tensor_137 = None\n",
      "        slice_tensor_629: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_628, 1, 0, 9223372036854775807);  slice_tensor_628 = None\n",
      "        slice_tensor_630: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_629, 2, 0, 9223372036854775807);  slice_tensor_629 = None\n",
      "        slice_tensor_631: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_630, 3, 0, 9223372036854775807);  slice_tensor_630 = None\n",
      "        _tensor_constant102 = self._tensor_constant102\n",
      "        slice_tensor_632: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant102, 0, 0, 9223372036854775807);  _tensor_constant102 = None\n",
      "        slice_tensor_633: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_632, 1, 0, 9223372036854775807);  slice_tensor_632 = None\n",
      "        slice_tensor_634: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_633, 2, 0, sym_size_1);  slice_tensor_633 = None\n",
      "        slice_tensor_635: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_634, 3, 0, 9223372036854775807);  slice_tensor_634 = None\n",
      "        copy__default_38: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_635, slice_tensor_631);  slice_tensor_635 = slice_tensor_631 = None\n",
      "        slice_tensor_636: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_97, 0, 0, 9223372036854775807);  transpose_int_97 = None\n",
      "        slice_tensor_637: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_636, 1, 0, 9223372036854775807);  slice_tensor_636 = None\n",
      "        slice_tensor_638: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_637, 2, 0, 9223372036854775807);  slice_tensor_637 = None\n",
      "        slice_tensor_639: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_638, 3, 0, 9223372036854775807);  slice_tensor_638 = None\n",
      "        _tensor_constant103 = self._tensor_constant103\n",
      "        slice_tensor_640: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant103, 0, 0, 9223372036854775807);  _tensor_constant103 = None\n",
      "        slice_tensor_641: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_640, 1, 0, 9223372036854775807);  slice_tensor_640 = None\n",
      "        slice_tensor_642: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_641, 2, 0, sym_size_1);  slice_tensor_641 = None\n",
      "        slice_tensor_643: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_642, 3, 0, 9223372036854775807);  slice_tensor_642 = None\n",
      "        copy__default_39: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_643, slice_tensor_639);  slice_tensor_643 = slice_tensor_639 = None\n",
      "        slice_tensor_644: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_144, 0, 0, 9223372036854775807);  add_tensor_144 = None\n",
      "        slice_tensor_645: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_644, 1, 0, 9223372036854775807);  slice_tensor_644 = None\n",
      "        slice_tensor_646: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_645, 2, 0, 9223372036854775807);  slice_tensor_645 = None\n",
      "        slice_tensor_647: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_646, 3, 0, 9223372036854775807);  slice_tensor_646 = None\n",
      "        _tensor_constant104 = self._tensor_constant104\n",
      "        slice_tensor_648: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant104, 0, 0, 9223372036854775807);  _tensor_constant104 = None\n",
      "        slice_tensor_649: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_648, 1, 0, 9223372036854775807);  slice_tensor_648 = None\n",
      "        slice_tensor_650: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_649, 2, 0, sym_size_1);  slice_tensor_649 = None\n",
      "        slice_tensor_651: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_650, 3, 0, 9223372036854775807);  slice_tensor_650 = None\n",
      "        copy__default_40: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_651, slice_tensor_647);  slice_tensor_651 = slice_tensor_647 = None\n",
      "        slice_tensor_652: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_102, 0, 0, 9223372036854775807);  transpose_int_102 = None\n",
      "        slice_tensor_653: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_652, 1, 0, 9223372036854775807);  slice_tensor_652 = None\n",
      "        slice_tensor_654: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_653, 2, 0, 9223372036854775807);  slice_tensor_653 = None\n",
      "        slice_tensor_655: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_654, 3, 0, 9223372036854775807);  slice_tensor_654 = None\n",
      "        _tensor_constant105 = self._tensor_constant105\n",
      "        slice_tensor_656: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant105, 0, 0, 9223372036854775807);  _tensor_constant105 = None\n",
      "        slice_tensor_657: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_656, 1, 0, 9223372036854775807);  slice_tensor_656 = None\n",
      "        slice_tensor_658: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_657, 2, 0, sym_size_1);  slice_tensor_657 = None\n",
      "        slice_tensor_659: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_658, 3, 0, 9223372036854775807);  slice_tensor_658 = None\n",
      "        copy__default_41: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_659, slice_tensor_655);  slice_tensor_659 = slice_tensor_655 = None\n",
      "        slice_tensor_660: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_151, 0, 0, 9223372036854775807);  add_tensor_151 = None\n",
      "        slice_tensor_661: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_660, 1, 0, 9223372036854775807);  slice_tensor_660 = None\n",
      "        slice_tensor_662: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_661, 2, 0, 9223372036854775807);  slice_tensor_661 = None\n",
      "        slice_tensor_663: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_662, 3, 0, 9223372036854775807);  slice_tensor_662 = None\n",
      "        _tensor_constant106 = self._tensor_constant106\n",
      "        slice_tensor_664: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant106, 0, 0, 9223372036854775807);  _tensor_constant106 = None\n",
      "        slice_tensor_665: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_664, 1, 0, 9223372036854775807);  slice_tensor_664 = None\n",
      "        slice_tensor_666: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_665, 2, 0, sym_size_1);  slice_tensor_665 = None\n",
      "        slice_tensor_667: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_666, 3, 0, 9223372036854775807);  slice_tensor_666 = None\n",
      "        copy__default_42: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_667, slice_tensor_663);  slice_tensor_667 = slice_tensor_663 = None\n",
      "        slice_tensor_668: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_107, 0, 0, 9223372036854775807);  transpose_int_107 = None\n",
      "        slice_tensor_669: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_668, 1, 0, 9223372036854775807);  slice_tensor_668 = None\n",
      "        slice_tensor_670: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_669, 2, 0, 9223372036854775807);  slice_tensor_669 = None\n",
      "        slice_tensor_671: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_670, 3, 0, 9223372036854775807);  slice_tensor_670 = None\n",
      "        _tensor_constant107 = self._tensor_constant107\n",
      "        slice_tensor_672: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant107, 0, 0, 9223372036854775807);  _tensor_constant107 = None\n",
      "        slice_tensor_673: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_672, 1, 0, 9223372036854775807);  slice_tensor_672 = None\n",
      "        slice_tensor_674: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_673, 2, 0, sym_size_1);  slice_tensor_673 = None\n",
      "        slice_tensor_675: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_674, 3, 0, 9223372036854775807);  slice_tensor_674 = None\n",
      "        copy__default_43: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_675, slice_tensor_671);  slice_tensor_675 = slice_tensor_671 = None\n",
      "        slice_tensor_676: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_158, 0, 0, 9223372036854775807);  add_tensor_158 = None\n",
      "        slice_tensor_677: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_676, 1, 0, 9223372036854775807);  slice_tensor_676 = None\n",
      "        slice_tensor_678: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_677, 2, 0, 9223372036854775807);  slice_tensor_677 = None\n",
      "        slice_tensor_679: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_678, 3, 0, 9223372036854775807);  slice_tensor_678 = None\n",
      "        _tensor_constant108 = self._tensor_constant108\n",
      "        slice_tensor_680: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant108, 0, 0, 9223372036854775807);  _tensor_constant108 = None\n",
      "        slice_tensor_681: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_680, 1, 0, 9223372036854775807);  slice_tensor_680 = None\n",
      "        slice_tensor_682: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_681, 2, 0, sym_size_1);  slice_tensor_681 = None\n",
      "        slice_tensor_683: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_682, 3, 0, 9223372036854775807);  slice_tensor_682 = None\n",
      "        copy__default_44: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_683, slice_tensor_679);  slice_tensor_683 = slice_tensor_679 = None\n",
      "        slice_tensor_684: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_112, 0, 0, 9223372036854775807);  transpose_int_112 = None\n",
      "        slice_tensor_685: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_684, 1, 0, 9223372036854775807);  slice_tensor_684 = None\n",
      "        slice_tensor_686: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_685, 2, 0, 9223372036854775807);  slice_tensor_685 = None\n",
      "        slice_tensor_687: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_686, 3, 0, 9223372036854775807);  slice_tensor_686 = None\n",
      "        _tensor_constant109 = self._tensor_constant109\n",
      "        slice_tensor_688: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant109, 0, 0, 9223372036854775807);  _tensor_constant109 = None\n",
      "        slice_tensor_689: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_688, 1, 0, 9223372036854775807);  slice_tensor_688 = None\n",
      "        slice_tensor_690: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_689, 2, 0, sym_size_1);  slice_tensor_689 = None\n",
      "        slice_tensor_691: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_690, 3, 0, 9223372036854775807);  slice_tensor_690 = None\n",
      "        copy__default_45: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_691, slice_tensor_687);  slice_tensor_691 = slice_tensor_687 = None\n",
      "        slice_tensor_692: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_165, 0, 0, 9223372036854775807);  add_tensor_165 = None\n",
      "        slice_tensor_693: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_692, 1, 0, 9223372036854775807);  slice_tensor_692 = None\n",
      "        slice_tensor_694: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_693, 2, 0, 9223372036854775807);  slice_tensor_693 = None\n",
      "        slice_tensor_695: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_694, 3, 0, 9223372036854775807);  slice_tensor_694 = None\n",
      "        _tensor_constant110 = self._tensor_constant110\n",
      "        slice_tensor_696: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant110, 0, 0, 9223372036854775807);  _tensor_constant110 = None\n",
      "        slice_tensor_697: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_696, 1, 0, 9223372036854775807);  slice_tensor_696 = None\n",
      "        slice_tensor_698: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_697, 2, 0, sym_size_1);  slice_tensor_697 = None\n",
      "        slice_tensor_699: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_698, 3, 0, 9223372036854775807);  slice_tensor_698 = None\n",
      "        copy__default_46: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_699, slice_tensor_695);  slice_tensor_699 = slice_tensor_695 = None\n",
      "        slice_tensor_700: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_117, 0, 0, 9223372036854775807);  transpose_int_117 = None\n",
      "        slice_tensor_701: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_700, 1, 0, 9223372036854775807);  slice_tensor_700 = None\n",
      "        slice_tensor_702: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_701, 2, 0, 9223372036854775807);  slice_tensor_701 = None\n",
      "        slice_tensor_703: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_702, 3, 0, 9223372036854775807);  slice_tensor_702 = None\n",
      "        _tensor_constant111 = self._tensor_constant111\n",
      "        slice_tensor_704: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant111, 0, 0, 9223372036854775807);  _tensor_constant111 = None\n",
      "        slice_tensor_705: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_704, 1, 0, 9223372036854775807);  slice_tensor_704 = None\n",
      "        slice_tensor_706: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_705, 2, 0, sym_size_1);  slice_tensor_705 = None\n",
      "        slice_tensor_707: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_706, 3, 0, 9223372036854775807);  slice_tensor_706 = None\n",
      "        copy__default_47: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_707, slice_tensor_703);  slice_tensor_707 = slice_tensor_703 = None\n",
      "        slice_tensor_708: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_172, 0, 0, 9223372036854775807);  add_tensor_172 = None\n",
      "        slice_tensor_709: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_708, 1, 0, 9223372036854775807);  slice_tensor_708 = None\n",
      "        slice_tensor_710: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_709, 2, 0, 9223372036854775807);  slice_tensor_709 = None\n",
      "        slice_tensor_711: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_710, 3, 0, 9223372036854775807);  slice_tensor_710 = None\n",
      "        _tensor_constant112 = self._tensor_constant112\n",
      "        slice_tensor_712: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant112, 0, 0, 9223372036854775807);  _tensor_constant112 = None\n",
      "        slice_tensor_713: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_712, 1, 0, 9223372036854775807);  slice_tensor_712 = None\n",
      "        slice_tensor_714: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_713, 2, 0, sym_size_1);  slice_tensor_713 = None\n",
      "        slice_tensor_715: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_714, 3, 0, 9223372036854775807);  slice_tensor_714 = None\n",
      "        copy__default_48: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_715, slice_tensor_711);  slice_tensor_715 = slice_tensor_711 = None\n",
      "        slice_tensor_716: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_122, 0, 0, 9223372036854775807);  transpose_int_122 = None\n",
      "        slice_tensor_717: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_716, 1, 0, 9223372036854775807);  slice_tensor_716 = None\n",
      "        slice_tensor_718: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_717, 2, 0, 9223372036854775807);  slice_tensor_717 = None\n",
      "        slice_tensor_719: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_718, 3, 0, 9223372036854775807);  slice_tensor_718 = None\n",
      "        _tensor_constant113 = self._tensor_constant113\n",
      "        slice_tensor_720: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant113, 0, 0, 9223372036854775807);  _tensor_constant113 = None\n",
      "        slice_tensor_721: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_720, 1, 0, 9223372036854775807);  slice_tensor_720 = None\n",
      "        slice_tensor_722: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_721, 2, 0, sym_size_1);  slice_tensor_721 = None\n",
      "        slice_tensor_723: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_722, 3, 0, 9223372036854775807);  slice_tensor_722 = None\n",
      "        copy__default_49: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_723, slice_tensor_719);  slice_tensor_723 = slice_tensor_719 = None\n",
      "        slice_tensor_724: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_179, 0, 0, 9223372036854775807);  add_tensor_179 = None\n",
      "        slice_tensor_725: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_724, 1, 0, 9223372036854775807);  slice_tensor_724 = None\n",
      "        slice_tensor_726: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_725, 2, 0, 9223372036854775807);  slice_tensor_725 = None\n",
      "        slice_tensor_727: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_726, 3, 0, 9223372036854775807);  slice_tensor_726 = None\n",
      "        _tensor_constant114 = self._tensor_constant114\n",
      "        slice_tensor_728: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant114, 0, 0, 9223372036854775807);  _tensor_constant114 = None\n",
      "        slice_tensor_729: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_728, 1, 0, 9223372036854775807);  slice_tensor_728 = None\n",
      "        slice_tensor_730: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_729, 2, 0, sym_size_1);  slice_tensor_729 = None\n",
      "        slice_tensor_731: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_730, 3, 0, 9223372036854775807);  slice_tensor_730 = None\n",
      "        copy__default_50: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_731, slice_tensor_727);  slice_tensor_731 = slice_tensor_727 = None\n",
      "        slice_tensor_732: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_127, 0, 0, 9223372036854775807);  transpose_int_127 = None\n",
      "        slice_tensor_733: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_732, 1, 0, 9223372036854775807);  slice_tensor_732 = None\n",
      "        slice_tensor_734: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_733, 2, 0, 9223372036854775807);  slice_tensor_733 = None\n",
      "        slice_tensor_735: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_734, 3, 0, 9223372036854775807);  slice_tensor_734 = None\n",
      "        _tensor_constant115 = self._tensor_constant115\n",
      "        slice_tensor_736: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant115, 0, 0, 9223372036854775807);  _tensor_constant115 = None\n",
      "        slice_tensor_737: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_736, 1, 0, 9223372036854775807);  slice_tensor_736 = None\n",
      "        slice_tensor_738: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_737, 2, 0, sym_size_1);  slice_tensor_737 = None\n",
      "        slice_tensor_739: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_738, 3, 0, 9223372036854775807);  slice_tensor_738 = None\n",
      "        copy__default_51: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_739, slice_tensor_735);  slice_tensor_739 = slice_tensor_735 = None\n",
      "        slice_tensor_740: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_186, 0, 0, 9223372036854775807);  add_tensor_186 = None\n",
      "        slice_tensor_741: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_740, 1, 0, 9223372036854775807);  slice_tensor_740 = None\n",
      "        slice_tensor_742: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_741, 2, 0, 9223372036854775807);  slice_tensor_741 = None\n",
      "        slice_tensor_743: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_742, 3, 0, 9223372036854775807);  slice_tensor_742 = None\n",
      "        _tensor_constant116 = self._tensor_constant116\n",
      "        slice_tensor_744: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant116, 0, 0, 9223372036854775807);  _tensor_constant116 = None\n",
      "        slice_tensor_745: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_744, 1, 0, 9223372036854775807);  slice_tensor_744 = None\n",
      "        slice_tensor_746: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_745, 2, 0, sym_size_1);  slice_tensor_745 = None\n",
      "        slice_tensor_747: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_746, 3, 0, 9223372036854775807);  slice_tensor_746 = None\n",
      "        copy__default_52: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_747, slice_tensor_743);  slice_tensor_747 = slice_tensor_743 = None\n",
      "        slice_tensor_748: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_132, 0, 0, 9223372036854775807);  transpose_int_132 = None\n",
      "        slice_tensor_749: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_748, 1, 0, 9223372036854775807);  slice_tensor_748 = None\n",
      "        slice_tensor_750: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_749, 2, 0, 9223372036854775807);  slice_tensor_749 = None\n",
      "        slice_tensor_751: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_750, 3, 0, 9223372036854775807);  slice_tensor_750 = None\n",
      "        _tensor_constant117 = self._tensor_constant117\n",
      "        slice_tensor_752: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant117, 0, 0, 9223372036854775807);  _tensor_constant117 = None\n",
      "        slice_tensor_753: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_752, 1, 0, 9223372036854775807);  slice_tensor_752 = None\n",
      "        slice_tensor_754: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_753, 2, 0, sym_size_1);  slice_tensor_753 = None\n",
      "        slice_tensor_755: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_754, 3, 0, 9223372036854775807);  slice_tensor_754 = None\n",
      "        copy__default_53: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_755, slice_tensor_751);  slice_tensor_755 = slice_tensor_751 = None\n",
      "        slice_tensor_756: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_193, 0, 0, 9223372036854775807);  add_tensor_193 = None\n",
      "        slice_tensor_757: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_756, 1, 0, 9223372036854775807);  slice_tensor_756 = None\n",
      "        slice_tensor_758: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_757, 2, 0, 9223372036854775807);  slice_tensor_757 = None\n",
      "        slice_tensor_759: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_758, 3, 0, 9223372036854775807);  slice_tensor_758 = None\n",
      "        _tensor_constant118 = self._tensor_constant118\n",
      "        slice_tensor_760: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant118, 0, 0, 9223372036854775807);  _tensor_constant118 = None\n",
      "        slice_tensor_761: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_760, 1, 0, 9223372036854775807);  slice_tensor_760 = None\n",
      "        slice_tensor_762: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_761, 2, 0, sym_size_1);  slice_tensor_761 = None\n",
      "        slice_tensor_763: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_762, 3, 0, 9223372036854775807);  slice_tensor_762 = None\n",
      "        copy__default_54: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_763, slice_tensor_759);  slice_tensor_763 = slice_tensor_759 = None\n",
      "        slice_tensor_764: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_137, 0, 0, 9223372036854775807);  transpose_int_137 = None\n",
      "        slice_tensor_765: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_764, 1, 0, 9223372036854775807);  slice_tensor_764 = None\n",
      "        slice_tensor_766: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_765, 2, 0, 9223372036854775807);  slice_tensor_765 = None\n",
      "        slice_tensor_767: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_766, 3, 0, 9223372036854775807);  slice_tensor_766 = None\n",
      "        _tensor_constant119 = self._tensor_constant119\n",
      "        slice_tensor_768: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant119, 0, 0, 9223372036854775807);  _tensor_constant119 = None\n",
      "        slice_tensor_769: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_768, 1, 0, 9223372036854775807);  slice_tensor_768 = None\n",
      "        slice_tensor_770: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_769, 2, 0, sym_size_1);  slice_tensor_769 = None\n",
      "        slice_tensor_771: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_770, 3, 0, 9223372036854775807);  slice_tensor_770 = None\n",
      "        copy__default_55: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_771, slice_tensor_767);  slice_tensor_771 = slice_tensor_767 = None\n",
      "        slice_tensor_772: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_200, 0, 0, 9223372036854775807);  add_tensor_200 = None\n",
      "        slice_tensor_773: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_772, 1, 0, 9223372036854775807);  slice_tensor_772 = None\n",
      "        slice_tensor_774: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_773, 2, 0, 9223372036854775807);  slice_tensor_773 = None\n",
      "        slice_tensor_775: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_774, 3, 0, 9223372036854775807);  slice_tensor_774 = None\n",
      "        _tensor_constant120 = self._tensor_constant120\n",
      "        slice_tensor_776: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant120, 0, 0, 9223372036854775807);  _tensor_constant120 = None\n",
      "        slice_tensor_777: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_776, 1, 0, 9223372036854775807);  slice_tensor_776 = None\n",
      "        slice_tensor_778: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_777, 2, 0, sym_size_1);  slice_tensor_777 = None\n",
      "        slice_tensor_779: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_778, 3, 0, 9223372036854775807);  slice_tensor_778 = None\n",
      "        copy__default_56: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_779, slice_tensor_775);  slice_tensor_779 = slice_tensor_775 = None\n",
      "        slice_tensor_780: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_142, 0, 0, 9223372036854775807);  transpose_int_142 = None\n",
      "        slice_tensor_781: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_780, 1, 0, 9223372036854775807);  slice_tensor_780 = None\n",
      "        slice_tensor_782: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_781, 2, 0, 9223372036854775807);  slice_tensor_781 = None\n",
      "        slice_tensor_783: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_782, 3, 0, 9223372036854775807);  slice_tensor_782 = None\n",
      "        _tensor_constant121 = self._tensor_constant121\n",
      "        slice_tensor_784: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant121, 0, 0, 9223372036854775807);  _tensor_constant121 = None\n",
      "        slice_tensor_785: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_784, 1, 0, 9223372036854775807);  slice_tensor_784 = None\n",
      "        slice_tensor_786: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_785, 2, 0, sym_size_1);  slice_tensor_785 = None\n",
      "        slice_tensor_787: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_786, 3, 0, 9223372036854775807);  slice_tensor_786 = None\n",
      "        copy__default_57: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_787, slice_tensor_783);  slice_tensor_787 = slice_tensor_783 = None\n",
      "        slice_tensor_788: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_207, 0, 0, 9223372036854775807);  add_tensor_207 = None\n",
      "        slice_tensor_789: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_788, 1, 0, 9223372036854775807);  slice_tensor_788 = None\n",
      "        slice_tensor_790: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_789, 2, 0, 9223372036854775807);  slice_tensor_789 = None\n",
      "        slice_tensor_791: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_790, 3, 0, 9223372036854775807);  slice_tensor_790 = None\n",
      "        _tensor_constant122 = self._tensor_constant122\n",
      "        slice_tensor_792: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant122, 0, 0, 9223372036854775807);  _tensor_constant122 = None\n",
      "        slice_tensor_793: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_792, 1, 0, 9223372036854775807);  slice_tensor_792 = None\n",
      "        slice_tensor_794: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_793, 2, 0, sym_size_1);  slice_tensor_793 = None\n",
      "        slice_tensor_795: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_794, 3, 0, 9223372036854775807);  slice_tensor_794 = None\n",
      "        copy__default_58: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_795, slice_tensor_791);  slice_tensor_795 = slice_tensor_791 = None\n",
      "        slice_tensor_796: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_147, 0, 0, 9223372036854775807);  transpose_int_147 = None\n",
      "        slice_tensor_797: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_796, 1, 0, 9223372036854775807);  slice_tensor_796 = None\n",
      "        slice_tensor_798: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_797, 2, 0, 9223372036854775807);  slice_tensor_797 = None\n",
      "        slice_tensor_799: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_798, 3, 0, 9223372036854775807);  slice_tensor_798 = None\n",
      "        _tensor_constant123 = self._tensor_constant123\n",
      "        slice_tensor_800: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant123, 0, 0, 9223372036854775807);  _tensor_constant123 = None\n",
      "        slice_tensor_801: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_800, 1, 0, 9223372036854775807);  slice_tensor_800 = None\n",
      "        slice_tensor_802: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_801, 2, 0, sym_size_1);  slice_tensor_801 = None\n",
      "        slice_tensor_803: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_802, 3, 0, 9223372036854775807);  slice_tensor_802 = None\n",
      "        copy__default_59: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_803, slice_tensor_799);  slice_tensor_803 = slice_tensor_799 = None\n",
      "        slice_tensor_804: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_214, 0, 0, 9223372036854775807);  add_tensor_214 = None\n",
      "        slice_tensor_805: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_804, 1, 0, 9223372036854775807);  slice_tensor_804 = None\n",
      "        slice_tensor_806: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_805, 2, 0, 9223372036854775807);  slice_tensor_805 = None\n",
      "        slice_tensor_807: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_806, 3, 0, 9223372036854775807);  slice_tensor_806 = None\n",
      "        _tensor_constant124 = self._tensor_constant124\n",
      "        slice_tensor_808: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant124, 0, 0, 9223372036854775807);  _tensor_constant124 = None\n",
      "        slice_tensor_809: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_808, 1, 0, 9223372036854775807);  slice_tensor_808 = None\n",
      "        slice_tensor_810: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_809, 2, 0, sym_size_1);  slice_tensor_809 = None\n",
      "        slice_tensor_811: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_810, 3, 0, 9223372036854775807);  slice_tensor_810 = None\n",
      "        copy__default_60: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_811, slice_tensor_807);  slice_tensor_811 = slice_tensor_807 = None\n",
      "        slice_tensor_812: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_152, 0, 0, 9223372036854775807);  transpose_int_152 = None\n",
      "        slice_tensor_813: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_812, 1, 0, 9223372036854775807);  slice_tensor_812 = None\n",
      "        slice_tensor_814: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_813, 2, 0, 9223372036854775807);  slice_tensor_813 = None\n",
      "        slice_tensor_815: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_814, 3, 0, 9223372036854775807);  slice_tensor_814 = None\n",
      "        _tensor_constant125 = self._tensor_constant125\n",
      "        slice_tensor_816: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant125, 0, 0, 9223372036854775807);  _tensor_constant125 = None\n",
      "        slice_tensor_817: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_816, 1, 0, 9223372036854775807);  slice_tensor_816 = None\n",
      "        slice_tensor_818: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_817, 2, 0, sym_size_1);  slice_tensor_817 = None\n",
      "        slice_tensor_819: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_818, 3, 0, 9223372036854775807);  slice_tensor_818 = None\n",
      "        copy__default_61: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_819, slice_tensor_815);  slice_tensor_819 = slice_tensor_815 = None\n",
      "        slice_tensor_820: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_221, 0, 0, 9223372036854775807);  add_tensor_221 = None\n",
      "        slice_tensor_821: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_820, 1, 0, 9223372036854775807);  slice_tensor_820 = None\n",
      "        slice_tensor_822: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_821, 2, 0, 9223372036854775807);  slice_tensor_821 = None\n",
      "        slice_tensor_823: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_822, 3, 0, 9223372036854775807);  slice_tensor_822 = None\n",
      "        _tensor_constant126 = self._tensor_constant126\n",
      "        slice_tensor_824: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant126, 0, 0, 9223372036854775807);  _tensor_constant126 = None\n",
      "        slice_tensor_825: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_824, 1, 0, 9223372036854775807);  slice_tensor_824 = None\n",
      "        slice_tensor_826: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_825, 2, 0, sym_size_1);  slice_tensor_825 = None\n",
      "        slice_tensor_827: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_826, 3, 0, 9223372036854775807);  slice_tensor_826 = None\n",
      "        copy__default_62: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_827, slice_tensor_823);  slice_tensor_827 = slice_tensor_823 = None\n",
      "        slice_tensor_828: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_157, 0, 0, 9223372036854775807);  transpose_int_157 = None\n",
      "        slice_tensor_829: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_828, 1, 0, 9223372036854775807);  slice_tensor_828 = None\n",
      "        slice_tensor_830: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_829, 2, 0, 9223372036854775807);  slice_tensor_829 = None\n",
      "        slice_tensor_831: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_830, 3, 0, 9223372036854775807);  slice_tensor_830 = None\n",
      "        _tensor_constant127 = self._tensor_constant127\n",
      "        slice_tensor_832: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant127, 0, 0, 9223372036854775807);  _tensor_constant127 = None\n",
      "        slice_tensor_833: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_832, 1, 0, 9223372036854775807);  slice_tensor_832 = None\n",
      "        slice_tensor_834: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_833, 2, 0, sym_size_1);  slice_tensor_833 = None\n",
      "        slice_tensor_835: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_834, 3, 0, 9223372036854775807);  slice_tensor_834 = None\n",
      "        copy__default_63: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_835, slice_tensor_831);  slice_tensor_835 = slice_tensor_831 = None\n",
      "        return pytree.tree_unflatten([view_default_771, sym_size_1], self._out_spec)\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"class GraphModule(torch.nn.Module):\\n    def forward(self, input_ids):\\n        arg0: i64[1, s0], = fx_pytree.tree_flatten_spec(([input_ids], {}), self._in_spec)\\n        # File: /tmp/ipykernel_881114/4291870228.py:32, code: torch.sym_constrain_range(input_ids.shape[1], min=2, max=4095)\\n        sym_size: Sym(s0) = torch.ops.aten.sym_size(arg0, 1)\\n        sym_constrain_range_default = torch.ops.aten.sym_constrain_range.default(sym_size, min = 2, max = 4095)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:643, code: position_ids = torch.arange(\\n        add: Sym(s0) = sym_size + 0\\n        arange_start: i64[s0] = torch.ops.aten.arange.start(0, add, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  add = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:646, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\\n        unsqueeze_default: i64[1, s0] = torch.ops.aten.unsqueeze.default(arange_start, 0);  arange_start = None\\n        view_default: i64[1, s0] = torch.ops.aten.view.default(unsqueeze_default, [-1, sym_size]);  unsqueeze_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:651, code: inputs_embeds = self.embed_tokens(input_ids)\\n        _param_constant0 = self._param_constant0\\n        embedding_default: f32[1, s0, 4096] = torch.ops.aten.embedding.default(_param_constant0, arg0);  _param_constant0 = arg0 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:654, code: attention_mask = torch.ones(\\n        ones_default: b8[1, s0] = torch.ops.aten.ones.default([1, sym_size], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:50, code: mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\\n        full_default: f32[s0, s0] = torch.ops.aten.full.default([sym_size, sym_size], -3.4028234663852886e+38, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:51, code: mask_cond = torch.arange(mask.size(-1), device=device)\\n        arange_default: i64[s0] = torch.ops.aten.arange.default(sym_size, device = device(type='cpu'), pin_memory = False)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:52, code: mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\\n        add_tensor: i64[s0] = torch.ops.aten.add.Tensor(arange_default, 1)\\n        view_default_1: i64[s0, 1] = torch.ops.aten.view.default(add_tensor, [sym_size, 1]);  add_tensor = None\\n        lt_tensor: b8[s0, s0] = torch.ops.aten.lt.Tensor(arange_default, view_default_1);  arange_default = view_default_1 = None\\n        masked_fill__scalar: f32[s0, s0] = torch.ops.aten.masked_fill_.Scalar(full_default, lt_tensor, 0);  full_default = lt_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:57, code: return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\\n        unsqueeze_default_1: f32[1, s0, s0] = torch.ops.aten.unsqueeze.default(masked_fill__scalar, 0);  masked_fill__scalar = None\\n        unsqueeze_default_2: f32[1, 1, s0, s0] = torch.ops.aten.unsqueeze.default(unsqueeze_default_1, 1);  unsqueeze_default_1 = None\\n        slice_tensor: f32[1, 1, s0, s0] = torch.ops.aten.slice.Tensor(unsqueeze_default_2, 2, 0, 9223372036854775807);  unsqueeze_default_2 = None\\n        slice_tensor_1: f32[1, 1, s0, s0] = torch.ops.aten.slice.Tensor(slice_tensor, 3, 0, 9223372036854775807);  slice_tensor = None\\n        add_1: Sym(s0) = sym_size + 0\\n        expand_default: f32[1, 1, s0, s0] = torch.ops.aten.expand.default(slice_tensor_1, [1, 1, sym_size, add_1]);  slice_tensor_1 = add_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:68, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\\n        slice_tensor_2: b8[1, s0] = torch.ops.aten.slice.Tensor(ones_default, 0, 0, 9223372036854775807);  ones_default = None\\n        unsqueeze_default_3: b8[1, 1, s0] = torch.ops.aten.unsqueeze.default(slice_tensor_2, 1);  slice_tensor_2 = None\\n        unsqueeze_default_4: b8[1, 1, 1, s0] = torch.ops.aten.unsqueeze.default(unsqueeze_default_3, 2);  unsqueeze_default_3 = None\\n        slice_tensor_3: b8[1, 1, 1, s0] = torch.ops.aten.slice.Tensor(unsqueeze_default_4, 3, 0, 9223372036854775807);  unsqueeze_default_4 = None\\n        expand_default_1: b8[1, 1, s0, s0] = torch.ops.aten.expand.default(slice_tensor_3, [1, 1, sym_size, sym_size]);  slice_tensor_3 = None\\n        _to_copy_default: f32[1, 1, s0, s0] = torch.ops.aten._to_copy.default(expand_default_1, dtype = torch.float32);  expand_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70, code: inverted_mask = 1.0 - expanded_mask\\n        rsub_scalar: f32[1, 1, s0, s0] = torch.ops.aten.rsub.Scalar(_to_copy_default, 1.0);  _to_copy_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\\n        _to_copy_default_1: b8[1, 1, s0, s0] = torch.ops.aten._to_copy.default(rsub_scalar, dtype = torch.bool)\\n        masked_fill_scalar: f32[1, 1, s0, s0] = torch.ops.aten.masked_fill.Scalar(rsub_scalar, _to_copy_default_1, -3.4028234663852886e+38);  rsub_scalar = _to_copy_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:598, code: expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\\n        add_tensor_1: f32[1, 1, s0, s0] = torch.ops.aten.add.Tensor(masked_fill_scalar, expand_default);  masked_fill_scalar = expand_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(embedding_default, 2)\\n        mean_dim: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar, [-1], True);  pow_tensor_scalar = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_2: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim, 1e-06);  mean_dim = None\\n        rsqrt_default: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_2);  add_tensor_2 = None\\n        detach_default: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default)\\n        mul_tensor: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(embedding_default, rsqrt_default);  rsqrt_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant1 = self._param_constant1\\n        mul_tensor_1: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant1, mul_tensor);  _param_constant1 = mul_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant2 = self._param_constant2\\n        t_default: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant2);  _param_constant2 = None\\n        view_default_2: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_1, [sym_size, 4096])\\n        mm_default: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_2, t_default);  view_default_2 = t_default = None\\n        view_default_3: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default, [1, sym_size, 4096]);  mm_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant3 = self._param_constant3\\n        t_default_1: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant3);  _param_constant3 = None\\n        view_default_4: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_1, [sym_size, 4096])\\n        mm_default_1: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_4, t_default_1);  view_default_4 = t_default_1 = None\\n        view_default_5: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_1, [1, sym_size, 4096]);  mm_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant4 = self._param_constant4\\n        t_default_2: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant4);  _param_constant4 = None\\n        view_default_6: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_1, [sym_size, 4096]);  mul_tensor_1 = None\\n        mm_default_2: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_6, t_default_2);  view_default_6 = t_default_2 = None\\n        view_default_7: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_2, [1, sym_size, 4096]);  mm_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_8: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_3, [1, sym_size, 32, 128])\\n        transpose_int: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_8, 1, 2);  view_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_9: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_5, [1, sym_size, 32, 128])\\n        transpose_int_1: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_9, 1, 2);  view_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_10: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_7, [1, sym_size, 32, 128])\\n        transpose_int_2: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_10, 1, 2);  view_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant0 = self._tensor_constant0\\n        slice_tensor_4: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant0, 0, 0, 9223372036854775807);  _tensor_constant0 = None\\n        slice_tensor_5: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_4, 1, 0, 9223372036854775807);  slice_tensor_4 = None\\n        sym_size_1: Sym(s0) = torch.ops.aten.sym_size(view_default_5, 1);  view_default_5 = None\\n        slice_tensor_6: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_5, 2, 0, sym_size_1);  slice_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant1 = self._tensor_constant1\\n        slice_tensor_7: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant1, 0, 0, 9223372036854775807);  _tensor_constant1 = None\\n        slice_tensor_8: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_7, 1, 0, 9223372036854775807);  slice_tensor_7 = None\\n        slice_tensor_9: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_8, 2, 0, sym_size_1);  slice_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_6, 1);  slice_tensor_6 = None\\n        squeeze_dim_1: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim, 0);  squeeze_dim = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_2: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_9, 1);  slice_tensor_9 = None\\n        squeeze_dim_3: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_2, 0);  squeeze_dim_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_1, [view_default]);  squeeze_dim_1 = None\\n        unsqueeze_default_5: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor, 1);  index_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_1: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_3, [view_default]);  squeeze_dim_3 = None\\n        unsqueeze_default_6: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_1, 1);  index_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_2: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int, unsqueeze_default_5)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_10: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_11: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int, 3, 64, 9223372036854775807);  transpose_int = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_11);  slice_tensor_11 = None\\n        cat_default: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default, slice_tensor_10], -1);  neg_default = slice_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_3: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default, unsqueeze_default_6);  cat_default = None\\n        add_tensor_3: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_2, mul_tensor_3);  mul_tensor_2 = mul_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_4: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_1, unsqueeze_default_5);  unsqueeze_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_12: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_13: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_1, 3, 64, 9223372036854775807);  transpose_int_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_1: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_13);  slice_tensor_13 = None\\n        cat_default_1: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_1, slice_tensor_12], -1);  neg_default_1 = slice_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_5: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_1, unsqueeze_default_6);  cat_default_1 = unsqueeze_default_6 = None\\n        add_tensor_4: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_4, mul_tensor_5);  mul_tensor_4 = mul_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_3: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_4, 2, 3)\\n        sym_size_2: Sym(s0) = torch.ops.aten.sym_size(view_default_3, 1);  view_default_3 = None\\n        expand_default_2: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_3, [1, 32, sym_size_2, 128]);  add_tensor_3 = None\\n        view_default_11: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_2, [32, sym_size_2, 128]);  expand_default_2 = None\\n        expand_default_3: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_3, [1, 32, 128, sym_size_1]);  transpose_int_3 = None\\n        view_default_12: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_3, [32, 128, sym_size_1]);  expand_default_3 = None\\n        bmm_default: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_11, view_default_12);  view_default_11 = view_default_12 = None\\n        view_default_13: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default, [1, 32, sym_size_2, sym_size_1]);  bmm_default = None\\n        div_tensor: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_13, 11.313708498984761);  view_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_5: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor, add_tensor_1);  div_tensor = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_5, -1, False);  add_tensor_5 = None\\n        detach_default_1: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_4: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default, [1, 32, sym_size_2, sym_size_1]);  _softmax_default = None\\n        view_default_14: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_4, [32, sym_size_2, sym_size_1]);  expand_default_4 = None\\n        sym_size_3: Sym(s0) = torch.ops.aten.sym_size(view_default_7, 1);  view_default_7 = None\\n        expand_default_5: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_2, [1, 32, sym_size_3, 128])\\n        view_default_15: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_5, [32, sym_size_3, 128]);  expand_default_5 = sym_size_3 = None\\n        bmm_default_1: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_14, view_default_15);  view_default_14 = view_default_15 = None\\n        view_default_16: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_1, [1, 32, sym_size_2, 128]);  bmm_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_4: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_16, 1, 2);  view_default_16 = None\\n        clone_default: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_4, memory_format = torch.contiguous_format);  transpose_int_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_17: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default, [1, sym_size, 4096]);  clone_default = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant5 = self._param_constant5\\n        t_default_3: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant5);  _param_constant5 = None\\n        view_default_18: f32[s0, 4096] = torch.ops.aten.view.default(view_default_17, [sym_size_2, 4096]);  view_default_17 = None\\n        mm_default_3: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_18, t_default_3);  view_default_18 = t_default_3 = None\\n        view_default_19: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_3, [1, sym_size_2, 4096]);  mm_default_3 = sym_size_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_6: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(embedding_default, view_default_19);  embedding_default = view_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_1: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_6, 2)\\n        mean_dim_1: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_1, [-1], True);  pow_tensor_scalar_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_7: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_1, 1e-06);  mean_dim_1 = None\\n        rsqrt_default_1: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_7);  add_tensor_7 = None\\n        detach_default_2: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_1)\\n        mul_tensor_6: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_6, rsqrt_default_1);  rsqrt_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant6 = self._param_constant6\\n        mul_tensor_7: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant6, mul_tensor_6);  _param_constant6 = mul_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant7 = self._param_constant7\\n        t_default_4: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant7);  _param_constant7 = None\\n        view_default_20: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_7, [sym_size, 4096])\\n        mm_default_4: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_20, t_default_4);  view_default_20 = t_default_4 = None\\n        view_default_21: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_4, [1, sym_size, 11008]);  mm_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant8 = self._param_constant8\\n        t_default_5: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant8);  _param_constant8 = None\\n        view_default_22: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_7, [sym_size, 4096]);  mul_tensor_7 = None\\n        mm_default_5: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_22, t_default_5);  view_default_22 = t_default_5 = None\\n        view_default_23: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_5, [1, sym_size, 11008]);  mm_default_5 = None\\n        mul_tensor_8: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default, view_default_23);  silu_default = view_default_23 = None\\n        _param_constant9 = self._param_constant9\\n        t_default_6: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant9);  _param_constant9 = None\\n        sym_size_4: Sym(s0) = torch.ops.aten.sym_size(view_default_21, 1);  view_default_21 = None\\n        view_default_24: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_8, [sym_size_4, 11008]);  mul_tensor_8 = None\\n        mm_default_6: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_24, t_default_6);  view_default_24 = t_default_6 = None\\n        view_default_25: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_6, [1, sym_size_4, 4096]);  mm_default_6 = sym_size_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_8: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_6, view_default_25);  add_tensor_6 = view_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_2: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_8, 2)\\n        mean_dim_2: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_2, [-1], True);  pow_tensor_scalar_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_9: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_2, 1e-06);  mean_dim_2 = None\\n        rsqrt_default_2: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_9);  add_tensor_9 = None\\n        detach_default_3: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_2)\\n        mul_tensor_9: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_8, rsqrt_default_2);  rsqrt_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant10 = self._param_constant10\\n        mul_tensor_10: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant10, mul_tensor_9);  _param_constant10 = mul_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant11 = self._param_constant11\\n        t_default_7: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant11);  _param_constant11 = None\\n        view_default_26: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_10, [sym_size, 4096])\\n        mm_default_7: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_26, t_default_7);  view_default_26 = t_default_7 = None\\n        view_default_27: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_7, [1, sym_size, 4096]);  mm_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant12 = self._param_constant12\\n        t_default_8: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant12);  _param_constant12 = None\\n        view_default_28: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_10, [sym_size, 4096])\\n        mm_default_8: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_28, t_default_8);  view_default_28 = t_default_8 = None\\n        view_default_29: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_8, [1, sym_size, 4096]);  mm_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant13 = self._param_constant13\\n        t_default_9: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant13);  _param_constant13 = None\\n        view_default_30: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_10, [sym_size, 4096]);  mul_tensor_10 = None\\n        mm_default_9: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_30, t_default_9);  view_default_30 = t_default_9 = None\\n        view_default_31: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_9, [1, sym_size, 4096]);  mm_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_32: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_27, [1, sym_size, 32, 128])\\n        transpose_int_5: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_32, 1, 2);  view_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_33: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_29, [1, sym_size, 32, 128])\\n        transpose_int_6: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_33, 1, 2);  view_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_34: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_31, [1, sym_size, 32, 128])\\n        transpose_int_7: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_34, 1, 2);  view_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant2 = self._tensor_constant2\\n        slice_tensor_14: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant2, 0, 0, 9223372036854775807);  _tensor_constant2 = None\\n        slice_tensor_15: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_14, 1, 0, 9223372036854775807);  slice_tensor_14 = None\\n        sym_size_5: Sym(s0) = torch.ops.aten.sym_size(view_default_29, 1);  view_default_29 = None\\n        slice_tensor_16: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_15, 2, 0, sym_size_5);  slice_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant3 = self._tensor_constant3\\n        slice_tensor_17: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant3, 0, 0, 9223372036854775807);  _tensor_constant3 = None\\n        slice_tensor_18: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_17, 1, 0, 9223372036854775807);  slice_tensor_17 = None\\n        slice_tensor_19: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_18, 2, 0, sym_size_5);  slice_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_4: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_16, 1);  slice_tensor_16 = None\\n        squeeze_dim_5: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_4, 0);  squeeze_dim_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_6: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_19, 1);  slice_tensor_19 = None\\n        squeeze_dim_7: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_6, 0);  squeeze_dim_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_2: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_5, [view_default]);  squeeze_dim_5 = None\\n        unsqueeze_default_7: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_2, 1);  index_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_3: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_7, [view_default]);  squeeze_dim_7 = None\\n        unsqueeze_default_8: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_3, 1);  index_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_11: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_5, unsqueeze_default_7)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_20: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_21: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_5, 3, 64, 9223372036854775807);  transpose_int_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_2: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_21);  slice_tensor_21 = None\\n        cat_default_2: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_2, slice_tensor_20], -1);  neg_default_2 = slice_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_12: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_2, unsqueeze_default_8);  cat_default_2 = None\\n        add_tensor_10: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_11, mul_tensor_12);  mul_tensor_11 = mul_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_13: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_6, unsqueeze_default_7);  unsqueeze_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_22: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_23: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_6, 3, 64, 9223372036854775807);  transpose_int_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_3: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_23);  slice_tensor_23 = None\\n        cat_default_3: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_3, slice_tensor_22], -1);  neg_default_3 = slice_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_14: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_3, unsqueeze_default_8);  cat_default_3 = unsqueeze_default_8 = None\\n        add_tensor_11: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_13, mul_tensor_14);  mul_tensor_13 = mul_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_8: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_11, 2, 3)\\n        sym_size_6: Sym(s0) = torch.ops.aten.sym_size(view_default_27, 1);  view_default_27 = None\\n        expand_default_6: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_10, [1, 32, sym_size_6, 128]);  add_tensor_10 = None\\n        view_default_35: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_6, [32, sym_size_6, 128]);  expand_default_6 = None\\n        expand_default_7: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_8, [1, 32, 128, sym_size_5]);  transpose_int_8 = None\\n        view_default_36: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_7, [32, 128, sym_size_5]);  expand_default_7 = None\\n        bmm_default_2: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_35, view_default_36);  view_default_35 = view_default_36 = None\\n        view_default_37: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_2, [1, 32, sym_size_6, sym_size_5]);  bmm_default_2 = None\\n        div_tensor_1: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_37, 11.313708498984761);  view_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_12: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_1, add_tensor_1);  div_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_1: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_12, -1, False);  add_tensor_12 = None\\n        detach_default_4: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_1)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_8: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_1, [1, 32, sym_size_6, sym_size_5]);  _softmax_default_1 = None\\n        view_default_38: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_8, [32, sym_size_6, sym_size_5]);  expand_default_8 = sym_size_5 = None\\n        sym_size_7: Sym(s0) = torch.ops.aten.sym_size(view_default_31, 1);  view_default_31 = None\\n        expand_default_9: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_7, [1, 32, sym_size_7, 128])\\n        view_default_39: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_9, [32, sym_size_7, 128]);  expand_default_9 = sym_size_7 = None\\n        bmm_default_3: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_38, view_default_39);  view_default_38 = view_default_39 = None\\n        view_default_40: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_3, [1, 32, sym_size_6, 128]);  bmm_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_9: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_40, 1, 2);  view_default_40 = None\\n        clone_default_1: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_9, memory_format = torch.contiguous_format);  transpose_int_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_41: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_1, [1, sym_size, 4096]);  clone_default_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant14 = self._param_constant14\\n        t_default_10: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant14);  _param_constant14 = None\\n        view_default_42: f32[s0, 4096] = torch.ops.aten.view.default(view_default_41, [sym_size_6, 4096]);  view_default_41 = None\\n        mm_default_10: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_42, t_default_10);  view_default_42 = t_default_10 = None\\n        view_default_43: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_10, [1, sym_size_6, 4096]);  mm_default_10 = sym_size_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_13: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_8, view_default_43);  add_tensor_8 = view_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_3: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_13, 2)\\n        mean_dim_3: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_3, [-1], True);  pow_tensor_scalar_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_14: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_3, 1e-06);  mean_dim_3 = None\\n        rsqrt_default_3: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_14);  add_tensor_14 = None\\n        detach_default_5: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_3)\\n        mul_tensor_15: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_13, rsqrt_default_3);  rsqrt_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant15 = self._param_constant15\\n        mul_tensor_16: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant15, mul_tensor_15);  _param_constant15 = mul_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant16 = self._param_constant16\\n        t_default_11: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant16);  _param_constant16 = None\\n        view_default_44: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_16, [sym_size, 4096])\\n        mm_default_11: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_44, t_default_11);  view_default_44 = t_default_11 = None\\n        view_default_45: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_11, [1, sym_size, 11008]);  mm_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_1: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_45)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant17 = self._param_constant17\\n        t_default_12: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant17);  _param_constant17 = None\\n        view_default_46: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_16, [sym_size, 4096]);  mul_tensor_16 = None\\n        mm_default_12: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_46, t_default_12);  view_default_46 = t_default_12 = None\\n        view_default_47: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_12, [1, sym_size, 11008]);  mm_default_12 = None\\n        mul_tensor_17: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_1, view_default_47);  silu_default_1 = view_default_47 = None\\n        _param_constant18 = self._param_constant18\\n        t_default_13: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant18);  _param_constant18 = None\\n        sym_size_8: Sym(s0) = torch.ops.aten.sym_size(view_default_45, 1);  view_default_45 = None\\n        view_default_48: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_17, [sym_size_8, 11008]);  mul_tensor_17 = None\\n        mm_default_13: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_48, t_default_13);  view_default_48 = t_default_13 = None\\n        view_default_49: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_13, [1, sym_size_8, 4096]);  mm_default_13 = sym_size_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_15: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_13, view_default_49);  add_tensor_13 = view_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_4: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_15, 2)\\n        mean_dim_4: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_4, [-1], True);  pow_tensor_scalar_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_16: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_4, 1e-06);  mean_dim_4 = None\\n        rsqrt_default_4: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_16);  add_tensor_16 = None\\n        detach_default_6: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_4)\\n        mul_tensor_18: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_15, rsqrt_default_4);  rsqrt_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant19 = self._param_constant19\\n        mul_tensor_19: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant19, mul_tensor_18);  _param_constant19 = mul_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant20 = self._param_constant20\\n        t_default_14: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant20);  _param_constant20 = None\\n        view_default_50: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_19, [sym_size, 4096])\\n        mm_default_14: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_50, t_default_14);  view_default_50 = t_default_14 = None\\n        view_default_51: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_14, [1, sym_size, 4096]);  mm_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant21 = self._param_constant21\\n        t_default_15: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant21);  _param_constant21 = None\\n        view_default_52: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_19, [sym_size, 4096])\\n        mm_default_15: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_52, t_default_15);  view_default_52 = t_default_15 = None\\n        view_default_53: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_15, [1, sym_size, 4096]);  mm_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant22 = self._param_constant22\\n        t_default_16: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant22);  _param_constant22 = None\\n        view_default_54: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_19, [sym_size, 4096]);  mul_tensor_19 = None\\n        mm_default_16: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_54, t_default_16);  view_default_54 = t_default_16 = None\\n        view_default_55: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_16, [1, sym_size, 4096]);  mm_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_56: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_51, [1, sym_size, 32, 128])\\n        transpose_int_10: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_56, 1, 2);  view_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_57: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_53, [1, sym_size, 32, 128])\\n        transpose_int_11: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_57, 1, 2);  view_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_58: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_55, [1, sym_size, 32, 128])\\n        transpose_int_12: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_58, 1, 2);  view_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant4 = self._tensor_constant4\\n        slice_tensor_24: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant4, 0, 0, 9223372036854775807);  _tensor_constant4 = None\\n        slice_tensor_25: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_24, 1, 0, 9223372036854775807);  slice_tensor_24 = None\\n        sym_size_9: Sym(s0) = torch.ops.aten.sym_size(view_default_53, 1);  view_default_53 = None\\n        slice_tensor_26: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_25, 2, 0, sym_size_9);  slice_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant5 = self._tensor_constant5\\n        slice_tensor_27: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant5, 0, 0, 9223372036854775807);  _tensor_constant5 = None\\n        slice_tensor_28: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_27, 1, 0, 9223372036854775807);  slice_tensor_27 = None\\n        slice_tensor_29: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_28, 2, 0, sym_size_9);  slice_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_8: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_26, 1);  slice_tensor_26 = None\\n        squeeze_dim_9: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_8, 0);  squeeze_dim_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_10: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_29, 1);  slice_tensor_29 = None\\n        squeeze_dim_11: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_10, 0);  squeeze_dim_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_4: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_9, [view_default]);  squeeze_dim_9 = None\\n        unsqueeze_default_9: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_4, 1);  index_tensor_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_5: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_11, [view_default]);  squeeze_dim_11 = None\\n        unsqueeze_default_10: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_5, 1);  index_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_20: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_10, unsqueeze_default_9)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_30: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_31: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_10, 3, 64, 9223372036854775807);  transpose_int_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_4: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_31);  slice_tensor_31 = None\\n        cat_default_4: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_4, slice_tensor_30], -1);  neg_default_4 = slice_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_21: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_4, unsqueeze_default_10);  cat_default_4 = None\\n        add_tensor_17: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_20, mul_tensor_21);  mul_tensor_20 = mul_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_22: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_11, unsqueeze_default_9);  unsqueeze_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_32: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_33: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_11, 3, 64, 9223372036854775807);  transpose_int_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_5: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_33);  slice_tensor_33 = None\\n        cat_default_5: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_5, slice_tensor_32], -1);  neg_default_5 = slice_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_23: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_5, unsqueeze_default_10);  cat_default_5 = unsqueeze_default_10 = None\\n        add_tensor_18: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_22, mul_tensor_23);  mul_tensor_22 = mul_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_13: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_18, 2, 3)\\n        sym_size_10: Sym(s0) = torch.ops.aten.sym_size(view_default_51, 1);  view_default_51 = None\\n        expand_default_10: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_17, [1, 32, sym_size_10, 128]);  add_tensor_17 = None\\n        view_default_59: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_10, [32, sym_size_10, 128]);  expand_default_10 = None\\n        expand_default_11: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_13, [1, 32, 128, sym_size_9]);  transpose_int_13 = None\\n        view_default_60: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_11, [32, 128, sym_size_9]);  expand_default_11 = None\\n        bmm_default_4: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_59, view_default_60);  view_default_59 = view_default_60 = None\\n        view_default_61: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_4, [1, 32, sym_size_10, sym_size_9]);  bmm_default_4 = None\\n        div_tensor_2: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_61, 11.313708498984761);  view_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_19: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_2, add_tensor_1);  div_tensor_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_2: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_19, -1, False);  add_tensor_19 = None\\n        detach_default_7: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_2)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_12: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_2, [1, 32, sym_size_10, sym_size_9]);  _softmax_default_2 = None\\n        view_default_62: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_12, [32, sym_size_10, sym_size_9]);  expand_default_12 = sym_size_9 = None\\n        sym_size_11: Sym(s0) = torch.ops.aten.sym_size(view_default_55, 1);  view_default_55 = None\\n        expand_default_13: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_12, [1, 32, sym_size_11, 128])\\n        view_default_63: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_13, [32, sym_size_11, 128]);  expand_default_13 = sym_size_11 = None\\n        bmm_default_5: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_62, view_default_63);  view_default_62 = view_default_63 = None\\n        view_default_64: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_5, [1, 32, sym_size_10, 128]);  bmm_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_14: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_64, 1, 2);  view_default_64 = None\\n        clone_default_2: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_14, memory_format = torch.contiguous_format);  transpose_int_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_65: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_2, [1, sym_size, 4096]);  clone_default_2 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant23 = self._param_constant23\\n        t_default_17: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant23);  _param_constant23 = None\\n        view_default_66: f32[s0, 4096] = torch.ops.aten.view.default(view_default_65, [sym_size_10, 4096]);  view_default_65 = None\\n        mm_default_17: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_66, t_default_17);  view_default_66 = t_default_17 = None\\n        view_default_67: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_17, [1, sym_size_10, 4096]);  mm_default_17 = sym_size_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_20: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_15, view_default_67);  add_tensor_15 = view_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_5: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_20, 2)\\n        mean_dim_5: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_5, [-1], True);  pow_tensor_scalar_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_21: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_5, 1e-06);  mean_dim_5 = None\\n        rsqrt_default_5: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_21);  add_tensor_21 = None\\n        detach_default_8: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_5)\\n        mul_tensor_24: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_20, rsqrt_default_5);  rsqrt_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant24 = self._param_constant24\\n        mul_tensor_25: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant24, mul_tensor_24);  _param_constant24 = mul_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant25 = self._param_constant25\\n        t_default_18: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant25);  _param_constant25 = None\\n        view_default_68: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_25, [sym_size, 4096])\\n        mm_default_18: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_68, t_default_18);  view_default_68 = t_default_18 = None\\n        view_default_69: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_18, [1, sym_size, 11008]);  mm_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_2: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_69)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant26 = self._param_constant26\\n        t_default_19: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant26);  _param_constant26 = None\\n        view_default_70: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_25, [sym_size, 4096]);  mul_tensor_25 = None\\n        mm_default_19: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_70, t_default_19);  view_default_70 = t_default_19 = None\\n        view_default_71: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_19, [1, sym_size, 11008]);  mm_default_19 = None\\n        mul_tensor_26: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_2, view_default_71);  silu_default_2 = view_default_71 = None\\n        _param_constant27 = self._param_constant27\\n        t_default_20: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant27);  _param_constant27 = None\\n        sym_size_12: Sym(s0) = torch.ops.aten.sym_size(view_default_69, 1);  view_default_69 = None\\n        view_default_72: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_26, [sym_size_12, 11008]);  mul_tensor_26 = None\\n        mm_default_20: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_72, t_default_20);  view_default_72 = t_default_20 = None\\n        view_default_73: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_20, [1, sym_size_12, 4096]);  mm_default_20 = sym_size_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_22: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_20, view_default_73);  add_tensor_20 = view_default_73 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_6: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_22, 2)\\n        mean_dim_6: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_6, [-1], True);  pow_tensor_scalar_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_23: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_6, 1e-06);  mean_dim_6 = None\\n        rsqrt_default_6: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_23);  add_tensor_23 = None\\n        detach_default_9: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_6)\\n        mul_tensor_27: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_22, rsqrt_default_6);  rsqrt_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant28 = self._param_constant28\\n        mul_tensor_28: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant28, mul_tensor_27);  _param_constant28 = mul_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant29 = self._param_constant29\\n        t_default_21: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant29);  _param_constant29 = None\\n        view_default_74: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_28, [sym_size, 4096])\\n        mm_default_21: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_74, t_default_21);  view_default_74 = t_default_21 = None\\n        view_default_75: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_21, [1, sym_size, 4096]);  mm_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant30 = self._param_constant30\\n        t_default_22: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant30);  _param_constant30 = None\\n        view_default_76: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_28, [sym_size, 4096])\\n        mm_default_22: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_76, t_default_22);  view_default_76 = t_default_22 = None\\n        view_default_77: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_22, [1, sym_size, 4096]);  mm_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant31 = self._param_constant31\\n        t_default_23: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant31);  _param_constant31 = None\\n        view_default_78: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_28, [sym_size, 4096]);  mul_tensor_28 = None\\n        mm_default_23: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_78, t_default_23);  view_default_78 = t_default_23 = None\\n        view_default_79: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_23, [1, sym_size, 4096]);  mm_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_80: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_75, [1, sym_size, 32, 128])\\n        transpose_int_15: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_80, 1, 2);  view_default_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_81: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_77, [1, sym_size, 32, 128])\\n        transpose_int_16: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_81, 1, 2);  view_default_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_82: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_79, [1, sym_size, 32, 128])\\n        transpose_int_17: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_82, 1, 2);  view_default_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant6 = self._tensor_constant6\\n        slice_tensor_34: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant6, 0, 0, 9223372036854775807);  _tensor_constant6 = None\\n        slice_tensor_35: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_34, 1, 0, 9223372036854775807);  slice_tensor_34 = None\\n        sym_size_13: Sym(s0) = torch.ops.aten.sym_size(view_default_77, 1);  view_default_77 = None\\n        slice_tensor_36: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_35, 2, 0, sym_size_13);  slice_tensor_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant7 = self._tensor_constant7\\n        slice_tensor_37: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant7, 0, 0, 9223372036854775807);  _tensor_constant7 = None\\n        slice_tensor_38: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_37, 1, 0, 9223372036854775807);  slice_tensor_37 = None\\n        slice_tensor_39: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_38, 2, 0, sym_size_13);  slice_tensor_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_12: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_36, 1);  slice_tensor_36 = None\\n        squeeze_dim_13: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_12, 0);  squeeze_dim_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_14: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_39, 1);  slice_tensor_39 = None\\n        squeeze_dim_15: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_14, 0);  squeeze_dim_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_6: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_13, [view_default]);  squeeze_dim_13 = None\\n        unsqueeze_default_11: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_6, 1);  index_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_7: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_15, [view_default]);  squeeze_dim_15 = None\\n        unsqueeze_default_12: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_7, 1);  index_tensor_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_29: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_15, unsqueeze_default_11)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_40: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_41: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_15, 3, 64, 9223372036854775807);  transpose_int_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_6: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_41);  slice_tensor_41 = None\\n        cat_default_6: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_6, slice_tensor_40], -1);  neg_default_6 = slice_tensor_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_30: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_6, unsqueeze_default_12);  cat_default_6 = None\\n        add_tensor_24: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_29, mul_tensor_30);  mul_tensor_29 = mul_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_31: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_16, unsqueeze_default_11);  unsqueeze_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_42: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_43: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_16, 3, 64, 9223372036854775807);  transpose_int_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_7: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_43);  slice_tensor_43 = None\\n        cat_default_7: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_7, slice_tensor_42], -1);  neg_default_7 = slice_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_32: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_7, unsqueeze_default_12);  cat_default_7 = unsqueeze_default_12 = None\\n        add_tensor_25: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_31, mul_tensor_32);  mul_tensor_31 = mul_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_18: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_25, 2, 3)\\n        sym_size_14: Sym(s0) = torch.ops.aten.sym_size(view_default_75, 1);  view_default_75 = None\\n        expand_default_14: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_24, [1, 32, sym_size_14, 128]);  add_tensor_24 = None\\n        view_default_83: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_14, [32, sym_size_14, 128]);  expand_default_14 = None\\n        expand_default_15: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_18, [1, 32, 128, sym_size_13]);  transpose_int_18 = None\\n        view_default_84: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_15, [32, 128, sym_size_13]);  expand_default_15 = None\\n        bmm_default_6: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_83, view_default_84);  view_default_83 = view_default_84 = None\\n        view_default_85: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_6, [1, 32, sym_size_14, sym_size_13]);  bmm_default_6 = None\\n        div_tensor_3: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_85, 11.313708498984761);  view_default_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_26: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_3, add_tensor_1);  div_tensor_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_3: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_26, -1, False);  add_tensor_26 = None\\n        detach_default_10: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_3)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_16: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_3, [1, 32, sym_size_14, sym_size_13]);  _softmax_default_3 = None\\n        view_default_86: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_16, [32, sym_size_14, sym_size_13]);  expand_default_16 = sym_size_13 = None\\n        sym_size_15: Sym(s0) = torch.ops.aten.sym_size(view_default_79, 1);  view_default_79 = None\\n        expand_default_17: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_17, [1, 32, sym_size_15, 128])\\n        view_default_87: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_17, [32, sym_size_15, 128]);  expand_default_17 = sym_size_15 = None\\n        bmm_default_7: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_86, view_default_87);  view_default_86 = view_default_87 = None\\n        view_default_88: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_7, [1, 32, sym_size_14, 128]);  bmm_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_19: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_88, 1, 2);  view_default_88 = None\\n        clone_default_3: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_19, memory_format = torch.contiguous_format);  transpose_int_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_89: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_3, [1, sym_size, 4096]);  clone_default_3 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant32 = self._param_constant32\\n        t_default_24: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant32);  _param_constant32 = None\\n        view_default_90: f32[s0, 4096] = torch.ops.aten.view.default(view_default_89, [sym_size_14, 4096]);  view_default_89 = None\\n        mm_default_24: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_90, t_default_24);  view_default_90 = t_default_24 = None\\n        view_default_91: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_24, [1, sym_size_14, 4096]);  mm_default_24 = sym_size_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_27: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_22, view_default_91);  add_tensor_22 = view_default_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_7: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_27, 2)\\n        mean_dim_7: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_7, [-1], True);  pow_tensor_scalar_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_28: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_7, 1e-06);  mean_dim_7 = None\\n        rsqrt_default_7: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_28);  add_tensor_28 = None\\n        detach_default_11: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_7)\\n        mul_tensor_33: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_27, rsqrt_default_7);  rsqrt_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant33 = self._param_constant33\\n        mul_tensor_34: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant33, mul_tensor_33);  _param_constant33 = mul_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant34 = self._param_constant34\\n        t_default_25: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant34);  _param_constant34 = None\\n        view_default_92: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_34, [sym_size, 4096])\\n        mm_default_25: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_92, t_default_25);  view_default_92 = t_default_25 = None\\n        view_default_93: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_25, [1, sym_size, 11008]);  mm_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_3: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_93)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant35 = self._param_constant35\\n        t_default_26: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant35);  _param_constant35 = None\\n        view_default_94: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_34, [sym_size, 4096]);  mul_tensor_34 = None\\n        mm_default_26: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_94, t_default_26);  view_default_94 = t_default_26 = None\\n        view_default_95: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_26, [1, sym_size, 11008]);  mm_default_26 = None\\n        mul_tensor_35: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_3, view_default_95);  silu_default_3 = view_default_95 = None\\n        _param_constant36 = self._param_constant36\\n        t_default_27: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant36);  _param_constant36 = None\\n        sym_size_16: Sym(s0) = torch.ops.aten.sym_size(view_default_93, 1);  view_default_93 = None\\n        view_default_96: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_35, [sym_size_16, 11008]);  mul_tensor_35 = None\\n        mm_default_27: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_96, t_default_27);  view_default_96 = t_default_27 = None\\n        view_default_97: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_27, [1, sym_size_16, 4096]);  mm_default_27 = sym_size_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_29: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_27, view_default_97);  add_tensor_27 = view_default_97 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_8: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_29, 2)\\n        mean_dim_8: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_8, [-1], True);  pow_tensor_scalar_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_30: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_8, 1e-06);  mean_dim_8 = None\\n        rsqrt_default_8: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_30);  add_tensor_30 = None\\n        detach_default_12: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_8)\\n        mul_tensor_36: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_29, rsqrt_default_8);  rsqrt_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant37 = self._param_constant37\\n        mul_tensor_37: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant37, mul_tensor_36);  _param_constant37 = mul_tensor_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant38 = self._param_constant38\\n        t_default_28: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant38);  _param_constant38 = None\\n        view_default_98: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_37, [sym_size, 4096])\\n        mm_default_28: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_98, t_default_28);  view_default_98 = t_default_28 = None\\n        view_default_99: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_28, [1, sym_size, 4096]);  mm_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant39 = self._param_constant39\\n        t_default_29: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant39);  _param_constant39 = None\\n        view_default_100: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_37, [sym_size, 4096])\\n        mm_default_29: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_100, t_default_29);  view_default_100 = t_default_29 = None\\n        view_default_101: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_29, [1, sym_size, 4096]);  mm_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant40 = self._param_constant40\\n        t_default_30: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant40);  _param_constant40 = None\\n        view_default_102: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_37, [sym_size, 4096]);  mul_tensor_37 = None\\n        mm_default_30: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_102, t_default_30);  view_default_102 = t_default_30 = None\\n        view_default_103: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_30, [1, sym_size, 4096]);  mm_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_104: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_99, [1, sym_size, 32, 128])\\n        transpose_int_20: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_104, 1, 2);  view_default_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_105: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_101, [1, sym_size, 32, 128])\\n        transpose_int_21: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_105, 1, 2);  view_default_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_106: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_103, [1, sym_size, 32, 128])\\n        transpose_int_22: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_106, 1, 2);  view_default_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant8 = self._tensor_constant8\\n        slice_tensor_44: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant8, 0, 0, 9223372036854775807);  _tensor_constant8 = None\\n        slice_tensor_45: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_44, 1, 0, 9223372036854775807);  slice_tensor_44 = None\\n        sym_size_17: Sym(s0) = torch.ops.aten.sym_size(view_default_101, 1);  view_default_101 = None\\n        slice_tensor_46: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_45, 2, 0, sym_size_17);  slice_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant9 = self._tensor_constant9\\n        slice_tensor_47: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant9, 0, 0, 9223372036854775807);  _tensor_constant9 = None\\n        slice_tensor_48: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_47, 1, 0, 9223372036854775807);  slice_tensor_47 = None\\n        slice_tensor_49: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_48, 2, 0, sym_size_17);  slice_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_16: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_46, 1);  slice_tensor_46 = None\\n        squeeze_dim_17: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_16, 0);  squeeze_dim_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_18: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_49, 1);  slice_tensor_49 = None\\n        squeeze_dim_19: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_18, 0);  squeeze_dim_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_8: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_17, [view_default]);  squeeze_dim_17 = None\\n        unsqueeze_default_13: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_8, 1);  index_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_9: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_19, [view_default]);  squeeze_dim_19 = None\\n        unsqueeze_default_14: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_9, 1);  index_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_38: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_20, unsqueeze_default_13)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_50: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_51: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_20, 3, 64, 9223372036854775807);  transpose_int_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_8: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_51);  slice_tensor_51 = None\\n        cat_default_8: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_8, slice_tensor_50], -1);  neg_default_8 = slice_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_39: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_8, unsqueeze_default_14);  cat_default_8 = None\\n        add_tensor_31: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_38, mul_tensor_39);  mul_tensor_38 = mul_tensor_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_40: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_21, unsqueeze_default_13);  unsqueeze_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_52: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_53: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_21, 3, 64, 9223372036854775807);  transpose_int_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_9: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_53);  slice_tensor_53 = None\\n        cat_default_9: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_9, slice_tensor_52], -1);  neg_default_9 = slice_tensor_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_41: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_9, unsqueeze_default_14);  cat_default_9 = unsqueeze_default_14 = None\\n        add_tensor_32: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_40, mul_tensor_41);  mul_tensor_40 = mul_tensor_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_23: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_32, 2, 3)\\n        sym_size_18: Sym(s0) = torch.ops.aten.sym_size(view_default_99, 1);  view_default_99 = None\\n        expand_default_18: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_31, [1, 32, sym_size_18, 128]);  add_tensor_31 = None\\n        view_default_107: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_18, [32, sym_size_18, 128]);  expand_default_18 = None\\n        expand_default_19: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_23, [1, 32, 128, sym_size_17]);  transpose_int_23 = None\\n        view_default_108: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_19, [32, 128, sym_size_17]);  expand_default_19 = None\\n        bmm_default_8: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_107, view_default_108);  view_default_107 = view_default_108 = None\\n        view_default_109: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_8, [1, 32, sym_size_18, sym_size_17]);  bmm_default_8 = None\\n        div_tensor_4: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_109, 11.313708498984761);  view_default_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_33: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_4, add_tensor_1);  div_tensor_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_4: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_33, -1, False);  add_tensor_33 = None\\n        detach_default_13: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_4)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_20: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_4, [1, 32, sym_size_18, sym_size_17]);  _softmax_default_4 = None\\n        view_default_110: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_20, [32, sym_size_18, sym_size_17]);  expand_default_20 = sym_size_17 = None\\n        sym_size_19: Sym(s0) = torch.ops.aten.sym_size(view_default_103, 1);  view_default_103 = None\\n        expand_default_21: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_22, [1, 32, sym_size_19, 128])\\n        view_default_111: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_21, [32, sym_size_19, 128]);  expand_default_21 = sym_size_19 = None\\n        bmm_default_9: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_110, view_default_111);  view_default_110 = view_default_111 = None\\n        view_default_112: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_9, [1, 32, sym_size_18, 128]);  bmm_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_24: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_112, 1, 2);  view_default_112 = None\\n        clone_default_4: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_24, memory_format = torch.contiguous_format);  transpose_int_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_113: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_4, [1, sym_size, 4096]);  clone_default_4 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant41 = self._param_constant41\\n        t_default_31: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant41);  _param_constant41 = None\\n        view_default_114: f32[s0, 4096] = torch.ops.aten.view.default(view_default_113, [sym_size_18, 4096]);  view_default_113 = None\\n        mm_default_31: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_114, t_default_31);  view_default_114 = t_default_31 = None\\n        view_default_115: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_31, [1, sym_size_18, 4096]);  mm_default_31 = sym_size_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_34: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_29, view_default_115);  add_tensor_29 = view_default_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_9: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_34, 2)\\n        mean_dim_9: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_9, [-1], True);  pow_tensor_scalar_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_35: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_9, 1e-06);  mean_dim_9 = None\\n        rsqrt_default_9: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_35);  add_tensor_35 = None\\n        detach_default_14: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_9)\\n        mul_tensor_42: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_34, rsqrt_default_9);  rsqrt_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant42 = self._param_constant42\\n        mul_tensor_43: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant42, mul_tensor_42);  _param_constant42 = mul_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant43 = self._param_constant43\\n        t_default_32: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant43);  _param_constant43 = None\\n        view_default_116: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_43, [sym_size, 4096])\\n        mm_default_32: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_116, t_default_32);  view_default_116 = t_default_32 = None\\n        view_default_117: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_32, [1, sym_size, 11008]);  mm_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_4: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_117)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant44 = self._param_constant44\\n        t_default_33: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant44);  _param_constant44 = None\\n        view_default_118: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_43, [sym_size, 4096]);  mul_tensor_43 = None\\n        mm_default_33: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_118, t_default_33);  view_default_118 = t_default_33 = None\\n        view_default_119: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_33, [1, sym_size, 11008]);  mm_default_33 = None\\n        mul_tensor_44: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_4, view_default_119);  silu_default_4 = view_default_119 = None\\n        _param_constant45 = self._param_constant45\\n        t_default_34: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant45);  _param_constant45 = None\\n        sym_size_20: Sym(s0) = torch.ops.aten.sym_size(view_default_117, 1);  view_default_117 = None\\n        view_default_120: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_44, [sym_size_20, 11008]);  mul_tensor_44 = None\\n        mm_default_34: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_120, t_default_34);  view_default_120 = t_default_34 = None\\n        view_default_121: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_34, [1, sym_size_20, 4096]);  mm_default_34 = sym_size_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_36: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_34, view_default_121);  add_tensor_34 = view_default_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_10: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_36, 2)\\n        mean_dim_10: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_10, [-1], True);  pow_tensor_scalar_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_37: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_10, 1e-06);  mean_dim_10 = None\\n        rsqrt_default_10: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_37);  add_tensor_37 = None\\n        detach_default_15: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_10)\\n        mul_tensor_45: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_36, rsqrt_default_10);  rsqrt_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant46 = self._param_constant46\\n        mul_tensor_46: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant46, mul_tensor_45);  _param_constant46 = mul_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant47 = self._param_constant47\\n        t_default_35: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant47);  _param_constant47 = None\\n        view_default_122: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_46, [sym_size, 4096])\\n        mm_default_35: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_122, t_default_35);  view_default_122 = t_default_35 = None\\n        view_default_123: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_35, [1, sym_size, 4096]);  mm_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant48 = self._param_constant48\\n        t_default_36: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant48);  _param_constant48 = None\\n        view_default_124: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_46, [sym_size, 4096])\\n        mm_default_36: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_124, t_default_36);  view_default_124 = t_default_36 = None\\n        view_default_125: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_36, [1, sym_size, 4096]);  mm_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant49 = self._param_constant49\\n        t_default_37: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant49);  _param_constant49 = None\\n        view_default_126: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_46, [sym_size, 4096]);  mul_tensor_46 = None\\n        mm_default_37: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_126, t_default_37);  view_default_126 = t_default_37 = None\\n        view_default_127: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_37, [1, sym_size, 4096]);  mm_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_128: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_123, [1, sym_size, 32, 128])\\n        transpose_int_25: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_128, 1, 2);  view_default_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_129: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_125, [1, sym_size, 32, 128])\\n        transpose_int_26: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_129, 1, 2);  view_default_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_130: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_127, [1, sym_size, 32, 128])\\n        transpose_int_27: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_130, 1, 2);  view_default_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant10 = self._tensor_constant10\\n        slice_tensor_54: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant10, 0, 0, 9223372036854775807);  _tensor_constant10 = None\\n        slice_tensor_55: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_54, 1, 0, 9223372036854775807);  slice_tensor_54 = None\\n        sym_size_21: Sym(s0) = torch.ops.aten.sym_size(view_default_125, 1);  view_default_125 = None\\n        slice_tensor_56: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_55, 2, 0, sym_size_21);  slice_tensor_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant11 = self._tensor_constant11\\n        slice_tensor_57: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant11, 0, 0, 9223372036854775807);  _tensor_constant11 = None\\n        slice_tensor_58: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_57, 1, 0, 9223372036854775807);  slice_tensor_57 = None\\n        slice_tensor_59: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_58, 2, 0, sym_size_21);  slice_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_20: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_56, 1);  slice_tensor_56 = None\\n        squeeze_dim_21: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_20, 0);  squeeze_dim_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_22: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_59, 1);  slice_tensor_59 = None\\n        squeeze_dim_23: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_22, 0);  squeeze_dim_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_10: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_21, [view_default]);  squeeze_dim_21 = None\\n        unsqueeze_default_15: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_10, 1);  index_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_11: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_23, [view_default]);  squeeze_dim_23 = None\\n        unsqueeze_default_16: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_11, 1);  index_tensor_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_47: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_25, unsqueeze_default_15)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_60: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_61: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_25, 3, 64, 9223372036854775807);  transpose_int_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_10: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_61);  slice_tensor_61 = None\\n        cat_default_10: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_10, slice_tensor_60], -1);  neg_default_10 = slice_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_48: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_10, unsqueeze_default_16);  cat_default_10 = None\\n        add_tensor_38: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_47, mul_tensor_48);  mul_tensor_47 = mul_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_49: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_26, unsqueeze_default_15);  unsqueeze_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_62: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_63: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_26, 3, 64, 9223372036854775807);  transpose_int_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_11: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_63);  slice_tensor_63 = None\\n        cat_default_11: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_11, slice_tensor_62], -1);  neg_default_11 = slice_tensor_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_50: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_11, unsqueeze_default_16);  cat_default_11 = unsqueeze_default_16 = None\\n        add_tensor_39: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_49, mul_tensor_50);  mul_tensor_49 = mul_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_28: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_39, 2, 3)\\n        sym_size_22: Sym(s0) = torch.ops.aten.sym_size(view_default_123, 1);  view_default_123 = None\\n        expand_default_22: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_38, [1, 32, sym_size_22, 128]);  add_tensor_38 = None\\n        view_default_131: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_22, [32, sym_size_22, 128]);  expand_default_22 = None\\n        expand_default_23: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_28, [1, 32, 128, sym_size_21]);  transpose_int_28 = None\\n        view_default_132: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_23, [32, 128, sym_size_21]);  expand_default_23 = None\\n        bmm_default_10: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_131, view_default_132);  view_default_131 = view_default_132 = None\\n        view_default_133: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_10, [1, 32, sym_size_22, sym_size_21]);  bmm_default_10 = None\\n        div_tensor_5: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_133, 11.313708498984761);  view_default_133 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_40: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_5, add_tensor_1);  div_tensor_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_5: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_40, -1, False);  add_tensor_40 = None\\n        detach_default_16: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_5)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_24: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_5, [1, 32, sym_size_22, sym_size_21]);  _softmax_default_5 = None\\n        view_default_134: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_24, [32, sym_size_22, sym_size_21]);  expand_default_24 = sym_size_21 = None\\n        sym_size_23: Sym(s0) = torch.ops.aten.sym_size(view_default_127, 1);  view_default_127 = None\\n        expand_default_25: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_27, [1, 32, sym_size_23, 128])\\n        view_default_135: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_25, [32, sym_size_23, 128]);  expand_default_25 = sym_size_23 = None\\n        bmm_default_11: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_134, view_default_135);  view_default_134 = view_default_135 = None\\n        view_default_136: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_11, [1, 32, sym_size_22, 128]);  bmm_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_29: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_136, 1, 2);  view_default_136 = None\\n        clone_default_5: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_29, memory_format = torch.contiguous_format);  transpose_int_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_137: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_5, [1, sym_size, 4096]);  clone_default_5 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant50 = self._param_constant50\\n        t_default_38: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant50);  _param_constant50 = None\\n        view_default_138: f32[s0, 4096] = torch.ops.aten.view.default(view_default_137, [sym_size_22, 4096]);  view_default_137 = None\\n        mm_default_38: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_138, t_default_38);  view_default_138 = t_default_38 = None\\n        view_default_139: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_38, [1, sym_size_22, 4096]);  mm_default_38 = sym_size_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_41: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_36, view_default_139);  add_tensor_36 = view_default_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_11: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_41, 2)\\n        mean_dim_11: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_11, [-1], True);  pow_tensor_scalar_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_42: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_11, 1e-06);  mean_dim_11 = None\\n        rsqrt_default_11: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_42);  add_tensor_42 = None\\n        detach_default_17: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_11)\\n        mul_tensor_51: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_41, rsqrt_default_11);  rsqrt_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant51 = self._param_constant51\\n        mul_tensor_52: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant51, mul_tensor_51);  _param_constant51 = mul_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant52 = self._param_constant52\\n        t_default_39: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant52);  _param_constant52 = None\\n        view_default_140: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_52, [sym_size, 4096])\\n        mm_default_39: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_140, t_default_39);  view_default_140 = t_default_39 = None\\n        view_default_141: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_39, [1, sym_size, 11008]);  mm_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_5: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_141)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant53 = self._param_constant53\\n        t_default_40: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant53);  _param_constant53 = None\\n        view_default_142: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_52, [sym_size, 4096]);  mul_tensor_52 = None\\n        mm_default_40: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_142, t_default_40);  view_default_142 = t_default_40 = None\\n        view_default_143: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_40, [1, sym_size, 11008]);  mm_default_40 = None\\n        mul_tensor_53: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_5, view_default_143);  silu_default_5 = view_default_143 = None\\n        _param_constant54 = self._param_constant54\\n        t_default_41: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant54);  _param_constant54 = None\\n        sym_size_24: Sym(s0) = torch.ops.aten.sym_size(view_default_141, 1);  view_default_141 = None\\n        view_default_144: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_53, [sym_size_24, 11008]);  mul_tensor_53 = None\\n        mm_default_41: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_144, t_default_41);  view_default_144 = t_default_41 = None\\n        view_default_145: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_41, [1, sym_size_24, 4096]);  mm_default_41 = sym_size_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_43: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_41, view_default_145);  add_tensor_41 = view_default_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_12: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_43, 2)\\n        mean_dim_12: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_12, [-1], True);  pow_tensor_scalar_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_44: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_12, 1e-06);  mean_dim_12 = None\\n        rsqrt_default_12: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_44);  add_tensor_44 = None\\n        detach_default_18: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_12)\\n        mul_tensor_54: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_43, rsqrt_default_12);  rsqrt_default_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant55 = self._param_constant55\\n        mul_tensor_55: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant55, mul_tensor_54);  _param_constant55 = mul_tensor_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant56 = self._param_constant56\\n        t_default_42: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant56);  _param_constant56 = None\\n        view_default_146: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_55, [sym_size, 4096])\\n        mm_default_42: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_146, t_default_42);  view_default_146 = t_default_42 = None\\n        view_default_147: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_42, [1, sym_size, 4096]);  mm_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant57 = self._param_constant57\\n        t_default_43: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant57);  _param_constant57 = None\\n        view_default_148: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_55, [sym_size, 4096])\\n        mm_default_43: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_148, t_default_43);  view_default_148 = t_default_43 = None\\n        view_default_149: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_43, [1, sym_size, 4096]);  mm_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant58 = self._param_constant58\\n        t_default_44: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant58);  _param_constant58 = None\\n        view_default_150: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_55, [sym_size, 4096]);  mul_tensor_55 = None\\n        mm_default_44: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_150, t_default_44);  view_default_150 = t_default_44 = None\\n        view_default_151: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_44, [1, sym_size, 4096]);  mm_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_152: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_147, [1, sym_size, 32, 128])\\n        transpose_int_30: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_152, 1, 2);  view_default_152 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_153: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_149, [1, sym_size, 32, 128])\\n        transpose_int_31: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_153, 1, 2);  view_default_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_154: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_151, [1, sym_size, 32, 128])\\n        transpose_int_32: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_154, 1, 2);  view_default_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant12 = self._tensor_constant12\\n        slice_tensor_64: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant12, 0, 0, 9223372036854775807);  _tensor_constant12 = None\\n        slice_tensor_65: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_64, 1, 0, 9223372036854775807);  slice_tensor_64 = None\\n        sym_size_25: Sym(s0) = torch.ops.aten.sym_size(view_default_149, 1);  view_default_149 = None\\n        slice_tensor_66: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_65, 2, 0, sym_size_25);  slice_tensor_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant13 = self._tensor_constant13\\n        slice_tensor_67: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant13, 0, 0, 9223372036854775807);  _tensor_constant13 = None\\n        slice_tensor_68: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_67, 1, 0, 9223372036854775807);  slice_tensor_67 = None\\n        slice_tensor_69: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_68, 2, 0, sym_size_25);  slice_tensor_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_24: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_66, 1);  slice_tensor_66 = None\\n        squeeze_dim_25: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_24, 0);  squeeze_dim_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_26: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_69, 1);  slice_tensor_69 = None\\n        squeeze_dim_27: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_26, 0);  squeeze_dim_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_12: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_25, [view_default]);  squeeze_dim_25 = None\\n        unsqueeze_default_17: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_12, 1);  index_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_13: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_27, [view_default]);  squeeze_dim_27 = None\\n        unsqueeze_default_18: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_13, 1);  index_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_56: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_30, unsqueeze_default_17)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_70: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_71: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_30, 3, 64, 9223372036854775807);  transpose_int_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_12: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_71);  slice_tensor_71 = None\\n        cat_default_12: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_12, slice_tensor_70], -1);  neg_default_12 = slice_tensor_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_57: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_12, unsqueeze_default_18);  cat_default_12 = None\\n        add_tensor_45: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_56, mul_tensor_57);  mul_tensor_56 = mul_tensor_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_58: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_31, unsqueeze_default_17);  unsqueeze_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_72: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_73: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_31, 3, 64, 9223372036854775807);  transpose_int_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_13: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_73);  slice_tensor_73 = None\\n        cat_default_13: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_13, slice_tensor_72], -1);  neg_default_13 = slice_tensor_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_59: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_13, unsqueeze_default_18);  cat_default_13 = unsqueeze_default_18 = None\\n        add_tensor_46: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_58, mul_tensor_59);  mul_tensor_58 = mul_tensor_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_33: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_46, 2, 3)\\n        sym_size_26: Sym(s0) = torch.ops.aten.sym_size(view_default_147, 1);  view_default_147 = None\\n        expand_default_26: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_45, [1, 32, sym_size_26, 128]);  add_tensor_45 = None\\n        view_default_155: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_26, [32, sym_size_26, 128]);  expand_default_26 = None\\n        expand_default_27: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_33, [1, 32, 128, sym_size_25]);  transpose_int_33 = None\\n        view_default_156: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_27, [32, 128, sym_size_25]);  expand_default_27 = None\\n        bmm_default_12: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_155, view_default_156);  view_default_155 = view_default_156 = None\\n        view_default_157: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_12, [1, 32, sym_size_26, sym_size_25]);  bmm_default_12 = None\\n        div_tensor_6: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_157, 11.313708498984761);  view_default_157 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_47: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_6, add_tensor_1);  div_tensor_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_6: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_47, -1, False);  add_tensor_47 = None\\n        detach_default_19: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_6)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_28: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_6, [1, 32, sym_size_26, sym_size_25]);  _softmax_default_6 = None\\n        view_default_158: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_28, [32, sym_size_26, sym_size_25]);  expand_default_28 = sym_size_25 = None\\n        sym_size_27: Sym(s0) = torch.ops.aten.sym_size(view_default_151, 1);  view_default_151 = None\\n        expand_default_29: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_32, [1, 32, sym_size_27, 128])\\n        view_default_159: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_29, [32, sym_size_27, 128]);  expand_default_29 = sym_size_27 = None\\n        bmm_default_13: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_158, view_default_159);  view_default_158 = view_default_159 = None\\n        view_default_160: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_13, [1, 32, sym_size_26, 128]);  bmm_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_34: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_160, 1, 2);  view_default_160 = None\\n        clone_default_6: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_34, memory_format = torch.contiguous_format);  transpose_int_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_161: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_6, [1, sym_size, 4096]);  clone_default_6 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant59 = self._param_constant59\\n        t_default_45: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant59);  _param_constant59 = None\\n        view_default_162: f32[s0, 4096] = torch.ops.aten.view.default(view_default_161, [sym_size_26, 4096]);  view_default_161 = None\\n        mm_default_45: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_162, t_default_45);  view_default_162 = t_default_45 = None\\n        view_default_163: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_45, [1, sym_size_26, 4096]);  mm_default_45 = sym_size_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_48: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_43, view_default_163);  add_tensor_43 = view_default_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_13: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_48, 2)\\n        mean_dim_13: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_13, [-1], True);  pow_tensor_scalar_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_49: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_13, 1e-06);  mean_dim_13 = None\\n        rsqrt_default_13: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_49);  add_tensor_49 = None\\n        detach_default_20: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_13)\\n        mul_tensor_60: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_48, rsqrt_default_13);  rsqrt_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant60 = self._param_constant60\\n        mul_tensor_61: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant60, mul_tensor_60);  _param_constant60 = mul_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant61 = self._param_constant61\\n        t_default_46: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant61);  _param_constant61 = None\\n        view_default_164: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_61, [sym_size, 4096])\\n        mm_default_46: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_164, t_default_46);  view_default_164 = t_default_46 = None\\n        view_default_165: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_46, [1, sym_size, 11008]);  mm_default_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_6: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_165)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant62 = self._param_constant62\\n        t_default_47: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant62);  _param_constant62 = None\\n        view_default_166: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_61, [sym_size, 4096]);  mul_tensor_61 = None\\n        mm_default_47: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_166, t_default_47);  view_default_166 = t_default_47 = None\\n        view_default_167: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_47, [1, sym_size, 11008]);  mm_default_47 = None\\n        mul_tensor_62: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_6, view_default_167);  silu_default_6 = view_default_167 = None\\n        _param_constant63 = self._param_constant63\\n        t_default_48: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant63);  _param_constant63 = None\\n        sym_size_28: Sym(s0) = torch.ops.aten.sym_size(view_default_165, 1);  view_default_165 = None\\n        view_default_168: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_62, [sym_size_28, 11008]);  mul_tensor_62 = None\\n        mm_default_48: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_168, t_default_48);  view_default_168 = t_default_48 = None\\n        view_default_169: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_48, [1, sym_size_28, 4096]);  mm_default_48 = sym_size_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_50: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_48, view_default_169);  add_tensor_48 = view_default_169 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_14: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_50, 2)\\n        mean_dim_14: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_14, [-1], True);  pow_tensor_scalar_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_51: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_14, 1e-06);  mean_dim_14 = None\\n        rsqrt_default_14: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_51);  add_tensor_51 = None\\n        detach_default_21: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_14)\\n        mul_tensor_63: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_50, rsqrt_default_14);  rsqrt_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant64 = self._param_constant64\\n        mul_tensor_64: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant64, mul_tensor_63);  _param_constant64 = mul_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant65 = self._param_constant65\\n        t_default_49: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant65);  _param_constant65 = None\\n        view_default_170: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_64, [sym_size, 4096])\\n        mm_default_49: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_170, t_default_49);  view_default_170 = t_default_49 = None\\n        view_default_171: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_49, [1, sym_size, 4096]);  mm_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant66 = self._param_constant66\\n        t_default_50: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant66);  _param_constant66 = None\\n        view_default_172: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_64, [sym_size, 4096])\\n        mm_default_50: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_172, t_default_50);  view_default_172 = t_default_50 = None\\n        view_default_173: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_50, [1, sym_size, 4096]);  mm_default_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant67 = self._param_constant67\\n        t_default_51: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant67);  _param_constant67 = None\\n        view_default_174: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_64, [sym_size, 4096]);  mul_tensor_64 = None\\n        mm_default_51: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_174, t_default_51);  view_default_174 = t_default_51 = None\\n        view_default_175: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_51, [1, sym_size, 4096]);  mm_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_176: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_171, [1, sym_size, 32, 128])\\n        transpose_int_35: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_176, 1, 2);  view_default_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_177: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_173, [1, sym_size, 32, 128])\\n        transpose_int_36: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_177, 1, 2);  view_default_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_178: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_175, [1, sym_size, 32, 128])\\n        transpose_int_37: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_178, 1, 2);  view_default_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant14 = self._tensor_constant14\\n        slice_tensor_74: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant14, 0, 0, 9223372036854775807);  _tensor_constant14 = None\\n        slice_tensor_75: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_74, 1, 0, 9223372036854775807);  slice_tensor_74 = None\\n        sym_size_29: Sym(s0) = torch.ops.aten.sym_size(view_default_173, 1);  view_default_173 = None\\n        slice_tensor_76: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_75, 2, 0, sym_size_29);  slice_tensor_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant15 = self._tensor_constant15\\n        slice_tensor_77: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant15, 0, 0, 9223372036854775807);  _tensor_constant15 = None\\n        slice_tensor_78: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_77, 1, 0, 9223372036854775807);  slice_tensor_77 = None\\n        slice_tensor_79: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_78, 2, 0, sym_size_29);  slice_tensor_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_28: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_76, 1);  slice_tensor_76 = None\\n        squeeze_dim_29: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_28, 0);  squeeze_dim_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_30: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_79, 1);  slice_tensor_79 = None\\n        squeeze_dim_31: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_30, 0);  squeeze_dim_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_14: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_29, [view_default]);  squeeze_dim_29 = None\\n        unsqueeze_default_19: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_14, 1);  index_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_15: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_31, [view_default]);  squeeze_dim_31 = None\\n        unsqueeze_default_20: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_15, 1);  index_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_65: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_35, unsqueeze_default_19)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_80: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_81: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_35, 3, 64, 9223372036854775807);  transpose_int_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_14: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_81);  slice_tensor_81 = None\\n        cat_default_14: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_14, slice_tensor_80], -1);  neg_default_14 = slice_tensor_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_66: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_14, unsqueeze_default_20);  cat_default_14 = None\\n        add_tensor_52: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_65, mul_tensor_66);  mul_tensor_65 = mul_tensor_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_67: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_36, unsqueeze_default_19);  unsqueeze_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_82: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_83: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_36, 3, 64, 9223372036854775807);  transpose_int_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_15: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_83);  slice_tensor_83 = None\\n        cat_default_15: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_15, slice_tensor_82], -1);  neg_default_15 = slice_tensor_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_68: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_15, unsqueeze_default_20);  cat_default_15 = unsqueeze_default_20 = None\\n        add_tensor_53: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_67, mul_tensor_68);  mul_tensor_67 = mul_tensor_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_38: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_53, 2, 3)\\n        sym_size_30: Sym(s0) = torch.ops.aten.sym_size(view_default_171, 1);  view_default_171 = None\\n        expand_default_30: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_52, [1, 32, sym_size_30, 128]);  add_tensor_52 = None\\n        view_default_179: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_30, [32, sym_size_30, 128]);  expand_default_30 = None\\n        expand_default_31: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_38, [1, 32, 128, sym_size_29]);  transpose_int_38 = None\\n        view_default_180: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_31, [32, 128, sym_size_29]);  expand_default_31 = None\\n        bmm_default_14: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_179, view_default_180);  view_default_179 = view_default_180 = None\\n        view_default_181: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_14, [1, 32, sym_size_30, sym_size_29]);  bmm_default_14 = None\\n        div_tensor_7: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_181, 11.313708498984761);  view_default_181 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_54: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_7, add_tensor_1);  div_tensor_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_7: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_54, -1, False);  add_tensor_54 = None\\n        detach_default_22: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_7)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_32: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_7, [1, 32, sym_size_30, sym_size_29]);  _softmax_default_7 = None\\n        view_default_182: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_32, [32, sym_size_30, sym_size_29]);  expand_default_32 = sym_size_29 = None\\n        sym_size_31: Sym(s0) = torch.ops.aten.sym_size(view_default_175, 1);  view_default_175 = None\\n        expand_default_33: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_37, [1, 32, sym_size_31, 128])\\n        view_default_183: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_33, [32, sym_size_31, 128]);  expand_default_33 = sym_size_31 = None\\n        bmm_default_15: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_182, view_default_183);  view_default_182 = view_default_183 = None\\n        view_default_184: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_15, [1, 32, sym_size_30, 128]);  bmm_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_39: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_184, 1, 2);  view_default_184 = None\\n        clone_default_7: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_39, memory_format = torch.contiguous_format);  transpose_int_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_185: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_7, [1, sym_size, 4096]);  clone_default_7 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant68 = self._param_constant68\\n        t_default_52: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant68);  _param_constant68 = None\\n        view_default_186: f32[s0, 4096] = torch.ops.aten.view.default(view_default_185, [sym_size_30, 4096]);  view_default_185 = None\\n        mm_default_52: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_186, t_default_52);  view_default_186 = t_default_52 = None\\n        view_default_187: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_52, [1, sym_size_30, 4096]);  mm_default_52 = sym_size_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_55: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_50, view_default_187);  add_tensor_50 = view_default_187 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_15: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_55, 2)\\n        mean_dim_15: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_15, [-1], True);  pow_tensor_scalar_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_56: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_15, 1e-06);  mean_dim_15 = None\\n        rsqrt_default_15: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_56);  add_tensor_56 = None\\n        detach_default_23: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_15)\\n        mul_tensor_69: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_55, rsqrt_default_15);  rsqrt_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant69 = self._param_constant69\\n        mul_tensor_70: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant69, mul_tensor_69);  _param_constant69 = mul_tensor_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant70 = self._param_constant70\\n        t_default_53: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant70);  _param_constant70 = None\\n        view_default_188: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_70, [sym_size, 4096])\\n        mm_default_53: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_188, t_default_53);  view_default_188 = t_default_53 = None\\n        view_default_189: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_53, [1, sym_size, 11008]);  mm_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_7: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_189)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant71 = self._param_constant71\\n        t_default_54: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant71);  _param_constant71 = None\\n        view_default_190: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_70, [sym_size, 4096]);  mul_tensor_70 = None\\n        mm_default_54: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_190, t_default_54);  view_default_190 = t_default_54 = None\\n        view_default_191: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_54, [1, sym_size, 11008]);  mm_default_54 = None\\n        mul_tensor_71: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_7, view_default_191);  silu_default_7 = view_default_191 = None\\n        _param_constant72 = self._param_constant72\\n        t_default_55: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant72);  _param_constant72 = None\\n        sym_size_32: Sym(s0) = torch.ops.aten.sym_size(view_default_189, 1);  view_default_189 = None\\n        view_default_192: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_71, [sym_size_32, 11008]);  mul_tensor_71 = None\\n        mm_default_55: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_192, t_default_55);  view_default_192 = t_default_55 = None\\n        view_default_193: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_55, [1, sym_size_32, 4096]);  mm_default_55 = sym_size_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_57: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_55, view_default_193);  add_tensor_55 = view_default_193 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_16: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_57, 2)\\n        mean_dim_16: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_16, [-1], True);  pow_tensor_scalar_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_58: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_16, 1e-06);  mean_dim_16 = None\\n        rsqrt_default_16: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_58);  add_tensor_58 = None\\n        detach_default_24: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_16)\\n        mul_tensor_72: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_57, rsqrt_default_16);  rsqrt_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant73 = self._param_constant73\\n        mul_tensor_73: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant73, mul_tensor_72);  _param_constant73 = mul_tensor_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant74 = self._param_constant74\\n        t_default_56: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant74);  _param_constant74 = None\\n        view_default_194: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_73, [sym_size, 4096])\\n        mm_default_56: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_194, t_default_56);  view_default_194 = t_default_56 = None\\n        view_default_195: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_56, [1, sym_size, 4096]);  mm_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant75 = self._param_constant75\\n        t_default_57: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant75);  _param_constant75 = None\\n        view_default_196: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_73, [sym_size, 4096])\\n        mm_default_57: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_196, t_default_57);  view_default_196 = t_default_57 = None\\n        view_default_197: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_57, [1, sym_size, 4096]);  mm_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant76 = self._param_constant76\\n        t_default_58: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant76);  _param_constant76 = None\\n        view_default_198: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_73, [sym_size, 4096]);  mul_tensor_73 = None\\n        mm_default_58: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_198, t_default_58);  view_default_198 = t_default_58 = None\\n        view_default_199: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_58, [1, sym_size, 4096]);  mm_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_200: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_195, [1, sym_size, 32, 128])\\n        transpose_int_40: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_200, 1, 2);  view_default_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_201: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_197, [1, sym_size, 32, 128])\\n        transpose_int_41: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_201, 1, 2);  view_default_201 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_202: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_199, [1, sym_size, 32, 128])\\n        transpose_int_42: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_202, 1, 2);  view_default_202 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant16 = self._tensor_constant16\\n        slice_tensor_84: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant16, 0, 0, 9223372036854775807);  _tensor_constant16 = None\\n        slice_tensor_85: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_84, 1, 0, 9223372036854775807);  slice_tensor_84 = None\\n        sym_size_33: Sym(s0) = torch.ops.aten.sym_size(view_default_197, 1);  view_default_197 = None\\n        slice_tensor_86: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_85, 2, 0, sym_size_33);  slice_tensor_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant17 = self._tensor_constant17\\n        slice_tensor_87: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant17, 0, 0, 9223372036854775807);  _tensor_constant17 = None\\n        slice_tensor_88: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_87, 1, 0, 9223372036854775807);  slice_tensor_87 = None\\n        slice_tensor_89: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_88, 2, 0, sym_size_33);  slice_tensor_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_32: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_86, 1);  slice_tensor_86 = None\\n        squeeze_dim_33: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_32, 0);  squeeze_dim_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_34: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_89, 1);  slice_tensor_89 = None\\n        squeeze_dim_35: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_34, 0);  squeeze_dim_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_16: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_33, [view_default]);  squeeze_dim_33 = None\\n        unsqueeze_default_21: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_16, 1);  index_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_17: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_35, [view_default]);  squeeze_dim_35 = None\\n        unsqueeze_default_22: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_17, 1);  index_tensor_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_74: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_40, unsqueeze_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_90: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_91: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_40, 3, 64, 9223372036854775807);  transpose_int_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_16: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_91);  slice_tensor_91 = None\\n        cat_default_16: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_16, slice_tensor_90], -1);  neg_default_16 = slice_tensor_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_75: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_16, unsqueeze_default_22);  cat_default_16 = None\\n        add_tensor_59: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_74, mul_tensor_75);  mul_tensor_74 = mul_tensor_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_76: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_41, unsqueeze_default_21);  unsqueeze_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_92: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_93: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_41, 3, 64, 9223372036854775807);  transpose_int_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_17: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_93);  slice_tensor_93 = None\\n        cat_default_17: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_17, slice_tensor_92], -1);  neg_default_17 = slice_tensor_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_77: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_17, unsqueeze_default_22);  cat_default_17 = unsqueeze_default_22 = None\\n        add_tensor_60: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_76, mul_tensor_77);  mul_tensor_76 = mul_tensor_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_43: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_60, 2, 3)\\n        sym_size_34: Sym(s0) = torch.ops.aten.sym_size(view_default_195, 1);  view_default_195 = None\\n        expand_default_34: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_59, [1, 32, sym_size_34, 128]);  add_tensor_59 = None\\n        view_default_203: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_34, [32, sym_size_34, 128]);  expand_default_34 = None\\n        expand_default_35: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_43, [1, 32, 128, sym_size_33]);  transpose_int_43 = None\\n        view_default_204: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_35, [32, 128, sym_size_33]);  expand_default_35 = None\\n        bmm_default_16: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_203, view_default_204);  view_default_203 = view_default_204 = None\\n        view_default_205: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_16, [1, 32, sym_size_34, sym_size_33]);  bmm_default_16 = None\\n        div_tensor_8: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_205, 11.313708498984761);  view_default_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_61: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_8, add_tensor_1);  div_tensor_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_8: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_61, -1, False);  add_tensor_61 = None\\n        detach_default_25: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_8)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_36: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_8, [1, 32, sym_size_34, sym_size_33]);  _softmax_default_8 = None\\n        view_default_206: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_36, [32, sym_size_34, sym_size_33]);  expand_default_36 = sym_size_33 = None\\n        sym_size_35: Sym(s0) = torch.ops.aten.sym_size(view_default_199, 1);  view_default_199 = None\\n        expand_default_37: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_42, [1, 32, sym_size_35, 128])\\n        view_default_207: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_37, [32, sym_size_35, 128]);  expand_default_37 = sym_size_35 = None\\n        bmm_default_17: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_206, view_default_207);  view_default_206 = view_default_207 = None\\n        view_default_208: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_17, [1, 32, sym_size_34, 128]);  bmm_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_44: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_208, 1, 2);  view_default_208 = None\\n        clone_default_8: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_44, memory_format = torch.contiguous_format);  transpose_int_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_209: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_8, [1, sym_size, 4096]);  clone_default_8 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant77 = self._param_constant77\\n        t_default_59: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant77);  _param_constant77 = None\\n        view_default_210: f32[s0, 4096] = torch.ops.aten.view.default(view_default_209, [sym_size_34, 4096]);  view_default_209 = None\\n        mm_default_59: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_210, t_default_59);  view_default_210 = t_default_59 = None\\n        view_default_211: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_59, [1, sym_size_34, 4096]);  mm_default_59 = sym_size_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_62: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_57, view_default_211);  add_tensor_57 = view_default_211 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_17: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_62, 2)\\n        mean_dim_17: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_17, [-1], True);  pow_tensor_scalar_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_63: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_17, 1e-06);  mean_dim_17 = None\\n        rsqrt_default_17: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_63);  add_tensor_63 = None\\n        detach_default_26: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_17)\\n        mul_tensor_78: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_62, rsqrt_default_17);  rsqrt_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant78 = self._param_constant78\\n        mul_tensor_79: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant78, mul_tensor_78);  _param_constant78 = mul_tensor_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant79 = self._param_constant79\\n        t_default_60: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant79);  _param_constant79 = None\\n        view_default_212: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_79, [sym_size, 4096])\\n        mm_default_60: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_212, t_default_60);  view_default_212 = t_default_60 = None\\n        view_default_213: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_60, [1, sym_size, 11008]);  mm_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_8: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_213)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant80 = self._param_constant80\\n        t_default_61: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant80);  _param_constant80 = None\\n        view_default_214: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_79, [sym_size, 4096]);  mul_tensor_79 = None\\n        mm_default_61: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_214, t_default_61);  view_default_214 = t_default_61 = None\\n        view_default_215: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_61, [1, sym_size, 11008]);  mm_default_61 = None\\n        mul_tensor_80: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_8, view_default_215);  silu_default_8 = view_default_215 = None\\n        _param_constant81 = self._param_constant81\\n        t_default_62: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant81);  _param_constant81 = None\\n        sym_size_36: Sym(s0) = torch.ops.aten.sym_size(view_default_213, 1);  view_default_213 = None\\n        view_default_216: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_80, [sym_size_36, 11008]);  mul_tensor_80 = None\\n        mm_default_62: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_216, t_default_62);  view_default_216 = t_default_62 = None\\n        view_default_217: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_62, [1, sym_size_36, 4096]);  mm_default_62 = sym_size_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_64: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_62, view_default_217);  add_tensor_62 = view_default_217 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_18: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_64, 2)\\n        mean_dim_18: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_18, [-1], True);  pow_tensor_scalar_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_65: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_18, 1e-06);  mean_dim_18 = None\\n        rsqrt_default_18: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_65);  add_tensor_65 = None\\n        detach_default_27: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_18)\\n        mul_tensor_81: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_64, rsqrt_default_18);  rsqrt_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant82 = self._param_constant82\\n        mul_tensor_82: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant82, mul_tensor_81);  _param_constant82 = mul_tensor_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant83 = self._param_constant83\\n        t_default_63: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant83);  _param_constant83 = None\\n        view_default_218: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_82, [sym_size, 4096])\\n        mm_default_63: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_218, t_default_63);  view_default_218 = t_default_63 = None\\n        view_default_219: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_63, [1, sym_size, 4096]);  mm_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant84 = self._param_constant84\\n        t_default_64: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant84);  _param_constant84 = None\\n        view_default_220: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_82, [sym_size, 4096])\\n        mm_default_64: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_220, t_default_64);  view_default_220 = t_default_64 = None\\n        view_default_221: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_64, [1, sym_size, 4096]);  mm_default_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant85 = self._param_constant85\\n        t_default_65: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant85);  _param_constant85 = None\\n        view_default_222: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_82, [sym_size, 4096]);  mul_tensor_82 = None\\n        mm_default_65: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_222, t_default_65);  view_default_222 = t_default_65 = None\\n        view_default_223: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_65, [1, sym_size, 4096]);  mm_default_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_224: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_219, [1, sym_size, 32, 128])\\n        transpose_int_45: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_224, 1, 2);  view_default_224 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_225: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_221, [1, sym_size, 32, 128])\\n        transpose_int_46: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_225, 1, 2);  view_default_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_226: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_223, [1, sym_size, 32, 128])\\n        transpose_int_47: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_226, 1, 2);  view_default_226 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant18 = self._tensor_constant18\\n        slice_tensor_94: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant18, 0, 0, 9223372036854775807);  _tensor_constant18 = None\\n        slice_tensor_95: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_94, 1, 0, 9223372036854775807);  slice_tensor_94 = None\\n        sym_size_37: Sym(s0) = torch.ops.aten.sym_size(view_default_221, 1);  view_default_221 = None\\n        slice_tensor_96: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_95, 2, 0, sym_size_37);  slice_tensor_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant19 = self._tensor_constant19\\n        slice_tensor_97: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant19, 0, 0, 9223372036854775807);  _tensor_constant19 = None\\n        slice_tensor_98: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_97, 1, 0, 9223372036854775807);  slice_tensor_97 = None\\n        slice_tensor_99: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_98, 2, 0, sym_size_37);  slice_tensor_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_36: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_96, 1);  slice_tensor_96 = None\\n        squeeze_dim_37: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_36, 0);  squeeze_dim_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_38: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_99, 1);  slice_tensor_99 = None\\n        squeeze_dim_39: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_38, 0);  squeeze_dim_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_18: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_37, [view_default]);  squeeze_dim_37 = None\\n        unsqueeze_default_23: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_18, 1);  index_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_19: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_39, [view_default]);  squeeze_dim_39 = None\\n        unsqueeze_default_24: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_19, 1);  index_tensor_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_83: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_45, unsqueeze_default_23)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_100: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_101: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_45, 3, 64, 9223372036854775807);  transpose_int_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_18: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_101);  slice_tensor_101 = None\\n        cat_default_18: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_18, slice_tensor_100], -1);  neg_default_18 = slice_tensor_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_84: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_18, unsqueeze_default_24);  cat_default_18 = None\\n        add_tensor_66: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_83, mul_tensor_84);  mul_tensor_83 = mul_tensor_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_85: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_46, unsqueeze_default_23);  unsqueeze_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_102: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_103: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_46, 3, 64, 9223372036854775807);  transpose_int_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_19: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_103);  slice_tensor_103 = None\\n        cat_default_19: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_19, slice_tensor_102], -1);  neg_default_19 = slice_tensor_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_86: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_19, unsqueeze_default_24);  cat_default_19 = unsqueeze_default_24 = None\\n        add_tensor_67: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_85, mul_tensor_86);  mul_tensor_85 = mul_tensor_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_48: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_67, 2, 3)\\n        sym_size_38: Sym(s0) = torch.ops.aten.sym_size(view_default_219, 1);  view_default_219 = None\\n        expand_default_38: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_66, [1, 32, sym_size_38, 128]);  add_tensor_66 = None\\n        view_default_227: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_38, [32, sym_size_38, 128]);  expand_default_38 = None\\n        expand_default_39: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_48, [1, 32, 128, sym_size_37]);  transpose_int_48 = None\\n        view_default_228: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_39, [32, 128, sym_size_37]);  expand_default_39 = None\\n        bmm_default_18: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_227, view_default_228);  view_default_227 = view_default_228 = None\\n        view_default_229: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_18, [1, 32, sym_size_38, sym_size_37]);  bmm_default_18 = None\\n        div_tensor_9: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_229, 11.313708498984761);  view_default_229 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_68: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_9, add_tensor_1);  div_tensor_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_9: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_68, -1, False);  add_tensor_68 = None\\n        detach_default_28: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_9)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_40: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_9, [1, 32, sym_size_38, sym_size_37]);  _softmax_default_9 = None\\n        view_default_230: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_40, [32, sym_size_38, sym_size_37]);  expand_default_40 = sym_size_37 = None\\n        sym_size_39: Sym(s0) = torch.ops.aten.sym_size(view_default_223, 1);  view_default_223 = None\\n        expand_default_41: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_47, [1, 32, sym_size_39, 128])\\n        view_default_231: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_41, [32, sym_size_39, 128]);  expand_default_41 = sym_size_39 = None\\n        bmm_default_19: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_230, view_default_231);  view_default_230 = view_default_231 = None\\n        view_default_232: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_19, [1, 32, sym_size_38, 128]);  bmm_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_49: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_232, 1, 2);  view_default_232 = None\\n        clone_default_9: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_49, memory_format = torch.contiguous_format);  transpose_int_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_233: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_9, [1, sym_size, 4096]);  clone_default_9 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant86 = self._param_constant86\\n        t_default_66: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant86);  _param_constant86 = None\\n        view_default_234: f32[s0, 4096] = torch.ops.aten.view.default(view_default_233, [sym_size_38, 4096]);  view_default_233 = None\\n        mm_default_66: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_234, t_default_66);  view_default_234 = t_default_66 = None\\n        view_default_235: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_66, [1, sym_size_38, 4096]);  mm_default_66 = sym_size_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_69: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_64, view_default_235);  add_tensor_64 = view_default_235 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_19: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_69, 2)\\n        mean_dim_19: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_19, [-1], True);  pow_tensor_scalar_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_70: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_19, 1e-06);  mean_dim_19 = None\\n        rsqrt_default_19: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_70);  add_tensor_70 = None\\n        detach_default_29: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_19)\\n        mul_tensor_87: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_69, rsqrt_default_19);  rsqrt_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant87 = self._param_constant87\\n        mul_tensor_88: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant87, mul_tensor_87);  _param_constant87 = mul_tensor_87 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant88 = self._param_constant88\\n        t_default_67: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant88);  _param_constant88 = None\\n        view_default_236: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_88, [sym_size, 4096])\\n        mm_default_67: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_236, t_default_67);  view_default_236 = t_default_67 = None\\n        view_default_237: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_67, [1, sym_size, 11008]);  mm_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_9: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_237)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant89 = self._param_constant89\\n        t_default_68: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant89);  _param_constant89 = None\\n        view_default_238: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_88, [sym_size, 4096]);  mul_tensor_88 = None\\n        mm_default_68: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_238, t_default_68);  view_default_238 = t_default_68 = None\\n        view_default_239: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_68, [1, sym_size, 11008]);  mm_default_68 = None\\n        mul_tensor_89: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_9, view_default_239);  silu_default_9 = view_default_239 = None\\n        _param_constant90 = self._param_constant90\\n        t_default_69: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant90);  _param_constant90 = None\\n        sym_size_40: Sym(s0) = torch.ops.aten.sym_size(view_default_237, 1);  view_default_237 = None\\n        view_default_240: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_89, [sym_size_40, 11008]);  mul_tensor_89 = None\\n        mm_default_69: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_240, t_default_69);  view_default_240 = t_default_69 = None\\n        view_default_241: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_69, [1, sym_size_40, 4096]);  mm_default_69 = sym_size_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_71: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_69, view_default_241);  add_tensor_69 = view_default_241 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_20: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_71, 2)\\n        mean_dim_20: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_20, [-1], True);  pow_tensor_scalar_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_72: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_20, 1e-06);  mean_dim_20 = None\\n        rsqrt_default_20: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_72);  add_tensor_72 = None\\n        detach_default_30: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_20)\\n        mul_tensor_90: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_71, rsqrt_default_20);  rsqrt_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant91 = self._param_constant91\\n        mul_tensor_91: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant91, mul_tensor_90);  _param_constant91 = mul_tensor_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant92 = self._param_constant92\\n        t_default_70: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant92);  _param_constant92 = None\\n        view_default_242: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_91, [sym_size, 4096])\\n        mm_default_70: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_242, t_default_70);  view_default_242 = t_default_70 = None\\n        view_default_243: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_70, [1, sym_size, 4096]);  mm_default_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant93 = self._param_constant93\\n        t_default_71: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant93);  _param_constant93 = None\\n        view_default_244: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_91, [sym_size, 4096])\\n        mm_default_71: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_244, t_default_71);  view_default_244 = t_default_71 = None\\n        view_default_245: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_71, [1, sym_size, 4096]);  mm_default_71 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant94 = self._param_constant94\\n        t_default_72: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant94);  _param_constant94 = None\\n        view_default_246: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_91, [sym_size, 4096]);  mul_tensor_91 = None\\n        mm_default_72: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_246, t_default_72);  view_default_246 = t_default_72 = None\\n        view_default_247: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_72, [1, sym_size, 4096]);  mm_default_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_248: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_243, [1, sym_size, 32, 128])\\n        transpose_int_50: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_248, 1, 2);  view_default_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_249: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_245, [1, sym_size, 32, 128])\\n        transpose_int_51: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_249, 1, 2);  view_default_249 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_250: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_247, [1, sym_size, 32, 128])\\n        transpose_int_52: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_250, 1, 2);  view_default_250 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant20 = self._tensor_constant20\\n        slice_tensor_104: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant20, 0, 0, 9223372036854775807);  _tensor_constant20 = None\\n        slice_tensor_105: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_104, 1, 0, 9223372036854775807);  slice_tensor_104 = None\\n        sym_size_41: Sym(s0) = torch.ops.aten.sym_size(view_default_245, 1);  view_default_245 = None\\n        slice_tensor_106: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_105, 2, 0, sym_size_41);  slice_tensor_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant21 = self._tensor_constant21\\n        slice_tensor_107: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant21, 0, 0, 9223372036854775807);  _tensor_constant21 = None\\n        slice_tensor_108: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_107, 1, 0, 9223372036854775807);  slice_tensor_107 = None\\n        slice_tensor_109: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_108, 2, 0, sym_size_41);  slice_tensor_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_40: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_106, 1);  slice_tensor_106 = None\\n        squeeze_dim_41: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_40, 0);  squeeze_dim_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_42: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_109, 1);  slice_tensor_109 = None\\n        squeeze_dim_43: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_42, 0);  squeeze_dim_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_20: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_41, [view_default]);  squeeze_dim_41 = None\\n        unsqueeze_default_25: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_20, 1);  index_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_21: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_43, [view_default]);  squeeze_dim_43 = None\\n        unsqueeze_default_26: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_21, 1);  index_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_92: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_50, unsqueeze_default_25)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_110: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_111: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_50, 3, 64, 9223372036854775807);  transpose_int_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_20: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_111);  slice_tensor_111 = None\\n        cat_default_20: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_20, slice_tensor_110], -1);  neg_default_20 = slice_tensor_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_93: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_20, unsqueeze_default_26);  cat_default_20 = None\\n        add_tensor_73: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_92, mul_tensor_93);  mul_tensor_92 = mul_tensor_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_94: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_51, unsqueeze_default_25);  unsqueeze_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_112: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_113: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_51, 3, 64, 9223372036854775807);  transpose_int_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_21: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_113);  slice_tensor_113 = None\\n        cat_default_21: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_21, slice_tensor_112], -1);  neg_default_21 = slice_tensor_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_95: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_21, unsqueeze_default_26);  cat_default_21 = unsqueeze_default_26 = None\\n        add_tensor_74: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_94, mul_tensor_95);  mul_tensor_94 = mul_tensor_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_53: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_74, 2, 3)\\n        sym_size_42: Sym(s0) = torch.ops.aten.sym_size(view_default_243, 1);  view_default_243 = None\\n        expand_default_42: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_73, [1, 32, sym_size_42, 128]);  add_tensor_73 = None\\n        view_default_251: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_42, [32, sym_size_42, 128]);  expand_default_42 = None\\n        expand_default_43: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_53, [1, 32, 128, sym_size_41]);  transpose_int_53 = None\\n        view_default_252: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_43, [32, 128, sym_size_41]);  expand_default_43 = None\\n        bmm_default_20: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_251, view_default_252);  view_default_251 = view_default_252 = None\\n        view_default_253: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_20, [1, 32, sym_size_42, sym_size_41]);  bmm_default_20 = None\\n        div_tensor_10: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_253, 11.313708498984761);  view_default_253 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_75: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_10, add_tensor_1);  div_tensor_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_10: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_75, -1, False);  add_tensor_75 = None\\n        detach_default_31: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_10)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_44: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_10, [1, 32, sym_size_42, sym_size_41]);  _softmax_default_10 = None\\n        view_default_254: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_44, [32, sym_size_42, sym_size_41]);  expand_default_44 = sym_size_41 = None\\n        sym_size_43: Sym(s0) = torch.ops.aten.sym_size(view_default_247, 1);  view_default_247 = None\\n        expand_default_45: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_52, [1, 32, sym_size_43, 128])\\n        view_default_255: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_45, [32, sym_size_43, 128]);  expand_default_45 = sym_size_43 = None\\n        bmm_default_21: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_254, view_default_255);  view_default_254 = view_default_255 = None\\n        view_default_256: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_21, [1, 32, sym_size_42, 128]);  bmm_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_54: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_256, 1, 2);  view_default_256 = None\\n        clone_default_10: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_54, memory_format = torch.contiguous_format);  transpose_int_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_257: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_10, [1, sym_size, 4096]);  clone_default_10 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant95 = self._param_constant95\\n        t_default_73: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant95);  _param_constant95 = None\\n        view_default_258: f32[s0, 4096] = torch.ops.aten.view.default(view_default_257, [sym_size_42, 4096]);  view_default_257 = None\\n        mm_default_73: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_258, t_default_73);  view_default_258 = t_default_73 = None\\n        view_default_259: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_73, [1, sym_size_42, 4096]);  mm_default_73 = sym_size_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_76: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_71, view_default_259);  add_tensor_71 = view_default_259 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_21: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_76, 2)\\n        mean_dim_21: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_21, [-1], True);  pow_tensor_scalar_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_77: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_21, 1e-06);  mean_dim_21 = None\\n        rsqrt_default_21: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_77);  add_tensor_77 = None\\n        detach_default_32: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_21)\\n        mul_tensor_96: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_76, rsqrt_default_21);  rsqrt_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant96 = self._param_constant96\\n        mul_tensor_97: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant96, mul_tensor_96);  _param_constant96 = mul_tensor_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant97 = self._param_constant97\\n        t_default_74: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant97);  _param_constant97 = None\\n        view_default_260: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_97, [sym_size, 4096])\\n        mm_default_74: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_260, t_default_74);  view_default_260 = t_default_74 = None\\n        view_default_261: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_74, [1, sym_size, 11008]);  mm_default_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_10: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_261)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant98 = self._param_constant98\\n        t_default_75: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant98);  _param_constant98 = None\\n        view_default_262: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_97, [sym_size, 4096]);  mul_tensor_97 = None\\n        mm_default_75: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_262, t_default_75);  view_default_262 = t_default_75 = None\\n        view_default_263: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_75, [1, sym_size, 11008]);  mm_default_75 = None\\n        mul_tensor_98: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_10, view_default_263);  silu_default_10 = view_default_263 = None\\n        _param_constant99 = self._param_constant99\\n        t_default_76: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant99);  _param_constant99 = None\\n        sym_size_44: Sym(s0) = torch.ops.aten.sym_size(view_default_261, 1);  view_default_261 = None\\n        view_default_264: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_98, [sym_size_44, 11008]);  mul_tensor_98 = None\\n        mm_default_76: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_264, t_default_76);  view_default_264 = t_default_76 = None\\n        view_default_265: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_76, [1, sym_size_44, 4096]);  mm_default_76 = sym_size_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_78: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_76, view_default_265);  add_tensor_76 = view_default_265 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_22: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_78, 2)\\n        mean_dim_22: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_22, [-1], True);  pow_tensor_scalar_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_79: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_22, 1e-06);  mean_dim_22 = None\\n        rsqrt_default_22: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_79);  add_tensor_79 = None\\n        detach_default_33: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_22)\\n        mul_tensor_99: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_78, rsqrt_default_22);  rsqrt_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant100 = self._param_constant100\\n        mul_tensor_100: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant100, mul_tensor_99);  _param_constant100 = mul_tensor_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant101 = self._param_constant101\\n        t_default_77: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant101);  _param_constant101 = None\\n        view_default_266: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_100, [sym_size, 4096])\\n        mm_default_77: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_266, t_default_77);  view_default_266 = t_default_77 = None\\n        view_default_267: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_77, [1, sym_size, 4096]);  mm_default_77 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant102 = self._param_constant102\\n        t_default_78: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant102);  _param_constant102 = None\\n        view_default_268: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_100, [sym_size, 4096])\\n        mm_default_78: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_268, t_default_78);  view_default_268 = t_default_78 = None\\n        view_default_269: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_78, [1, sym_size, 4096]);  mm_default_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant103 = self._param_constant103\\n        t_default_79: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant103);  _param_constant103 = None\\n        view_default_270: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_100, [sym_size, 4096]);  mul_tensor_100 = None\\n        mm_default_79: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_270, t_default_79);  view_default_270 = t_default_79 = None\\n        view_default_271: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_79, [1, sym_size, 4096]);  mm_default_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_272: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_267, [1, sym_size, 32, 128])\\n        transpose_int_55: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_272, 1, 2);  view_default_272 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_273: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_269, [1, sym_size, 32, 128])\\n        transpose_int_56: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_273, 1, 2);  view_default_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_274: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_271, [1, sym_size, 32, 128])\\n        transpose_int_57: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_274, 1, 2);  view_default_274 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant22 = self._tensor_constant22\\n        slice_tensor_114: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant22, 0, 0, 9223372036854775807);  _tensor_constant22 = None\\n        slice_tensor_115: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_114, 1, 0, 9223372036854775807);  slice_tensor_114 = None\\n        sym_size_45: Sym(s0) = torch.ops.aten.sym_size(view_default_269, 1);  view_default_269 = None\\n        slice_tensor_116: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_115, 2, 0, sym_size_45);  slice_tensor_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant23 = self._tensor_constant23\\n        slice_tensor_117: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant23, 0, 0, 9223372036854775807);  _tensor_constant23 = None\\n        slice_tensor_118: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_117, 1, 0, 9223372036854775807);  slice_tensor_117 = None\\n        slice_tensor_119: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_118, 2, 0, sym_size_45);  slice_tensor_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_44: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_116, 1);  slice_tensor_116 = None\\n        squeeze_dim_45: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_44, 0);  squeeze_dim_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_46: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_119, 1);  slice_tensor_119 = None\\n        squeeze_dim_47: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_46, 0);  squeeze_dim_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_22: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_45, [view_default]);  squeeze_dim_45 = None\\n        unsqueeze_default_27: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_22, 1);  index_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_23: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_47, [view_default]);  squeeze_dim_47 = None\\n        unsqueeze_default_28: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_23, 1);  index_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_101: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_55, unsqueeze_default_27)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_120: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_121: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_55, 3, 64, 9223372036854775807);  transpose_int_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_22: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_121);  slice_tensor_121 = None\\n        cat_default_22: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_22, slice_tensor_120], -1);  neg_default_22 = slice_tensor_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_102: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_22, unsqueeze_default_28);  cat_default_22 = None\\n        add_tensor_80: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_101, mul_tensor_102);  mul_tensor_101 = mul_tensor_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_103: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_56, unsqueeze_default_27);  unsqueeze_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_122: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_123: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_56, 3, 64, 9223372036854775807);  transpose_int_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_23: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_123);  slice_tensor_123 = None\\n        cat_default_23: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_23, slice_tensor_122], -1);  neg_default_23 = slice_tensor_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_104: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_23, unsqueeze_default_28);  cat_default_23 = unsqueeze_default_28 = None\\n        add_tensor_81: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_103, mul_tensor_104);  mul_tensor_103 = mul_tensor_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_58: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_81, 2, 3)\\n        sym_size_46: Sym(s0) = torch.ops.aten.sym_size(view_default_267, 1);  view_default_267 = None\\n        expand_default_46: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_80, [1, 32, sym_size_46, 128]);  add_tensor_80 = None\\n        view_default_275: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_46, [32, sym_size_46, 128]);  expand_default_46 = None\\n        expand_default_47: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_58, [1, 32, 128, sym_size_45]);  transpose_int_58 = None\\n        view_default_276: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_47, [32, 128, sym_size_45]);  expand_default_47 = None\\n        bmm_default_22: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_275, view_default_276);  view_default_275 = view_default_276 = None\\n        view_default_277: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_22, [1, 32, sym_size_46, sym_size_45]);  bmm_default_22 = None\\n        div_tensor_11: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_277, 11.313708498984761);  view_default_277 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_82: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_11, add_tensor_1);  div_tensor_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_11: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_82, -1, False);  add_tensor_82 = None\\n        detach_default_34: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_11)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_48: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_11, [1, 32, sym_size_46, sym_size_45]);  _softmax_default_11 = None\\n        view_default_278: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_48, [32, sym_size_46, sym_size_45]);  expand_default_48 = sym_size_45 = None\\n        sym_size_47: Sym(s0) = torch.ops.aten.sym_size(view_default_271, 1);  view_default_271 = None\\n        expand_default_49: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_57, [1, 32, sym_size_47, 128])\\n        view_default_279: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_49, [32, sym_size_47, 128]);  expand_default_49 = sym_size_47 = None\\n        bmm_default_23: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_278, view_default_279);  view_default_278 = view_default_279 = None\\n        view_default_280: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_23, [1, 32, sym_size_46, 128]);  bmm_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_59: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_280, 1, 2);  view_default_280 = None\\n        clone_default_11: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_59, memory_format = torch.contiguous_format);  transpose_int_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_281: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_11, [1, sym_size, 4096]);  clone_default_11 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant104 = self._param_constant104\\n        t_default_80: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant104);  _param_constant104 = None\\n        view_default_282: f32[s0, 4096] = torch.ops.aten.view.default(view_default_281, [sym_size_46, 4096]);  view_default_281 = None\\n        mm_default_80: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_282, t_default_80);  view_default_282 = t_default_80 = None\\n        view_default_283: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_80, [1, sym_size_46, 4096]);  mm_default_80 = sym_size_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_83: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_78, view_default_283);  add_tensor_78 = view_default_283 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_23: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_83, 2)\\n        mean_dim_23: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_23, [-1], True);  pow_tensor_scalar_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_84: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_23, 1e-06);  mean_dim_23 = None\\n        rsqrt_default_23: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_84);  add_tensor_84 = None\\n        detach_default_35: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_23)\\n        mul_tensor_105: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_83, rsqrt_default_23);  rsqrt_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant105 = self._param_constant105\\n        mul_tensor_106: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant105, mul_tensor_105);  _param_constant105 = mul_tensor_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant106 = self._param_constant106\\n        t_default_81: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant106);  _param_constant106 = None\\n        view_default_284: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_106, [sym_size, 4096])\\n        mm_default_81: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_284, t_default_81);  view_default_284 = t_default_81 = None\\n        view_default_285: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_81, [1, sym_size, 11008]);  mm_default_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_11: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_285)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant107 = self._param_constant107\\n        t_default_82: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant107);  _param_constant107 = None\\n        view_default_286: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_106, [sym_size, 4096]);  mul_tensor_106 = None\\n        mm_default_82: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_286, t_default_82);  view_default_286 = t_default_82 = None\\n        view_default_287: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_82, [1, sym_size, 11008]);  mm_default_82 = None\\n        mul_tensor_107: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_11, view_default_287);  silu_default_11 = view_default_287 = None\\n        _param_constant108 = self._param_constant108\\n        t_default_83: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant108);  _param_constant108 = None\\n        sym_size_48: Sym(s0) = torch.ops.aten.sym_size(view_default_285, 1);  view_default_285 = None\\n        view_default_288: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_107, [sym_size_48, 11008]);  mul_tensor_107 = None\\n        mm_default_83: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_288, t_default_83);  view_default_288 = t_default_83 = None\\n        view_default_289: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_83, [1, sym_size_48, 4096]);  mm_default_83 = sym_size_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_85: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_83, view_default_289);  add_tensor_83 = view_default_289 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_24: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_85, 2)\\n        mean_dim_24: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_24, [-1], True);  pow_tensor_scalar_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_86: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_24, 1e-06);  mean_dim_24 = None\\n        rsqrt_default_24: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_86);  add_tensor_86 = None\\n        detach_default_36: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_24)\\n        mul_tensor_108: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_85, rsqrt_default_24);  rsqrt_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant109 = self._param_constant109\\n        mul_tensor_109: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant109, mul_tensor_108);  _param_constant109 = mul_tensor_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant110 = self._param_constant110\\n        t_default_84: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant110);  _param_constant110 = None\\n        view_default_290: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_109, [sym_size, 4096])\\n        mm_default_84: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_290, t_default_84);  view_default_290 = t_default_84 = None\\n        view_default_291: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_84, [1, sym_size, 4096]);  mm_default_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant111 = self._param_constant111\\n        t_default_85: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant111);  _param_constant111 = None\\n        view_default_292: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_109, [sym_size, 4096])\\n        mm_default_85: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_292, t_default_85);  view_default_292 = t_default_85 = None\\n        view_default_293: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_85, [1, sym_size, 4096]);  mm_default_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant112 = self._param_constant112\\n        t_default_86: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant112);  _param_constant112 = None\\n        view_default_294: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_109, [sym_size, 4096]);  mul_tensor_109 = None\\n        mm_default_86: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_294, t_default_86);  view_default_294 = t_default_86 = None\\n        view_default_295: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_86, [1, sym_size, 4096]);  mm_default_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_296: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_291, [1, sym_size, 32, 128])\\n        transpose_int_60: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_296, 1, 2);  view_default_296 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_297: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_293, [1, sym_size, 32, 128])\\n        transpose_int_61: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_297, 1, 2);  view_default_297 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_298: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_295, [1, sym_size, 32, 128])\\n        transpose_int_62: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_298, 1, 2);  view_default_298 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant24 = self._tensor_constant24\\n        slice_tensor_124: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant24, 0, 0, 9223372036854775807);  _tensor_constant24 = None\\n        slice_tensor_125: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_124, 1, 0, 9223372036854775807);  slice_tensor_124 = None\\n        sym_size_49: Sym(s0) = torch.ops.aten.sym_size(view_default_293, 1);  view_default_293 = None\\n        slice_tensor_126: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_125, 2, 0, sym_size_49);  slice_tensor_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant25 = self._tensor_constant25\\n        slice_tensor_127: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant25, 0, 0, 9223372036854775807);  _tensor_constant25 = None\\n        slice_tensor_128: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_127, 1, 0, 9223372036854775807);  slice_tensor_127 = None\\n        slice_tensor_129: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_128, 2, 0, sym_size_49);  slice_tensor_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_48: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_126, 1);  slice_tensor_126 = None\\n        squeeze_dim_49: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_48, 0);  squeeze_dim_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_50: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_129, 1);  slice_tensor_129 = None\\n        squeeze_dim_51: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_50, 0);  squeeze_dim_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_24: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_49, [view_default]);  squeeze_dim_49 = None\\n        unsqueeze_default_29: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_24, 1);  index_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_25: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_51, [view_default]);  squeeze_dim_51 = None\\n        unsqueeze_default_30: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_25, 1);  index_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_110: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_60, unsqueeze_default_29)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_130: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_131: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_60, 3, 64, 9223372036854775807);  transpose_int_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_24: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_131);  slice_tensor_131 = None\\n        cat_default_24: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_24, slice_tensor_130], -1);  neg_default_24 = slice_tensor_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_111: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_24, unsqueeze_default_30);  cat_default_24 = None\\n        add_tensor_87: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_110, mul_tensor_111);  mul_tensor_110 = mul_tensor_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_112: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_61, unsqueeze_default_29);  unsqueeze_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_132: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_133: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_61, 3, 64, 9223372036854775807);  transpose_int_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_25: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_133);  slice_tensor_133 = None\\n        cat_default_25: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_25, slice_tensor_132], -1);  neg_default_25 = slice_tensor_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_113: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_25, unsqueeze_default_30);  cat_default_25 = unsqueeze_default_30 = None\\n        add_tensor_88: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_112, mul_tensor_113);  mul_tensor_112 = mul_tensor_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_63: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_88, 2, 3)\\n        sym_size_50: Sym(s0) = torch.ops.aten.sym_size(view_default_291, 1);  view_default_291 = None\\n        expand_default_50: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_87, [1, 32, sym_size_50, 128]);  add_tensor_87 = None\\n        view_default_299: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_50, [32, sym_size_50, 128]);  expand_default_50 = None\\n        expand_default_51: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_63, [1, 32, 128, sym_size_49]);  transpose_int_63 = None\\n        view_default_300: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_51, [32, 128, sym_size_49]);  expand_default_51 = None\\n        bmm_default_24: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_299, view_default_300);  view_default_299 = view_default_300 = None\\n        view_default_301: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_24, [1, 32, sym_size_50, sym_size_49]);  bmm_default_24 = None\\n        div_tensor_12: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_301, 11.313708498984761);  view_default_301 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_89: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_12, add_tensor_1);  div_tensor_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_12: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_89, -1, False);  add_tensor_89 = None\\n        detach_default_37: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_12)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_52: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_12, [1, 32, sym_size_50, sym_size_49]);  _softmax_default_12 = None\\n        view_default_302: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_52, [32, sym_size_50, sym_size_49]);  expand_default_52 = sym_size_49 = None\\n        sym_size_51: Sym(s0) = torch.ops.aten.sym_size(view_default_295, 1);  view_default_295 = None\\n        expand_default_53: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_62, [1, 32, sym_size_51, 128])\\n        view_default_303: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_53, [32, sym_size_51, 128]);  expand_default_53 = sym_size_51 = None\\n        bmm_default_25: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_302, view_default_303);  view_default_302 = view_default_303 = None\\n        view_default_304: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_25, [1, 32, sym_size_50, 128]);  bmm_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_64: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_304, 1, 2);  view_default_304 = None\\n        clone_default_12: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_64, memory_format = torch.contiguous_format);  transpose_int_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_305: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_12, [1, sym_size, 4096]);  clone_default_12 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant113 = self._param_constant113\\n        t_default_87: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant113);  _param_constant113 = None\\n        view_default_306: f32[s0, 4096] = torch.ops.aten.view.default(view_default_305, [sym_size_50, 4096]);  view_default_305 = None\\n        mm_default_87: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_306, t_default_87);  view_default_306 = t_default_87 = None\\n        view_default_307: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_87, [1, sym_size_50, 4096]);  mm_default_87 = sym_size_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_90: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_85, view_default_307);  add_tensor_85 = view_default_307 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_25: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_90, 2)\\n        mean_dim_25: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_25, [-1], True);  pow_tensor_scalar_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_91: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_25, 1e-06);  mean_dim_25 = None\\n        rsqrt_default_25: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_91);  add_tensor_91 = None\\n        detach_default_38: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_25)\\n        mul_tensor_114: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_90, rsqrt_default_25);  rsqrt_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant114 = self._param_constant114\\n        mul_tensor_115: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant114, mul_tensor_114);  _param_constant114 = mul_tensor_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant115 = self._param_constant115\\n        t_default_88: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant115);  _param_constant115 = None\\n        view_default_308: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_115, [sym_size, 4096])\\n        mm_default_88: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_308, t_default_88);  view_default_308 = t_default_88 = None\\n        view_default_309: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_88, [1, sym_size, 11008]);  mm_default_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_12: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_309)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant116 = self._param_constant116\\n        t_default_89: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant116);  _param_constant116 = None\\n        view_default_310: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_115, [sym_size, 4096]);  mul_tensor_115 = None\\n        mm_default_89: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_310, t_default_89);  view_default_310 = t_default_89 = None\\n        view_default_311: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_89, [1, sym_size, 11008]);  mm_default_89 = None\\n        mul_tensor_116: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_12, view_default_311);  silu_default_12 = view_default_311 = None\\n        _param_constant117 = self._param_constant117\\n        t_default_90: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant117);  _param_constant117 = None\\n        sym_size_52: Sym(s0) = torch.ops.aten.sym_size(view_default_309, 1);  view_default_309 = None\\n        view_default_312: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_116, [sym_size_52, 11008]);  mul_tensor_116 = None\\n        mm_default_90: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_312, t_default_90);  view_default_312 = t_default_90 = None\\n        view_default_313: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_90, [1, sym_size_52, 4096]);  mm_default_90 = sym_size_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_92: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_90, view_default_313);  add_tensor_90 = view_default_313 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_26: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_92, 2)\\n        mean_dim_26: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_26, [-1], True);  pow_tensor_scalar_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_93: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_26, 1e-06);  mean_dim_26 = None\\n        rsqrt_default_26: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_93);  add_tensor_93 = None\\n        detach_default_39: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_26)\\n        mul_tensor_117: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_92, rsqrt_default_26);  rsqrt_default_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant118 = self._param_constant118\\n        mul_tensor_118: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant118, mul_tensor_117);  _param_constant118 = mul_tensor_117 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant119 = self._param_constant119\\n        t_default_91: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant119);  _param_constant119 = None\\n        view_default_314: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_118, [sym_size, 4096])\\n        mm_default_91: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_314, t_default_91);  view_default_314 = t_default_91 = None\\n        view_default_315: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_91, [1, sym_size, 4096]);  mm_default_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant120 = self._param_constant120\\n        t_default_92: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant120);  _param_constant120 = None\\n        view_default_316: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_118, [sym_size, 4096])\\n        mm_default_92: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_316, t_default_92);  view_default_316 = t_default_92 = None\\n        view_default_317: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_92, [1, sym_size, 4096]);  mm_default_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant121 = self._param_constant121\\n        t_default_93: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant121);  _param_constant121 = None\\n        view_default_318: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_118, [sym_size, 4096]);  mul_tensor_118 = None\\n        mm_default_93: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_318, t_default_93);  view_default_318 = t_default_93 = None\\n        view_default_319: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_93, [1, sym_size, 4096]);  mm_default_93 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_320: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_315, [1, sym_size, 32, 128])\\n        transpose_int_65: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_320, 1, 2);  view_default_320 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_321: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_317, [1, sym_size, 32, 128])\\n        transpose_int_66: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_321, 1, 2);  view_default_321 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_322: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_319, [1, sym_size, 32, 128])\\n        transpose_int_67: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_322, 1, 2);  view_default_322 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant26 = self._tensor_constant26\\n        slice_tensor_134: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant26, 0, 0, 9223372036854775807);  _tensor_constant26 = None\\n        slice_tensor_135: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_134, 1, 0, 9223372036854775807);  slice_tensor_134 = None\\n        sym_size_53: Sym(s0) = torch.ops.aten.sym_size(view_default_317, 1);  view_default_317 = None\\n        slice_tensor_136: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_135, 2, 0, sym_size_53);  slice_tensor_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant27 = self._tensor_constant27\\n        slice_tensor_137: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant27, 0, 0, 9223372036854775807);  _tensor_constant27 = None\\n        slice_tensor_138: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_137, 1, 0, 9223372036854775807);  slice_tensor_137 = None\\n        slice_tensor_139: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_138, 2, 0, sym_size_53);  slice_tensor_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_52: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_136, 1);  slice_tensor_136 = None\\n        squeeze_dim_53: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_52, 0);  squeeze_dim_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_54: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_139, 1);  slice_tensor_139 = None\\n        squeeze_dim_55: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_54, 0);  squeeze_dim_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_26: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_53, [view_default]);  squeeze_dim_53 = None\\n        unsqueeze_default_31: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_26, 1);  index_tensor_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_27: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_55, [view_default]);  squeeze_dim_55 = None\\n        unsqueeze_default_32: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_27, 1);  index_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_119: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_65, unsqueeze_default_31)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_140: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_141: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_65, 3, 64, 9223372036854775807);  transpose_int_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_26: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_141);  slice_tensor_141 = None\\n        cat_default_26: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_26, slice_tensor_140], -1);  neg_default_26 = slice_tensor_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_120: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_26, unsqueeze_default_32);  cat_default_26 = None\\n        add_tensor_94: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_119, mul_tensor_120);  mul_tensor_119 = mul_tensor_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_121: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_66, unsqueeze_default_31);  unsqueeze_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_142: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_143: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_66, 3, 64, 9223372036854775807);  transpose_int_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_27: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_143);  slice_tensor_143 = None\\n        cat_default_27: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_27, slice_tensor_142], -1);  neg_default_27 = slice_tensor_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_122: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_27, unsqueeze_default_32);  cat_default_27 = unsqueeze_default_32 = None\\n        add_tensor_95: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_121, mul_tensor_122);  mul_tensor_121 = mul_tensor_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_68: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_95, 2, 3)\\n        sym_size_54: Sym(s0) = torch.ops.aten.sym_size(view_default_315, 1);  view_default_315 = None\\n        expand_default_54: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_94, [1, 32, sym_size_54, 128]);  add_tensor_94 = None\\n        view_default_323: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_54, [32, sym_size_54, 128]);  expand_default_54 = None\\n        expand_default_55: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_68, [1, 32, 128, sym_size_53]);  transpose_int_68 = None\\n        view_default_324: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_55, [32, 128, sym_size_53]);  expand_default_55 = None\\n        bmm_default_26: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_323, view_default_324);  view_default_323 = view_default_324 = None\\n        view_default_325: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_26, [1, 32, sym_size_54, sym_size_53]);  bmm_default_26 = None\\n        div_tensor_13: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_325, 11.313708498984761);  view_default_325 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_96: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_13, add_tensor_1);  div_tensor_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_13: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_96, -1, False);  add_tensor_96 = None\\n        detach_default_40: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_13)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_56: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_13, [1, 32, sym_size_54, sym_size_53]);  _softmax_default_13 = None\\n        view_default_326: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_56, [32, sym_size_54, sym_size_53]);  expand_default_56 = sym_size_53 = None\\n        sym_size_55: Sym(s0) = torch.ops.aten.sym_size(view_default_319, 1);  view_default_319 = None\\n        expand_default_57: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_67, [1, 32, sym_size_55, 128])\\n        view_default_327: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_57, [32, sym_size_55, 128]);  expand_default_57 = sym_size_55 = None\\n        bmm_default_27: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_326, view_default_327);  view_default_326 = view_default_327 = None\\n        view_default_328: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_27, [1, 32, sym_size_54, 128]);  bmm_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_69: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_328, 1, 2);  view_default_328 = None\\n        clone_default_13: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_69, memory_format = torch.contiguous_format);  transpose_int_69 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_329: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_13, [1, sym_size, 4096]);  clone_default_13 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant122 = self._param_constant122\\n        t_default_94: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant122);  _param_constant122 = None\\n        view_default_330: f32[s0, 4096] = torch.ops.aten.view.default(view_default_329, [sym_size_54, 4096]);  view_default_329 = None\\n        mm_default_94: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_330, t_default_94);  view_default_330 = t_default_94 = None\\n        view_default_331: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_94, [1, sym_size_54, 4096]);  mm_default_94 = sym_size_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_97: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_92, view_default_331);  add_tensor_92 = view_default_331 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_27: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_97, 2)\\n        mean_dim_27: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_27, [-1], True);  pow_tensor_scalar_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_98: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_27, 1e-06);  mean_dim_27 = None\\n        rsqrt_default_27: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_98);  add_tensor_98 = None\\n        detach_default_41: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_27)\\n        mul_tensor_123: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_97, rsqrt_default_27);  rsqrt_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant123 = self._param_constant123\\n        mul_tensor_124: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant123, mul_tensor_123);  _param_constant123 = mul_tensor_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant124 = self._param_constant124\\n        t_default_95: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant124);  _param_constant124 = None\\n        view_default_332: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_124, [sym_size, 4096])\\n        mm_default_95: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_332, t_default_95);  view_default_332 = t_default_95 = None\\n        view_default_333: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_95, [1, sym_size, 11008]);  mm_default_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_13: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_333)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant125 = self._param_constant125\\n        t_default_96: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant125);  _param_constant125 = None\\n        view_default_334: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_124, [sym_size, 4096]);  mul_tensor_124 = None\\n        mm_default_96: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_334, t_default_96);  view_default_334 = t_default_96 = None\\n        view_default_335: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_96, [1, sym_size, 11008]);  mm_default_96 = None\\n        mul_tensor_125: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_13, view_default_335);  silu_default_13 = view_default_335 = None\\n        _param_constant126 = self._param_constant126\\n        t_default_97: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant126);  _param_constant126 = None\\n        sym_size_56: Sym(s0) = torch.ops.aten.sym_size(view_default_333, 1);  view_default_333 = None\\n        view_default_336: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_125, [sym_size_56, 11008]);  mul_tensor_125 = None\\n        mm_default_97: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_336, t_default_97);  view_default_336 = t_default_97 = None\\n        view_default_337: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_97, [1, sym_size_56, 4096]);  mm_default_97 = sym_size_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_99: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_97, view_default_337);  add_tensor_97 = view_default_337 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_28: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_99, 2)\\n        mean_dim_28: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_28, [-1], True);  pow_tensor_scalar_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_100: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_28, 1e-06);  mean_dim_28 = None\\n        rsqrt_default_28: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_100);  add_tensor_100 = None\\n        detach_default_42: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_28)\\n        mul_tensor_126: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_99, rsqrt_default_28);  rsqrt_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant127 = self._param_constant127\\n        mul_tensor_127: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant127, mul_tensor_126);  _param_constant127 = mul_tensor_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant128 = self._param_constant128\\n        t_default_98: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant128);  _param_constant128 = None\\n        view_default_338: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_127, [sym_size, 4096])\\n        mm_default_98: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_338, t_default_98);  view_default_338 = t_default_98 = None\\n        view_default_339: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_98, [1, sym_size, 4096]);  mm_default_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant129 = self._param_constant129\\n        t_default_99: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant129);  _param_constant129 = None\\n        view_default_340: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_127, [sym_size, 4096])\\n        mm_default_99: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_340, t_default_99);  view_default_340 = t_default_99 = None\\n        view_default_341: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_99, [1, sym_size, 4096]);  mm_default_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant130 = self._param_constant130\\n        t_default_100: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant130);  _param_constant130 = None\\n        view_default_342: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_127, [sym_size, 4096]);  mul_tensor_127 = None\\n        mm_default_100: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_342, t_default_100);  view_default_342 = t_default_100 = None\\n        view_default_343: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_100, [1, sym_size, 4096]);  mm_default_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_344: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_339, [1, sym_size, 32, 128])\\n        transpose_int_70: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_344, 1, 2);  view_default_344 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_345: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_341, [1, sym_size, 32, 128])\\n        transpose_int_71: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_345, 1, 2);  view_default_345 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_346: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_343, [1, sym_size, 32, 128])\\n        transpose_int_72: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_346, 1, 2);  view_default_346 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant28 = self._tensor_constant28\\n        slice_tensor_144: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant28, 0, 0, 9223372036854775807);  _tensor_constant28 = None\\n        slice_tensor_145: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_144, 1, 0, 9223372036854775807);  slice_tensor_144 = None\\n        sym_size_57: Sym(s0) = torch.ops.aten.sym_size(view_default_341, 1);  view_default_341 = None\\n        slice_tensor_146: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_145, 2, 0, sym_size_57);  slice_tensor_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant29 = self._tensor_constant29\\n        slice_tensor_147: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant29, 0, 0, 9223372036854775807);  _tensor_constant29 = None\\n        slice_tensor_148: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_147, 1, 0, 9223372036854775807);  slice_tensor_147 = None\\n        slice_tensor_149: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_148, 2, 0, sym_size_57);  slice_tensor_148 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_56: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_146, 1);  slice_tensor_146 = None\\n        squeeze_dim_57: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_56, 0);  squeeze_dim_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_58: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_149, 1);  slice_tensor_149 = None\\n        squeeze_dim_59: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_58, 0);  squeeze_dim_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_28: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_57, [view_default]);  squeeze_dim_57 = None\\n        unsqueeze_default_33: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_28, 1);  index_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_29: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_59, [view_default]);  squeeze_dim_59 = None\\n        unsqueeze_default_34: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_29, 1);  index_tensor_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_128: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_70, unsqueeze_default_33)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_150: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_151: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_70, 3, 64, 9223372036854775807);  transpose_int_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_28: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_151);  slice_tensor_151 = None\\n        cat_default_28: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_28, slice_tensor_150], -1);  neg_default_28 = slice_tensor_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_129: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_28, unsqueeze_default_34);  cat_default_28 = None\\n        add_tensor_101: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_128, mul_tensor_129);  mul_tensor_128 = mul_tensor_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_130: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_71, unsqueeze_default_33);  unsqueeze_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_152: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_153: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_71, 3, 64, 9223372036854775807);  transpose_int_71 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_29: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_153);  slice_tensor_153 = None\\n        cat_default_29: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_29, slice_tensor_152], -1);  neg_default_29 = slice_tensor_152 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_131: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_29, unsqueeze_default_34);  cat_default_29 = unsqueeze_default_34 = None\\n        add_tensor_102: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_130, mul_tensor_131);  mul_tensor_130 = mul_tensor_131 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_73: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_102, 2, 3)\\n        sym_size_58: Sym(s0) = torch.ops.aten.sym_size(view_default_339, 1);  view_default_339 = None\\n        expand_default_58: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_101, [1, 32, sym_size_58, 128]);  add_tensor_101 = None\\n        view_default_347: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_58, [32, sym_size_58, 128]);  expand_default_58 = None\\n        expand_default_59: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_73, [1, 32, 128, sym_size_57]);  transpose_int_73 = None\\n        view_default_348: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_59, [32, 128, sym_size_57]);  expand_default_59 = None\\n        bmm_default_28: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_347, view_default_348);  view_default_347 = view_default_348 = None\\n        view_default_349: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_28, [1, 32, sym_size_58, sym_size_57]);  bmm_default_28 = None\\n        div_tensor_14: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_349, 11.313708498984761);  view_default_349 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_103: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_14, add_tensor_1);  div_tensor_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_14: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_103, -1, False);  add_tensor_103 = None\\n        detach_default_43: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_14)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_60: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_14, [1, 32, sym_size_58, sym_size_57]);  _softmax_default_14 = None\\n        view_default_350: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_60, [32, sym_size_58, sym_size_57]);  expand_default_60 = sym_size_57 = None\\n        sym_size_59: Sym(s0) = torch.ops.aten.sym_size(view_default_343, 1);  view_default_343 = None\\n        expand_default_61: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_72, [1, 32, sym_size_59, 128])\\n        view_default_351: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_61, [32, sym_size_59, 128]);  expand_default_61 = sym_size_59 = None\\n        bmm_default_29: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_350, view_default_351);  view_default_350 = view_default_351 = None\\n        view_default_352: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_29, [1, 32, sym_size_58, 128]);  bmm_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_74: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_352, 1, 2);  view_default_352 = None\\n        clone_default_14: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_74, memory_format = torch.contiguous_format);  transpose_int_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_353: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_14, [1, sym_size, 4096]);  clone_default_14 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant131 = self._param_constant131\\n        t_default_101: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant131);  _param_constant131 = None\\n        view_default_354: f32[s0, 4096] = torch.ops.aten.view.default(view_default_353, [sym_size_58, 4096]);  view_default_353 = None\\n        mm_default_101: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_354, t_default_101);  view_default_354 = t_default_101 = None\\n        view_default_355: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_101, [1, sym_size_58, 4096]);  mm_default_101 = sym_size_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_104: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_99, view_default_355);  add_tensor_99 = view_default_355 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_29: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_104, 2)\\n        mean_dim_29: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_29, [-1], True);  pow_tensor_scalar_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_105: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_29, 1e-06);  mean_dim_29 = None\\n        rsqrt_default_29: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_105);  add_tensor_105 = None\\n        detach_default_44: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_29)\\n        mul_tensor_132: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_104, rsqrt_default_29);  rsqrt_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant132 = self._param_constant132\\n        mul_tensor_133: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant132, mul_tensor_132);  _param_constant132 = mul_tensor_132 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant133 = self._param_constant133\\n        t_default_102: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant133);  _param_constant133 = None\\n        view_default_356: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_133, [sym_size, 4096])\\n        mm_default_102: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_356, t_default_102);  view_default_356 = t_default_102 = None\\n        view_default_357: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_102, [1, sym_size, 11008]);  mm_default_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_14: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_357)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant134 = self._param_constant134\\n        t_default_103: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant134);  _param_constant134 = None\\n        view_default_358: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_133, [sym_size, 4096]);  mul_tensor_133 = None\\n        mm_default_103: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_358, t_default_103);  view_default_358 = t_default_103 = None\\n        view_default_359: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_103, [1, sym_size, 11008]);  mm_default_103 = None\\n        mul_tensor_134: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_14, view_default_359);  silu_default_14 = view_default_359 = None\\n        _param_constant135 = self._param_constant135\\n        t_default_104: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant135);  _param_constant135 = None\\n        sym_size_60: Sym(s0) = torch.ops.aten.sym_size(view_default_357, 1);  view_default_357 = None\\n        view_default_360: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_134, [sym_size_60, 11008]);  mul_tensor_134 = None\\n        mm_default_104: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_360, t_default_104);  view_default_360 = t_default_104 = None\\n        view_default_361: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_104, [1, sym_size_60, 4096]);  mm_default_104 = sym_size_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_106: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_104, view_default_361);  add_tensor_104 = view_default_361 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_30: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_106, 2)\\n        mean_dim_30: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_30, [-1], True);  pow_tensor_scalar_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_107: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_30, 1e-06);  mean_dim_30 = None\\n        rsqrt_default_30: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_107);  add_tensor_107 = None\\n        detach_default_45: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_30)\\n        mul_tensor_135: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_106, rsqrt_default_30);  rsqrt_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant136 = self._param_constant136\\n        mul_tensor_136: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant136, mul_tensor_135);  _param_constant136 = mul_tensor_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant137 = self._param_constant137\\n        t_default_105: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant137);  _param_constant137 = None\\n        view_default_362: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_136, [sym_size, 4096])\\n        mm_default_105: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_362, t_default_105);  view_default_362 = t_default_105 = None\\n        view_default_363: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_105, [1, sym_size, 4096]);  mm_default_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant138 = self._param_constant138\\n        t_default_106: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant138);  _param_constant138 = None\\n        view_default_364: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_136, [sym_size, 4096])\\n        mm_default_106: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_364, t_default_106);  view_default_364 = t_default_106 = None\\n        view_default_365: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_106, [1, sym_size, 4096]);  mm_default_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant139 = self._param_constant139\\n        t_default_107: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant139);  _param_constant139 = None\\n        view_default_366: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_136, [sym_size, 4096]);  mul_tensor_136 = None\\n        mm_default_107: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_366, t_default_107);  view_default_366 = t_default_107 = None\\n        view_default_367: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_107, [1, sym_size, 4096]);  mm_default_107 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_368: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_363, [1, sym_size, 32, 128])\\n        transpose_int_75: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_368, 1, 2);  view_default_368 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_369: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_365, [1, sym_size, 32, 128])\\n        transpose_int_76: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_369, 1, 2);  view_default_369 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_370: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_367, [1, sym_size, 32, 128])\\n        transpose_int_77: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_370, 1, 2);  view_default_370 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant30 = self._tensor_constant30\\n        slice_tensor_154: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant30, 0, 0, 9223372036854775807);  _tensor_constant30 = None\\n        slice_tensor_155: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_154, 1, 0, 9223372036854775807);  slice_tensor_154 = None\\n        sym_size_61: Sym(s0) = torch.ops.aten.sym_size(view_default_365, 1);  view_default_365 = None\\n        slice_tensor_156: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_155, 2, 0, sym_size_61);  slice_tensor_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant31 = self._tensor_constant31\\n        slice_tensor_157: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant31, 0, 0, 9223372036854775807);  _tensor_constant31 = None\\n        slice_tensor_158: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_157, 1, 0, 9223372036854775807);  slice_tensor_157 = None\\n        slice_tensor_159: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_158, 2, 0, sym_size_61);  slice_tensor_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_60: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_156, 1);  slice_tensor_156 = None\\n        squeeze_dim_61: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_60, 0);  squeeze_dim_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_62: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_159, 1);  slice_tensor_159 = None\\n        squeeze_dim_63: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_62, 0);  squeeze_dim_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_30: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_61, [view_default]);  squeeze_dim_61 = None\\n        unsqueeze_default_35: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_30, 1);  index_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_31: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_63, [view_default]);  squeeze_dim_63 = None\\n        unsqueeze_default_36: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_31, 1);  index_tensor_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_137: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_75, unsqueeze_default_35)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_160: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_161: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_75, 3, 64, 9223372036854775807);  transpose_int_75 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_30: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_161);  slice_tensor_161 = None\\n        cat_default_30: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_30, slice_tensor_160], -1);  neg_default_30 = slice_tensor_160 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_138: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_30, unsqueeze_default_36);  cat_default_30 = None\\n        add_tensor_108: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_137, mul_tensor_138);  mul_tensor_137 = mul_tensor_138 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_139: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_76, unsqueeze_default_35);  unsqueeze_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_162: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_163: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_76, 3, 64, 9223372036854775807);  transpose_int_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_31: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_163);  slice_tensor_163 = None\\n        cat_default_31: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_31, slice_tensor_162], -1);  neg_default_31 = slice_tensor_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_140: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_31, unsqueeze_default_36);  cat_default_31 = unsqueeze_default_36 = None\\n        add_tensor_109: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_139, mul_tensor_140);  mul_tensor_139 = mul_tensor_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_78: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_109, 2, 3)\\n        sym_size_62: Sym(s0) = torch.ops.aten.sym_size(view_default_363, 1);  view_default_363 = None\\n        expand_default_62: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_108, [1, 32, sym_size_62, 128]);  add_tensor_108 = None\\n        view_default_371: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_62, [32, sym_size_62, 128]);  expand_default_62 = None\\n        expand_default_63: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_78, [1, 32, 128, sym_size_61]);  transpose_int_78 = None\\n        view_default_372: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_63, [32, 128, sym_size_61]);  expand_default_63 = None\\n        bmm_default_30: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_371, view_default_372);  view_default_371 = view_default_372 = None\\n        view_default_373: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_30, [1, 32, sym_size_62, sym_size_61]);  bmm_default_30 = None\\n        div_tensor_15: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_373, 11.313708498984761);  view_default_373 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_110: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_15, add_tensor_1);  div_tensor_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_15: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_110, -1, False);  add_tensor_110 = None\\n        detach_default_46: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_15)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_64: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_15, [1, 32, sym_size_62, sym_size_61]);  _softmax_default_15 = None\\n        view_default_374: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_64, [32, sym_size_62, sym_size_61]);  expand_default_64 = sym_size_61 = None\\n        sym_size_63: Sym(s0) = torch.ops.aten.sym_size(view_default_367, 1);  view_default_367 = None\\n        expand_default_65: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_77, [1, 32, sym_size_63, 128])\\n        view_default_375: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_65, [32, sym_size_63, 128]);  expand_default_65 = sym_size_63 = None\\n        bmm_default_31: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_374, view_default_375);  view_default_374 = view_default_375 = None\\n        view_default_376: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_31, [1, 32, sym_size_62, 128]);  bmm_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_79: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_376, 1, 2);  view_default_376 = None\\n        clone_default_15: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_79, memory_format = torch.contiguous_format);  transpose_int_79 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_377: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_15, [1, sym_size, 4096]);  clone_default_15 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant140 = self._param_constant140\\n        t_default_108: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant140);  _param_constant140 = None\\n        view_default_378: f32[s0, 4096] = torch.ops.aten.view.default(view_default_377, [sym_size_62, 4096]);  view_default_377 = None\\n        mm_default_108: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_378, t_default_108);  view_default_378 = t_default_108 = None\\n        view_default_379: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_108, [1, sym_size_62, 4096]);  mm_default_108 = sym_size_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_111: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_106, view_default_379);  add_tensor_106 = view_default_379 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_31: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_111, 2)\\n        mean_dim_31: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_31, [-1], True);  pow_tensor_scalar_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_112: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_31, 1e-06);  mean_dim_31 = None\\n        rsqrt_default_31: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_112);  add_tensor_112 = None\\n        detach_default_47: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_31)\\n        mul_tensor_141: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_111, rsqrt_default_31);  rsqrt_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant141 = self._param_constant141\\n        mul_tensor_142: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant141, mul_tensor_141);  _param_constant141 = mul_tensor_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant142 = self._param_constant142\\n        t_default_109: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant142);  _param_constant142 = None\\n        view_default_380: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_142, [sym_size, 4096])\\n        mm_default_109: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_380, t_default_109);  view_default_380 = t_default_109 = None\\n        view_default_381: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_109, [1, sym_size, 11008]);  mm_default_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_15: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_381)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant143 = self._param_constant143\\n        t_default_110: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant143);  _param_constant143 = None\\n        view_default_382: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_142, [sym_size, 4096]);  mul_tensor_142 = None\\n        mm_default_110: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_382, t_default_110);  view_default_382 = t_default_110 = None\\n        view_default_383: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_110, [1, sym_size, 11008]);  mm_default_110 = None\\n        mul_tensor_143: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_15, view_default_383);  silu_default_15 = view_default_383 = None\\n        _param_constant144 = self._param_constant144\\n        t_default_111: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant144);  _param_constant144 = None\\n        sym_size_64: Sym(s0) = torch.ops.aten.sym_size(view_default_381, 1);  view_default_381 = None\\n        view_default_384: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_143, [sym_size_64, 11008]);  mul_tensor_143 = None\\n        mm_default_111: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_384, t_default_111);  view_default_384 = t_default_111 = None\\n        view_default_385: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_111, [1, sym_size_64, 4096]);  mm_default_111 = sym_size_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_113: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_111, view_default_385);  add_tensor_111 = view_default_385 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_32: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_113, 2)\\n        mean_dim_32: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_32, [-1], True);  pow_tensor_scalar_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_114: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_32, 1e-06);  mean_dim_32 = None\\n        rsqrt_default_32: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_114);  add_tensor_114 = None\\n        detach_default_48: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_32)\\n        mul_tensor_144: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_113, rsqrt_default_32);  rsqrt_default_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant145 = self._param_constant145\\n        mul_tensor_145: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant145, mul_tensor_144);  _param_constant145 = mul_tensor_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant146 = self._param_constant146\\n        t_default_112: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant146);  _param_constant146 = None\\n        view_default_386: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_145, [sym_size, 4096])\\n        mm_default_112: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_386, t_default_112);  view_default_386 = t_default_112 = None\\n        view_default_387: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_112, [1, sym_size, 4096]);  mm_default_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant147 = self._param_constant147\\n        t_default_113: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant147);  _param_constant147 = None\\n        view_default_388: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_145, [sym_size, 4096])\\n        mm_default_113: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_388, t_default_113);  view_default_388 = t_default_113 = None\\n        view_default_389: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_113, [1, sym_size, 4096]);  mm_default_113 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant148 = self._param_constant148\\n        t_default_114: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant148);  _param_constant148 = None\\n        view_default_390: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_145, [sym_size, 4096]);  mul_tensor_145 = None\\n        mm_default_114: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_390, t_default_114);  view_default_390 = t_default_114 = None\\n        view_default_391: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_114, [1, sym_size, 4096]);  mm_default_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_392: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_387, [1, sym_size, 32, 128])\\n        transpose_int_80: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_392, 1, 2);  view_default_392 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_393: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_389, [1, sym_size, 32, 128])\\n        transpose_int_81: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_393, 1, 2);  view_default_393 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_394: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_391, [1, sym_size, 32, 128])\\n        transpose_int_82: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_394, 1, 2);  view_default_394 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant32 = self._tensor_constant32\\n        slice_tensor_164: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant32, 0, 0, 9223372036854775807);  _tensor_constant32 = None\\n        slice_tensor_165: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_164, 1, 0, 9223372036854775807);  slice_tensor_164 = None\\n        sym_size_65: Sym(s0) = torch.ops.aten.sym_size(view_default_389, 1);  view_default_389 = None\\n        slice_tensor_166: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_165, 2, 0, sym_size_65);  slice_tensor_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant33 = self._tensor_constant33\\n        slice_tensor_167: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant33, 0, 0, 9223372036854775807);  _tensor_constant33 = None\\n        slice_tensor_168: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_167, 1, 0, 9223372036854775807);  slice_tensor_167 = None\\n        slice_tensor_169: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_168, 2, 0, sym_size_65);  slice_tensor_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_64: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_166, 1);  slice_tensor_166 = None\\n        squeeze_dim_65: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_64, 0);  squeeze_dim_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_66: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_169, 1);  slice_tensor_169 = None\\n        squeeze_dim_67: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_66, 0);  squeeze_dim_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_32: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_65, [view_default]);  squeeze_dim_65 = None\\n        unsqueeze_default_37: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_32, 1);  index_tensor_32 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_33: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_67, [view_default]);  squeeze_dim_67 = None\\n        unsqueeze_default_38: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_33, 1);  index_tensor_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_146: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_80, unsqueeze_default_37)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_170: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_171: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_80, 3, 64, 9223372036854775807);  transpose_int_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_32: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_171);  slice_tensor_171 = None\\n        cat_default_32: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_32, slice_tensor_170], -1);  neg_default_32 = slice_tensor_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_147: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_32, unsqueeze_default_38);  cat_default_32 = None\\n        add_tensor_115: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_146, mul_tensor_147);  mul_tensor_146 = mul_tensor_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_148: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_81, unsqueeze_default_37);  unsqueeze_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_172: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_173: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_81, 3, 64, 9223372036854775807);  transpose_int_81 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_33: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_173);  slice_tensor_173 = None\\n        cat_default_33: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_33, slice_tensor_172], -1);  neg_default_33 = slice_tensor_172 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_149: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_33, unsqueeze_default_38);  cat_default_33 = unsqueeze_default_38 = None\\n        add_tensor_116: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_148, mul_tensor_149);  mul_tensor_148 = mul_tensor_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_83: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_116, 2, 3)\\n        sym_size_66: Sym(s0) = torch.ops.aten.sym_size(view_default_387, 1);  view_default_387 = None\\n        expand_default_66: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_115, [1, 32, sym_size_66, 128]);  add_tensor_115 = None\\n        view_default_395: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_66, [32, sym_size_66, 128]);  expand_default_66 = None\\n        expand_default_67: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_83, [1, 32, 128, sym_size_65]);  transpose_int_83 = None\\n        view_default_396: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_67, [32, 128, sym_size_65]);  expand_default_67 = None\\n        bmm_default_32: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_395, view_default_396);  view_default_395 = view_default_396 = None\\n        view_default_397: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_32, [1, 32, sym_size_66, sym_size_65]);  bmm_default_32 = None\\n        div_tensor_16: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_397, 11.313708498984761);  view_default_397 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_117: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_16, add_tensor_1);  div_tensor_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_16: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_117, -1, False);  add_tensor_117 = None\\n        detach_default_49: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_16)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_68: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_16, [1, 32, sym_size_66, sym_size_65]);  _softmax_default_16 = None\\n        view_default_398: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_68, [32, sym_size_66, sym_size_65]);  expand_default_68 = sym_size_65 = None\\n        sym_size_67: Sym(s0) = torch.ops.aten.sym_size(view_default_391, 1);  view_default_391 = None\\n        expand_default_69: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_82, [1, 32, sym_size_67, 128])\\n        view_default_399: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_69, [32, sym_size_67, 128]);  expand_default_69 = sym_size_67 = None\\n        bmm_default_33: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_398, view_default_399);  view_default_398 = view_default_399 = None\\n        view_default_400: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_33, [1, 32, sym_size_66, 128]);  bmm_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_84: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_400, 1, 2);  view_default_400 = None\\n        clone_default_16: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_84, memory_format = torch.contiguous_format);  transpose_int_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_401: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_16, [1, sym_size, 4096]);  clone_default_16 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant149 = self._param_constant149\\n        t_default_115: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant149);  _param_constant149 = None\\n        view_default_402: f32[s0, 4096] = torch.ops.aten.view.default(view_default_401, [sym_size_66, 4096]);  view_default_401 = None\\n        mm_default_115: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_402, t_default_115);  view_default_402 = t_default_115 = None\\n        view_default_403: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_115, [1, sym_size_66, 4096]);  mm_default_115 = sym_size_66 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_118: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_113, view_default_403);  add_tensor_113 = view_default_403 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_33: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_118, 2)\\n        mean_dim_33: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_33, [-1], True);  pow_tensor_scalar_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_119: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_33, 1e-06);  mean_dim_33 = None\\n        rsqrt_default_33: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_119);  add_tensor_119 = None\\n        detach_default_50: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_33)\\n        mul_tensor_150: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_118, rsqrt_default_33);  rsqrt_default_33 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant150 = self._param_constant150\\n        mul_tensor_151: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant150, mul_tensor_150);  _param_constant150 = mul_tensor_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant151 = self._param_constant151\\n        t_default_116: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant151);  _param_constant151 = None\\n        view_default_404: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_151, [sym_size, 4096])\\n        mm_default_116: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_404, t_default_116);  view_default_404 = t_default_116 = None\\n        view_default_405: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_116, [1, sym_size, 11008]);  mm_default_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_16: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_405)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant152 = self._param_constant152\\n        t_default_117: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant152);  _param_constant152 = None\\n        view_default_406: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_151, [sym_size, 4096]);  mul_tensor_151 = None\\n        mm_default_117: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_406, t_default_117);  view_default_406 = t_default_117 = None\\n        view_default_407: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_117, [1, sym_size, 11008]);  mm_default_117 = None\\n        mul_tensor_152: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_16, view_default_407);  silu_default_16 = view_default_407 = None\\n        _param_constant153 = self._param_constant153\\n        t_default_118: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant153);  _param_constant153 = None\\n        sym_size_68: Sym(s0) = torch.ops.aten.sym_size(view_default_405, 1);  view_default_405 = None\\n        view_default_408: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_152, [sym_size_68, 11008]);  mul_tensor_152 = None\\n        mm_default_118: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_408, t_default_118);  view_default_408 = t_default_118 = None\\n        view_default_409: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_118, [1, sym_size_68, 4096]);  mm_default_118 = sym_size_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_120: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_118, view_default_409);  add_tensor_118 = view_default_409 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_34: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_120, 2)\\n        mean_dim_34: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_34, [-1], True);  pow_tensor_scalar_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_121: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_34, 1e-06);  mean_dim_34 = None\\n        rsqrt_default_34: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_121);  add_tensor_121 = None\\n        detach_default_51: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_34)\\n        mul_tensor_153: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_120, rsqrt_default_34);  rsqrt_default_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant154 = self._param_constant154\\n        mul_tensor_154: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant154, mul_tensor_153);  _param_constant154 = mul_tensor_153 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant155 = self._param_constant155\\n        t_default_119: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant155);  _param_constant155 = None\\n        view_default_410: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_154, [sym_size, 4096])\\n        mm_default_119: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_410, t_default_119);  view_default_410 = t_default_119 = None\\n        view_default_411: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_119, [1, sym_size, 4096]);  mm_default_119 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant156 = self._param_constant156\\n        t_default_120: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant156);  _param_constant156 = None\\n        view_default_412: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_154, [sym_size, 4096])\\n        mm_default_120: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_412, t_default_120);  view_default_412 = t_default_120 = None\\n        view_default_413: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_120, [1, sym_size, 4096]);  mm_default_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant157 = self._param_constant157\\n        t_default_121: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant157);  _param_constant157 = None\\n        view_default_414: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_154, [sym_size, 4096]);  mul_tensor_154 = None\\n        mm_default_121: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_414, t_default_121);  view_default_414 = t_default_121 = None\\n        view_default_415: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_121, [1, sym_size, 4096]);  mm_default_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_416: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_411, [1, sym_size, 32, 128])\\n        transpose_int_85: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_416, 1, 2);  view_default_416 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_417: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_413, [1, sym_size, 32, 128])\\n        transpose_int_86: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_417, 1, 2);  view_default_417 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_418: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_415, [1, sym_size, 32, 128])\\n        transpose_int_87: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_418, 1, 2);  view_default_418 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant34 = self._tensor_constant34\\n        slice_tensor_174: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant34, 0, 0, 9223372036854775807);  _tensor_constant34 = None\\n        slice_tensor_175: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_174, 1, 0, 9223372036854775807);  slice_tensor_174 = None\\n        sym_size_69: Sym(s0) = torch.ops.aten.sym_size(view_default_413, 1);  view_default_413 = None\\n        slice_tensor_176: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_175, 2, 0, sym_size_69);  slice_tensor_175 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant35 = self._tensor_constant35\\n        slice_tensor_177: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant35, 0, 0, 9223372036854775807);  _tensor_constant35 = None\\n        slice_tensor_178: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_177, 1, 0, 9223372036854775807);  slice_tensor_177 = None\\n        slice_tensor_179: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_178, 2, 0, sym_size_69);  slice_tensor_178 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_68: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_176, 1);  slice_tensor_176 = None\\n        squeeze_dim_69: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_68, 0);  squeeze_dim_68 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_70: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_179, 1);  slice_tensor_179 = None\\n        squeeze_dim_71: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_70, 0);  squeeze_dim_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_34: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_69, [view_default]);  squeeze_dim_69 = None\\n        unsqueeze_default_39: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_34, 1);  index_tensor_34 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_35: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_71, [view_default]);  squeeze_dim_71 = None\\n        unsqueeze_default_40: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_35, 1);  index_tensor_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_155: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_85, unsqueeze_default_39)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_180: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_181: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_85, 3, 64, 9223372036854775807);  transpose_int_85 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_34: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_181);  slice_tensor_181 = None\\n        cat_default_34: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_34, slice_tensor_180], -1);  neg_default_34 = slice_tensor_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_156: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_34, unsqueeze_default_40);  cat_default_34 = None\\n        add_tensor_122: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_155, mul_tensor_156);  mul_tensor_155 = mul_tensor_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_157: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_86, unsqueeze_default_39);  unsqueeze_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_182: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_183: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_86, 3, 64, 9223372036854775807);  transpose_int_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_35: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_183);  slice_tensor_183 = None\\n        cat_default_35: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_35, slice_tensor_182], -1);  neg_default_35 = slice_tensor_182 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_158: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_35, unsqueeze_default_40);  cat_default_35 = unsqueeze_default_40 = None\\n        add_tensor_123: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_157, mul_tensor_158);  mul_tensor_157 = mul_tensor_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_88: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_123, 2, 3)\\n        sym_size_70: Sym(s0) = torch.ops.aten.sym_size(view_default_411, 1);  view_default_411 = None\\n        expand_default_70: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_122, [1, 32, sym_size_70, 128]);  add_tensor_122 = None\\n        view_default_419: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_70, [32, sym_size_70, 128]);  expand_default_70 = None\\n        expand_default_71: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_88, [1, 32, 128, sym_size_69]);  transpose_int_88 = None\\n        view_default_420: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_71, [32, 128, sym_size_69]);  expand_default_71 = None\\n        bmm_default_34: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_419, view_default_420);  view_default_419 = view_default_420 = None\\n        view_default_421: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_34, [1, 32, sym_size_70, sym_size_69]);  bmm_default_34 = None\\n        div_tensor_17: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_421, 11.313708498984761);  view_default_421 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_124: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_17, add_tensor_1);  div_tensor_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_17: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_124, -1, False);  add_tensor_124 = None\\n        detach_default_52: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_17)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_72: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_17, [1, 32, sym_size_70, sym_size_69]);  _softmax_default_17 = None\\n        view_default_422: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_72, [32, sym_size_70, sym_size_69]);  expand_default_72 = sym_size_69 = None\\n        sym_size_71: Sym(s0) = torch.ops.aten.sym_size(view_default_415, 1);  view_default_415 = None\\n        expand_default_73: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_87, [1, 32, sym_size_71, 128])\\n        view_default_423: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_73, [32, sym_size_71, 128]);  expand_default_73 = sym_size_71 = None\\n        bmm_default_35: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_422, view_default_423);  view_default_422 = view_default_423 = None\\n        view_default_424: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_35, [1, 32, sym_size_70, 128]);  bmm_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_89: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_424, 1, 2);  view_default_424 = None\\n        clone_default_17: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_89, memory_format = torch.contiguous_format);  transpose_int_89 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_425: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_17, [1, sym_size, 4096]);  clone_default_17 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant158 = self._param_constant158\\n        t_default_122: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant158);  _param_constant158 = None\\n        view_default_426: f32[s0, 4096] = torch.ops.aten.view.default(view_default_425, [sym_size_70, 4096]);  view_default_425 = None\\n        mm_default_122: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_426, t_default_122);  view_default_426 = t_default_122 = None\\n        view_default_427: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_122, [1, sym_size_70, 4096]);  mm_default_122 = sym_size_70 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_125: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_120, view_default_427);  add_tensor_120 = view_default_427 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_35: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_125, 2)\\n        mean_dim_35: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_35, [-1], True);  pow_tensor_scalar_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_126: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_35, 1e-06);  mean_dim_35 = None\\n        rsqrt_default_35: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_126);  add_tensor_126 = None\\n        detach_default_53: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_35)\\n        mul_tensor_159: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_125, rsqrt_default_35);  rsqrt_default_35 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant159 = self._param_constant159\\n        mul_tensor_160: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant159, mul_tensor_159);  _param_constant159 = mul_tensor_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant160 = self._param_constant160\\n        t_default_123: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant160);  _param_constant160 = None\\n        view_default_428: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_160, [sym_size, 4096])\\n        mm_default_123: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_428, t_default_123);  view_default_428 = t_default_123 = None\\n        view_default_429: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_123, [1, sym_size, 11008]);  mm_default_123 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_17: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_429)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant161 = self._param_constant161\\n        t_default_124: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant161);  _param_constant161 = None\\n        view_default_430: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_160, [sym_size, 4096]);  mul_tensor_160 = None\\n        mm_default_124: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_430, t_default_124);  view_default_430 = t_default_124 = None\\n        view_default_431: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_124, [1, sym_size, 11008]);  mm_default_124 = None\\n        mul_tensor_161: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_17, view_default_431);  silu_default_17 = view_default_431 = None\\n        _param_constant162 = self._param_constant162\\n        t_default_125: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant162);  _param_constant162 = None\\n        sym_size_72: Sym(s0) = torch.ops.aten.sym_size(view_default_429, 1);  view_default_429 = None\\n        view_default_432: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_161, [sym_size_72, 11008]);  mul_tensor_161 = None\\n        mm_default_125: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_432, t_default_125);  view_default_432 = t_default_125 = None\\n        view_default_433: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_125, [1, sym_size_72, 4096]);  mm_default_125 = sym_size_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_127: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_125, view_default_433);  add_tensor_125 = view_default_433 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_36: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_127, 2)\\n        mean_dim_36: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_36, [-1], True);  pow_tensor_scalar_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_128: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_36, 1e-06);  mean_dim_36 = None\\n        rsqrt_default_36: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_128);  add_tensor_128 = None\\n        detach_default_54: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_36)\\n        mul_tensor_162: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_127, rsqrt_default_36);  rsqrt_default_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant163 = self._param_constant163\\n        mul_tensor_163: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant163, mul_tensor_162);  _param_constant163 = mul_tensor_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant164 = self._param_constant164\\n        t_default_126: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant164);  _param_constant164 = None\\n        view_default_434: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_163, [sym_size, 4096])\\n        mm_default_126: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_434, t_default_126);  view_default_434 = t_default_126 = None\\n        view_default_435: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_126, [1, sym_size, 4096]);  mm_default_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant165 = self._param_constant165\\n        t_default_127: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant165);  _param_constant165 = None\\n        view_default_436: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_163, [sym_size, 4096])\\n        mm_default_127: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_436, t_default_127);  view_default_436 = t_default_127 = None\\n        view_default_437: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_127, [1, sym_size, 4096]);  mm_default_127 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant166 = self._param_constant166\\n        t_default_128: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant166);  _param_constant166 = None\\n        view_default_438: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_163, [sym_size, 4096]);  mul_tensor_163 = None\\n        mm_default_128: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_438, t_default_128);  view_default_438 = t_default_128 = None\\n        view_default_439: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_128, [1, sym_size, 4096]);  mm_default_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_440: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_435, [1, sym_size, 32, 128])\\n        transpose_int_90: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_440, 1, 2);  view_default_440 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_441: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_437, [1, sym_size, 32, 128])\\n        transpose_int_91: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_441, 1, 2);  view_default_441 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_442: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_439, [1, sym_size, 32, 128])\\n        transpose_int_92: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_442, 1, 2);  view_default_442 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant36 = self._tensor_constant36\\n        slice_tensor_184: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant36, 0, 0, 9223372036854775807);  _tensor_constant36 = None\\n        slice_tensor_185: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_184, 1, 0, 9223372036854775807);  slice_tensor_184 = None\\n        sym_size_73: Sym(s0) = torch.ops.aten.sym_size(view_default_437, 1);  view_default_437 = None\\n        slice_tensor_186: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_185, 2, 0, sym_size_73);  slice_tensor_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant37 = self._tensor_constant37\\n        slice_tensor_187: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant37, 0, 0, 9223372036854775807);  _tensor_constant37 = None\\n        slice_tensor_188: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_187, 1, 0, 9223372036854775807);  slice_tensor_187 = None\\n        slice_tensor_189: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_188, 2, 0, sym_size_73);  slice_tensor_188 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_72: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_186, 1);  slice_tensor_186 = None\\n        squeeze_dim_73: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_72, 0);  squeeze_dim_72 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_74: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_189, 1);  slice_tensor_189 = None\\n        squeeze_dim_75: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_74, 0);  squeeze_dim_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_36: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_73, [view_default]);  squeeze_dim_73 = None\\n        unsqueeze_default_41: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_36, 1);  index_tensor_36 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_37: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_75, [view_default]);  squeeze_dim_75 = None\\n        unsqueeze_default_42: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_37, 1);  index_tensor_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_164: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_90, unsqueeze_default_41)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_190: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_191: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_90, 3, 64, 9223372036854775807);  transpose_int_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_36: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_191);  slice_tensor_191 = None\\n        cat_default_36: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_36, slice_tensor_190], -1);  neg_default_36 = slice_tensor_190 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_165: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_36, unsqueeze_default_42);  cat_default_36 = None\\n        add_tensor_129: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_164, mul_tensor_165);  mul_tensor_164 = mul_tensor_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_166: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_91, unsqueeze_default_41);  unsqueeze_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_192: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_193: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_91, 3, 64, 9223372036854775807);  transpose_int_91 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_37: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_193);  slice_tensor_193 = None\\n        cat_default_37: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_37, slice_tensor_192], -1);  neg_default_37 = slice_tensor_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_167: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_37, unsqueeze_default_42);  cat_default_37 = unsqueeze_default_42 = None\\n        add_tensor_130: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_166, mul_tensor_167);  mul_tensor_166 = mul_tensor_167 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_93: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_130, 2, 3)\\n        sym_size_74: Sym(s0) = torch.ops.aten.sym_size(view_default_435, 1);  view_default_435 = None\\n        expand_default_74: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_129, [1, 32, sym_size_74, 128]);  add_tensor_129 = None\\n        view_default_443: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_74, [32, sym_size_74, 128]);  expand_default_74 = None\\n        expand_default_75: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_93, [1, 32, 128, sym_size_73]);  transpose_int_93 = None\\n        view_default_444: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_75, [32, 128, sym_size_73]);  expand_default_75 = None\\n        bmm_default_36: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_443, view_default_444);  view_default_443 = view_default_444 = None\\n        view_default_445: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_36, [1, 32, sym_size_74, sym_size_73]);  bmm_default_36 = None\\n        div_tensor_18: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_445, 11.313708498984761);  view_default_445 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_131: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_18, add_tensor_1);  div_tensor_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_18: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_131, -1, False);  add_tensor_131 = None\\n        detach_default_55: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_18)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_76: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_18, [1, 32, sym_size_74, sym_size_73]);  _softmax_default_18 = None\\n        view_default_446: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_76, [32, sym_size_74, sym_size_73]);  expand_default_76 = sym_size_73 = None\\n        sym_size_75: Sym(s0) = torch.ops.aten.sym_size(view_default_439, 1);  view_default_439 = None\\n        expand_default_77: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_92, [1, 32, sym_size_75, 128])\\n        view_default_447: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_77, [32, sym_size_75, 128]);  expand_default_77 = sym_size_75 = None\\n        bmm_default_37: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_446, view_default_447);  view_default_446 = view_default_447 = None\\n        view_default_448: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_37, [1, 32, sym_size_74, 128]);  bmm_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_94: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_448, 1, 2);  view_default_448 = None\\n        clone_default_18: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_94, memory_format = torch.contiguous_format);  transpose_int_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_449: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_18, [1, sym_size, 4096]);  clone_default_18 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant167 = self._param_constant167\\n        t_default_129: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant167);  _param_constant167 = None\\n        view_default_450: f32[s0, 4096] = torch.ops.aten.view.default(view_default_449, [sym_size_74, 4096]);  view_default_449 = None\\n        mm_default_129: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_450, t_default_129);  view_default_450 = t_default_129 = None\\n        view_default_451: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_129, [1, sym_size_74, 4096]);  mm_default_129 = sym_size_74 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_132: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_127, view_default_451);  add_tensor_127 = view_default_451 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_37: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_132, 2)\\n        mean_dim_37: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_37, [-1], True);  pow_tensor_scalar_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_133: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_37, 1e-06);  mean_dim_37 = None\\n        rsqrt_default_37: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_133);  add_tensor_133 = None\\n        detach_default_56: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_37)\\n        mul_tensor_168: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_132, rsqrt_default_37);  rsqrt_default_37 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant168 = self._param_constant168\\n        mul_tensor_169: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant168, mul_tensor_168);  _param_constant168 = mul_tensor_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant169 = self._param_constant169\\n        t_default_130: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant169);  _param_constant169 = None\\n        view_default_452: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_169, [sym_size, 4096])\\n        mm_default_130: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_452, t_default_130);  view_default_452 = t_default_130 = None\\n        view_default_453: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_130, [1, sym_size, 11008]);  mm_default_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_18: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_453)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant170 = self._param_constant170\\n        t_default_131: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant170);  _param_constant170 = None\\n        view_default_454: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_169, [sym_size, 4096]);  mul_tensor_169 = None\\n        mm_default_131: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_454, t_default_131);  view_default_454 = t_default_131 = None\\n        view_default_455: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_131, [1, sym_size, 11008]);  mm_default_131 = None\\n        mul_tensor_170: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_18, view_default_455);  silu_default_18 = view_default_455 = None\\n        _param_constant171 = self._param_constant171\\n        t_default_132: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant171);  _param_constant171 = None\\n        sym_size_76: Sym(s0) = torch.ops.aten.sym_size(view_default_453, 1);  view_default_453 = None\\n        view_default_456: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_170, [sym_size_76, 11008]);  mul_tensor_170 = None\\n        mm_default_132: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_456, t_default_132);  view_default_456 = t_default_132 = None\\n        view_default_457: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_132, [1, sym_size_76, 4096]);  mm_default_132 = sym_size_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_134: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_132, view_default_457);  add_tensor_132 = view_default_457 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_38: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_134, 2)\\n        mean_dim_38: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_38, [-1], True);  pow_tensor_scalar_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_135: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_38, 1e-06);  mean_dim_38 = None\\n        rsqrt_default_38: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_135);  add_tensor_135 = None\\n        detach_default_57: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_38)\\n        mul_tensor_171: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_134, rsqrt_default_38);  rsqrt_default_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant172 = self._param_constant172\\n        mul_tensor_172: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant172, mul_tensor_171);  _param_constant172 = mul_tensor_171 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant173 = self._param_constant173\\n        t_default_133: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant173);  _param_constant173 = None\\n        view_default_458: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_172, [sym_size, 4096])\\n        mm_default_133: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_458, t_default_133);  view_default_458 = t_default_133 = None\\n        view_default_459: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_133, [1, sym_size, 4096]);  mm_default_133 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant174 = self._param_constant174\\n        t_default_134: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant174);  _param_constant174 = None\\n        view_default_460: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_172, [sym_size, 4096])\\n        mm_default_134: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_460, t_default_134);  view_default_460 = t_default_134 = None\\n        view_default_461: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_134, [1, sym_size, 4096]);  mm_default_134 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant175 = self._param_constant175\\n        t_default_135: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant175);  _param_constant175 = None\\n        view_default_462: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_172, [sym_size, 4096]);  mul_tensor_172 = None\\n        mm_default_135: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_462, t_default_135);  view_default_462 = t_default_135 = None\\n        view_default_463: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_135, [1, sym_size, 4096]);  mm_default_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_464: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_459, [1, sym_size, 32, 128])\\n        transpose_int_95: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_464, 1, 2);  view_default_464 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_465: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_461, [1, sym_size, 32, 128])\\n        transpose_int_96: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_465, 1, 2);  view_default_465 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_466: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_463, [1, sym_size, 32, 128])\\n        transpose_int_97: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_466, 1, 2);  view_default_466 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant38 = self._tensor_constant38\\n        slice_tensor_194: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant38, 0, 0, 9223372036854775807);  _tensor_constant38 = None\\n        slice_tensor_195: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_194, 1, 0, 9223372036854775807);  slice_tensor_194 = None\\n        sym_size_77: Sym(s0) = torch.ops.aten.sym_size(view_default_461, 1);  view_default_461 = None\\n        slice_tensor_196: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_195, 2, 0, sym_size_77);  slice_tensor_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant39 = self._tensor_constant39\\n        slice_tensor_197: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant39, 0, 0, 9223372036854775807);  _tensor_constant39 = None\\n        slice_tensor_198: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_197, 1, 0, 9223372036854775807);  slice_tensor_197 = None\\n        slice_tensor_199: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_198, 2, 0, sym_size_77);  slice_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_76: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_196, 1);  slice_tensor_196 = None\\n        squeeze_dim_77: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_76, 0);  squeeze_dim_76 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_78: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_199, 1);  slice_tensor_199 = None\\n        squeeze_dim_79: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_78, 0);  squeeze_dim_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_38: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_77, [view_default]);  squeeze_dim_77 = None\\n        unsqueeze_default_43: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_38, 1);  index_tensor_38 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_39: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_79, [view_default]);  squeeze_dim_79 = None\\n        unsqueeze_default_44: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_39, 1);  index_tensor_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_173: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_95, unsqueeze_default_43)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_200: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_201: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_95, 3, 64, 9223372036854775807);  transpose_int_95 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_38: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_201);  slice_tensor_201 = None\\n        cat_default_38: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_38, slice_tensor_200], -1);  neg_default_38 = slice_tensor_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_174: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_38, unsqueeze_default_44);  cat_default_38 = None\\n        add_tensor_136: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_173, mul_tensor_174);  mul_tensor_173 = mul_tensor_174 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_175: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_96, unsqueeze_default_43);  unsqueeze_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_202: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_203: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_96, 3, 64, 9223372036854775807);  transpose_int_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_39: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_203);  slice_tensor_203 = None\\n        cat_default_39: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_39, slice_tensor_202], -1);  neg_default_39 = slice_tensor_202 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_176: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_39, unsqueeze_default_44);  cat_default_39 = unsqueeze_default_44 = None\\n        add_tensor_137: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_175, mul_tensor_176);  mul_tensor_175 = mul_tensor_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_98: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_137, 2, 3)\\n        sym_size_78: Sym(s0) = torch.ops.aten.sym_size(view_default_459, 1);  view_default_459 = None\\n        expand_default_78: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_136, [1, 32, sym_size_78, 128]);  add_tensor_136 = None\\n        view_default_467: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_78, [32, sym_size_78, 128]);  expand_default_78 = None\\n        expand_default_79: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_98, [1, 32, 128, sym_size_77]);  transpose_int_98 = None\\n        view_default_468: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_79, [32, 128, sym_size_77]);  expand_default_79 = None\\n        bmm_default_38: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_467, view_default_468);  view_default_467 = view_default_468 = None\\n        view_default_469: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_38, [1, 32, sym_size_78, sym_size_77]);  bmm_default_38 = None\\n        div_tensor_19: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_469, 11.313708498984761);  view_default_469 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_138: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_19, add_tensor_1);  div_tensor_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_19: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_138, -1, False);  add_tensor_138 = None\\n        detach_default_58: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_19)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_80: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_19, [1, 32, sym_size_78, sym_size_77]);  _softmax_default_19 = None\\n        view_default_470: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_80, [32, sym_size_78, sym_size_77]);  expand_default_80 = sym_size_77 = None\\n        sym_size_79: Sym(s0) = torch.ops.aten.sym_size(view_default_463, 1);  view_default_463 = None\\n        expand_default_81: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_97, [1, 32, sym_size_79, 128])\\n        view_default_471: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_81, [32, sym_size_79, 128]);  expand_default_81 = sym_size_79 = None\\n        bmm_default_39: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_470, view_default_471);  view_default_470 = view_default_471 = None\\n        view_default_472: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_39, [1, 32, sym_size_78, 128]);  bmm_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_99: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_472, 1, 2);  view_default_472 = None\\n        clone_default_19: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_99, memory_format = torch.contiguous_format);  transpose_int_99 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_473: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_19, [1, sym_size, 4096]);  clone_default_19 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant176 = self._param_constant176\\n        t_default_136: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant176);  _param_constant176 = None\\n        view_default_474: f32[s0, 4096] = torch.ops.aten.view.default(view_default_473, [sym_size_78, 4096]);  view_default_473 = None\\n        mm_default_136: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_474, t_default_136);  view_default_474 = t_default_136 = None\\n        view_default_475: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_136, [1, sym_size_78, 4096]);  mm_default_136 = sym_size_78 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_139: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_134, view_default_475);  add_tensor_134 = view_default_475 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_39: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_139, 2)\\n        mean_dim_39: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_39, [-1], True);  pow_tensor_scalar_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_140: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_39, 1e-06);  mean_dim_39 = None\\n        rsqrt_default_39: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_140);  add_tensor_140 = None\\n        detach_default_59: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_39)\\n        mul_tensor_177: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_139, rsqrt_default_39);  rsqrt_default_39 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant177 = self._param_constant177\\n        mul_tensor_178: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant177, mul_tensor_177);  _param_constant177 = mul_tensor_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant178 = self._param_constant178\\n        t_default_137: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant178);  _param_constant178 = None\\n        view_default_476: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_178, [sym_size, 4096])\\n        mm_default_137: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_476, t_default_137);  view_default_476 = t_default_137 = None\\n        view_default_477: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_137, [1, sym_size, 11008]);  mm_default_137 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_19: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_477)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant179 = self._param_constant179\\n        t_default_138: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant179);  _param_constant179 = None\\n        view_default_478: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_178, [sym_size, 4096]);  mul_tensor_178 = None\\n        mm_default_138: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_478, t_default_138);  view_default_478 = t_default_138 = None\\n        view_default_479: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_138, [1, sym_size, 11008]);  mm_default_138 = None\\n        mul_tensor_179: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_19, view_default_479);  silu_default_19 = view_default_479 = None\\n        _param_constant180 = self._param_constant180\\n        t_default_139: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant180);  _param_constant180 = None\\n        sym_size_80: Sym(s0) = torch.ops.aten.sym_size(view_default_477, 1);  view_default_477 = None\\n        view_default_480: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_179, [sym_size_80, 11008]);  mul_tensor_179 = None\\n        mm_default_139: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_480, t_default_139);  view_default_480 = t_default_139 = None\\n        view_default_481: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_139, [1, sym_size_80, 4096]);  mm_default_139 = sym_size_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_141: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_139, view_default_481);  add_tensor_139 = view_default_481 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_40: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_141, 2)\\n        mean_dim_40: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_40, [-1], True);  pow_tensor_scalar_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_142: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_40, 1e-06);  mean_dim_40 = None\\n        rsqrt_default_40: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_142);  add_tensor_142 = None\\n        detach_default_60: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_40)\\n        mul_tensor_180: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_141, rsqrt_default_40);  rsqrt_default_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant181 = self._param_constant181\\n        mul_tensor_181: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant181, mul_tensor_180);  _param_constant181 = mul_tensor_180 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant182 = self._param_constant182\\n        t_default_140: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant182);  _param_constant182 = None\\n        view_default_482: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_181, [sym_size, 4096])\\n        mm_default_140: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_482, t_default_140);  view_default_482 = t_default_140 = None\\n        view_default_483: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_140, [1, sym_size, 4096]);  mm_default_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant183 = self._param_constant183\\n        t_default_141: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant183);  _param_constant183 = None\\n        view_default_484: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_181, [sym_size, 4096])\\n        mm_default_141: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_484, t_default_141);  view_default_484 = t_default_141 = None\\n        view_default_485: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_141, [1, sym_size, 4096]);  mm_default_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant184 = self._param_constant184\\n        t_default_142: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant184);  _param_constant184 = None\\n        view_default_486: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_181, [sym_size, 4096]);  mul_tensor_181 = None\\n        mm_default_142: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_486, t_default_142);  view_default_486 = t_default_142 = None\\n        view_default_487: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_142, [1, sym_size, 4096]);  mm_default_142 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_488: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_483, [1, sym_size, 32, 128])\\n        transpose_int_100: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_488, 1, 2);  view_default_488 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_489: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_485, [1, sym_size, 32, 128])\\n        transpose_int_101: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_489, 1, 2);  view_default_489 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_490: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_487, [1, sym_size, 32, 128])\\n        transpose_int_102: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_490, 1, 2);  view_default_490 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant40 = self._tensor_constant40\\n        slice_tensor_204: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant40, 0, 0, 9223372036854775807);  _tensor_constant40 = None\\n        slice_tensor_205: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_204, 1, 0, 9223372036854775807);  slice_tensor_204 = None\\n        sym_size_81: Sym(s0) = torch.ops.aten.sym_size(view_default_485, 1);  view_default_485 = None\\n        slice_tensor_206: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_205, 2, 0, sym_size_81);  slice_tensor_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant41 = self._tensor_constant41\\n        slice_tensor_207: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant41, 0, 0, 9223372036854775807);  _tensor_constant41 = None\\n        slice_tensor_208: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_207, 1, 0, 9223372036854775807);  slice_tensor_207 = None\\n        slice_tensor_209: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_208, 2, 0, sym_size_81);  slice_tensor_208 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_80: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_206, 1);  slice_tensor_206 = None\\n        squeeze_dim_81: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_80, 0);  squeeze_dim_80 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_82: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_209, 1);  slice_tensor_209 = None\\n        squeeze_dim_83: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_82, 0);  squeeze_dim_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_40: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_81, [view_default]);  squeeze_dim_81 = None\\n        unsqueeze_default_45: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_40, 1);  index_tensor_40 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_41: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_83, [view_default]);  squeeze_dim_83 = None\\n        unsqueeze_default_46: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_41, 1);  index_tensor_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_182: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_100, unsqueeze_default_45)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_210: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_211: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_100, 3, 64, 9223372036854775807);  transpose_int_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_40: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_211);  slice_tensor_211 = None\\n        cat_default_40: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_40, slice_tensor_210], -1);  neg_default_40 = slice_tensor_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_183: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_40, unsqueeze_default_46);  cat_default_40 = None\\n        add_tensor_143: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_182, mul_tensor_183);  mul_tensor_182 = mul_tensor_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_184: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_101, unsqueeze_default_45);  unsqueeze_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_212: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_213: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_101, 3, 64, 9223372036854775807);  transpose_int_101 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_41: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_213);  slice_tensor_213 = None\\n        cat_default_41: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_41, slice_tensor_212], -1);  neg_default_41 = slice_tensor_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_185: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_41, unsqueeze_default_46);  cat_default_41 = unsqueeze_default_46 = None\\n        add_tensor_144: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_184, mul_tensor_185);  mul_tensor_184 = mul_tensor_185 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_103: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_144, 2, 3)\\n        sym_size_82: Sym(s0) = torch.ops.aten.sym_size(view_default_483, 1);  view_default_483 = None\\n        expand_default_82: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_143, [1, 32, sym_size_82, 128]);  add_tensor_143 = None\\n        view_default_491: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_82, [32, sym_size_82, 128]);  expand_default_82 = None\\n        expand_default_83: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_103, [1, 32, 128, sym_size_81]);  transpose_int_103 = None\\n        view_default_492: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_83, [32, 128, sym_size_81]);  expand_default_83 = None\\n        bmm_default_40: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_491, view_default_492);  view_default_491 = view_default_492 = None\\n        view_default_493: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_40, [1, 32, sym_size_82, sym_size_81]);  bmm_default_40 = None\\n        div_tensor_20: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_493, 11.313708498984761);  view_default_493 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_145: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_20, add_tensor_1);  div_tensor_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_20: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_145, -1, False);  add_tensor_145 = None\\n        detach_default_61: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_20)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_84: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_20, [1, 32, sym_size_82, sym_size_81]);  _softmax_default_20 = None\\n        view_default_494: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_84, [32, sym_size_82, sym_size_81]);  expand_default_84 = sym_size_81 = None\\n        sym_size_83: Sym(s0) = torch.ops.aten.sym_size(view_default_487, 1);  view_default_487 = None\\n        expand_default_85: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_102, [1, 32, sym_size_83, 128])\\n        view_default_495: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_85, [32, sym_size_83, 128]);  expand_default_85 = sym_size_83 = None\\n        bmm_default_41: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_494, view_default_495);  view_default_494 = view_default_495 = None\\n        view_default_496: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_41, [1, 32, sym_size_82, 128]);  bmm_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_104: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_496, 1, 2);  view_default_496 = None\\n        clone_default_20: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_104, memory_format = torch.contiguous_format);  transpose_int_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_497: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_20, [1, sym_size, 4096]);  clone_default_20 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant185 = self._param_constant185\\n        t_default_143: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant185);  _param_constant185 = None\\n        view_default_498: f32[s0, 4096] = torch.ops.aten.view.default(view_default_497, [sym_size_82, 4096]);  view_default_497 = None\\n        mm_default_143: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_498, t_default_143);  view_default_498 = t_default_143 = None\\n        view_default_499: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_143, [1, sym_size_82, 4096]);  mm_default_143 = sym_size_82 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_146: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_141, view_default_499);  add_tensor_141 = view_default_499 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_41: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_146, 2)\\n        mean_dim_41: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_41, [-1], True);  pow_tensor_scalar_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_147: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_41, 1e-06);  mean_dim_41 = None\\n        rsqrt_default_41: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_147);  add_tensor_147 = None\\n        detach_default_62: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_41)\\n        mul_tensor_186: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_146, rsqrt_default_41);  rsqrt_default_41 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant186 = self._param_constant186\\n        mul_tensor_187: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant186, mul_tensor_186);  _param_constant186 = mul_tensor_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant187 = self._param_constant187\\n        t_default_144: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant187);  _param_constant187 = None\\n        view_default_500: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_187, [sym_size, 4096])\\n        mm_default_144: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_500, t_default_144);  view_default_500 = t_default_144 = None\\n        view_default_501: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_144, [1, sym_size, 11008]);  mm_default_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_20: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_501)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant188 = self._param_constant188\\n        t_default_145: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant188);  _param_constant188 = None\\n        view_default_502: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_187, [sym_size, 4096]);  mul_tensor_187 = None\\n        mm_default_145: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_502, t_default_145);  view_default_502 = t_default_145 = None\\n        view_default_503: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_145, [1, sym_size, 11008]);  mm_default_145 = None\\n        mul_tensor_188: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_20, view_default_503);  silu_default_20 = view_default_503 = None\\n        _param_constant189 = self._param_constant189\\n        t_default_146: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant189);  _param_constant189 = None\\n        sym_size_84: Sym(s0) = torch.ops.aten.sym_size(view_default_501, 1);  view_default_501 = None\\n        view_default_504: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_188, [sym_size_84, 11008]);  mul_tensor_188 = None\\n        mm_default_146: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_504, t_default_146);  view_default_504 = t_default_146 = None\\n        view_default_505: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_146, [1, sym_size_84, 4096]);  mm_default_146 = sym_size_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_148: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_146, view_default_505);  add_tensor_146 = view_default_505 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_42: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_148, 2)\\n        mean_dim_42: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_42, [-1], True);  pow_tensor_scalar_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_149: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_42, 1e-06);  mean_dim_42 = None\\n        rsqrt_default_42: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_149);  add_tensor_149 = None\\n        detach_default_63: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_42)\\n        mul_tensor_189: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_148, rsqrt_default_42);  rsqrt_default_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant190 = self._param_constant190\\n        mul_tensor_190: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant190, mul_tensor_189);  _param_constant190 = mul_tensor_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant191 = self._param_constant191\\n        t_default_147: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant191);  _param_constant191 = None\\n        view_default_506: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_190, [sym_size, 4096])\\n        mm_default_147: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_506, t_default_147);  view_default_506 = t_default_147 = None\\n        view_default_507: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_147, [1, sym_size, 4096]);  mm_default_147 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant192 = self._param_constant192\\n        t_default_148: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant192);  _param_constant192 = None\\n        view_default_508: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_190, [sym_size, 4096])\\n        mm_default_148: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_508, t_default_148);  view_default_508 = t_default_148 = None\\n        view_default_509: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_148, [1, sym_size, 4096]);  mm_default_148 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant193 = self._param_constant193\\n        t_default_149: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant193);  _param_constant193 = None\\n        view_default_510: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_190, [sym_size, 4096]);  mul_tensor_190 = None\\n        mm_default_149: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_510, t_default_149);  view_default_510 = t_default_149 = None\\n        view_default_511: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_149, [1, sym_size, 4096]);  mm_default_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_512: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_507, [1, sym_size, 32, 128])\\n        transpose_int_105: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_512, 1, 2);  view_default_512 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_513: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_509, [1, sym_size, 32, 128])\\n        transpose_int_106: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_513, 1, 2);  view_default_513 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_514: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_511, [1, sym_size, 32, 128])\\n        transpose_int_107: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_514, 1, 2);  view_default_514 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant42 = self._tensor_constant42\\n        slice_tensor_214: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant42, 0, 0, 9223372036854775807);  _tensor_constant42 = None\\n        slice_tensor_215: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_214, 1, 0, 9223372036854775807);  slice_tensor_214 = None\\n        sym_size_85: Sym(s0) = torch.ops.aten.sym_size(view_default_509, 1);  view_default_509 = None\\n        slice_tensor_216: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_215, 2, 0, sym_size_85);  slice_tensor_215 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant43 = self._tensor_constant43\\n        slice_tensor_217: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant43, 0, 0, 9223372036854775807);  _tensor_constant43 = None\\n        slice_tensor_218: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_217, 1, 0, 9223372036854775807);  slice_tensor_217 = None\\n        slice_tensor_219: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_218, 2, 0, sym_size_85);  slice_tensor_218 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_84: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_216, 1);  slice_tensor_216 = None\\n        squeeze_dim_85: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_84, 0);  squeeze_dim_84 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_86: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_219, 1);  slice_tensor_219 = None\\n        squeeze_dim_87: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_86, 0);  squeeze_dim_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_42: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_85, [view_default]);  squeeze_dim_85 = None\\n        unsqueeze_default_47: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_42, 1);  index_tensor_42 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_43: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_87, [view_default]);  squeeze_dim_87 = None\\n        unsqueeze_default_48: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_43, 1);  index_tensor_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_191: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_105, unsqueeze_default_47)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_220: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_221: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_105, 3, 64, 9223372036854775807);  transpose_int_105 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_42: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_221);  slice_tensor_221 = None\\n        cat_default_42: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_42, slice_tensor_220], -1);  neg_default_42 = slice_tensor_220 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_192: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_42, unsqueeze_default_48);  cat_default_42 = None\\n        add_tensor_150: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_191, mul_tensor_192);  mul_tensor_191 = mul_tensor_192 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_193: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_106, unsqueeze_default_47);  unsqueeze_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_222: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_223: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_106, 3, 64, 9223372036854775807);  transpose_int_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_43: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_223);  slice_tensor_223 = None\\n        cat_default_43: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_43, slice_tensor_222], -1);  neg_default_43 = slice_tensor_222 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_194: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_43, unsqueeze_default_48);  cat_default_43 = unsqueeze_default_48 = None\\n        add_tensor_151: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_193, mul_tensor_194);  mul_tensor_193 = mul_tensor_194 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_108: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_151, 2, 3)\\n        sym_size_86: Sym(s0) = torch.ops.aten.sym_size(view_default_507, 1);  view_default_507 = None\\n        expand_default_86: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_150, [1, 32, sym_size_86, 128]);  add_tensor_150 = None\\n        view_default_515: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_86, [32, sym_size_86, 128]);  expand_default_86 = None\\n        expand_default_87: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_108, [1, 32, 128, sym_size_85]);  transpose_int_108 = None\\n        view_default_516: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_87, [32, 128, sym_size_85]);  expand_default_87 = None\\n        bmm_default_42: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_515, view_default_516);  view_default_515 = view_default_516 = None\\n        view_default_517: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_42, [1, 32, sym_size_86, sym_size_85]);  bmm_default_42 = None\\n        div_tensor_21: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_517, 11.313708498984761);  view_default_517 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_152: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_21, add_tensor_1);  div_tensor_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_21: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_152, -1, False);  add_tensor_152 = None\\n        detach_default_64: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_21)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_88: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_21, [1, 32, sym_size_86, sym_size_85]);  _softmax_default_21 = None\\n        view_default_518: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_88, [32, sym_size_86, sym_size_85]);  expand_default_88 = sym_size_85 = None\\n        sym_size_87: Sym(s0) = torch.ops.aten.sym_size(view_default_511, 1);  view_default_511 = None\\n        expand_default_89: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_107, [1, 32, sym_size_87, 128])\\n        view_default_519: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_89, [32, sym_size_87, 128]);  expand_default_89 = sym_size_87 = None\\n        bmm_default_43: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_518, view_default_519);  view_default_518 = view_default_519 = None\\n        view_default_520: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_43, [1, 32, sym_size_86, 128]);  bmm_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_109: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_520, 1, 2);  view_default_520 = None\\n        clone_default_21: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_109, memory_format = torch.contiguous_format);  transpose_int_109 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_521: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_21, [1, sym_size, 4096]);  clone_default_21 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant194 = self._param_constant194\\n        t_default_150: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant194);  _param_constant194 = None\\n        view_default_522: f32[s0, 4096] = torch.ops.aten.view.default(view_default_521, [sym_size_86, 4096]);  view_default_521 = None\\n        mm_default_150: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_522, t_default_150);  view_default_522 = t_default_150 = None\\n        view_default_523: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_150, [1, sym_size_86, 4096]);  mm_default_150 = sym_size_86 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_153: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_148, view_default_523);  add_tensor_148 = view_default_523 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_43: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_153, 2)\\n        mean_dim_43: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_43, [-1], True);  pow_tensor_scalar_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_154: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_43, 1e-06);  mean_dim_43 = None\\n        rsqrt_default_43: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_154);  add_tensor_154 = None\\n        detach_default_65: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_43)\\n        mul_tensor_195: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_153, rsqrt_default_43);  rsqrt_default_43 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant195 = self._param_constant195\\n        mul_tensor_196: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant195, mul_tensor_195);  _param_constant195 = mul_tensor_195 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant196 = self._param_constant196\\n        t_default_151: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant196);  _param_constant196 = None\\n        view_default_524: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_196, [sym_size, 4096])\\n        mm_default_151: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_524, t_default_151);  view_default_524 = t_default_151 = None\\n        view_default_525: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_151, [1, sym_size, 11008]);  mm_default_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_21: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_525)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant197 = self._param_constant197\\n        t_default_152: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant197);  _param_constant197 = None\\n        view_default_526: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_196, [sym_size, 4096]);  mul_tensor_196 = None\\n        mm_default_152: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_526, t_default_152);  view_default_526 = t_default_152 = None\\n        view_default_527: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_152, [1, sym_size, 11008]);  mm_default_152 = None\\n        mul_tensor_197: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_21, view_default_527);  silu_default_21 = view_default_527 = None\\n        _param_constant198 = self._param_constant198\\n        t_default_153: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant198);  _param_constant198 = None\\n        sym_size_88: Sym(s0) = torch.ops.aten.sym_size(view_default_525, 1);  view_default_525 = None\\n        view_default_528: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_197, [sym_size_88, 11008]);  mul_tensor_197 = None\\n        mm_default_153: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_528, t_default_153);  view_default_528 = t_default_153 = None\\n        view_default_529: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_153, [1, sym_size_88, 4096]);  mm_default_153 = sym_size_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_155: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_153, view_default_529);  add_tensor_153 = view_default_529 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_44: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_155, 2)\\n        mean_dim_44: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_44, [-1], True);  pow_tensor_scalar_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_156: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_44, 1e-06);  mean_dim_44 = None\\n        rsqrt_default_44: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_156);  add_tensor_156 = None\\n        detach_default_66: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_44)\\n        mul_tensor_198: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_155, rsqrt_default_44);  rsqrt_default_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant199 = self._param_constant199\\n        mul_tensor_199: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant199, mul_tensor_198);  _param_constant199 = mul_tensor_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant200 = self._param_constant200\\n        t_default_154: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant200);  _param_constant200 = None\\n        view_default_530: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_199, [sym_size, 4096])\\n        mm_default_154: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_530, t_default_154);  view_default_530 = t_default_154 = None\\n        view_default_531: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_154, [1, sym_size, 4096]);  mm_default_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant201 = self._param_constant201\\n        t_default_155: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant201);  _param_constant201 = None\\n        view_default_532: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_199, [sym_size, 4096])\\n        mm_default_155: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_532, t_default_155);  view_default_532 = t_default_155 = None\\n        view_default_533: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_155, [1, sym_size, 4096]);  mm_default_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant202 = self._param_constant202\\n        t_default_156: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant202);  _param_constant202 = None\\n        view_default_534: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_199, [sym_size, 4096]);  mul_tensor_199 = None\\n        mm_default_156: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_534, t_default_156);  view_default_534 = t_default_156 = None\\n        view_default_535: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_156, [1, sym_size, 4096]);  mm_default_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_536: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_531, [1, sym_size, 32, 128])\\n        transpose_int_110: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_536, 1, 2);  view_default_536 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_537: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_533, [1, sym_size, 32, 128])\\n        transpose_int_111: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_537, 1, 2);  view_default_537 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_538: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_535, [1, sym_size, 32, 128])\\n        transpose_int_112: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_538, 1, 2);  view_default_538 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant44 = self._tensor_constant44\\n        slice_tensor_224: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant44, 0, 0, 9223372036854775807);  _tensor_constant44 = None\\n        slice_tensor_225: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_224, 1, 0, 9223372036854775807);  slice_tensor_224 = None\\n        sym_size_89: Sym(s0) = torch.ops.aten.sym_size(view_default_533, 1);  view_default_533 = None\\n        slice_tensor_226: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_225, 2, 0, sym_size_89);  slice_tensor_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant45 = self._tensor_constant45\\n        slice_tensor_227: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant45, 0, 0, 9223372036854775807);  _tensor_constant45 = None\\n        slice_tensor_228: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_227, 1, 0, 9223372036854775807);  slice_tensor_227 = None\\n        slice_tensor_229: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_228, 2, 0, sym_size_89);  slice_tensor_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_88: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_226, 1);  slice_tensor_226 = None\\n        squeeze_dim_89: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_88, 0);  squeeze_dim_88 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_90: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_229, 1);  slice_tensor_229 = None\\n        squeeze_dim_91: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_90, 0);  squeeze_dim_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_44: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_89, [view_default]);  squeeze_dim_89 = None\\n        unsqueeze_default_49: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_44, 1);  index_tensor_44 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_45: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_91, [view_default]);  squeeze_dim_91 = None\\n        unsqueeze_default_50: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_45, 1);  index_tensor_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_200: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_110, unsqueeze_default_49)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_230: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_231: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_110, 3, 64, 9223372036854775807);  transpose_int_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_44: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_231);  slice_tensor_231 = None\\n        cat_default_44: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_44, slice_tensor_230], -1);  neg_default_44 = slice_tensor_230 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_201: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_44, unsqueeze_default_50);  cat_default_44 = None\\n        add_tensor_157: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_200, mul_tensor_201);  mul_tensor_200 = mul_tensor_201 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_202: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_111, unsqueeze_default_49);  unsqueeze_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_232: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_233: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_111, 3, 64, 9223372036854775807);  transpose_int_111 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_45: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_233);  slice_tensor_233 = None\\n        cat_default_45: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_45, slice_tensor_232], -1);  neg_default_45 = slice_tensor_232 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_203: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_45, unsqueeze_default_50);  cat_default_45 = unsqueeze_default_50 = None\\n        add_tensor_158: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_202, mul_tensor_203);  mul_tensor_202 = mul_tensor_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_113: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_158, 2, 3)\\n        sym_size_90: Sym(s0) = torch.ops.aten.sym_size(view_default_531, 1);  view_default_531 = None\\n        expand_default_90: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_157, [1, 32, sym_size_90, 128]);  add_tensor_157 = None\\n        view_default_539: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_90, [32, sym_size_90, 128]);  expand_default_90 = None\\n        expand_default_91: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_113, [1, 32, 128, sym_size_89]);  transpose_int_113 = None\\n        view_default_540: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_91, [32, 128, sym_size_89]);  expand_default_91 = None\\n        bmm_default_44: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_539, view_default_540);  view_default_539 = view_default_540 = None\\n        view_default_541: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_44, [1, 32, sym_size_90, sym_size_89]);  bmm_default_44 = None\\n        div_tensor_22: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_541, 11.313708498984761);  view_default_541 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_159: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_22, add_tensor_1);  div_tensor_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_22: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_159, -1, False);  add_tensor_159 = None\\n        detach_default_67: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_22)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_92: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_22, [1, 32, sym_size_90, sym_size_89]);  _softmax_default_22 = None\\n        view_default_542: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_92, [32, sym_size_90, sym_size_89]);  expand_default_92 = sym_size_89 = None\\n        sym_size_91: Sym(s0) = torch.ops.aten.sym_size(view_default_535, 1);  view_default_535 = None\\n        expand_default_93: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_112, [1, 32, sym_size_91, 128])\\n        view_default_543: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_93, [32, sym_size_91, 128]);  expand_default_93 = sym_size_91 = None\\n        bmm_default_45: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_542, view_default_543);  view_default_542 = view_default_543 = None\\n        view_default_544: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_45, [1, 32, sym_size_90, 128]);  bmm_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_114: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_544, 1, 2);  view_default_544 = None\\n        clone_default_22: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_114, memory_format = torch.contiguous_format);  transpose_int_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_545: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_22, [1, sym_size, 4096]);  clone_default_22 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant203 = self._param_constant203\\n        t_default_157: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant203);  _param_constant203 = None\\n        view_default_546: f32[s0, 4096] = torch.ops.aten.view.default(view_default_545, [sym_size_90, 4096]);  view_default_545 = None\\n        mm_default_157: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_546, t_default_157);  view_default_546 = t_default_157 = None\\n        view_default_547: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_157, [1, sym_size_90, 4096]);  mm_default_157 = sym_size_90 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_160: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_155, view_default_547);  add_tensor_155 = view_default_547 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_45: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_160, 2)\\n        mean_dim_45: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_45, [-1], True);  pow_tensor_scalar_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_161: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_45, 1e-06);  mean_dim_45 = None\\n        rsqrt_default_45: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_161);  add_tensor_161 = None\\n        detach_default_68: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_45)\\n        mul_tensor_204: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_160, rsqrt_default_45);  rsqrt_default_45 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant204 = self._param_constant204\\n        mul_tensor_205: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant204, mul_tensor_204);  _param_constant204 = mul_tensor_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant205 = self._param_constant205\\n        t_default_158: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant205);  _param_constant205 = None\\n        view_default_548: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_205, [sym_size, 4096])\\n        mm_default_158: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_548, t_default_158);  view_default_548 = t_default_158 = None\\n        view_default_549: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_158, [1, sym_size, 11008]);  mm_default_158 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_22: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_549)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant206 = self._param_constant206\\n        t_default_159: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant206);  _param_constant206 = None\\n        view_default_550: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_205, [sym_size, 4096]);  mul_tensor_205 = None\\n        mm_default_159: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_550, t_default_159);  view_default_550 = t_default_159 = None\\n        view_default_551: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_159, [1, sym_size, 11008]);  mm_default_159 = None\\n        mul_tensor_206: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_22, view_default_551);  silu_default_22 = view_default_551 = None\\n        _param_constant207 = self._param_constant207\\n        t_default_160: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant207);  _param_constant207 = None\\n        sym_size_92: Sym(s0) = torch.ops.aten.sym_size(view_default_549, 1);  view_default_549 = None\\n        view_default_552: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_206, [sym_size_92, 11008]);  mul_tensor_206 = None\\n        mm_default_160: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_552, t_default_160);  view_default_552 = t_default_160 = None\\n        view_default_553: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_160, [1, sym_size_92, 4096]);  mm_default_160 = sym_size_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_162: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_160, view_default_553);  add_tensor_160 = view_default_553 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_46: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_162, 2)\\n        mean_dim_46: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_46, [-1], True);  pow_tensor_scalar_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_163: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_46, 1e-06);  mean_dim_46 = None\\n        rsqrt_default_46: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_163);  add_tensor_163 = None\\n        detach_default_69: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_46)\\n        mul_tensor_207: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_162, rsqrt_default_46);  rsqrt_default_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant208 = self._param_constant208\\n        mul_tensor_208: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant208, mul_tensor_207);  _param_constant208 = mul_tensor_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant209 = self._param_constant209\\n        t_default_161: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant209);  _param_constant209 = None\\n        view_default_554: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_208, [sym_size, 4096])\\n        mm_default_161: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_554, t_default_161);  view_default_554 = t_default_161 = None\\n        view_default_555: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_161, [1, sym_size, 4096]);  mm_default_161 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant210 = self._param_constant210\\n        t_default_162: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant210);  _param_constant210 = None\\n        view_default_556: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_208, [sym_size, 4096])\\n        mm_default_162: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_556, t_default_162);  view_default_556 = t_default_162 = None\\n        view_default_557: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_162, [1, sym_size, 4096]);  mm_default_162 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant211 = self._param_constant211\\n        t_default_163: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant211);  _param_constant211 = None\\n        view_default_558: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_208, [sym_size, 4096]);  mul_tensor_208 = None\\n        mm_default_163: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_558, t_default_163);  view_default_558 = t_default_163 = None\\n        view_default_559: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_163, [1, sym_size, 4096]);  mm_default_163 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_560: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_555, [1, sym_size, 32, 128])\\n        transpose_int_115: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_560, 1, 2);  view_default_560 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_561: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_557, [1, sym_size, 32, 128])\\n        transpose_int_116: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_561, 1, 2);  view_default_561 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_562: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_559, [1, sym_size, 32, 128])\\n        transpose_int_117: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_562, 1, 2);  view_default_562 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant46 = self._tensor_constant46\\n        slice_tensor_234: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant46, 0, 0, 9223372036854775807);  _tensor_constant46 = None\\n        slice_tensor_235: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_234, 1, 0, 9223372036854775807);  slice_tensor_234 = None\\n        sym_size_93: Sym(s0) = torch.ops.aten.sym_size(view_default_557, 1);  view_default_557 = None\\n        slice_tensor_236: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_235, 2, 0, sym_size_93);  slice_tensor_235 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant47 = self._tensor_constant47\\n        slice_tensor_237: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant47, 0, 0, 9223372036854775807);  _tensor_constant47 = None\\n        slice_tensor_238: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_237, 1, 0, 9223372036854775807);  slice_tensor_237 = None\\n        slice_tensor_239: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_238, 2, 0, sym_size_93);  slice_tensor_238 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_92: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_236, 1);  slice_tensor_236 = None\\n        squeeze_dim_93: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_92, 0);  squeeze_dim_92 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_94: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_239, 1);  slice_tensor_239 = None\\n        squeeze_dim_95: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_94, 0);  squeeze_dim_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_46: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_93, [view_default]);  squeeze_dim_93 = None\\n        unsqueeze_default_51: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_46, 1);  index_tensor_46 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_47: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_95, [view_default]);  squeeze_dim_95 = None\\n        unsqueeze_default_52: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_47, 1);  index_tensor_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_209: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_115, unsqueeze_default_51)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_240: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_241: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_115, 3, 64, 9223372036854775807);  transpose_int_115 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_46: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_241);  slice_tensor_241 = None\\n        cat_default_46: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_46, slice_tensor_240], -1);  neg_default_46 = slice_tensor_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_210: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_46, unsqueeze_default_52);  cat_default_46 = None\\n        add_tensor_164: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_209, mul_tensor_210);  mul_tensor_209 = mul_tensor_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_211: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_116, unsqueeze_default_51);  unsqueeze_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_242: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_243: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_116, 3, 64, 9223372036854775807);  transpose_int_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_47: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_243);  slice_tensor_243 = None\\n        cat_default_47: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_47, slice_tensor_242], -1);  neg_default_47 = slice_tensor_242 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_212: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_47, unsqueeze_default_52);  cat_default_47 = unsqueeze_default_52 = None\\n        add_tensor_165: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_211, mul_tensor_212);  mul_tensor_211 = mul_tensor_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_118: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_165, 2, 3)\\n        sym_size_94: Sym(s0) = torch.ops.aten.sym_size(view_default_555, 1);  view_default_555 = None\\n        expand_default_94: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_164, [1, 32, sym_size_94, 128]);  add_tensor_164 = None\\n        view_default_563: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_94, [32, sym_size_94, 128]);  expand_default_94 = None\\n        expand_default_95: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_118, [1, 32, 128, sym_size_93]);  transpose_int_118 = None\\n        view_default_564: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_95, [32, 128, sym_size_93]);  expand_default_95 = None\\n        bmm_default_46: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_563, view_default_564);  view_default_563 = view_default_564 = None\\n        view_default_565: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_46, [1, 32, sym_size_94, sym_size_93]);  bmm_default_46 = None\\n        div_tensor_23: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_565, 11.313708498984761);  view_default_565 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_166: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_23, add_tensor_1);  div_tensor_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_23: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_166, -1, False);  add_tensor_166 = None\\n        detach_default_70: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_23)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_96: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_23, [1, 32, sym_size_94, sym_size_93]);  _softmax_default_23 = None\\n        view_default_566: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_96, [32, sym_size_94, sym_size_93]);  expand_default_96 = sym_size_93 = None\\n        sym_size_95: Sym(s0) = torch.ops.aten.sym_size(view_default_559, 1);  view_default_559 = None\\n        expand_default_97: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_117, [1, 32, sym_size_95, 128])\\n        view_default_567: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_97, [32, sym_size_95, 128]);  expand_default_97 = sym_size_95 = None\\n        bmm_default_47: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_566, view_default_567);  view_default_566 = view_default_567 = None\\n        view_default_568: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_47, [1, 32, sym_size_94, 128]);  bmm_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_119: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_568, 1, 2);  view_default_568 = None\\n        clone_default_23: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_119, memory_format = torch.contiguous_format);  transpose_int_119 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_569: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_23, [1, sym_size, 4096]);  clone_default_23 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant212 = self._param_constant212\\n        t_default_164: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant212);  _param_constant212 = None\\n        view_default_570: f32[s0, 4096] = torch.ops.aten.view.default(view_default_569, [sym_size_94, 4096]);  view_default_569 = None\\n        mm_default_164: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_570, t_default_164);  view_default_570 = t_default_164 = None\\n        view_default_571: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_164, [1, sym_size_94, 4096]);  mm_default_164 = sym_size_94 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_167: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_162, view_default_571);  add_tensor_162 = view_default_571 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_47: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_167, 2)\\n        mean_dim_47: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_47, [-1], True);  pow_tensor_scalar_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_168: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_47, 1e-06);  mean_dim_47 = None\\n        rsqrt_default_47: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_168);  add_tensor_168 = None\\n        detach_default_71: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_47)\\n        mul_tensor_213: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_167, rsqrt_default_47);  rsqrt_default_47 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant213 = self._param_constant213\\n        mul_tensor_214: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant213, mul_tensor_213);  _param_constant213 = mul_tensor_213 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant214 = self._param_constant214\\n        t_default_165: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant214);  _param_constant214 = None\\n        view_default_572: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_214, [sym_size, 4096])\\n        mm_default_165: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_572, t_default_165);  view_default_572 = t_default_165 = None\\n        view_default_573: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_165, [1, sym_size, 11008]);  mm_default_165 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_23: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_573)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant215 = self._param_constant215\\n        t_default_166: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant215);  _param_constant215 = None\\n        view_default_574: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_214, [sym_size, 4096]);  mul_tensor_214 = None\\n        mm_default_166: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_574, t_default_166);  view_default_574 = t_default_166 = None\\n        view_default_575: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_166, [1, sym_size, 11008]);  mm_default_166 = None\\n        mul_tensor_215: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_23, view_default_575);  silu_default_23 = view_default_575 = None\\n        _param_constant216 = self._param_constant216\\n        t_default_167: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant216);  _param_constant216 = None\\n        sym_size_96: Sym(s0) = torch.ops.aten.sym_size(view_default_573, 1);  view_default_573 = None\\n        view_default_576: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_215, [sym_size_96, 11008]);  mul_tensor_215 = None\\n        mm_default_167: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_576, t_default_167);  view_default_576 = t_default_167 = None\\n        view_default_577: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_167, [1, sym_size_96, 4096]);  mm_default_167 = sym_size_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_169: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_167, view_default_577);  add_tensor_167 = view_default_577 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_48: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_169, 2)\\n        mean_dim_48: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_48, [-1], True);  pow_tensor_scalar_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_170: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_48, 1e-06);  mean_dim_48 = None\\n        rsqrt_default_48: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_170);  add_tensor_170 = None\\n        detach_default_72: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_48)\\n        mul_tensor_216: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_169, rsqrt_default_48);  rsqrt_default_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant217 = self._param_constant217\\n        mul_tensor_217: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant217, mul_tensor_216);  _param_constant217 = mul_tensor_216 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant218 = self._param_constant218\\n        t_default_168: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant218);  _param_constant218 = None\\n        view_default_578: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_217, [sym_size, 4096])\\n        mm_default_168: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_578, t_default_168);  view_default_578 = t_default_168 = None\\n        view_default_579: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_168, [1, sym_size, 4096]);  mm_default_168 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant219 = self._param_constant219\\n        t_default_169: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant219);  _param_constant219 = None\\n        view_default_580: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_217, [sym_size, 4096])\\n        mm_default_169: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_580, t_default_169);  view_default_580 = t_default_169 = None\\n        view_default_581: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_169, [1, sym_size, 4096]);  mm_default_169 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant220 = self._param_constant220\\n        t_default_170: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant220);  _param_constant220 = None\\n        view_default_582: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_217, [sym_size, 4096]);  mul_tensor_217 = None\\n        mm_default_170: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_582, t_default_170);  view_default_582 = t_default_170 = None\\n        view_default_583: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_170, [1, sym_size, 4096]);  mm_default_170 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_584: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_579, [1, sym_size, 32, 128])\\n        transpose_int_120: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_584, 1, 2);  view_default_584 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_585: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_581, [1, sym_size, 32, 128])\\n        transpose_int_121: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_585, 1, 2);  view_default_585 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_586: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_583, [1, sym_size, 32, 128])\\n        transpose_int_122: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_586, 1, 2);  view_default_586 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant48 = self._tensor_constant48\\n        slice_tensor_244: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant48, 0, 0, 9223372036854775807);  _tensor_constant48 = None\\n        slice_tensor_245: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_244, 1, 0, 9223372036854775807);  slice_tensor_244 = None\\n        sym_size_97: Sym(s0) = torch.ops.aten.sym_size(view_default_581, 1);  view_default_581 = None\\n        slice_tensor_246: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_245, 2, 0, sym_size_97);  slice_tensor_245 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant49 = self._tensor_constant49\\n        slice_tensor_247: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant49, 0, 0, 9223372036854775807);  _tensor_constant49 = None\\n        slice_tensor_248: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_247, 1, 0, 9223372036854775807);  slice_tensor_247 = None\\n        slice_tensor_249: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_248, 2, 0, sym_size_97);  slice_tensor_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_96: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_246, 1);  slice_tensor_246 = None\\n        squeeze_dim_97: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_96, 0);  squeeze_dim_96 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_98: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_249, 1);  slice_tensor_249 = None\\n        squeeze_dim_99: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_98, 0);  squeeze_dim_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_48: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_97, [view_default]);  squeeze_dim_97 = None\\n        unsqueeze_default_53: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_48, 1);  index_tensor_48 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_49: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_99, [view_default]);  squeeze_dim_99 = None\\n        unsqueeze_default_54: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_49, 1);  index_tensor_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_218: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_120, unsqueeze_default_53)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_250: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_251: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_120, 3, 64, 9223372036854775807);  transpose_int_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_48: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_251);  slice_tensor_251 = None\\n        cat_default_48: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_48, slice_tensor_250], -1);  neg_default_48 = slice_tensor_250 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_219: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_48, unsqueeze_default_54);  cat_default_48 = None\\n        add_tensor_171: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_218, mul_tensor_219);  mul_tensor_218 = mul_tensor_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_220: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_121, unsqueeze_default_53);  unsqueeze_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_252: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_253: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_121, 3, 64, 9223372036854775807);  transpose_int_121 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_49: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_253);  slice_tensor_253 = None\\n        cat_default_49: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_49, slice_tensor_252], -1);  neg_default_49 = slice_tensor_252 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_221: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_49, unsqueeze_default_54);  cat_default_49 = unsqueeze_default_54 = None\\n        add_tensor_172: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_220, mul_tensor_221);  mul_tensor_220 = mul_tensor_221 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_123: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_172, 2, 3)\\n        sym_size_98: Sym(s0) = torch.ops.aten.sym_size(view_default_579, 1);  view_default_579 = None\\n        expand_default_98: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_171, [1, 32, sym_size_98, 128]);  add_tensor_171 = None\\n        view_default_587: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_98, [32, sym_size_98, 128]);  expand_default_98 = None\\n        expand_default_99: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_123, [1, 32, 128, sym_size_97]);  transpose_int_123 = None\\n        view_default_588: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_99, [32, 128, sym_size_97]);  expand_default_99 = None\\n        bmm_default_48: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_587, view_default_588);  view_default_587 = view_default_588 = None\\n        view_default_589: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_48, [1, 32, sym_size_98, sym_size_97]);  bmm_default_48 = None\\n        div_tensor_24: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_589, 11.313708498984761);  view_default_589 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_173: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_24, add_tensor_1);  div_tensor_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_24: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_173, -1, False);  add_tensor_173 = None\\n        detach_default_73: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_24)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_100: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_24, [1, 32, sym_size_98, sym_size_97]);  _softmax_default_24 = None\\n        view_default_590: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_100, [32, sym_size_98, sym_size_97]);  expand_default_100 = sym_size_97 = None\\n        sym_size_99: Sym(s0) = torch.ops.aten.sym_size(view_default_583, 1);  view_default_583 = None\\n        expand_default_101: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_122, [1, 32, sym_size_99, 128])\\n        view_default_591: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_101, [32, sym_size_99, 128]);  expand_default_101 = sym_size_99 = None\\n        bmm_default_49: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_590, view_default_591);  view_default_590 = view_default_591 = None\\n        view_default_592: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_49, [1, 32, sym_size_98, 128]);  bmm_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_124: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_592, 1, 2);  view_default_592 = None\\n        clone_default_24: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_124, memory_format = torch.contiguous_format);  transpose_int_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_593: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_24, [1, sym_size, 4096]);  clone_default_24 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant221 = self._param_constant221\\n        t_default_171: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant221);  _param_constant221 = None\\n        view_default_594: f32[s0, 4096] = torch.ops.aten.view.default(view_default_593, [sym_size_98, 4096]);  view_default_593 = None\\n        mm_default_171: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_594, t_default_171);  view_default_594 = t_default_171 = None\\n        view_default_595: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_171, [1, sym_size_98, 4096]);  mm_default_171 = sym_size_98 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_174: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_169, view_default_595);  add_tensor_169 = view_default_595 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_49: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_174, 2)\\n        mean_dim_49: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_49, [-1], True);  pow_tensor_scalar_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_175: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_49, 1e-06);  mean_dim_49 = None\\n        rsqrt_default_49: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_175);  add_tensor_175 = None\\n        detach_default_74: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_49)\\n        mul_tensor_222: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_174, rsqrt_default_49);  rsqrt_default_49 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant222 = self._param_constant222\\n        mul_tensor_223: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant222, mul_tensor_222);  _param_constant222 = mul_tensor_222 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant223 = self._param_constant223\\n        t_default_172: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant223);  _param_constant223 = None\\n        view_default_596: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_223, [sym_size, 4096])\\n        mm_default_172: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_596, t_default_172);  view_default_596 = t_default_172 = None\\n        view_default_597: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_172, [1, sym_size, 11008]);  mm_default_172 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_24: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_597)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant224 = self._param_constant224\\n        t_default_173: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant224);  _param_constant224 = None\\n        view_default_598: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_223, [sym_size, 4096]);  mul_tensor_223 = None\\n        mm_default_173: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_598, t_default_173);  view_default_598 = t_default_173 = None\\n        view_default_599: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_173, [1, sym_size, 11008]);  mm_default_173 = None\\n        mul_tensor_224: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_24, view_default_599);  silu_default_24 = view_default_599 = None\\n        _param_constant225 = self._param_constant225\\n        t_default_174: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant225);  _param_constant225 = None\\n        sym_size_100: Sym(s0) = torch.ops.aten.sym_size(view_default_597, 1);  view_default_597 = None\\n        view_default_600: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_224, [sym_size_100, 11008]);  mul_tensor_224 = None\\n        mm_default_174: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_600, t_default_174);  view_default_600 = t_default_174 = None\\n        view_default_601: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_174, [1, sym_size_100, 4096]);  mm_default_174 = sym_size_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_176: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_174, view_default_601);  add_tensor_174 = view_default_601 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_50: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_176, 2)\\n        mean_dim_50: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_50, [-1], True);  pow_tensor_scalar_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_177: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_50, 1e-06);  mean_dim_50 = None\\n        rsqrt_default_50: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_177);  add_tensor_177 = None\\n        detach_default_75: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_50)\\n        mul_tensor_225: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_176, rsqrt_default_50);  rsqrt_default_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant226 = self._param_constant226\\n        mul_tensor_226: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant226, mul_tensor_225);  _param_constant226 = mul_tensor_225 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant227 = self._param_constant227\\n        t_default_175: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant227);  _param_constant227 = None\\n        view_default_602: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_226, [sym_size, 4096])\\n        mm_default_175: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_602, t_default_175);  view_default_602 = t_default_175 = None\\n        view_default_603: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_175, [1, sym_size, 4096]);  mm_default_175 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant228 = self._param_constant228\\n        t_default_176: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant228);  _param_constant228 = None\\n        view_default_604: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_226, [sym_size, 4096])\\n        mm_default_176: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_604, t_default_176);  view_default_604 = t_default_176 = None\\n        view_default_605: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_176, [1, sym_size, 4096]);  mm_default_176 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant229 = self._param_constant229\\n        t_default_177: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant229);  _param_constant229 = None\\n        view_default_606: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_226, [sym_size, 4096]);  mul_tensor_226 = None\\n        mm_default_177: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_606, t_default_177);  view_default_606 = t_default_177 = None\\n        view_default_607: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_177, [1, sym_size, 4096]);  mm_default_177 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_608: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_603, [1, sym_size, 32, 128])\\n        transpose_int_125: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_608, 1, 2);  view_default_608 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_609: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_605, [1, sym_size, 32, 128])\\n        transpose_int_126: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_609, 1, 2);  view_default_609 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_610: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_607, [1, sym_size, 32, 128])\\n        transpose_int_127: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_610, 1, 2);  view_default_610 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant50 = self._tensor_constant50\\n        slice_tensor_254: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant50, 0, 0, 9223372036854775807);  _tensor_constant50 = None\\n        slice_tensor_255: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_254, 1, 0, 9223372036854775807);  slice_tensor_254 = None\\n        sym_size_101: Sym(s0) = torch.ops.aten.sym_size(view_default_605, 1);  view_default_605 = None\\n        slice_tensor_256: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_255, 2, 0, sym_size_101);  slice_tensor_255 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant51 = self._tensor_constant51\\n        slice_tensor_257: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant51, 0, 0, 9223372036854775807);  _tensor_constant51 = None\\n        slice_tensor_258: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_257, 1, 0, 9223372036854775807);  slice_tensor_257 = None\\n        slice_tensor_259: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_258, 2, 0, sym_size_101);  slice_tensor_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_100: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_256, 1);  slice_tensor_256 = None\\n        squeeze_dim_101: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_100, 0);  squeeze_dim_100 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_102: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_259, 1);  slice_tensor_259 = None\\n        squeeze_dim_103: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_102, 0);  squeeze_dim_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_50: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_101, [view_default]);  squeeze_dim_101 = None\\n        unsqueeze_default_55: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_50, 1);  index_tensor_50 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_51: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_103, [view_default]);  squeeze_dim_103 = None\\n        unsqueeze_default_56: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_51, 1);  index_tensor_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_227: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_125, unsqueeze_default_55)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_260: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_261: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_125, 3, 64, 9223372036854775807);  transpose_int_125 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_50: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_261);  slice_tensor_261 = None\\n        cat_default_50: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_50, slice_tensor_260], -1);  neg_default_50 = slice_tensor_260 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_228: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_50, unsqueeze_default_56);  cat_default_50 = None\\n        add_tensor_178: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_227, mul_tensor_228);  mul_tensor_227 = mul_tensor_228 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_229: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_126, unsqueeze_default_55);  unsqueeze_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_262: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_263: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_126, 3, 64, 9223372036854775807);  transpose_int_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_51: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_263);  slice_tensor_263 = None\\n        cat_default_51: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_51, slice_tensor_262], -1);  neg_default_51 = slice_tensor_262 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_230: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_51, unsqueeze_default_56);  cat_default_51 = unsqueeze_default_56 = None\\n        add_tensor_179: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_229, mul_tensor_230);  mul_tensor_229 = mul_tensor_230 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_128: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_179, 2, 3)\\n        sym_size_102: Sym(s0) = torch.ops.aten.sym_size(view_default_603, 1);  view_default_603 = None\\n        expand_default_102: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_178, [1, 32, sym_size_102, 128]);  add_tensor_178 = None\\n        view_default_611: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_102, [32, sym_size_102, 128]);  expand_default_102 = None\\n        expand_default_103: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_128, [1, 32, 128, sym_size_101]);  transpose_int_128 = None\\n        view_default_612: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_103, [32, 128, sym_size_101]);  expand_default_103 = None\\n        bmm_default_50: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_611, view_default_612);  view_default_611 = view_default_612 = None\\n        view_default_613: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_50, [1, 32, sym_size_102, sym_size_101]);  bmm_default_50 = None\\n        div_tensor_25: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_613, 11.313708498984761);  view_default_613 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_180: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_25, add_tensor_1);  div_tensor_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_25: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_180, -1, False);  add_tensor_180 = None\\n        detach_default_76: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_25)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_104: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_25, [1, 32, sym_size_102, sym_size_101]);  _softmax_default_25 = None\\n        view_default_614: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_104, [32, sym_size_102, sym_size_101]);  expand_default_104 = sym_size_101 = None\\n        sym_size_103: Sym(s0) = torch.ops.aten.sym_size(view_default_607, 1);  view_default_607 = None\\n        expand_default_105: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_127, [1, 32, sym_size_103, 128])\\n        view_default_615: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_105, [32, sym_size_103, 128]);  expand_default_105 = sym_size_103 = None\\n        bmm_default_51: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_614, view_default_615);  view_default_614 = view_default_615 = None\\n        view_default_616: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_51, [1, 32, sym_size_102, 128]);  bmm_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_129: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_616, 1, 2);  view_default_616 = None\\n        clone_default_25: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_129, memory_format = torch.contiguous_format);  transpose_int_129 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_617: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_25, [1, sym_size, 4096]);  clone_default_25 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant230 = self._param_constant230\\n        t_default_178: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant230);  _param_constant230 = None\\n        view_default_618: f32[s0, 4096] = torch.ops.aten.view.default(view_default_617, [sym_size_102, 4096]);  view_default_617 = None\\n        mm_default_178: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_618, t_default_178);  view_default_618 = t_default_178 = None\\n        view_default_619: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_178, [1, sym_size_102, 4096]);  mm_default_178 = sym_size_102 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_181: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_176, view_default_619);  add_tensor_176 = view_default_619 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_51: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_181, 2)\\n        mean_dim_51: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_51, [-1], True);  pow_tensor_scalar_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_182: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_51, 1e-06);  mean_dim_51 = None\\n        rsqrt_default_51: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_182);  add_tensor_182 = None\\n        detach_default_77: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_51)\\n        mul_tensor_231: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_181, rsqrt_default_51);  rsqrt_default_51 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant231 = self._param_constant231\\n        mul_tensor_232: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant231, mul_tensor_231);  _param_constant231 = mul_tensor_231 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant232 = self._param_constant232\\n        t_default_179: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant232);  _param_constant232 = None\\n        view_default_620: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_232, [sym_size, 4096])\\n        mm_default_179: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_620, t_default_179);  view_default_620 = t_default_179 = None\\n        view_default_621: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_179, [1, sym_size, 11008]);  mm_default_179 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_25: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_621)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant233 = self._param_constant233\\n        t_default_180: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant233);  _param_constant233 = None\\n        view_default_622: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_232, [sym_size, 4096]);  mul_tensor_232 = None\\n        mm_default_180: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_622, t_default_180);  view_default_622 = t_default_180 = None\\n        view_default_623: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_180, [1, sym_size, 11008]);  mm_default_180 = None\\n        mul_tensor_233: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_25, view_default_623);  silu_default_25 = view_default_623 = None\\n        _param_constant234 = self._param_constant234\\n        t_default_181: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant234);  _param_constant234 = None\\n        sym_size_104: Sym(s0) = torch.ops.aten.sym_size(view_default_621, 1);  view_default_621 = None\\n        view_default_624: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_233, [sym_size_104, 11008]);  mul_tensor_233 = None\\n        mm_default_181: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_624, t_default_181);  view_default_624 = t_default_181 = None\\n        view_default_625: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_181, [1, sym_size_104, 4096]);  mm_default_181 = sym_size_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_183: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_181, view_default_625);  add_tensor_181 = view_default_625 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_52: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_183, 2)\\n        mean_dim_52: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_52, [-1], True);  pow_tensor_scalar_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_184: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_52, 1e-06);  mean_dim_52 = None\\n        rsqrt_default_52: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_184);  add_tensor_184 = None\\n        detach_default_78: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_52)\\n        mul_tensor_234: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_183, rsqrt_default_52);  rsqrt_default_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant235 = self._param_constant235\\n        mul_tensor_235: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant235, mul_tensor_234);  _param_constant235 = mul_tensor_234 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant236 = self._param_constant236\\n        t_default_182: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant236);  _param_constant236 = None\\n        view_default_626: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_235, [sym_size, 4096])\\n        mm_default_182: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_626, t_default_182);  view_default_626 = t_default_182 = None\\n        view_default_627: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_182, [1, sym_size, 4096]);  mm_default_182 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant237 = self._param_constant237\\n        t_default_183: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant237);  _param_constant237 = None\\n        view_default_628: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_235, [sym_size, 4096])\\n        mm_default_183: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_628, t_default_183);  view_default_628 = t_default_183 = None\\n        view_default_629: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_183, [1, sym_size, 4096]);  mm_default_183 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant238 = self._param_constant238\\n        t_default_184: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant238);  _param_constant238 = None\\n        view_default_630: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_235, [sym_size, 4096]);  mul_tensor_235 = None\\n        mm_default_184: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_630, t_default_184);  view_default_630 = t_default_184 = None\\n        view_default_631: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_184, [1, sym_size, 4096]);  mm_default_184 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_632: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_627, [1, sym_size, 32, 128])\\n        transpose_int_130: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_632, 1, 2);  view_default_632 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_633: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_629, [1, sym_size, 32, 128])\\n        transpose_int_131: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_633, 1, 2);  view_default_633 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_634: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_631, [1, sym_size, 32, 128])\\n        transpose_int_132: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_634, 1, 2);  view_default_634 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant52 = self._tensor_constant52\\n        slice_tensor_264: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant52, 0, 0, 9223372036854775807);  _tensor_constant52 = None\\n        slice_tensor_265: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_264, 1, 0, 9223372036854775807);  slice_tensor_264 = None\\n        sym_size_105: Sym(s0) = torch.ops.aten.sym_size(view_default_629, 1);  view_default_629 = None\\n        slice_tensor_266: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_265, 2, 0, sym_size_105);  slice_tensor_265 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant53 = self._tensor_constant53\\n        slice_tensor_267: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant53, 0, 0, 9223372036854775807);  _tensor_constant53 = None\\n        slice_tensor_268: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_267, 1, 0, 9223372036854775807);  slice_tensor_267 = None\\n        slice_tensor_269: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_268, 2, 0, sym_size_105);  slice_tensor_268 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_104: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_266, 1);  slice_tensor_266 = None\\n        squeeze_dim_105: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_104, 0);  squeeze_dim_104 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_106: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_269, 1);  slice_tensor_269 = None\\n        squeeze_dim_107: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_106, 0);  squeeze_dim_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_52: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_105, [view_default]);  squeeze_dim_105 = None\\n        unsqueeze_default_57: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_52, 1);  index_tensor_52 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_53: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_107, [view_default]);  squeeze_dim_107 = None\\n        unsqueeze_default_58: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_53, 1);  index_tensor_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_236: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_130, unsqueeze_default_57)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_270: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_271: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_130, 3, 64, 9223372036854775807);  transpose_int_130 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_52: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_271);  slice_tensor_271 = None\\n        cat_default_52: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_52, slice_tensor_270], -1);  neg_default_52 = slice_tensor_270 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_237: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_52, unsqueeze_default_58);  cat_default_52 = None\\n        add_tensor_185: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_236, mul_tensor_237);  mul_tensor_236 = mul_tensor_237 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_238: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_131, unsqueeze_default_57);  unsqueeze_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_272: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_273: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_131, 3, 64, 9223372036854775807);  transpose_int_131 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_53: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_273);  slice_tensor_273 = None\\n        cat_default_53: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_53, slice_tensor_272], -1);  neg_default_53 = slice_tensor_272 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_239: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_53, unsqueeze_default_58);  cat_default_53 = unsqueeze_default_58 = None\\n        add_tensor_186: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_238, mul_tensor_239);  mul_tensor_238 = mul_tensor_239 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_133: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_186, 2, 3)\\n        sym_size_106: Sym(s0) = torch.ops.aten.sym_size(view_default_627, 1);  view_default_627 = None\\n        expand_default_106: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_185, [1, 32, sym_size_106, 128]);  add_tensor_185 = None\\n        view_default_635: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_106, [32, sym_size_106, 128]);  expand_default_106 = None\\n        expand_default_107: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_133, [1, 32, 128, sym_size_105]);  transpose_int_133 = None\\n        view_default_636: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_107, [32, 128, sym_size_105]);  expand_default_107 = None\\n        bmm_default_52: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_635, view_default_636);  view_default_635 = view_default_636 = None\\n        view_default_637: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_52, [1, 32, sym_size_106, sym_size_105]);  bmm_default_52 = None\\n        div_tensor_26: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_637, 11.313708498984761);  view_default_637 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_187: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_26, add_tensor_1);  div_tensor_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_26: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_187, -1, False);  add_tensor_187 = None\\n        detach_default_79: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_26)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_108: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_26, [1, 32, sym_size_106, sym_size_105]);  _softmax_default_26 = None\\n        view_default_638: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_108, [32, sym_size_106, sym_size_105]);  expand_default_108 = sym_size_105 = None\\n        sym_size_107: Sym(s0) = torch.ops.aten.sym_size(view_default_631, 1);  view_default_631 = None\\n        expand_default_109: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_132, [1, 32, sym_size_107, 128])\\n        view_default_639: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_109, [32, sym_size_107, 128]);  expand_default_109 = sym_size_107 = None\\n        bmm_default_53: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_638, view_default_639);  view_default_638 = view_default_639 = None\\n        view_default_640: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_53, [1, 32, sym_size_106, 128]);  bmm_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_134: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_640, 1, 2);  view_default_640 = None\\n        clone_default_26: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_134, memory_format = torch.contiguous_format);  transpose_int_134 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_641: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_26, [1, sym_size, 4096]);  clone_default_26 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant239 = self._param_constant239\\n        t_default_185: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant239);  _param_constant239 = None\\n        view_default_642: f32[s0, 4096] = torch.ops.aten.view.default(view_default_641, [sym_size_106, 4096]);  view_default_641 = None\\n        mm_default_185: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_642, t_default_185);  view_default_642 = t_default_185 = None\\n        view_default_643: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_185, [1, sym_size_106, 4096]);  mm_default_185 = sym_size_106 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_188: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_183, view_default_643);  add_tensor_183 = view_default_643 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_53: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_188, 2)\\n        mean_dim_53: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_53, [-1], True);  pow_tensor_scalar_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_189: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_53, 1e-06);  mean_dim_53 = None\\n        rsqrt_default_53: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_189);  add_tensor_189 = None\\n        detach_default_80: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_53)\\n        mul_tensor_240: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_188, rsqrt_default_53);  rsqrt_default_53 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant240 = self._param_constant240\\n        mul_tensor_241: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant240, mul_tensor_240);  _param_constant240 = mul_tensor_240 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant241 = self._param_constant241\\n        t_default_186: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant241);  _param_constant241 = None\\n        view_default_644: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_241, [sym_size, 4096])\\n        mm_default_186: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_644, t_default_186);  view_default_644 = t_default_186 = None\\n        view_default_645: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_186, [1, sym_size, 11008]);  mm_default_186 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_26: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_645)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant242 = self._param_constant242\\n        t_default_187: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant242);  _param_constant242 = None\\n        view_default_646: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_241, [sym_size, 4096]);  mul_tensor_241 = None\\n        mm_default_187: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_646, t_default_187);  view_default_646 = t_default_187 = None\\n        view_default_647: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_187, [1, sym_size, 11008]);  mm_default_187 = None\\n        mul_tensor_242: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_26, view_default_647);  silu_default_26 = view_default_647 = None\\n        _param_constant243 = self._param_constant243\\n        t_default_188: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant243);  _param_constant243 = None\\n        sym_size_108: Sym(s0) = torch.ops.aten.sym_size(view_default_645, 1);  view_default_645 = None\\n        view_default_648: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_242, [sym_size_108, 11008]);  mul_tensor_242 = None\\n        mm_default_188: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_648, t_default_188);  view_default_648 = t_default_188 = None\\n        view_default_649: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_188, [1, sym_size_108, 4096]);  mm_default_188 = sym_size_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_190: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_188, view_default_649);  add_tensor_188 = view_default_649 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_54: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_190, 2)\\n        mean_dim_54: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_54, [-1], True);  pow_tensor_scalar_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_191: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_54, 1e-06);  mean_dim_54 = None\\n        rsqrt_default_54: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_191);  add_tensor_191 = None\\n        detach_default_81: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_54)\\n        mul_tensor_243: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_190, rsqrt_default_54);  rsqrt_default_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant244 = self._param_constant244\\n        mul_tensor_244: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant244, mul_tensor_243);  _param_constant244 = mul_tensor_243 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant245 = self._param_constant245\\n        t_default_189: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant245);  _param_constant245 = None\\n        view_default_650: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_244, [sym_size, 4096])\\n        mm_default_189: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_650, t_default_189);  view_default_650 = t_default_189 = None\\n        view_default_651: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_189, [1, sym_size, 4096]);  mm_default_189 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant246 = self._param_constant246\\n        t_default_190: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant246);  _param_constant246 = None\\n        view_default_652: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_244, [sym_size, 4096])\\n        mm_default_190: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_652, t_default_190);  view_default_652 = t_default_190 = None\\n        view_default_653: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_190, [1, sym_size, 4096]);  mm_default_190 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant247 = self._param_constant247\\n        t_default_191: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant247);  _param_constant247 = None\\n        view_default_654: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_244, [sym_size, 4096]);  mul_tensor_244 = None\\n        mm_default_191: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_654, t_default_191);  view_default_654 = t_default_191 = None\\n        view_default_655: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_191, [1, sym_size, 4096]);  mm_default_191 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_656: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_651, [1, sym_size, 32, 128])\\n        transpose_int_135: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_656, 1, 2);  view_default_656 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_657: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_653, [1, sym_size, 32, 128])\\n        transpose_int_136: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_657, 1, 2);  view_default_657 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_658: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_655, [1, sym_size, 32, 128])\\n        transpose_int_137: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_658, 1, 2);  view_default_658 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant54 = self._tensor_constant54\\n        slice_tensor_274: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant54, 0, 0, 9223372036854775807);  _tensor_constant54 = None\\n        slice_tensor_275: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_274, 1, 0, 9223372036854775807);  slice_tensor_274 = None\\n        sym_size_109: Sym(s0) = torch.ops.aten.sym_size(view_default_653, 1);  view_default_653 = None\\n        slice_tensor_276: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_275, 2, 0, sym_size_109);  slice_tensor_275 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant55 = self._tensor_constant55\\n        slice_tensor_277: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant55, 0, 0, 9223372036854775807);  _tensor_constant55 = None\\n        slice_tensor_278: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_277, 1, 0, 9223372036854775807);  slice_tensor_277 = None\\n        slice_tensor_279: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_278, 2, 0, sym_size_109);  slice_tensor_278 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_108: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_276, 1);  slice_tensor_276 = None\\n        squeeze_dim_109: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_108, 0);  squeeze_dim_108 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_110: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_279, 1);  slice_tensor_279 = None\\n        squeeze_dim_111: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_110, 0);  squeeze_dim_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_54: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_109, [view_default]);  squeeze_dim_109 = None\\n        unsqueeze_default_59: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_54, 1);  index_tensor_54 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_55: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_111, [view_default]);  squeeze_dim_111 = None\\n        unsqueeze_default_60: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_55, 1);  index_tensor_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_245: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_135, unsqueeze_default_59)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_280: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_281: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_135, 3, 64, 9223372036854775807);  transpose_int_135 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_54: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_281);  slice_tensor_281 = None\\n        cat_default_54: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_54, slice_tensor_280], -1);  neg_default_54 = slice_tensor_280 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_246: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_54, unsqueeze_default_60);  cat_default_54 = None\\n        add_tensor_192: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_245, mul_tensor_246);  mul_tensor_245 = mul_tensor_246 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_247: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_136, unsqueeze_default_59);  unsqueeze_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_282: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_283: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_136, 3, 64, 9223372036854775807);  transpose_int_136 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_55: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_283);  slice_tensor_283 = None\\n        cat_default_55: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_55, slice_tensor_282], -1);  neg_default_55 = slice_tensor_282 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_248: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_55, unsqueeze_default_60);  cat_default_55 = unsqueeze_default_60 = None\\n        add_tensor_193: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_247, mul_tensor_248);  mul_tensor_247 = mul_tensor_248 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_138: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_193, 2, 3)\\n        sym_size_110: Sym(s0) = torch.ops.aten.sym_size(view_default_651, 1);  view_default_651 = None\\n        expand_default_110: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_192, [1, 32, sym_size_110, 128]);  add_tensor_192 = None\\n        view_default_659: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_110, [32, sym_size_110, 128]);  expand_default_110 = None\\n        expand_default_111: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_138, [1, 32, 128, sym_size_109]);  transpose_int_138 = None\\n        view_default_660: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_111, [32, 128, sym_size_109]);  expand_default_111 = None\\n        bmm_default_54: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_659, view_default_660);  view_default_659 = view_default_660 = None\\n        view_default_661: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_54, [1, 32, sym_size_110, sym_size_109]);  bmm_default_54 = None\\n        div_tensor_27: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_661, 11.313708498984761);  view_default_661 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_194: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_27, add_tensor_1);  div_tensor_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_27: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_194, -1, False);  add_tensor_194 = None\\n        detach_default_82: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_27)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_112: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_27, [1, 32, sym_size_110, sym_size_109]);  _softmax_default_27 = None\\n        view_default_662: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_112, [32, sym_size_110, sym_size_109]);  expand_default_112 = sym_size_109 = None\\n        sym_size_111: Sym(s0) = torch.ops.aten.sym_size(view_default_655, 1);  view_default_655 = None\\n        expand_default_113: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_137, [1, 32, sym_size_111, 128])\\n        view_default_663: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_113, [32, sym_size_111, 128]);  expand_default_113 = sym_size_111 = None\\n        bmm_default_55: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_662, view_default_663);  view_default_662 = view_default_663 = None\\n        view_default_664: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_55, [1, 32, sym_size_110, 128]);  bmm_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_139: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_664, 1, 2);  view_default_664 = None\\n        clone_default_27: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_139, memory_format = torch.contiguous_format);  transpose_int_139 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_665: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_27, [1, sym_size, 4096]);  clone_default_27 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant248 = self._param_constant248\\n        t_default_192: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant248);  _param_constant248 = None\\n        view_default_666: f32[s0, 4096] = torch.ops.aten.view.default(view_default_665, [sym_size_110, 4096]);  view_default_665 = None\\n        mm_default_192: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_666, t_default_192);  view_default_666 = t_default_192 = None\\n        view_default_667: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_192, [1, sym_size_110, 4096]);  mm_default_192 = sym_size_110 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_195: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_190, view_default_667);  add_tensor_190 = view_default_667 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_55: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_195, 2)\\n        mean_dim_55: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_55, [-1], True);  pow_tensor_scalar_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_196: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_55, 1e-06);  mean_dim_55 = None\\n        rsqrt_default_55: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_196);  add_tensor_196 = None\\n        detach_default_83: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_55)\\n        mul_tensor_249: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_195, rsqrt_default_55);  rsqrt_default_55 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant249 = self._param_constant249\\n        mul_tensor_250: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant249, mul_tensor_249);  _param_constant249 = mul_tensor_249 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant250 = self._param_constant250\\n        t_default_193: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant250);  _param_constant250 = None\\n        view_default_668: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_250, [sym_size, 4096])\\n        mm_default_193: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_668, t_default_193);  view_default_668 = t_default_193 = None\\n        view_default_669: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_193, [1, sym_size, 11008]);  mm_default_193 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_27: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_669)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant251 = self._param_constant251\\n        t_default_194: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant251);  _param_constant251 = None\\n        view_default_670: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_250, [sym_size, 4096]);  mul_tensor_250 = None\\n        mm_default_194: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_670, t_default_194);  view_default_670 = t_default_194 = None\\n        view_default_671: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_194, [1, sym_size, 11008]);  mm_default_194 = None\\n        mul_tensor_251: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_27, view_default_671);  silu_default_27 = view_default_671 = None\\n        _param_constant252 = self._param_constant252\\n        t_default_195: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant252);  _param_constant252 = None\\n        sym_size_112: Sym(s0) = torch.ops.aten.sym_size(view_default_669, 1);  view_default_669 = None\\n        view_default_672: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_251, [sym_size_112, 11008]);  mul_tensor_251 = None\\n        mm_default_195: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_672, t_default_195);  view_default_672 = t_default_195 = None\\n        view_default_673: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_195, [1, sym_size_112, 4096]);  mm_default_195 = sym_size_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_197: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_195, view_default_673);  add_tensor_195 = view_default_673 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_56: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_197, 2)\\n        mean_dim_56: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_56, [-1], True);  pow_tensor_scalar_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_198: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_56, 1e-06);  mean_dim_56 = None\\n        rsqrt_default_56: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_198);  add_tensor_198 = None\\n        detach_default_84: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_56)\\n        mul_tensor_252: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_197, rsqrt_default_56);  rsqrt_default_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant253 = self._param_constant253\\n        mul_tensor_253: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant253, mul_tensor_252);  _param_constant253 = mul_tensor_252 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant254 = self._param_constant254\\n        t_default_196: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant254);  _param_constant254 = None\\n        view_default_674: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_253, [sym_size, 4096])\\n        mm_default_196: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_674, t_default_196);  view_default_674 = t_default_196 = None\\n        view_default_675: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_196, [1, sym_size, 4096]);  mm_default_196 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant255 = self._param_constant255\\n        t_default_197: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant255);  _param_constant255 = None\\n        view_default_676: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_253, [sym_size, 4096])\\n        mm_default_197: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_676, t_default_197);  view_default_676 = t_default_197 = None\\n        view_default_677: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_197, [1, sym_size, 4096]);  mm_default_197 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant256 = self._param_constant256\\n        t_default_198: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant256);  _param_constant256 = None\\n        view_default_678: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_253, [sym_size, 4096]);  mul_tensor_253 = None\\n        mm_default_198: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_678, t_default_198);  view_default_678 = t_default_198 = None\\n        view_default_679: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_198, [1, sym_size, 4096]);  mm_default_198 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_680: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_675, [1, sym_size, 32, 128])\\n        transpose_int_140: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_680, 1, 2);  view_default_680 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_681: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_677, [1, sym_size, 32, 128])\\n        transpose_int_141: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_681, 1, 2);  view_default_681 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_682: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_679, [1, sym_size, 32, 128])\\n        transpose_int_142: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_682, 1, 2);  view_default_682 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant56 = self._tensor_constant56\\n        slice_tensor_284: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant56, 0, 0, 9223372036854775807);  _tensor_constant56 = None\\n        slice_tensor_285: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_284, 1, 0, 9223372036854775807);  slice_tensor_284 = None\\n        sym_size_113: Sym(s0) = torch.ops.aten.sym_size(view_default_677, 1);  view_default_677 = None\\n        slice_tensor_286: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_285, 2, 0, sym_size_113);  slice_tensor_285 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant57 = self._tensor_constant57\\n        slice_tensor_287: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant57, 0, 0, 9223372036854775807);  _tensor_constant57 = None\\n        slice_tensor_288: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_287, 1, 0, 9223372036854775807);  slice_tensor_287 = None\\n        slice_tensor_289: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_288, 2, 0, sym_size_113);  slice_tensor_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_112: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_286, 1);  slice_tensor_286 = None\\n        squeeze_dim_113: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_112, 0);  squeeze_dim_112 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_114: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_289, 1);  slice_tensor_289 = None\\n        squeeze_dim_115: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_114, 0);  squeeze_dim_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_56: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_113, [view_default]);  squeeze_dim_113 = None\\n        unsqueeze_default_61: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_56, 1);  index_tensor_56 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_57: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_115, [view_default]);  squeeze_dim_115 = None\\n        unsqueeze_default_62: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_57, 1);  index_tensor_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_254: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_140, unsqueeze_default_61)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_290: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_291: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_140, 3, 64, 9223372036854775807);  transpose_int_140 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_56: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_291);  slice_tensor_291 = None\\n        cat_default_56: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_56, slice_tensor_290], -1);  neg_default_56 = slice_tensor_290 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_255: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_56, unsqueeze_default_62);  cat_default_56 = None\\n        add_tensor_199: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_254, mul_tensor_255);  mul_tensor_254 = mul_tensor_255 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_256: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_141, unsqueeze_default_61);  unsqueeze_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_292: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_293: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_141, 3, 64, 9223372036854775807);  transpose_int_141 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_57: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_293);  slice_tensor_293 = None\\n        cat_default_57: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_57, slice_tensor_292], -1);  neg_default_57 = slice_tensor_292 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_257: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_57, unsqueeze_default_62);  cat_default_57 = unsqueeze_default_62 = None\\n        add_tensor_200: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_256, mul_tensor_257);  mul_tensor_256 = mul_tensor_257 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_143: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_200, 2, 3)\\n        sym_size_114: Sym(s0) = torch.ops.aten.sym_size(view_default_675, 1);  view_default_675 = None\\n        expand_default_114: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_199, [1, 32, sym_size_114, 128]);  add_tensor_199 = None\\n        view_default_683: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_114, [32, sym_size_114, 128]);  expand_default_114 = None\\n        expand_default_115: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_143, [1, 32, 128, sym_size_113]);  transpose_int_143 = None\\n        view_default_684: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_115, [32, 128, sym_size_113]);  expand_default_115 = None\\n        bmm_default_56: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_683, view_default_684);  view_default_683 = view_default_684 = None\\n        view_default_685: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_56, [1, 32, sym_size_114, sym_size_113]);  bmm_default_56 = None\\n        div_tensor_28: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_685, 11.313708498984761);  view_default_685 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_201: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_28, add_tensor_1);  div_tensor_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_28: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_201, -1, False);  add_tensor_201 = None\\n        detach_default_85: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_28)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_116: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_28, [1, 32, sym_size_114, sym_size_113]);  _softmax_default_28 = None\\n        view_default_686: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_116, [32, sym_size_114, sym_size_113]);  expand_default_116 = sym_size_113 = None\\n        sym_size_115: Sym(s0) = torch.ops.aten.sym_size(view_default_679, 1);  view_default_679 = None\\n        expand_default_117: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_142, [1, 32, sym_size_115, 128])\\n        view_default_687: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_117, [32, sym_size_115, 128]);  expand_default_117 = sym_size_115 = None\\n        bmm_default_57: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_686, view_default_687);  view_default_686 = view_default_687 = None\\n        view_default_688: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_57, [1, 32, sym_size_114, 128]);  bmm_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_144: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_688, 1, 2);  view_default_688 = None\\n        clone_default_28: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_144, memory_format = torch.contiguous_format);  transpose_int_144 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_689: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_28, [1, sym_size, 4096]);  clone_default_28 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant257 = self._param_constant257\\n        t_default_199: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant257);  _param_constant257 = None\\n        view_default_690: f32[s0, 4096] = torch.ops.aten.view.default(view_default_689, [sym_size_114, 4096]);  view_default_689 = None\\n        mm_default_199: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_690, t_default_199);  view_default_690 = t_default_199 = None\\n        view_default_691: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_199, [1, sym_size_114, 4096]);  mm_default_199 = sym_size_114 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_202: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_197, view_default_691);  add_tensor_197 = view_default_691 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_57: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_202, 2)\\n        mean_dim_57: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_57, [-1], True);  pow_tensor_scalar_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_203: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_57, 1e-06);  mean_dim_57 = None\\n        rsqrt_default_57: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_203);  add_tensor_203 = None\\n        detach_default_86: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_57)\\n        mul_tensor_258: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_202, rsqrt_default_57);  rsqrt_default_57 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant258 = self._param_constant258\\n        mul_tensor_259: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant258, mul_tensor_258);  _param_constant258 = mul_tensor_258 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant259 = self._param_constant259\\n        t_default_200: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant259);  _param_constant259 = None\\n        view_default_692: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_259, [sym_size, 4096])\\n        mm_default_200: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_692, t_default_200);  view_default_692 = t_default_200 = None\\n        view_default_693: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_200, [1, sym_size, 11008]);  mm_default_200 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_28: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_693)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant260 = self._param_constant260\\n        t_default_201: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant260);  _param_constant260 = None\\n        view_default_694: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_259, [sym_size, 4096]);  mul_tensor_259 = None\\n        mm_default_201: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_694, t_default_201);  view_default_694 = t_default_201 = None\\n        view_default_695: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_201, [1, sym_size, 11008]);  mm_default_201 = None\\n        mul_tensor_260: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_28, view_default_695);  silu_default_28 = view_default_695 = None\\n        _param_constant261 = self._param_constant261\\n        t_default_202: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant261);  _param_constant261 = None\\n        sym_size_116: Sym(s0) = torch.ops.aten.sym_size(view_default_693, 1);  view_default_693 = None\\n        view_default_696: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_260, [sym_size_116, 11008]);  mul_tensor_260 = None\\n        mm_default_202: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_696, t_default_202);  view_default_696 = t_default_202 = None\\n        view_default_697: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_202, [1, sym_size_116, 4096]);  mm_default_202 = sym_size_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_204: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_202, view_default_697);  add_tensor_202 = view_default_697 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_58: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_204, 2)\\n        mean_dim_58: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_58, [-1], True);  pow_tensor_scalar_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_205: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_58, 1e-06);  mean_dim_58 = None\\n        rsqrt_default_58: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_205);  add_tensor_205 = None\\n        detach_default_87: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_58)\\n        mul_tensor_261: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_204, rsqrt_default_58);  rsqrt_default_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant262 = self._param_constant262\\n        mul_tensor_262: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant262, mul_tensor_261);  _param_constant262 = mul_tensor_261 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant263 = self._param_constant263\\n        t_default_203: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant263);  _param_constant263 = None\\n        view_default_698: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_262, [sym_size, 4096])\\n        mm_default_203: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_698, t_default_203);  view_default_698 = t_default_203 = None\\n        view_default_699: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_203, [1, sym_size, 4096]);  mm_default_203 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant264 = self._param_constant264\\n        t_default_204: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant264);  _param_constant264 = None\\n        view_default_700: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_262, [sym_size, 4096])\\n        mm_default_204: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_700, t_default_204);  view_default_700 = t_default_204 = None\\n        view_default_701: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_204, [1, sym_size, 4096]);  mm_default_204 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant265 = self._param_constant265\\n        t_default_205: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant265);  _param_constant265 = None\\n        view_default_702: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_262, [sym_size, 4096]);  mul_tensor_262 = None\\n        mm_default_205: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_702, t_default_205);  view_default_702 = t_default_205 = None\\n        view_default_703: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_205, [1, sym_size, 4096]);  mm_default_205 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_704: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_699, [1, sym_size, 32, 128])\\n        transpose_int_145: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_704, 1, 2);  view_default_704 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_705: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_701, [1, sym_size, 32, 128])\\n        transpose_int_146: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_705, 1, 2);  view_default_705 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_706: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_703, [1, sym_size, 32, 128])\\n        transpose_int_147: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_706, 1, 2);  view_default_706 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant58 = self._tensor_constant58\\n        slice_tensor_294: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant58, 0, 0, 9223372036854775807);  _tensor_constant58 = None\\n        slice_tensor_295: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_294, 1, 0, 9223372036854775807);  slice_tensor_294 = None\\n        sym_size_117: Sym(s0) = torch.ops.aten.sym_size(view_default_701, 1);  view_default_701 = None\\n        slice_tensor_296: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_295, 2, 0, sym_size_117);  slice_tensor_295 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant59 = self._tensor_constant59\\n        slice_tensor_297: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant59, 0, 0, 9223372036854775807);  _tensor_constant59 = None\\n        slice_tensor_298: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_297, 1, 0, 9223372036854775807);  slice_tensor_297 = None\\n        slice_tensor_299: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_298, 2, 0, sym_size_117);  slice_tensor_298 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_116: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_296, 1);  slice_tensor_296 = None\\n        squeeze_dim_117: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_116, 0);  squeeze_dim_116 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_118: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_299, 1);  slice_tensor_299 = None\\n        squeeze_dim_119: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_118, 0);  squeeze_dim_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_58: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_117, [view_default]);  squeeze_dim_117 = None\\n        unsqueeze_default_63: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_58, 1);  index_tensor_58 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_59: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_119, [view_default]);  squeeze_dim_119 = None\\n        unsqueeze_default_64: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_59, 1);  index_tensor_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_263: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_145, unsqueeze_default_63)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_300: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_301: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_145, 3, 64, 9223372036854775807);  transpose_int_145 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_58: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_301);  slice_tensor_301 = None\\n        cat_default_58: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_58, slice_tensor_300], -1);  neg_default_58 = slice_tensor_300 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_264: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_58, unsqueeze_default_64);  cat_default_58 = None\\n        add_tensor_206: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_263, mul_tensor_264);  mul_tensor_263 = mul_tensor_264 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_265: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_146, unsqueeze_default_63);  unsqueeze_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_302: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_303: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_146, 3, 64, 9223372036854775807);  transpose_int_146 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_59: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_303);  slice_tensor_303 = None\\n        cat_default_59: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_59, slice_tensor_302], -1);  neg_default_59 = slice_tensor_302 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_266: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_59, unsqueeze_default_64);  cat_default_59 = unsqueeze_default_64 = None\\n        add_tensor_207: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_265, mul_tensor_266);  mul_tensor_265 = mul_tensor_266 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_148: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_207, 2, 3)\\n        sym_size_118: Sym(s0) = torch.ops.aten.sym_size(view_default_699, 1);  view_default_699 = None\\n        expand_default_118: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_206, [1, 32, sym_size_118, 128]);  add_tensor_206 = None\\n        view_default_707: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_118, [32, sym_size_118, 128]);  expand_default_118 = None\\n        expand_default_119: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_148, [1, 32, 128, sym_size_117]);  transpose_int_148 = None\\n        view_default_708: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_119, [32, 128, sym_size_117]);  expand_default_119 = None\\n        bmm_default_58: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_707, view_default_708);  view_default_707 = view_default_708 = None\\n        view_default_709: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_58, [1, 32, sym_size_118, sym_size_117]);  bmm_default_58 = None\\n        div_tensor_29: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_709, 11.313708498984761);  view_default_709 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_208: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_29, add_tensor_1);  div_tensor_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_29: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_208, -1, False);  add_tensor_208 = None\\n        detach_default_88: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_29)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_120: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_29, [1, 32, sym_size_118, sym_size_117]);  _softmax_default_29 = None\\n        view_default_710: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_120, [32, sym_size_118, sym_size_117]);  expand_default_120 = sym_size_117 = None\\n        sym_size_119: Sym(s0) = torch.ops.aten.sym_size(view_default_703, 1);  view_default_703 = None\\n        expand_default_121: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_147, [1, 32, sym_size_119, 128])\\n        view_default_711: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_121, [32, sym_size_119, 128]);  expand_default_121 = sym_size_119 = None\\n        bmm_default_59: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_710, view_default_711);  view_default_710 = view_default_711 = None\\n        view_default_712: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_59, [1, 32, sym_size_118, 128]);  bmm_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_149: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_712, 1, 2);  view_default_712 = None\\n        clone_default_29: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_149, memory_format = torch.contiguous_format);  transpose_int_149 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_713: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_29, [1, sym_size, 4096]);  clone_default_29 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant266 = self._param_constant266\\n        t_default_206: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant266);  _param_constant266 = None\\n        view_default_714: f32[s0, 4096] = torch.ops.aten.view.default(view_default_713, [sym_size_118, 4096]);  view_default_713 = None\\n        mm_default_206: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_714, t_default_206);  view_default_714 = t_default_206 = None\\n        view_default_715: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_206, [1, sym_size_118, 4096]);  mm_default_206 = sym_size_118 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_209: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_204, view_default_715);  add_tensor_204 = view_default_715 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_59: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_209, 2)\\n        mean_dim_59: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_59, [-1], True);  pow_tensor_scalar_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_210: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_59, 1e-06);  mean_dim_59 = None\\n        rsqrt_default_59: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_210);  add_tensor_210 = None\\n        detach_default_89: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_59)\\n        mul_tensor_267: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_209, rsqrt_default_59);  rsqrt_default_59 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant267 = self._param_constant267\\n        mul_tensor_268: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant267, mul_tensor_267);  _param_constant267 = mul_tensor_267 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant268 = self._param_constant268\\n        t_default_207: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant268);  _param_constant268 = None\\n        view_default_716: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_268, [sym_size, 4096])\\n        mm_default_207: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_716, t_default_207);  view_default_716 = t_default_207 = None\\n        view_default_717: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_207, [1, sym_size, 11008]);  mm_default_207 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_29: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_717)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant269 = self._param_constant269\\n        t_default_208: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant269);  _param_constant269 = None\\n        view_default_718: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_268, [sym_size, 4096]);  mul_tensor_268 = None\\n        mm_default_208: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_718, t_default_208);  view_default_718 = t_default_208 = None\\n        view_default_719: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_208, [1, sym_size, 11008]);  mm_default_208 = None\\n        mul_tensor_269: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_29, view_default_719);  silu_default_29 = view_default_719 = None\\n        _param_constant270 = self._param_constant270\\n        t_default_209: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant270);  _param_constant270 = None\\n        sym_size_120: Sym(s0) = torch.ops.aten.sym_size(view_default_717, 1);  view_default_717 = None\\n        view_default_720: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_269, [sym_size_120, 11008]);  mul_tensor_269 = None\\n        mm_default_209: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_720, t_default_209);  view_default_720 = t_default_209 = None\\n        view_default_721: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_209, [1, sym_size_120, 4096]);  mm_default_209 = sym_size_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_211: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_209, view_default_721);  add_tensor_209 = view_default_721 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_60: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_211, 2)\\n        mean_dim_60: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_60, [-1], True);  pow_tensor_scalar_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_212: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_60, 1e-06);  mean_dim_60 = None\\n        rsqrt_default_60: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_212);  add_tensor_212 = None\\n        detach_default_90: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_60)\\n        mul_tensor_270: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_211, rsqrt_default_60);  rsqrt_default_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant271 = self._param_constant271\\n        mul_tensor_271: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant271, mul_tensor_270);  _param_constant271 = mul_tensor_270 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant272 = self._param_constant272\\n        t_default_210: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant272);  _param_constant272 = None\\n        view_default_722: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_271, [sym_size, 4096])\\n        mm_default_210: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_722, t_default_210);  view_default_722 = t_default_210 = None\\n        view_default_723: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_210, [1, sym_size, 4096]);  mm_default_210 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant273 = self._param_constant273\\n        t_default_211: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant273);  _param_constant273 = None\\n        view_default_724: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_271, [sym_size, 4096])\\n        mm_default_211: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_724, t_default_211);  view_default_724 = t_default_211 = None\\n        view_default_725: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_211, [1, sym_size, 4096]);  mm_default_211 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant274 = self._param_constant274\\n        t_default_212: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant274);  _param_constant274 = None\\n        view_default_726: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_271, [sym_size, 4096]);  mul_tensor_271 = None\\n        mm_default_212: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_726, t_default_212);  view_default_726 = t_default_212 = None\\n        view_default_727: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_212, [1, sym_size, 4096]);  mm_default_212 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_728: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_723, [1, sym_size, 32, 128])\\n        transpose_int_150: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_728, 1, 2);  view_default_728 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_729: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_725, [1, sym_size, 32, 128])\\n        transpose_int_151: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_729, 1, 2);  view_default_729 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_730: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_727, [1, sym_size, 32, 128])\\n        transpose_int_152: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_730, 1, 2);  view_default_730 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant60 = self._tensor_constant60\\n        slice_tensor_304: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant60, 0, 0, 9223372036854775807);  _tensor_constant60 = None\\n        slice_tensor_305: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_304, 1, 0, 9223372036854775807);  slice_tensor_304 = None\\n        sym_size_121: Sym(s0) = torch.ops.aten.sym_size(view_default_725, 1);  view_default_725 = None\\n        slice_tensor_306: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_305, 2, 0, sym_size_121);  slice_tensor_305 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant61 = self._tensor_constant61\\n        slice_tensor_307: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant61, 0, 0, 9223372036854775807);  _tensor_constant61 = None\\n        slice_tensor_308: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_307, 1, 0, 9223372036854775807);  slice_tensor_307 = None\\n        slice_tensor_309: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_308, 2, 0, sym_size_121);  slice_tensor_308 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_120: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_306, 1);  slice_tensor_306 = None\\n        squeeze_dim_121: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_120, 0);  squeeze_dim_120 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_122: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_309, 1);  slice_tensor_309 = None\\n        squeeze_dim_123: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_122, 0);  squeeze_dim_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_60: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_121, [view_default]);  squeeze_dim_121 = None\\n        unsqueeze_default_65: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_60, 1);  index_tensor_60 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_61: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_123, [view_default]);  squeeze_dim_123 = None\\n        unsqueeze_default_66: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_61, 1);  index_tensor_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_272: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_150, unsqueeze_default_65)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_310: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_311: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_150, 3, 64, 9223372036854775807);  transpose_int_150 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_60: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_311);  slice_tensor_311 = None\\n        cat_default_60: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_60, slice_tensor_310], -1);  neg_default_60 = slice_tensor_310 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_273: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_60, unsqueeze_default_66);  cat_default_60 = None\\n        add_tensor_213: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_272, mul_tensor_273);  mul_tensor_272 = mul_tensor_273 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_274: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_151, unsqueeze_default_65);  unsqueeze_default_65 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_312: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_313: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_151, 3, 64, 9223372036854775807);  transpose_int_151 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_61: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_313);  slice_tensor_313 = None\\n        cat_default_61: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_61, slice_tensor_312], -1);  neg_default_61 = slice_tensor_312 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_275: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_61, unsqueeze_default_66);  cat_default_61 = unsqueeze_default_66 = None\\n        add_tensor_214: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_274, mul_tensor_275);  mul_tensor_274 = mul_tensor_275 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_153: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_214, 2, 3)\\n        sym_size_122: Sym(s0) = torch.ops.aten.sym_size(view_default_723, 1);  view_default_723 = None\\n        expand_default_122: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_213, [1, 32, sym_size_122, 128]);  add_tensor_213 = None\\n        view_default_731: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_122, [32, sym_size_122, 128]);  expand_default_122 = None\\n        expand_default_123: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_153, [1, 32, 128, sym_size_121]);  transpose_int_153 = None\\n        view_default_732: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_123, [32, 128, sym_size_121]);  expand_default_123 = None\\n        bmm_default_60: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_731, view_default_732);  view_default_731 = view_default_732 = None\\n        view_default_733: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_60, [1, 32, sym_size_122, sym_size_121]);  bmm_default_60 = None\\n        div_tensor_30: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_733, 11.313708498984761);  view_default_733 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_215: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_30, add_tensor_1);  div_tensor_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_30: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_215, -1, False);  add_tensor_215 = None\\n        detach_default_91: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_30)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_124: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_30, [1, 32, sym_size_122, sym_size_121]);  _softmax_default_30 = None\\n        view_default_734: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_124, [32, sym_size_122, sym_size_121]);  expand_default_124 = sym_size_121 = None\\n        sym_size_123: Sym(s0) = torch.ops.aten.sym_size(view_default_727, 1);  view_default_727 = None\\n        expand_default_125: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_152, [1, 32, sym_size_123, 128])\\n        view_default_735: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_125, [32, sym_size_123, 128]);  expand_default_125 = sym_size_123 = None\\n        bmm_default_61: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_734, view_default_735);  view_default_734 = view_default_735 = None\\n        view_default_736: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_61, [1, 32, sym_size_122, 128]);  bmm_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_154: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_736, 1, 2);  view_default_736 = None\\n        clone_default_30: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_154, memory_format = torch.contiguous_format);  transpose_int_154 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_737: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_30, [1, sym_size, 4096]);  clone_default_30 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant275 = self._param_constant275\\n        t_default_213: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant275);  _param_constant275 = None\\n        view_default_738: f32[s0, 4096] = torch.ops.aten.view.default(view_default_737, [sym_size_122, 4096]);  view_default_737 = None\\n        mm_default_213: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_738, t_default_213);  view_default_738 = t_default_213 = None\\n        view_default_739: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_213, [1, sym_size_122, 4096]);  mm_default_213 = sym_size_122 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_216: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_211, view_default_739);  add_tensor_211 = view_default_739 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_61: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_216, 2)\\n        mean_dim_61: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_61, [-1], True);  pow_tensor_scalar_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_217: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_61, 1e-06);  mean_dim_61 = None\\n        rsqrt_default_61: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_217);  add_tensor_217 = None\\n        detach_default_92: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_61)\\n        mul_tensor_276: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_216, rsqrt_default_61);  rsqrt_default_61 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant276 = self._param_constant276\\n        mul_tensor_277: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant276, mul_tensor_276);  _param_constant276 = mul_tensor_276 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant277 = self._param_constant277\\n        t_default_214: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant277);  _param_constant277 = None\\n        view_default_740: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_277, [sym_size, 4096])\\n        mm_default_214: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_740, t_default_214);  view_default_740 = t_default_214 = None\\n        view_default_741: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_214, [1, sym_size, 11008]);  mm_default_214 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_30: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_741)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant278 = self._param_constant278\\n        t_default_215: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant278);  _param_constant278 = None\\n        view_default_742: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_277, [sym_size, 4096]);  mul_tensor_277 = None\\n        mm_default_215: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_742, t_default_215);  view_default_742 = t_default_215 = None\\n        view_default_743: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_215, [1, sym_size, 11008]);  mm_default_215 = None\\n        mul_tensor_278: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_30, view_default_743);  silu_default_30 = view_default_743 = None\\n        _param_constant279 = self._param_constant279\\n        t_default_216: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant279);  _param_constant279 = None\\n        sym_size_124: Sym(s0) = torch.ops.aten.sym_size(view_default_741, 1);  view_default_741 = None\\n        view_default_744: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_278, [sym_size_124, 11008]);  mul_tensor_278 = None\\n        mm_default_216: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_744, t_default_216);  view_default_744 = t_default_216 = None\\n        view_default_745: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_216, [1, sym_size_124, 4096]);  mm_default_216 = sym_size_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_218: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_216, view_default_745);  add_tensor_216 = view_default_745 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_62: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_218, 2)\\n        mean_dim_62: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_62, [-1], True);  pow_tensor_scalar_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_219: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_62, 1e-06);  mean_dim_62 = None\\n        rsqrt_default_62: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_219);  add_tensor_219 = None\\n        detach_default_93: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_62)\\n        mul_tensor_279: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_218, rsqrt_default_62);  rsqrt_default_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant280 = self._param_constant280\\n        mul_tensor_280: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant280, mul_tensor_279);  _param_constant280 = mul_tensor_279 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:310, code: query_states = self.q_proj(hidden_states)\\n        _param_constant281 = self._param_constant281\\n        t_default_217: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant281);  _param_constant281 = None\\n        view_default_746: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_280, [sym_size, 4096])\\n        mm_default_217: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_746, t_default_217);  view_default_746 = t_default_217 = None\\n        view_default_747: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_217, [1, sym_size, 4096]);  mm_default_217 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311, code: key_states = self.k_proj(hidden_states)\\n        _param_constant282 = self._param_constant282\\n        t_default_218: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant282);  _param_constant282 = None\\n        view_default_748: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_280, [sym_size, 4096])\\n        mm_default_218: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_748, t_default_218);  view_default_748 = t_default_218 = None\\n        view_default_749: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_218, [1, sym_size, 4096]);  mm_default_218 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:312, code: value_states = self.v_proj(hidden_states)\\n        _param_constant283 = self._param_constant283\\n        t_default_219: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant283);  _param_constant283 = None\\n        view_default_750: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_280, [sym_size, 4096]);  mul_tensor_280 = None\\n        mm_default_219: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_750, t_default_219);  view_default_750 = t_default_219 = None\\n        view_default_751: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_219, [1, sym_size, 4096]);  mm_default_219 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:314, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\\n        view_default_752: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_747, [1, sym_size, 32, 128])\\n        transpose_int_155: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_752, 1, 2);  view_default_752 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_753: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_749, [1, sym_size, 32, 128])\\n        transpose_int_156: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_753, 1, 2);  view_default_753 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:316, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\\n        view_default_754: f32[1, s0, 32, 128] = torch.ops.aten.view.default(view_default_751, [1, sym_size, 32, 128])\\n        transpose_int_157: f32[1, 32, s0, 128] = torch.ops.aten.transpose.int(view_default_754, 1, 2);  view_default_754 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123, code: self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant62 = self._tensor_constant62\\n        slice_tensor_314: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant62, 0, 0, 9223372036854775807);  _tensor_constant62 = None\\n        slice_tensor_315: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_314, 1, 0, 9223372036854775807);  slice_tensor_314 = None\\n        sym_size_125: Sym(s0) = torch.ops.aten.sym_size(view_default_749, 1);  view_default_749 = None\\n        slice_tensor_316: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_315, 2, 0, sym_size_125);  slice_tensor_315 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124, code: self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\\n        _tensor_constant63 = self._tensor_constant63\\n        slice_tensor_317: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(_tensor_constant63, 0, 0, 9223372036854775807);  _tensor_constant63 = None\\n        slice_tensor_318: f32[1, 1, 4096, 128] = torch.ops.aten.slice.Tensor(slice_tensor_317, 1, 0, 9223372036854775807);  slice_tensor_317 = None\\n        slice_tensor_319: f32[1, 1, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_318, 2, 0, sym_size_125);  slice_tensor_318 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:182, code: cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_124: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_316, 1);  slice_tensor_316 = None\\n        squeeze_dim_125: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_124, 0);  squeeze_dim_124 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183, code: sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\\n        squeeze_dim_126: f32[1, s0, 128] = torch.ops.aten.squeeze.dim(slice_tensor_319, 1);  slice_tensor_319 = None\\n        squeeze_dim_127: f32[s0, 128] = torch.ops.aten.squeeze.dim(squeeze_dim_126, 0);  squeeze_dim_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:184, code: cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_62: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_125, [view_default]);  squeeze_dim_125 = None\\n        unsqueeze_default_67: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_62, 1);  index_tensor_62 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:185, code: sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\\n        index_tensor_63: f32[1, s0, 128] = torch.ops.aten.index.Tensor(squeeze_dim_127, [view_default]);  squeeze_dim_127 = view_default = None\\n        unsqueeze_default_68: f32[1, 1, s0, 128] = torch.ops.aten.unsqueeze.default(index_tensor_63, 1);  index_tensor_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_281: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_155, unsqueeze_default_67)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_320: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_321: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_155, 3, 64, 9223372036854775807);  transpose_int_155 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_62: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_321);  slice_tensor_321 = None\\n        cat_default_62: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_62, slice_tensor_320], -1);  neg_default_62 = slice_tensor_320 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:186, code: q_embed = (q * cos) + (rotate_half(q) * sin)\\n        mul_tensor_282: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_62, unsqueeze_default_68);  cat_default_62 = None\\n        add_tensor_220: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_281, mul_tensor_282);  mul_tensor_281 = mul_tensor_282 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_283: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(transpose_int_156, unsqueeze_default_67);  unsqueeze_default_67 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175, code: x1 = x[..., : x.shape[-1] // 2]\\n        slice_tensor_322: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 0, 64)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:176, code: x2 = x[..., x.shape[-1] // 2 :]\\n        slice_tensor_323: f32[1, 32, s0, 64] = torch.ops.aten.slice.Tensor(transpose_int_156, 3, 64, 9223372036854775807);  transpose_int_156 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:177, code: return torch.cat((-x2, x1), dim=-1)\\n        neg_default_63: f32[1, 32, s0, 64] = torch.ops.aten.neg.default(slice_tensor_323);  slice_tensor_323 = None\\n        cat_default_63: f32[1, 32, s0, 128] = torch.ops.aten.cat.default([neg_default_63, slice_tensor_322], -1);  neg_default_63 = slice_tensor_322 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:187, code: k_embed = (k * cos) + (rotate_half(k) * sin)\\n        mul_tensor_284: f32[1, 32, s0, 128] = torch.ops.aten.mul.Tensor(cat_default_63, unsqueeze_default_68);  cat_default_63 = unsqueeze_default_68 = None\\n        add_tensor_221: f32[1, 32, s0, 128] = torch.ops.aten.add.Tensor(mul_tensor_283, mul_tensor_284);  mul_tensor_283 = mul_tensor_284 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:335, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\\n        transpose_int_158: f32[1, 32, 128, s0] = torch.ops.aten.transpose.int(add_tensor_221, 2, 3)\\n        sym_size_126: Sym(s0) = torch.ops.aten.sym_size(view_default_747, 1);  view_default_747 = None\\n        expand_default_126: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(add_tensor_220, [1, 32, sym_size_126, 128]);  add_tensor_220 = None\\n        view_default_755: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_126, [32, sym_size_126, 128]);  expand_default_126 = None\\n        expand_default_127: f32[1, 32, 128, s0] = torch.ops.aten.expand.default(transpose_int_158, [1, 32, 128, sym_size_125]);  transpose_int_158 = None\\n        view_default_756: f32[32, 128, s0] = torch.ops.aten.view.default(expand_default_127, [32, 128, sym_size_125]);  expand_default_127 = None\\n        bmm_default_62: f32[32, s0, s0] = torch.ops.aten.bmm.default(view_default_755, view_default_756);  view_default_755 = view_default_756 = None\\n        view_default_757: f32[1, 32, s0, s0] = torch.ops.aten.view.default(bmm_default_62, [1, 32, sym_size_126, sym_size_125]);  bmm_default_62 = None\\n        div_tensor_31: f32[1, 32, s0, s0] = torch.ops.aten.div.Tensor(view_default_757, 11.313708498984761);  view_default_757 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:348, code: attn_weights = attn_weights + attention_mask\\n        add_tensor_222: f32[1, 32, s0, s0] = torch.ops.aten.add.Tensor(div_tensor_31, add_tensor_1);  div_tensor_31 = add_tensor_1 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:351, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\\n        _softmax_default_31: f32[1, 32, s0, s0] = torch.ops.aten._softmax.default(add_tensor_222, -1, False);  add_tensor_222 = None\\n        detach_default_94: f32[1, 32, s0, s0] = torch.ops.aten.detach.default(_softmax_default_31)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352, code: attn_output = torch.matmul(attn_weights, value_states)\\n        expand_default_128: f32[1, 32, s0, s0] = torch.ops.aten.expand.default(_softmax_default_31, [1, 32, sym_size_126, sym_size_125]);  _softmax_default_31 = None\\n        view_default_758: f32[32, s0, s0] = torch.ops.aten.view.default(expand_default_128, [32, sym_size_126, sym_size_125]);  expand_default_128 = sym_size_125 = None\\n        sym_size_127: Sym(s0) = torch.ops.aten.sym_size(view_default_751, 1);  view_default_751 = None\\n        expand_default_129: f32[1, 32, s0, 128] = torch.ops.aten.expand.default(transpose_int_157, [1, 32, sym_size_127, 128])\\n        view_default_759: f32[32, s0, 128] = torch.ops.aten.view.default(expand_default_129, [32, sym_size_127, 128]);  expand_default_129 = sym_size_127 = None\\n        bmm_default_63: f32[32, s0, 128] = torch.ops.aten.bmm.default(view_default_758, view_default_759);  view_default_758 = view_default_759 = None\\n        view_default_760: f32[1, 32, s0, 128] = torch.ops.aten.view.default(bmm_default_63, [1, 32, sym_size_126, 128]);  bmm_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:360, code: attn_output = attn_output.transpose(1, 2).contiguous()\\n        transpose_int_159: f32[1, s0, 32, 128] = torch.ops.aten.transpose.int(view_default_760, 1, 2);  view_default_760 = None\\n        clone_default_31: f32[1, s0, 32, 128] = torch.ops.aten.clone.default(transpose_int_159, memory_format = torch.contiguous_format);  transpose_int_159 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:361, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\\n        view_default_761: f32[1, s0, 4096] = torch.ops.aten.view.default(clone_default_31, [1, sym_size, 4096]);  clone_default_31 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:368, code: attn_output = self.o_proj(attn_output)\\n        _param_constant284 = self._param_constant284\\n        t_default_220: f32[4096, 4096] = torch.ops.aten.t.default(_param_constant284);  _param_constant284 = None\\n        view_default_762: f32[s0, 4096] = torch.ops.aten.view.default(view_default_761, [sym_size_126, 4096]);  view_default_761 = None\\n        mm_default_220: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_762, t_default_220);  view_default_762 = t_default_220 = None\\n        view_default_763: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_220, [1, sym_size_126, 4096]);  mm_default_220 = sym_size_126 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421, code: hidden_states = residual + hidden_states\\n        add_tensor_223: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_218, view_default_763);  add_tensor_218 = view_default_763 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_63: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_223, 2)\\n        mean_dim_63: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_63, [-1], True);  pow_tensor_scalar_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_224: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_63, 1e-06);  mean_dim_63 = None\\n        rsqrt_default_63: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_224);  add_tensor_224 = None\\n        detach_default_95: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_63)\\n        mul_tensor_285: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_223, rsqrt_default_63);  rsqrt_default_63 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant285 = self._param_constant285\\n        mul_tensor_286: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant285, mul_tensor_285);  _param_constant285 = mul_tensor_285 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant286 = self._param_constant286\\n        t_default_221: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant286);  _param_constant286 = None\\n        view_default_764: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_286, [sym_size, 4096])\\n        mm_default_221: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_764, t_default_221);  view_default_764 = t_default_221 = None\\n        view_default_765: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_221, [1, sym_size, 11008]);  mm_default_221 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/activations.py:150, code: return nn.functional.silu(input)\\n        silu_default_31: f32[1, s0, 11008] = torch.ops.aten.silu.default(view_default_765)\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:220, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n        _param_constant287 = self._param_constant287\\n        t_default_222: f32[4096, 11008] = torch.ops.aten.t.default(_param_constant287);  _param_constant287 = None\\n        view_default_766: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_286, [sym_size, 4096]);  mul_tensor_286 = None\\n        mm_default_222: f32[s0, 11008] = torch.ops.aten.mm.default(view_default_766, t_default_222);  view_default_766 = t_default_222 = None\\n        view_default_767: f32[1, s0, 11008] = torch.ops.aten.view.default(mm_default_222, [1, sym_size, 11008]);  mm_default_222 = None\\n        mul_tensor_287: f32[1, s0, 11008] = torch.ops.aten.mul.Tensor(silu_default_31, view_default_767);  silu_default_31 = view_default_767 = None\\n        _param_constant288 = self._param_constant288\\n        t_default_223: f32[11008, 4096] = torch.ops.aten.t.default(_param_constant288);  _param_constant288 = None\\n        sym_size_128: Sym(s0) = torch.ops.aten.sym_size(view_default_765, 1);  view_default_765 = None\\n        view_default_768: f32[s0, 11008] = torch.ops.aten.view.default(mul_tensor_287, [sym_size_128, 11008]);  mul_tensor_287 = None\\n        mm_default_223: f32[s0, 4096] = torch.ops.aten.mm.default(view_default_768, t_default_223);  view_default_768 = t_default_223 = None\\n        view_default_769: f32[1, s0, 4096] = torch.ops.aten.view.default(mm_default_223, [1, sym_size_128, 4096]);  mm_default_223 = sym_size_128 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:427, code: hidden_states = residual + hidden_states\\n        add_tensor_225: f32[1, s0, 4096] = torch.ops.aten.add.Tensor(add_tensor_223, view_default_769);  add_tensor_223 = view_default_769 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:87, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        pow_tensor_scalar_64: f32[1, s0, 4096] = torch.ops.aten.pow.Tensor_Scalar(add_tensor_225, 2)\\n        mean_dim_64: f32[1, s0, 1] = torch.ops.aten.mean.dim(pow_tensor_scalar_64, [-1], True);  pow_tensor_scalar_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:88, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\\n        add_tensor_226: f32[1, s0, 1] = torch.ops.aten.add.Tensor(mean_dim_64, 1e-06);  mean_dim_64 = None\\n        rsqrt_default_64: f32[1, s0, 1] = torch.ops.aten.rsqrt.default(add_tensor_226);  add_tensor_226 = None\\n        detach_default_96: f32[1, s0, 1] = torch.ops.aten.detach.default(rsqrt_default_64)\\n        mul_tensor_288: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(add_tensor_225, rsqrt_default_64);  add_tensor_225 = rsqrt_default_64 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:89, code: return self.weight * hidden_states.to(input_dtype)\\n        _param_constant289 = self._param_constant289\\n        mul_tensor_289: f32[1, s0, 4096] = torch.ops.aten.mul.Tensor(_param_constant289, mul_tensor_288);  _param_constant289 = mul_tensor_288 = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:827, code: logits = self.lm_head(hidden_states)\\n        _param_constant290 = self._param_constant290\\n        t_default_224: f32[4096, 32000] = torch.ops.aten.t.default(_param_constant290);  _param_constant290 = None\\n        view_default_770: f32[s0, 4096] = torch.ops.aten.view.default(mul_tensor_289, [sym_size, 4096]);  mul_tensor_289 = None\\n        mm_default_224: f32[s0, 32000] = torch.ops.aten.mm.default(view_default_770, t_default_224);  view_default_770 = t_default_224 = None\\n        view_default_771: f32[1, s0, 32000] = torch.ops.aten.view.default(mm_default_224, [1, sym_size, 32000]);  mm_default_224 = sym_size = None\\n        \\n        # File: /home/stella/src/venv/Turbine/lib/python3.11/site-packages/torch/_export/constraints.py:13, code: torch.sym_constrain_range(symbol, min=min, max=max)\\n        sym_constrain_range_default_1 = torch.ops.aten.sym_constrain_range.default(sym_size_1)\\n        \\n        # File: /tmp/ipykernel_881114/4291870228.py:46, code: state[:, :, 0:seq_length, :] = update[:, :, :, :]\\n        slice_tensor_324: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_4, 0, 0, 9223372036854775807);  add_tensor_4 = None\\n        slice_tensor_325: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_324, 1, 0, 9223372036854775807);  slice_tensor_324 = None\\n        slice_tensor_326: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_325, 2, 0, 9223372036854775807);  slice_tensor_325 = None\\n        slice_tensor_327: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_326, 3, 0, 9223372036854775807);  slice_tensor_326 = None\\n        _tensor_constant64 = self._tensor_constant64\\n        slice_tensor_328: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant64, 0, 0, 9223372036854775807);  _tensor_constant64 = None\\n        slice_tensor_329: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_328, 1, 0, 9223372036854775807);  slice_tensor_328 = None\\n        slice_tensor_330: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_329, 2, 0, sym_size_1);  slice_tensor_329 = None\\n        slice_tensor_331: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_330, 3, 0, 9223372036854775807);  slice_tensor_330 = None\\n        copy__default: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_331, slice_tensor_327);  slice_tensor_331 = slice_tensor_327 = None\\n        slice_tensor_332: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_2, 0, 0, 9223372036854775807);  transpose_int_2 = None\\n        slice_tensor_333: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_332, 1, 0, 9223372036854775807);  slice_tensor_332 = None\\n        slice_tensor_334: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_333, 2, 0, 9223372036854775807);  slice_tensor_333 = None\\n        slice_tensor_335: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_334, 3, 0, 9223372036854775807);  slice_tensor_334 = None\\n        _tensor_constant65 = self._tensor_constant65\\n        slice_tensor_336: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant65, 0, 0, 9223372036854775807);  _tensor_constant65 = None\\n        slice_tensor_337: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_336, 1, 0, 9223372036854775807);  slice_tensor_336 = None\\n        slice_tensor_338: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_337, 2, 0, sym_size_1);  slice_tensor_337 = None\\n        slice_tensor_339: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_338, 3, 0, 9223372036854775807);  slice_tensor_338 = None\\n        copy__default_1: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_339, slice_tensor_335);  slice_tensor_339 = slice_tensor_335 = None\\n        slice_tensor_340: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_11, 0, 0, 9223372036854775807);  add_tensor_11 = None\\n        slice_tensor_341: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_340, 1, 0, 9223372036854775807);  slice_tensor_340 = None\\n        slice_tensor_342: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_341, 2, 0, 9223372036854775807);  slice_tensor_341 = None\\n        slice_tensor_343: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_342, 3, 0, 9223372036854775807);  slice_tensor_342 = None\\n        _tensor_constant66 = self._tensor_constant66\\n        slice_tensor_344: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant66, 0, 0, 9223372036854775807);  _tensor_constant66 = None\\n        slice_tensor_345: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_344, 1, 0, 9223372036854775807);  slice_tensor_344 = None\\n        slice_tensor_346: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_345, 2, 0, sym_size_1);  slice_tensor_345 = None\\n        slice_tensor_347: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_346, 3, 0, 9223372036854775807);  slice_tensor_346 = None\\n        copy__default_2: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_347, slice_tensor_343);  slice_tensor_347 = slice_tensor_343 = None\\n        slice_tensor_348: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_7, 0, 0, 9223372036854775807);  transpose_int_7 = None\\n        slice_tensor_349: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_348, 1, 0, 9223372036854775807);  slice_tensor_348 = None\\n        slice_tensor_350: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_349, 2, 0, 9223372036854775807);  slice_tensor_349 = None\\n        slice_tensor_351: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_350, 3, 0, 9223372036854775807);  slice_tensor_350 = None\\n        _tensor_constant67 = self._tensor_constant67\\n        slice_tensor_352: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant67, 0, 0, 9223372036854775807);  _tensor_constant67 = None\\n        slice_tensor_353: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_352, 1, 0, 9223372036854775807);  slice_tensor_352 = None\\n        slice_tensor_354: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_353, 2, 0, sym_size_1);  slice_tensor_353 = None\\n        slice_tensor_355: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_354, 3, 0, 9223372036854775807);  slice_tensor_354 = None\\n        copy__default_3: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_355, slice_tensor_351);  slice_tensor_355 = slice_tensor_351 = None\\n        slice_tensor_356: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_18, 0, 0, 9223372036854775807);  add_tensor_18 = None\\n        slice_tensor_357: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_356, 1, 0, 9223372036854775807);  slice_tensor_356 = None\\n        slice_tensor_358: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_357, 2, 0, 9223372036854775807);  slice_tensor_357 = None\\n        slice_tensor_359: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_358, 3, 0, 9223372036854775807);  slice_tensor_358 = None\\n        _tensor_constant68 = self._tensor_constant68\\n        slice_tensor_360: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant68, 0, 0, 9223372036854775807);  _tensor_constant68 = None\\n        slice_tensor_361: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_360, 1, 0, 9223372036854775807);  slice_tensor_360 = None\\n        slice_tensor_362: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_361, 2, 0, sym_size_1);  slice_tensor_361 = None\\n        slice_tensor_363: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_362, 3, 0, 9223372036854775807);  slice_tensor_362 = None\\n        copy__default_4: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_363, slice_tensor_359);  slice_tensor_363 = slice_tensor_359 = None\\n        slice_tensor_364: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_12, 0, 0, 9223372036854775807);  transpose_int_12 = None\\n        slice_tensor_365: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_364, 1, 0, 9223372036854775807);  slice_tensor_364 = None\\n        slice_tensor_366: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_365, 2, 0, 9223372036854775807);  slice_tensor_365 = None\\n        slice_tensor_367: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_366, 3, 0, 9223372036854775807);  slice_tensor_366 = None\\n        _tensor_constant69 = self._tensor_constant69\\n        slice_tensor_368: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant69, 0, 0, 9223372036854775807);  _tensor_constant69 = None\\n        slice_tensor_369: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_368, 1, 0, 9223372036854775807);  slice_tensor_368 = None\\n        slice_tensor_370: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_369, 2, 0, sym_size_1);  slice_tensor_369 = None\\n        slice_tensor_371: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_370, 3, 0, 9223372036854775807);  slice_tensor_370 = None\\n        copy__default_5: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_371, slice_tensor_367);  slice_tensor_371 = slice_tensor_367 = None\\n        slice_tensor_372: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_25, 0, 0, 9223372036854775807);  add_tensor_25 = None\\n        slice_tensor_373: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_372, 1, 0, 9223372036854775807);  slice_tensor_372 = None\\n        slice_tensor_374: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_373, 2, 0, 9223372036854775807);  slice_tensor_373 = None\\n        slice_tensor_375: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_374, 3, 0, 9223372036854775807);  slice_tensor_374 = None\\n        _tensor_constant70 = self._tensor_constant70\\n        slice_tensor_376: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant70, 0, 0, 9223372036854775807);  _tensor_constant70 = None\\n        slice_tensor_377: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_376, 1, 0, 9223372036854775807);  slice_tensor_376 = None\\n        slice_tensor_378: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_377, 2, 0, sym_size_1);  slice_tensor_377 = None\\n        slice_tensor_379: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_378, 3, 0, 9223372036854775807);  slice_tensor_378 = None\\n        copy__default_6: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_379, slice_tensor_375);  slice_tensor_379 = slice_tensor_375 = None\\n        slice_tensor_380: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_17, 0, 0, 9223372036854775807);  transpose_int_17 = None\\n        slice_tensor_381: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_380, 1, 0, 9223372036854775807);  slice_tensor_380 = None\\n        slice_tensor_382: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_381, 2, 0, 9223372036854775807);  slice_tensor_381 = None\\n        slice_tensor_383: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_382, 3, 0, 9223372036854775807);  slice_tensor_382 = None\\n        _tensor_constant71 = self._tensor_constant71\\n        slice_tensor_384: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant71, 0, 0, 9223372036854775807);  _tensor_constant71 = None\\n        slice_tensor_385: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_384, 1, 0, 9223372036854775807);  slice_tensor_384 = None\\n        slice_tensor_386: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_385, 2, 0, sym_size_1);  slice_tensor_385 = None\\n        slice_tensor_387: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_386, 3, 0, 9223372036854775807);  slice_tensor_386 = None\\n        copy__default_7: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_387, slice_tensor_383);  slice_tensor_387 = slice_tensor_383 = None\\n        slice_tensor_388: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_32, 0, 0, 9223372036854775807);  add_tensor_32 = None\\n        slice_tensor_389: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_388, 1, 0, 9223372036854775807);  slice_tensor_388 = None\\n        slice_tensor_390: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_389, 2, 0, 9223372036854775807);  slice_tensor_389 = None\\n        slice_tensor_391: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_390, 3, 0, 9223372036854775807);  slice_tensor_390 = None\\n        _tensor_constant72 = self._tensor_constant72\\n        slice_tensor_392: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant72, 0, 0, 9223372036854775807);  _tensor_constant72 = None\\n        slice_tensor_393: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_392, 1, 0, 9223372036854775807);  slice_tensor_392 = None\\n        slice_tensor_394: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_393, 2, 0, sym_size_1);  slice_tensor_393 = None\\n        slice_tensor_395: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_394, 3, 0, 9223372036854775807);  slice_tensor_394 = None\\n        copy__default_8: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_395, slice_tensor_391);  slice_tensor_395 = slice_tensor_391 = None\\n        slice_tensor_396: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_22, 0, 0, 9223372036854775807);  transpose_int_22 = None\\n        slice_tensor_397: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_396, 1, 0, 9223372036854775807);  slice_tensor_396 = None\\n        slice_tensor_398: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_397, 2, 0, 9223372036854775807);  slice_tensor_397 = None\\n        slice_tensor_399: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_398, 3, 0, 9223372036854775807);  slice_tensor_398 = None\\n        _tensor_constant73 = self._tensor_constant73\\n        slice_tensor_400: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant73, 0, 0, 9223372036854775807);  _tensor_constant73 = None\\n        slice_tensor_401: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_400, 1, 0, 9223372036854775807);  slice_tensor_400 = None\\n        slice_tensor_402: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_401, 2, 0, sym_size_1);  slice_tensor_401 = None\\n        slice_tensor_403: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_402, 3, 0, 9223372036854775807);  slice_tensor_402 = None\\n        copy__default_9: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_403, slice_tensor_399);  slice_tensor_403 = slice_tensor_399 = None\\n        slice_tensor_404: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_39, 0, 0, 9223372036854775807);  add_tensor_39 = None\\n        slice_tensor_405: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_404, 1, 0, 9223372036854775807);  slice_tensor_404 = None\\n        slice_tensor_406: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_405, 2, 0, 9223372036854775807);  slice_tensor_405 = None\\n        slice_tensor_407: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_406, 3, 0, 9223372036854775807);  slice_tensor_406 = None\\n        _tensor_constant74 = self._tensor_constant74\\n        slice_tensor_408: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant74, 0, 0, 9223372036854775807);  _tensor_constant74 = None\\n        slice_tensor_409: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_408, 1, 0, 9223372036854775807);  slice_tensor_408 = None\\n        slice_tensor_410: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_409, 2, 0, sym_size_1);  slice_tensor_409 = None\\n        slice_tensor_411: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_410, 3, 0, 9223372036854775807);  slice_tensor_410 = None\\n        copy__default_10: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_411, slice_tensor_407);  slice_tensor_411 = slice_tensor_407 = None\\n        slice_tensor_412: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_27, 0, 0, 9223372036854775807);  transpose_int_27 = None\\n        slice_tensor_413: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_412, 1, 0, 9223372036854775807);  slice_tensor_412 = None\\n        slice_tensor_414: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_413, 2, 0, 9223372036854775807);  slice_tensor_413 = None\\n        slice_tensor_415: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_414, 3, 0, 9223372036854775807);  slice_tensor_414 = None\\n        _tensor_constant75 = self._tensor_constant75\\n        slice_tensor_416: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant75, 0, 0, 9223372036854775807);  _tensor_constant75 = None\\n        slice_tensor_417: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_416, 1, 0, 9223372036854775807);  slice_tensor_416 = None\\n        slice_tensor_418: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_417, 2, 0, sym_size_1);  slice_tensor_417 = None\\n        slice_tensor_419: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_418, 3, 0, 9223372036854775807);  slice_tensor_418 = None\\n        copy__default_11: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_419, slice_tensor_415);  slice_tensor_419 = slice_tensor_415 = None\\n        slice_tensor_420: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_46, 0, 0, 9223372036854775807);  add_tensor_46 = None\\n        slice_tensor_421: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_420, 1, 0, 9223372036854775807);  slice_tensor_420 = None\\n        slice_tensor_422: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_421, 2, 0, 9223372036854775807);  slice_tensor_421 = None\\n        slice_tensor_423: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_422, 3, 0, 9223372036854775807);  slice_tensor_422 = None\\n        _tensor_constant76 = self._tensor_constant76\\n        slice_tensor_424: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant76, 0, 0, 9223372036854775807);  _tensor_constant76 = None\\n        slice_tensor_425: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_424, 1, 0, 9223372036854775807);  slice_tensor_424 = None\\n        slice_tensor_426: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_425, 2, 0, sym_size_1);  slice_tensor_425 = None\\n        slice_tensor_427: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_426, 3, 0, 9223372036854775807);  slice_tensor_426 = None\\n        copy__default_12: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_427, slice_tensor_423);  slice_tensor_427 = slice_tensor_423 = None\\n        slice_tensor_428: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_32, 0, 0, 9223372036854775807);  transpose_int_32 = None\\n        slice_tensor_429: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_428, 1, 0, 9223372036854775807);  slice_tensor_428 = None\\n        slice_tensor_430: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_429, 2, 0, 9223372036854775807);  slice_tensor_429 = None\\n        slice_tensor_431: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_430, 3, 0, 9223372036854775807);  slice_tensor_430 = None\\n        _tensor_constant77 = self._tensor_constant77\\n        slice_tensor_432: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant77, 0, 0, 9223372036854775807);  _tensor_constant77 = None\\n        slice_tensor_433: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_432, 1, 0, 9223372036854775807);  slice_tensor_432 = None\\n        slice_tensor_434: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_433, 2, 0, sym_size_1);  slice_tensor_433 = None\\n        slice_tensor_435: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_434, 3, 0, 9223372036854775807);  slice_tensor_434 = None\\n        copy__default_13: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_435, slice_tensor_431);  slice_tensor_435 = slice_tensor_431 = None\\n        slice_tensor_436: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_53, 0, 0, 9223372036854775807);  add_tensor_53 = None\\n        slice_tensor_437: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_436, 1, 0, 9223372036854775807);  slice_tensor_436 = None\\n        slice_tensor_438: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_437, 2, 0, 9223372036854775807);  slice_tensor_437 = None\\n        slice_tensor_439: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_438, 3, 0, 9223372036854775807);  slice_tensor_438 = None\\n        _tensor_constant78 = self._tensor_constant78\\n        slice_tensor_440: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant78, 0, 0, 9223372036854775807);  _tensor_constant78 = None\\n        slice_tensor_441: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_440, 1, 0, 9223372036854775807);  slice_tensor_440 = None\\n        slice_tensor_442: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_441, 2, 0, sym_size_1);  slice_tensor_441 = None\\n        slice_tensor_443: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_442, 3, 0, 9223372036854775807);  slice_tensor_442 = None\\n        copy__default_14: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_443, slice_tensor_439);  slice_tensor_443 = slice_tensor_439 = None\\n        slice_tensor_444: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_37, 0, 0, 9223372036854775807);  transpose_int_37 = None\\n        slice_tensor_445: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_444, 1, 0, 9223372036854775807);  slice_tensor_444 = None\\n        slice_tensor_446: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_445, 2, 0, 9223372036854775807);  slice_tensor_445 = None\\n        slice_tensor_447: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_446, 3, 0, 9223372036854775807);  slice_tensor_446 = None\\n        _tensor_constant79 = self._tensor_constant79\\n        slice_tensor_448: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant79, 0, 0, 9223372036854775807);  _tensor_constant79 = None\\n        slice_tensor_449: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_448, 1, 0, 9223372036854775807);  slice_tensor_448 = None\\n        slice_tensor_450: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_449, 2, 0, sym_size_1);  slice_tensor_449 = None\\n        slice_tensor_451: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_450, 3, 0, 9223372036854775807);  slice_tensor_450 = None\\n        copy__default_15: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_451, slice_tensor_447);  slice_tensor_451 = slice_tensor_447 = None\\n        slice_tensor_452: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_60, 0, 0, 9223372036854775807);  add_tensor_60 = None\\n        slice_tensor_453: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_452, 1, 0, 9223372036854775807);  slice_tensor_452 = None\\n        slice_tensor_454: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_453, 2, 0, 9223372036854775807);  slice_tensor_453 = None\\n        slice_tensor_455: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_454, 3, 0, 9223372036854775807);  slice_tensor_454 = None\\n        _tensor_constant80 = self._tensor_constant80\\n        slice_tensor_456: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant80, 0, 0, 9223372036854775807);  _tensor_constant80 = None\\n        slice_tensor_457: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_456, 1, 0, 9223372036854775807);  slice_tensor_456 = None\\n        slice_tensor_458: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_457, 2, 0, sym_size_1);  slice_tensor_457 = None\\n        slice_tensor_459: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_458, 3, 0, 9223372036854775807);  slice_tensor_458 = None\\n        copy__default_16: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_459, slice_tensor_455);  slice_tensor_459 = slice_tensor_455 = None\\n        slice_tensor_460: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_42, 0, 0, 9223372036854775807);  transpose_int_42 = None\\n        slice_tensor_461: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_460, 1, 0, 9223372036854775807);  slice_tensor_460 = None\\n        slice_tensor_462: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_461, 2, 0, 9223372036854775807);  slice_tensor_461 = None\\n        slice_tensor_463: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_462, 3, 0, 9223372036854775807);  slice_tensor_462 = None\\n        _tensor_constant81 = self._tensor_constant81\\n        slice_tensor_464: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant81, 0, 0, 9223372036854775807);  _tensor_constant81 = None\\n        slice_tensor_465: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_464, 1, 0, 9223372036854775807);  slice_tensor_464 = None\\n        slice_tensor_466: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_465, 2, 0, sym_size_1);  slice_tensor_465 = None\\n        slice_tensor_467: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_466, 3, 0, 9223372036854775807);  slice_tensor_466 = None\\n        copy__default_17: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_467, slice_tensor_463);  slice_tensor_467 = slice_tensor_463 = None\\n        slice_tensor_468: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_67, 0, 0, 9223372036854775807);  add_tensor_67 = None\\n        slice_tensor_469: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_468, 1, 0, 9223372036854775807);  slice_tensor_468 = None\\n        slice_tensor_470: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_469, 2, 0, 9223372036854775807);  slice_tensor_469 = None\\n        slice_tensor_471: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_470, 3, 0, 9223372036854775807);  slice_tensor_470 = None\\n        _tensor_constant82 = self._tensor_constant82\\n        slice_tensor_472: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant82, 0, 0, 9223372036854775807);  _tensor_constant82 = None\\n        slice_tensor_473: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_472, 1, 0, 9223372036854775807);  slice_tensor_472 = None\\n        slice_tensor_474: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_473, 2, 0, sym_size_1);  slice_tensor_473 = None\\n        slice_tensor_475: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_474, 3, 0, 9223372036854775807);  slice_tensor_474 = None\\n        copy__default_18: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_475, slice_tensor_471);  slice_tensor_475 = slice_tensor_471 = None\\n        slice_tensor_476: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_47, 0, 0, 9223372036854775807);  transpose_int_47 = None\\n        slice_tensor_477: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_476, 1, 0, 9223372036854775807);  slice_tensor_476 = None\\n        slice_tensor_478: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_477, 2, 0, 9223372036854775807);  slice_tensor_477 = None\\n        slice_tensor_479: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_478, 3, 0, 9223372036854775807);  slice_tensor_478 = None\\n        _tensor_constant83 = self._tensor_constant83\\n        slice_tensor_480: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant83, 0, 0, 9223372036854775807);  _tensor_constant83 = None\\n        slice_tensor_481: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_480, 1, 0, 9223372036854775807);  slice_tensor_480 = None\\n        slice_tensor_482: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_481, 2, 0, sym_size_1);  slice_tensor_481 = None\\n        slice_tensor_483: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_482, 3, 0, 9223372036854775807);  slice_tensor_482 = None\\n        copy__default_19: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_483, slice_tensor_479);  slice_tensor_483 = slice_tensor_479 = None\\n        slice_tensor_484: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_74, 0, 0, 9223372036854775807);  add_tensor_74 = None\\n        slice_tensor_485: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_484, 1, 0, 9223372036854775807);  slice_tensor_484 = None\\n        slice_tensor_486: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_485, 2, 0, 9223372036854775807);  slice_tensor_485 = None\\n        slice_tensor_487: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_486, 3, 0, 9223372036854775807);  slice_tensor_486 = None\\n        _tensor_constant84 = self._tensor_constant84\\n        slice_tensor_488: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant84, 0, 0, 9223372036854775807);  _tensor_constant84 = None\\n        slice_tensor_489: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_488, 1, 0, 9223372036854775807);  slice_tensor_488 = None\\n        slice_tensor_490: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_489, 2, 0, sym_size_1);  slice_tensor_489 = None\\n        slice_tensor_491: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_490, 3, 0, 9223372036854775807);  slice_tensor_490 = None\\n        copy__default_20: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_491, slice_tensor_487);  slice_tensor_491 = slice_tensor_487 = None\\n        slice_tensor_492: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_52, 0, 0, 9223372036854775807);  transpose_int_52 = None\\n        slice_tensor_493: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_492, 1, 0, 9223372036854775807);  slice_tensor_492 = None\\n        slice_tensor_494: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_493, 2, 0, 9223372036854775807);  slice_tensor_493 = None\\n        slice_tensor_495: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_494, 3, 0, 9223372036854775807);  slice_tensor_494 = None\\n        _tensor_constant85 = self._tensor_constant85\\n        slice_tensor_496: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant85, 0, 0, 9223372036854775807);  _tensor_constant85 = None\\n        slice_tensor_497: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_496, 1, 0, 9223372036854775807);  slice_tensor_496 = None\\n        slice_tensor_498: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_497, 2, 0, sym_size_1);  slice_tensor_497 = None\\n        slice_tensor_499: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_498, 3, 0, 9223372036854775807);  slice_tensor_498 = None\\n        copy__default_21: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_499, slice_tensor_495);  slice_tensor_499 = slice_tensor_495 = None\\n        slice_tensor_500: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_81, 0, 0, 9223372036854775807);  add_tensor_81 = None\\n        slice_tensor_501: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_500, 1, 0, 9223372036854775807);  slice_tensor_500 = None\\n        slice_tensor_502: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_501, 2, 0, 9223372036854775807);  slice_tensor_501 = None\\n        slice_tensor_503: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_502, 3, 0, 9223372036854775807);  slice_tensor_502 = None\\n        _tensor_constant86 = self._tensor_constant86\\n        slice_tensor_504: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant86, 0, 0, 9223372036854775807);  _tensor_constant86 = None\\n        slice_tensor_505: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_504, 1, 0, 9223372036854775807);  slice_tensor_504 = None\\n        slice_tensor_506: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_505, 2, 0, sym_size_1);  slice_tensor_505 = None\\n        slice_tensor_507: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_506, 3, 0, 9223372036854775807);  slice_tensor_506 = None\\n        copy__default_22: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_507, slice_tensor_503);  slice_tensor_507 = slice_tensor_503 = None\\n        slice_tensor_508: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_57, 0, 0, 9223372036854775807);  transpose_int_57 = None\\n        slice_tensor_509: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_508, 1, 0, 9223372036854775807);  slice_tensor_508 = None\\n        slice_tensor_510: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_509, 2, 0, 9223372036854775807);  slice_tensor_509 = None\\n        slice_tensor_511: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_510, 3, 0, 9223372036854775807);  slice_tensor_510 = None\\n        _tensor_constant87 = self._tensor_constant87\\n        slice_tensor_512: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant87, 0, 0, 9223372036854775807);  _tensor_constant87 = None\\n        slice_tensor_513: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_512, 1, 0, 9223372036854775807);  slice_tensor_512 = None\\n        slice_tensor_514: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_513, 2, 0, sym_size_1);  slice_tensor_513 = None\\n        slice_tensor_515: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_514, 3, 0, 9223372036854775807);  slice_tensor_514 = None\\n        copy__default_23: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_515, slice_tensor_511);  slice_tensor_515 = slice_tensor_511 = None\\n        slice_tensor_516: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_88, 0, 0, 9223372036854775807);  add_tensor_88 = None\\n        slice_tensor_517: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_516, 1, 0, 9223372036854775807);  slice_tensor_516 = None\\n        slice_tensor_518: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_517, 2, 0, 9223372036854775807);  slice_tensor_517 = None\\n        slice_tensor_519: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_518, 3, 0, 9223372036854775807);  slice_tensor_518 = None\\n        _tensor_constant88 = self._tensor_constant88\\n        slice_tensor_520: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant88, 0, 0, 9223372036854775807);  _tensor_constant88 = None\\n        slice_tensor_521: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_520, 1, 0, 9223372036854775807);  slice_tensor_520 = None\\n        slice_tensor_522: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_521, 2, 0, sym_size_1);  slice_tensor_521 = None\\n        slice_tensor_523: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_522, 3, 0, 9223372036854775807);  slice_tensor_522 = None\\n        copy__default_24: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_523, slice_tensor_519);  slice_tensor_523 = slice_tensor_519 = None\\n        slice_tensor_524: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_62, 0, 0, 9223372036854775807);  transpose_int_62 = None\\n        slice_tensor_525: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_524, 1, 0, 9223372036854775807);  slice_tensor_524 = None\\n        slice_tensor_526: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_525, 2, 0, 9223372036854775807);  slice_tensor_525 = None\\n        slice_tensor_527: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_526, 3, 0, 9223372036854775807);  slice_tensor_526 = None\\n        _tensor_constant89 = self._tensor_constant89\\n        slice_tensor_528: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant89, 0, 0, 9223372036854775807);  _tensor_constant89 = None\\n        slice_tensor_529: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_528, 1, 0, 9223372036854775807);  slice_tensor_528 = None\\n        slice_tensor_530: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_529, 2, 0, sym_size_1);  slice_tensor_529 = None\\n        slice_tensor_531: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_530, 3, 0, 9223372036854775807);  slice_tensor_530 = None\\n        copy__default_25: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_531, slice_tensor_527);  slice_tensor_531 = slice_tensor_527 = None\\n        slice_tensor_532: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_95, 0, 0, 9223372036854775807);  add_tensor_95 = None\\n        slice_tensor_533: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_532, 1, 0, 9223372036854775807);  slice_tensor_532 = None\\n        slice_tensor_534: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_533, 2, 0, 9223372036854775807);  slice_tensor_533 = None\\n        slice_tensor_535: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_534, 3, 0, 9223372036854775807);  slice_tensor_534 = None\\n        _tensor_constant90 = self._tensor_constant90\\n        slice_tensor_536: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant90, 0, 0, 9223372036854775807);  _tensor_constant90 = None\\n        slice_tensor_537: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_536, 1, 0, 9223372036854775807);  slice_tensor_536 = None\\n        slice_tensor_538: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_537, 2, 0, sym_size_1);  slice_tensor_537 = None\\n        slice_tensor_539: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_538, 3, 0, 9223372036854775807);  slice_tensor_538 = None\\n        copy__default_26: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_539, slice_tensor_535);  slice_tensor_539 = slice_tensor_535 = None\\n        slice_tensor_540: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_67, 0, 0, 9223372036854775807);  transpose_int_67 = None\\n        slice_tensor_541: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_540, 1, 0, 9223372036854775807);  slice_tensor_540 = None\\n        slice_tensor_542: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_541, 2, 0, 9223372036854775807);  slice_tensor_541 = None\\n        slice_tensor_543: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_542, 3, 0, 9223372036854775807);  slice_tensor_542 = None\\n        _tensor_constant91 = self._tensor_constant91\\n        slice_tensor_544: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant91, 0, 0, 9223372036854775807);  _tensor_constant91 = None\\n        slice_tensor_545: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_544, 1, 0, 9223372036854775807);  slice_tensor_544 = None\\n        slice_tensor_546: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_545, 2, 0, sym_size_1);  slice_tensor_545 = None\\n        slice_tensor_547: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_546, 3, 0, 9223372036854775807);  slice_tensor_546 = None\\n        copy__default_27: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_547, slice_tensor_543);  slice_tensor_547 = slice_tensor_543 = None\\n        slice_tensor_548: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_102, 0, 0, 9223372036854775807);  add_tensor_102 = None\\n        slice_tensor_549: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_548, 1, 0, 9223372036854775807);  slice_tensor_548 = None\\n        slice_tensor_550: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_549, 2, 0, 9223372036854775807);  slice_tensor_549 = None\\n        slice_tensor_551: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_550, 3, 0, 9223372036854775807);  slice_tensor_550 = None\\n        _tensor_constant92 = self._tensor_constant92\\n        slice_tensor_552: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant92, 0, 0, 9223372036854775807);  _tensor_constant92 = None\\n        slice_tensor_553: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_552, 1, 0, 9223372036854775807);  slice_tensor_552 = None\\n        slice_tensor_554: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_553, 2, 0, sym_size_1);  slice_tensor_553 = None\\n        slice_tensor_555: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_554, 3, 0, 9223372036854775807);  slice_tensor_554 = None\\n        copy__default_28: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_555, slice_tensor_551);  slice_tensor_555 = slice_tensor_551 = None\\n        slice_tensor_556: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_72, 0, 0, 9223372036854775807);  transpose_int_72 = None\\n        slice_tensor_557: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_556, 1, 0, 9223372036854775807);  slice_tensor_556 = None\\n        slice_tensor_558: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_557, 2, 0, 9223372036854775807);  slice_tensor_557 = None\\n        slice_tensor_559: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_558, 3, 0, 9223372036854775807);  slice_tensor_558 = None\\n        _tensor_constant93 = self._tensor_constant93\\n        slice_tensor_560: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant93, 0, 0, 9223372036854775807);  _tensor_constant93 = None\\n        slice_tensor_561: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_560, 1, 0, 9223372036854775807);  slice_tensor_560 = None\\n        slice_tensor_562: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_561, 2, 0, sym_size_1);  slice_tensor_561 = None\\n        slice_tensor_563: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_562, 3, 0, 9223372036854775807);  slice_tensor_562 = None\\n        copy__default_29: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_563, slice_tensor_559);  slice_tensor_563 = slice_tensor_559 = None\\n        slice_tensor_564: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_109, 0, 0, 9223372036854775807);  add_tensor_109 = None\\n        slice_tensor_565: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_564, 1, 0, 9223372036854775807);  slice_tensor_564 = None\\n        slice_tensor_566: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_565, 2, 0, 9223372036854775807);  slice_tensor_565 = None\\n        slice_tensor_567: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_566, 3, 0, 9223372036854775807);  slice_tensor_566 = None\\n        _tensor_constant94 = self._tensor_constant94\\n        slice_tensor_568: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant94, 0, 0, 9223372036854775807);  _tensor_constant94 = None\\n        slice_tensor_569: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_568, 1, 0, 9223372036854775807);  slice_tensor_568 = None\\n        slice_tensor_570: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_569, 2, 0, sym_size_1);  slice_tensor_569 = None\\n        slice_tensor_571: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_570, 3, 0, 9223372036854775807);  slice_tensor_570 = None\\n        copy__default_30: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_571, slice_tensor_567);  slice_tensor_571 = slice_tensor_567 = None\\n        slice_tensor_572: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_77, 0, 0, 9223372036854775807);  transpose_int_77 = None\\n        slice_tensor_573: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_572, 1, 0, 9223372036854775807);  slice_tensor_572 = None\\n        slice_tensor_574: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_573, 2, 0, 9223372036854775807);  slice_tensor_573 = None\\n        slice_tensor_575: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_574, 3, 0, 9223372036854775807);  slice_tensor_574 = None\\n        _tensor_constant95 = self._tensor_constant95\\n        slice_tensor_576: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant95, 0, 0, 9223372036854775807);  _tensor_constant95 = None\\n        slice_tensor_577: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_576, 1, 0, 9223372036854775807);  slice_tensor_576 = None\\n        slice_tensor_578: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_577, 2, 0, sym_size_1);  slice_tensor_577 = None\\n        slice_tensor_579: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_578, 3, 0, 9223372036854775807);  slice_tensor_578 = None\\n        copy__default_31: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_579, slice_tensor_575);  slice_tensor_579 = slice_tensor_575 = None\\n        slice_tensor_580: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_116, 0, 0, 9223372036854775807);  add_tensor_116 = None\\n        slice_tensor_581: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_580, 1, 0, 9223372036854775807);  slice_tensor_580 = None\\n        slice_tensor_582: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_581, 2, 0, 9223372036854775807);  slice_tensor_581 = None\\n        slice_tensor_583: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_582, 3, 0, 9223372036854775807);  slice_tensor_582 = None\\n        _tensor_constant96 = self._tensor_constant96\\n        slice_tensor_584: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant96, 0, 0, 9223372036854775807);  _tensor_constant96 = None\\n        slice_tensor_585: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_584, 1, 0, 9223372036854775807);  slice_tensor_584 = None\\n        slice_tensor_586: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_585, 2, 0, sym_size_1);  slice_tensor_585 = None\\n        slice_tensor_587: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_586, 3, 0, 9223372036854775807);  slice_tensor_586 = None\\n        copy__default_32: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_587, slice_tensor_583);  slice_tensor_587 = slice_tensor_583 = None\\n        slice_tensor_588: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_82, 0, 0, 9223372036854775807);  transpose_int_82 = None\\n        slice_tensor_589: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_588, 1, 0, 9223372036854775807);  slice_tensor_588 = None\\n        slice_tensor_590: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_589, 2, 0, 9223372036854775807);  slice_tensor_589 = None\\n        slice_tensor_591: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_590, 3, 0, 9223372036854775807);  slice_tensor_590 = None\\n        _tensor_constant97 = self._tensor_constant97\\n        slice_tensor_592: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant97, 0, 0, 9223372036854775807);  _tensor_constant97 = None\\n        slice_tensor_593: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_592, 1, 0, 9223372036854775807);  slice_tensor_592 = None\\n        slice_tensor_594: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_593, 2, 0, sym_size_1);  slice_tensor_593 = None\\n        slice_tensor_595: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_594, 3, 0, 9223372036854775807);  slice_tensor_594 = None\\n        copy__default_33: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_595, slice_tensor_591);  slice_tensor_595 = slice_tensor_591 = None\\n        slice_tensor_596: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_123, 0, 0, 9223372036854775807);  add_tensor_123 = None\\n        slice_tensor_597: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_596, 1, 0, 9223372036854775807);  slice_tensor_596 = None\\n        slice_tensor_598: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_597, 2, 0, 9223372036854775807);  slice_tensor_597 = None\\n        slice_tensor_599: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_598, 3, 0, 9223372036854775807);  slice_tensor_598 = None\\n        _tensor_constant98 = self._tensor_constant98\\n        slice_tensor_600: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant98, 0, 0, 9223372036854775807);  _tensor_constant98 = None\\n        slice_tensor_601: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_600, 1, 0, 9223372036854775807);  slice_tensor_600 = None\\n        slice_tensor_602: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_601, 2, 0, sym_size_1);  slice_tensor_601 = None\\n        slice_tensor_603: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_602, 3, 0, 9223372036854775807);  slice_tensor_602 = None\\n        copy__default_34: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_603, slice_tensor_599);  slice_tensor_603 = slice_tensor_599 = None\\n        slice_tensor_604: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_87, 0, 0, 9223372036854775807);  transpose_int_87 = None\\n        slice_tensor_605: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_604, 1, 0, 9223372036854775807);  slice_tensor_604 = None\\n        slice_tensor_606: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_605, 2, 0, 9223372036854775807);  slice_tensor_605 = None\\n        slice_tensor_607: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_606, 3, 0, 9223372036854775807);  slice_tensor_606 = None\\n        _tensor_constant99 = self._tensor_constant99\\n        slice_tensor_608: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant99, 0, 0, 9223372036854775807);  _tensor_constant99 = None\\n        slice_tensor_609: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_608, 1, 0, 9223372036854775807);  slice_tensor_608 = None\\n        slice_tensor_610: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_609, 2, 0, sym_size_1);  slice_tensor_609 = None\\n        slice_tensor_611: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_610, 3, 0, 9223372036854775807);  slice_tensor_610 = None\\n        copy__default_35: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_611, slice_tensor_607);  slice_tensor_611 = slice_tensor_607 = None\\n        slice_tensor_612: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_130, 0, 0, 9223372036854775807);  add_tensor_130 = None\\n        slice_tensor_613: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_612, 1, 0, 9223372036854775807);  slice_tensor_612 = None\\n        slice_tensor_614: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_613, 2, 0, 9223372036854775807);  slice_tensor_613 = None\\n        slice_tensor_615: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_614, 3, 0, 9223372036854775807);  slice_tensor_614 = None\\n        _tensor_constant100 = self._tensor_constant100\\n        slice_tensor_616: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant100, 0, 0, 9223372036854775807);  _tensor_constant100 = None\\n        slice_tensor_617: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_616, 1, 0, 9223372036854775807);  slice_tensor_616 = None\\n        slice_tensor_618: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_617, 2, 0, sym_size_1);  slice_tensor_617 = None\\n        slice_tensor_619: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_618, 3, 0, 9223372036854775807);  slice_tensor_618 = None\\n        copy__default_36: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_619, slice_tensor_615);  slice_tensor_619 = slice_tensor_615 = None\\n        slice_tensor_620: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_92, 0, 0, 9223372036854775807);  transpose_int_92 = None\\n        slice_tensor_621: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_620, 1, 0, 9223372036854775807);  slice_tensor_620 = None\\n        slice_tensor_622: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_621, 2, 0, 9223372036854775807);  slice_tensor_621 = None\\n        slice_tensor_623: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_622, 3, 0, 9223372036854775807);  slice_tensor_622 = None\\n        _tensor_constant101 = self._tensor_constant101\\n        slice_tensor_624: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant101, 0, 0, 9223372036854775807);  _tensor_constant101 = None\\n        slice_tensor_625: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_624, 1, 0, 9223372036854775807);  slice_tensor_624 = None\\n        slice_tensor_626: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_625, 2, 0, sym_size_1);  slice_tensor_625 = None\\n        slice_tensor_627: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_626, 3, 0, 9223372036854775807);  slice_tensor_626 = None\\n        copy__default_37: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_627, slice_tensor_623);  slice_tensor_627 = slice_tensor_623 = None\\n        slice_tensor_628: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_137, 0, 0, 9223372036854775807);  add_tensor_137 = None\\n        slice_tensor_629: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_628, 1, 0, 9223372036854775807);  slice_tensor_628 = None\\n        slice_tensor_630: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_629, 2, 0, 9223372036854775807);  slice_tensor_629 = None\\n        slice_tensor_631: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_630, 3, 0, 9223372036854775807);  slice_tensor_630 = None\\n        _tensor_constant102 = self._tensor_constant102\\n        slice_tensor_632: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant102, 0, 0, 9223372036854775807);  _tensor_constant102 = None\\n        slice_tensor_633: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_632, 1, 0, 9223372036854775807);  slice_tensor_632 = None\\n        slice_tensor_634: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_633, 2, 0, sym_size_1);  slice_tensor_633 = None\\n        slice_tensor_635: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_634, 3, 0, 9223372036854775807);  slice_tensor_634 = None\\n        copy__default_38: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_635, slice_tensor_631);  slice_tensor_635 = slice_tensor_631 = None\\n        slice_tensor_636: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_97, 0, 0, 9223372036854775807);  transpose_int_97 = None\\n        slice_tensor_637: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_636, 1, 0, 9223372036854775807);  slice_tensor_636 = None\\n        slice_tensor_638: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_637, 2, 0, 9223372036854775807);  slice_tensor_637 = None\\n        slice_tensor_639: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_638, 3, 0, 9223372036854775807);  slice_tensor_638 = None\\n        _tensor_constant103 = self._tensor_constant103\\n        slice_tensor_640: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant103, 0, 0, 9223372036854775807);  _tensor_constant103 = None\\n        slice_tensor_641: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_640, 1, 0, 9223372036854775807);  slice_tensor_640 = None\\n        slice_tensor_642: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_641, 2, 0, sym_size_1);  slice_tensor_641 = None\\n        slice_tensor_643: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_642, 3, 0, 9223372036854775807);  slice_tensor_642 = None\\n        copy__default_39: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_643, slice_tensor_639);  slice_tensor_643 = slice_tensor_639 = None\\n        slice_tensor_644: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_144, 0, 0, 9223372036854775807);  add_tensor_144 = None\\n        slice_tensor_645: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_644, 1, 0, 9223372036854775807);  slice_tensor_644 = None\\n        slice_tensor_646: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_645, 2, 0, 9223372036854775807);  slice_tensor_645 = None\\n        slice_tensor_647: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_646, 3, 0, 9223372036854775807);  slice_tensor_646 = None\\n        _tensor_constant104 = self._tensor_constant104\\n        slice_tensor_648: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant104, 0, 0, 9223372036854775807);  _tensor_constant104 = None\\n        slice_tensor_649: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_648, 1, 0, 9223372036854775807);  slice_tensor_648 = None\\n        slice_tensor_650: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_649, 2, 0, sym_size_1);  slice_tensor_649 = None\\n        slice_tensor_651: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_650, 3, 0, 9223372036854775807);  slice_tensor_650 = None\\n        copy__default_40: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_651, slice_tensor_647);  slice_tensor_651 = slice_tensor_647 = None\\n        slice_tensor_652: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_102, 0, 0, 9223372036854775807);  transpose_int_102 = None\\n        slice_tensor_653: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_652, 1, 0, 9223372036854775807);  slice_tensor_652 = None\\n        slice_tensor_654: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_653, 2, 0, 9223372036854775807);  slice_tensor_653 = None\\n        slice_tensor_655: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_654, 3, 0, 9223372036854775807);  slice_tensor_654 = None\\n        _tensor_constant105 = self._tensor_constant105\\n        slice_tensor_656: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant105, 0, 0, 9223372036854775807);  _tensor_constant105 = None\\n        slice_tensor_657: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_656, 1, 0, 9223372036854775807);  slice_tensor_656 = None\\n        slice_tensor_658: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_657, 2, 0, sym_size_1);  slice_tensor_657 = None\\n        slice_tensor_659: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_658, 3, 0, 9223372036854775807);  slice_tensor_658 = None\\n        copy__default_41: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_659, slice_tensor_655);  slice_tensor_659 = slice_tensor_655 = None\\n        slice_tensor_660: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_151, 0, 0, 9223372036854775807);  add_tensor_151 = None\\n        slice_tensor_661: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_660, 1, 0, 9223372036854775807);  slice_tensor_660 = None\\n        slice_tensor_662: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_661, 2, 0, 9223372036854775807);  slice_tensor_661 = None\\n        slice_tensor_663: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_662, 3, 0, 9223372036854775807);  slice_tensor_662 = None\\n        _tensor_constant106 = self._tensor_constant106\\n        slice_tensor_664: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant106, 0, 0, 9223372036854775807);  _tensor_constant106 = None\\n        slice_tensor_665: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_664, 1, 0, 9223372036854775807);  slice_tensor_664 = None\\n        slice_tensor_666: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_665, 2, 0, sym_size_1);  slice_tensor_665 = None\\n        slice_tensor_667: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_666, 3, 0, 9223372036854775807);  slice_tensor_666 = None\\n        copy__default_42: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_667, slice_tensor_663);  slice_tensor_667 = slice_tensor_663 = None\\n        slice_tensor_668: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_107, 0, 0, 9223372036854775807);  transpose_int_107 = None\\n        slice_tensor_669: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_668, 1, 0, 9223372036854775807);  slice_tensor_668 = None\\n        slice_tensor_670: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_669, 2, 0, 9223372036854775807);  slice_tensor_669 = None\\n        slice_tensor_671: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_670, 3, 0, 9223372036854775807);  slice_tensor_670 = None\\n        _tensor_constant107 = self._tensor_constant107\\n        slice_tensor_672: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant107, 0, 0, 9223372036854775807);  _tensor_constant107 = None\\n        slice_tensor_673: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_672, 1, 0, 9223372036854775807);  slice_tensor_672 = None\\n        slice_tensor_674: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_673, 2, 0, sym_size_1);  slice_tensor_673 = None\\n        slice_tensor_675: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_674, 3, 0, 9223372036854775807);  slice_tensor_674 = None\\n        copy__default_43: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_675, slice_tensor_671);  slice_tensor_675 = slice_tensor_671 = None\\n        slice_tensor_676: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_158, 0, 0, 9223372036854775807);  add_tensor_158 = None\\n        slice_tensor_677: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_676, 1, 0, 9223372036854775807);  slice_tensor_676 = None\\n        slice_tensor_678: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_677, 2, 0, 9223372036854775807);  slice_tensor_677 = None\\n        slice_tensor_679: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_678, 3, 0, 9223372036854775807);  slice_tensor_678 = None\\n        _tensor_constant108 = self._tensor_constant108\\n        slice_tensor_680: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant108, 0, 0, 9223372036854775807);  _tensor_constant108 = None\\n        slice_tensor_681: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_680, 1, 0, 9223372036854775807);  slice_tensor_680 = None\\n        slice_tensor_682: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_681, 2, 0, sym_size_1);  slice_tensor_681 = None\\n        slice_tensor_683: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_682, 3, 0, 9223372036854775807);  slice_tensor_682 = None\\n        copy__default_44: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_683, slice_tensor_679);  slice_tensor_683 = slice_tensor_679 = None\\n        slice_tensor_684: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_112, 0, 0, 9223372036854775807);  transpose_int_112 = None\\n        slice_tensor_685: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_684, 1, 0, 9223372036854775807);  slice_tensor_684 = None\\n        slice_tensor_686: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_685, 2, 0, 9223372036854775807);  slice_tensor_685 = None\\n        slice_tensor_687: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_686, 3, 0, 9223372036854775807);  slice_tensor_686 = None\\n        _tensor_constant109 = self._tensor_constant109\\n        slice_tensor_688: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant109, 0, 0, 9223372036854775807);  _tensor_constant109 = None\\n        slice_tensor_689: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_688, 1, 0, 9223372036854775807);  slice_tensor_688 = None\\n        slice_tensor_690: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_689, 2, 0, sym_size_1);  slice_tensor_689 = None\\n        slice_tensor_691: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_690, 3, 0, 9223372036854775807);  slice_tensor_690 = None\\n        copy__default_45: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_691, slice_tensor_687);  slice_tensor_691 = slice_tensor_687 = None\\n        slice_tensor_692: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_165, 0, 0, 9223372036854775807);  add_tensor_165 = None\\n        slice_tensor_693: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_692, 1, 0, 9223372036854775807);  slice_tensor_692 = None\\n        slice_tensor_694: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_693, 2, 0, 9223372036854775807);  slice_tensor_693 = None\\n        slice_tensor_695: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_694, 3, 0, 9223372036854775807);  slice_tensor_694 = None\\n        _tensor_constant110 = self._tensor_constant110\\n        slice_tensor_696: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant110, 0, 0, 9223372036854775807);  _tensor_constant110 = None\\n        slice_tensor_697: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_696, 1, 0, 9223372036854775807);  slice_tensor_696 = None\\n        slice_tensor_698: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_697, 2, 0, sym_size_1);  slice_tensor_697 = None\\n        slice_tensor_699: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_698, 3, 0, 9223372036854775807);  slice_tensor_698 = None\\n        copy__default_46: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_699, slice_tensor_695);  slice_tensor_699 = slice_tensor_695 = None\\n        slice_tensor_700: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_117, 0, 0, 9223372036854775807);  transpose_int_117 = None\\n        slice_tensor_701: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_700, 1, 0, 9223372036854775807);  slice_tensor_700 = None\\n        slice_tensor_702: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_701, 2, 0, 9223372036854775807);  slice_tensor_701 = None\\n        slice_tensor_703: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_702, 3, 0, 9223372036854775807);  slice_tensor_702 = None\\n        _tensor_constant111 = self._tensor_constant111\\n        slice_tensor_704: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant111, 0, 0, 9223372036854775807);  _tensor_constant111 = None\\n        slice_tensor_705: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_704, 1, 0, 9223372036854775807);  slice_tensor_704 = None\\n        slice_tensor_706: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_705, 2, 0, sym_size_1);  slice_tensor_705 = None\\n        slice_tensor_707: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_706, 3, 0, 9223372036854775807);  slice_tensor_706 = None\\n        copy__default_47: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_707, slice_tensor_703);  slice_tensor_707 = slice_tensor_703 = None\\n        slice_tensor_708: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_172, 0, 0, 9223372036854775807);  add_tensor_172 = None\\n        slice_tensor_709: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_708, 1, 0, 9223372036854775807);  slice_tensor_708 = None\\n        slice_tensor_710: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_709, 2, 0, 9223372036854775807);  slice_tensor_709 = None\\n        slice_tensor_711: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_710, 3, 0, 9223372036854775807);  slice_tensor_710 = None\\n        _tensor_constant112 = self._tensor_constant112\\n        slice_tensor_712: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant112, 0, 0, 9223372036854775807);  _tensor_constant112 = None\\n        slice_tensor_713: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_712, 1, 0, 9223372036854775807);  slice_tensor_712 = None\\n        slice_tensor_714: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_713, 2, 0, sym_size_1);  slice_tensor_713 = None\\n        slice_tensor_715: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_714, 3, 0, 9223372036854775807);  slice_tensor_714 = None\\n        copy__default_48: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_715, slice_tensor_711);  slice_tensor_715 = slice_tensor_711 = None\\n        slice_tensor_716: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_122, 0, 0, 9223372036854775807);  transpose_int_122 = None\\n        slice_tensor_717: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_716, 1, 0, 9223372036854775807);  slice_tensor_716 = None\\n        slice_tensor_718: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_717, 2, 0, 9223372036854775807);  slice_tensor_717 = None\\n        slice_tensor_719: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_718, 3, 0, 9223372036854775807);  slice_tensor_718 = None\\n        _tensor_constant113 = self._tensor_constant113\\n        slice_tensor_720: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant113, 0, 0, 9223372036854775807);  _tensor_constant113 = None\\n        slice_tensor_721: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_720, 1, 0, 9223372036854775807);  slice_tensor_720 = None\\n        slice_tensor_722: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_721, 2, 0, sym_size_1);  slice_tensor_721 = None\\n        slice_tensor_723: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_722, 3, 0, 9223372036854775807);  slice_tensor_722 = None\\n        copy__default_49: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_723, slice_tensor_719);  slice_tensor_723 = slice_tensor_719 = None\\n        slice_tensor_724: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_179, 0, 0, 9223372036854775807);  add_tensor_179 = None\\n        slice_tensor_725: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_724, 1, 0, 9223372036854775807);  slice_tensor_724 = None\\n        slice_tensor_726: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_725, 2, 0, 9223372036854775807);  slice_tensor_725 = None\\n        slice_tensor_727: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_726, 3, 0, 9223372036854775807);  slice_tensor_726 = None\\n        _tensor_constant114 = self._tensor_constant114\\n        slice_tensor_728: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant114, 0, 0, 9223372036854775807);  _tensor_constant114 = None\\n        slice_tensor_729: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_728, 1, 0, 9223372036854775807);  slice_tensor_728 = None\\n        slice_tensor_730: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_729, 2, 0, sym_size_1);  slice_tensor_729 = None\\n        slice_tensor_731: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_730, 3, 0, 9223372036854775807);  slice_tensor_730 = None\\n        copy__default_50: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_731, slice_tensor_727);  slice_tensor_731 = slice_tensor_727 = None\\n        slice_tensor_732: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_127, 0, 0, 9223372036854775807);  transpose_int_127 = None\\n        slice_tensor_733: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_732, 1, 0, 9223372036854775807);  slice_tensor_732 = None\\n        slice_tensor_734: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_733, 2, 0, 9223372036854775807);  slice_tensor_733 = None\\n        slice_tensor_735: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_734, 3, 0, 9223372036854775807);  slice_tensor_734 = None\\n        _tensor_constant115 = self._tensor_constant115\\n        slice_tensor_736: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant115, 0, 0, 9223372036854775807);  _tensor_constant115 = None\\n        slice_tensor_737: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_736, 1, 0, 9223372036854775807);  slice_tensor_736 = None\\n        slice_tensor_738: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_737, 2, 0, sym_size_1);  slice_tensor_737 = None\\n        slice_tensor_739: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_738, 3, 0, 9223372036854775807);  slice_tensor_738 = None\\n        copy__default_51: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_739, slice_tensor_735);  slice_tensor_739 = slice_tensor_735 = None\\n        slice_tensor_740: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_186, 0, 0, 9223372036854775807);  add_tensor_186 = None\\n        slice_tensor_741: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_740, 1, 0, 9223372036854775807);  slice_tensor_740 = None\\n        slice_tensor_742: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_741, 2, 0, 9223372036854775807);  slice_tensor_741 = None\\n        slice_tensor_743: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_742, 3, 0, 9223372036854775807);  slice_tensor_742 = None\\n        _tensor_constant116 = self._tensor_constant116\\n        slice_tensor_744: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant116, 0, 0, 9223372036854775807);  _tensor_constant116 = None\\n        slice_tensor_745: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_744, 1, 0, 9223372036854775807);  slice_tensor_744 = None\\n        slice_tensor_746: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_745, 2, 0, sym_size_1);  slice_tensor_745 = None\\n        slice_tensor_747: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_746, 3, 0, 9223372036854775807);  slice_tensor_746 = None\\n        copy__default_52: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_747, slice_tensor_743);  slice_tensor_747 = slice_tensor_743 = None\\n        slice_tensor_748: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_132, 0, 0, 9223372036854775807);  transpose_int_132 = None\\n        slice_tensor_749: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_748, 1, 0, 9223372036854775807);  slice_tensor_748 = None\\n        slice_tensor_750: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_749, 2, 0, 9223372036854775807);  slice_tensor_749 = None\\n        slice_tensor_751: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_750, 3, 0, 9223372036854775807);  slice_tensor_750 = None\\n        _tensor_constant117 = self._tensor_constant117\\n        slice_tensor_752: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant117, 0, 0, 9223372036854775807);  _tensor_constant117 = None\\n        slice_tensor_753: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_752, 1, 0, 9223372036854775807);  slice_tensor_752 = None\\n        slice_tensor_754: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_753, 2, 0, sym_size_1);  slice_tensor_753 = None\\n        slice_tensor_755: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_754, 3, 0, 9223372036854775807);  slice_tensor_754 = None\\n        copy__default_53: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_755, slice_tensor_751);  slice_tensor_755 = slice_tensor_751 = None\\n        slice_tensor_756: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_193, 0, 0, 9223372036854775807);  add_tensor_193 = None\\n        slice_tensor_757: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_756, 1, 0, 9223372036854775807);  slice_tensor_756 = None\\n        slice_tensor_758: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_757, 2, 0, 9223372036854775807);  slice_tensor_757 = None\\n        slice_tensor_759: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_758, 3, 0, 9223372036854775807);  slice_tensor_758 = None\\n        _tensor_constant118 = self._tensor_constant118\\n        slice_tensor_760: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant118, 0, 0, 9223372036854775807);  _tensor_constant118 = None\\n        slice_tensor_761: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_760, 1, 0, 9223372036854775807);  slice_tensor_760 = None\\n        slice_tensor_762: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_761, 2, 0, sym_size_1);  slice_tensor_761 = None\\n        slice_tensor_763: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_762, 3, 0, 9223372036854775807);  slice_tensor_762 = None\\n        copy__default_54: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_763, slice_tensor_759);  slice_tensor_763 = slice_tensor_759 = None\\n        slice_tensor_764: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_137, 0, 0, 9223372036854775807);  transpose_int_137 = None\\n        slice_tensor_765: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_764, 1, 0, 9223372036854775807);  slice_tensor_764 = None\\n        slice_tensor_766: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_765, 2, 0, 9223372036854775807);  slice_tensor_765 = None\\n        slice_tensor_767: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_766, 3, 0, 9223372036854775807);  slice_tensor_766 = None\\n        _tensor_constant119 = self._tensor_constant119\\n        slice_tensor_768: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant119, 0, 0, 9223372036854775807);  _tensor_constant119 = None\\n        slice_tensor_769: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_768, 1, 0, 9223372036854775807);  slice_tensor_768 = None\\n        slice_tensor_770: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_769, 2, 0, sym_size_1);  slice_tensor_769 = None\\n        slice_tensor_771: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_770, 3, 0, 9223372036854775807);  slice_tensor_770 = None\\n        copy__default_55: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_771, slice_tensor_767);  slice_tensor_771 = slice_tensor_767 = None\\n        slice_tensor_772: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_200, 0, 0, 9223372036854775807);  add_tensor_200 = None\\n        slice_tensor_773: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_772, 1, 0, 9223372036854775807);  slice_tensor_772 = None\\n        slice_tensor_774: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_773, 2, 0, 9223372036854775807);  slice_tensor_773 = None\\n        slice_tensor_775: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_774, 3, 0, 9223372036854775807);  slice_tensor_774 = None\\n        _tensor_constant120 = self._tensor_constant120\\n        slice_tensor_776: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant120, 0, 0, 9223372036854775807);  _tensor_constant120 = None\\n        slice_tensor_777: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_776, 1, 0, 9223372036854775807);  slice_tensor_776 = None\\n        slice_tensor_778: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_777, 2, 0, sym_size_1);  slice_tensor_777 = None\\n        slice_tensor_779: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_778, 3, 0, 9223372036854775807);  slice_tensor_778 = None\\n        copy__default_56: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_779, slice_tensor_775);  slice_tensor_779 = slice_tensor_775 = None\\n        slice_tensor_780: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_142, 0, 0, 9223372036854775807);  transpose_int_142 = None\\n        slice_tensor_781: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_780, 1, 0, 9223372036854775807);  slice_tensor_780 = None\\n        slice_tensor_782: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_781, 2, 0, 9223372036854775807);  slice_tensor_781 = None\\n        slice_tensor_783: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_782, 3, 0, 9223372036854775807);  slice_tensor_782 = None\\n        _tensor_constant121 = self._tensor_constant121\\n        slice_tensor_784: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant121, 0, 0, 9223372036854775807);  _tensor_constant121 = None\\n        slice_tensor_785: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_784, 1, 0, 9223372036854775807);  slice_tensor_784 = None\\n        slice_tensor_786: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_785, 2, 0, sym_size_1);  slice_tensor_785 = None\\n        slice_tensor_787: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_786, 3, 0, 9223372036854775807);  slice_tensor_786 = None\\n        copy__default_57: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_787, slice_tensor_783);  slice_tensor_787 = slice_tensor_783 = None\\n        slice_tensor_788: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_207, 0, 0, 9223372036854775807);  add_tensor_207 = None\\n        slice_tensor_789: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_788, 1, 0, 9223372036854775807);  slice_tensor_788 = None\\n        slice_tensor_790: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_789, 2, 0, 9223372036854775807);  slice_tensor_789 = None\\n        slice_tensor_791: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_790, 3, 0, 9223372036854775807);  slice_tensor_790 = None\\n        _tensor_constant122 = self._tensor_constant122\\n        slice_tensor_792: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant122, 0, 0, 9223372036854775807);  _tensor_constant122 = None\\n        slice_tensor_793: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_792, 1, 0, 9223372036854775807);  slice_tensor_792 = None\\n        slice_tensor_794: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_793, 2, 0, sym_size_1);  slice_tensor_793 = None\\n        slice_tensor_795: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_794, 3, 0, 9223372036854775807);  slice_tensor_794 = None\\n        copy__default_58: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_795, slice_tensor_791);  slice_tensor_795 = slice_tensor_791 = None\\n        slice_tensor_796: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_147, 0, 0, 9223372036854775807);  transpose_int_147 = None\\n        slice_tensor_797: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_796, 1, 0, 9223372036854775807);  slice_tensor_796 = None\\n        slice_tensor_798: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_797, 2, 0, 9223372036854775807);  slice_tensor_797 = None\\n        slice_tensor_799: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_798, 3, 0, 9223372036854775807);  slice_tensor_798 = None\\n        _tensor_constant123 = self._tensor_constant123\\n        slice_tensor_800: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant123, 0, 0, 9223372036854775807);  _tensor_constant123 = None\\n        slice_tensor_801: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_800, 1, 0, 9223372036854775807);  slice_tensor_800 = None\\n        slice_tensor_802: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_801, 2, 0, sym_size_1);  slice_tensor_801 = None\\n        slice_tensor_803: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_802, 3, 0, 9223372036854775807);  slice_tensor_802 = None\\n        copy__default_59: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_803, slice_tensor_799);  slice_tensor_803 = slice_tensor_799 = None\\n        slice_tensor_804: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_214, 0, 0, 9223372036854775807);  add_tensor_214 = None\\n        slice_tensor_805: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_804, 1, 0, 9223372036854775807);  slice_tensor_804 = None\\n        slice_tensor_806: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_805, 2, 0, 9223372036854775807);  slice_tensor_805 = None\\n        slice_tensor_807: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_806, 3, 0, 9223372036854775807);  slice_tensor_806 = None\\n        _tensor_constant124 = self._tensor_constant124\\n        slice_tensor_808: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant124, 0, 0, 9223372036854775807);  _tensor_constant124 = None\\n        slice_tensor_809: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_808, 1, 0, 9223372036854775807);  slice_tensor_808 = None\\n        slice_tensor_810: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_809, 2, 0, sym_size_1);  slice_tensor_809 = None\\n        slice_tensor_811: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_810, 3, 0, 9223372036854775807);  slice_tensor_810 = None\\n        copy__default_60: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_811, slice_tensor_807);  slice_tensor_811 = slice_tensor_807 = None\\n        slice_tensor_812: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_152, 0, 0, 9223372036854775807);  transpose_int_152 = None\\n        slice_tensor_813: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_812, 1, 0, 9223372036854775807);  slice_tensor_812 = None\\n        slice_tensor_814: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_813, 2, 0, 9223372036854775807);  slice_tensor_813 = None\\n        slice_tensor_815: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_814, 3, 0, 9223372036854775807);  slice_tensor_814 = None\\n        _tensor_constant125 = self._tensor_constant125\\n        slice_tensor_816: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant125, 0, 0, 9223372036854775807);  _tensor_constant125 = None\\n        slice_tensor_817: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_816, 1, 0, 9223372036854775807);  slice_tensor_816 = None\\n        slice_tensor_818: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_817, 2, 0, sym_size_1);  slice_tensor_817 = None\\n        slice_tensor_819: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_818, 3, 0, 9223372036854775807);  slice_tensor_818 = None\\n        copy__default_61: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_819, slice_tensor_815);  slice_tensor_819 = slice_tensor_815 = None\\n        slice_tensor_820: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(add_tensor_221, 0, 0, 9223372036854775807);  add_tensor_221 = None\\n        slice_tensor_821: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_820, 1, 0, 9223372036854775807);  slice_tensor_820 = None\\n        slice_tensor_822: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_821, 2, 0, 9223372036854775807);  slice_tensor_821 = None\\n        slice_tensor_823: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_822, 3, 0, 9223372036854775807);  slice_tensor_822 = None\\n        _tensor_constant126 = self._tensor_constant126\\n        slice_tensor_824: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant126, 0, 0, 9223372036854775807);  _tensor_constant126 = None\\n        slice_tensor_825: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_824, 1, 0, 9223372036854775807);  slice_tensor_824 = None\\n        slice_tensor_826: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_825, 2, 0, sym_size_1);  slice_tensor_825 = None\\n        slice_tensor_827: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_826, 3, 0, 9223372036854775807);  slice_tensor_826 = None\\n        copy__default_62: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_827, slice_tensor_823);  slice_tensor_827 = slice_tensor_823 = None\\n        slice_tensor_828: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(transpose_int_157, 0, 0, 9223372036854775807);  transpose_int_157 = None\\n        slice_tensor_829: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_828, 1, 0, 9223372036854775807);  slice_tensor_828 = None\\n        slice_tensor_830: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_829, 2, 0, 9223372036854775807);  slice_tensor_829 = None\\n        slice_tensor_831: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_830, 3, 0, 9223372036854775807);  slice_tensor_830 = None\\n        _tensor_constant127 = self._tensor_constant127\\n        slice_tensor_832: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(_tensor_constant127, 0, 0, 9223372036854775807);  _tensor_constant127 = None\\n        slice_tensor_833: f32[1, 32, 512, 128] = torch.ops.aten.slice.Tensor(slice_tensor_832, 1, 0, 9223372036854775807);  slice_tensor_832 = None\\n        slice_tensor_834: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_833, 2, 0, sym_size_1);  slice_tensor_833 = None\\n        slice_tensor_835: f32[1, 32, s0, 128] = torch.ops.aten.slice.Tensor(slice_tensor_834, 3, 0, 9223372036854775807);  slice_tensor_834 = None\\n        copy__default_63: f32[1, 32, s0, 128] = torch.ops.aten.copy_.default(slice_tensor_835, slice_tensor_831);  slice_tensor_835 = slice_tensor_831 = None\\n        return pytree.tree_unflatten([view_default_771, sym_size_1], self._out_spec)\\n        \""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch._dynamo as dynamo\n",
    "from torch._export import dynamic_dim\n",
    "from torch._export.constraints import constrain_as_size, constrain_as_value\n",
    "import os\n",
    "\n",
    "os.environ[\"TORCH_LOGS\"] = \"dynamic\"\n",
    "\n",
    "print(input_ids.size())\n",
    "exp = dynamo.export(sm.forward, aten_graph=True, constraints=[\n",
    "    # dynamic_dim(input_ids, 1) >= 1,\n",
    "    # dynamic_dim(input_ids, 1) < 4096,\n",
    "])\n",
    "print(exp)\n",
    "g, guards = exp(input_ids)\n",
    "g.print_readable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
